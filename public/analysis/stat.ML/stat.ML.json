{
  "topics": {
    "data": {
      "0": {
        "name": "0_networks_neural_gradient_neural networks",
        "keywords": [
          [
            "networks",
            0.01651351838964716
          ],
          [
            "neural",
            0.015552491069885176
          ],
          [
            "gradient",
            0.012820191294465336
          ],
          [
            "neural networks",
            0.01202641869297848
          ],
          [
            "training",
            0.011293767941719907
          ],
          [
            "network",
            0.011007958114192097
          ],
          [
            "convergence",
            0.009604113337772219
          ],
          [
            "deep",
            0.009104859164368472
          ],
          [
            "descent",
            0.008978052408533074
          ],
          [
            "SGD",
            0.008948389832032508
          ]
        ],
        "count": 3493
      },
      "1": {
        "name": "1_policy_RL_reinforcement_reinforcement learning",
        "keywords": [
          [
            "policy",
            0.02397432172670536
          ],
          [
            "RL",
            0.02080437045439549
          ],
          [
            "reinforcement",
            0.01772941387202465
          ],
          [
            "reinforcement learning",
            0.017613262793026937
          ],
          [
            "learning",
            0.01737802776256704
          ],
          [
            "Reinforcement",
            0.013130818588451299
          ],
          [
            "agent",
            0.012409400749045943
          ],
          [
            "Learning",
            0.012359301641867451
          ],
          [
            "algorithm",
            0.010942367576672342
          ],
          [
            "state",
            0.010398100264856618
          ]
        ],
        "count": 2635
      },
      "2": {
        "name": "2_causal_treatment_Causal_effect",
        "keywords": [
          [
            "causal",
            0.04231843327192277
          ],
          [
            "treatment",
            0.022926415860340513
          ],
          [
            "Causal",
            0.01563168020967955
          ],
          [
            "effect",
            0.013382996169994568
          ],
          [
            "effects",
            0.012750879407991819
          ],
          [
            "data",
            0.012301030458905696
          ],
          [
            "variables",
            0.011832435079404357
          ],
          [
            "observational",
            0.010357337964820879
          ],
          [
            "estimation",
            0.009101538356105974
          ],
          [
            "inference",
            0.008696624548420375
          ]
        ],
        "count": 1904
      },
      "3": {
        "name": "3_series_time series_time_forecasting",
        "keywords": [
          [
            "series",
            0.021209839926851083
          ],
          [
            "time series",
            0.019698897222274407
          ],
          [
            "time",
            0.019162718303474553
          ],
          [
            "forecasting",
            0.01595467260024574
          ],
          [
            "detection",
            0.01591937516172695
          ],
          [
            "data",
            0.013659763347338014
          ],
          [
            "anomaly",
            0.010762175041070922
          ],
          [
            "model",
            0.010473749924156197
          ],
          [
            "models",
            0.010317421262053903
          ],
          [
            "methods",
            0.008249597790160627
          ]
        ],
        "count": 1898
      },
      "4": {
        "name": "4_Bayesian_optimization_model_systems",
        "keywords": [
          [
            "Bayesian",
            0.014302070854936691
          ],
          [
            "optimization",
            0.011210517708036582
          ],
          [
            "model",
            0.010658508038903464
          ],
          [
            "systems",
            0.010056594484593827
          ],
          [
            "equations",
            0.009855186845426574
          ],
          [
            "data",
            0.00938809930985837
          ],
          [
            "BO",
            0.008929974470273898
          ],
          [
            "surrogate",
            0.008299055519912849
          ],
          [
            "Gaussian",
            0.008270961416392641
          ],
          [
            "function",
            0.008270078382993252
          ]
        ],
        "count": 1812
      },
      "5": {
        "name": "5_graph_Graph_graphs_node",
        "keywords": [
          [
            "graph",
            0.04341247007835866
          ],
          [
            "Graph",
            0.020955617238846033
          ],
          [
            "graphs",
            0.02032644374834697
          ],
          [
            "node",
            0.018685785640721743
          ],
          [
            "GNNs",
            0.014987231739510137
          ],
          [
            "nodes",
            0.013101630871345483
          ],
          [
            "networks",
            0.012834126177396083
          ],
          [
            "network",
            0.011767488840476346
          ],
          [
            "GNN",
            0.009703245854926143
          ],
          [
            "structure",
            0.009093644591134452
          ]
        ],
        "count": 1766
      },
      "6": {
        "name": "6_generative_diffusion_models_latent",
        "keywords": [
          [
            "generative",
            0.0227392260567056
          ],
          [
            "diffusion",
            0.01938678528840647
          ],
          [
            "models",
            0.016876948250861303
          ],
          [
            "latent",
            0.012138525758928112
          ],
          [
            "distribution",
            0.01194577060201929
          ],
          [
            "GANs",
            0.011842156836265853
          ],
          [
            "generative models",
            0.011427032618440022
          ],
          [
            "data",
            0.011140406328379015
          ],
          [
            "score",
            0.010695329192312865
          ],
          [
            "Generative",
            0.010356715436578295
          ]
        ],
        "count": 1371
      },
      "7": {
        "name": "7_regret_bandit_bandits_arm",
        "keywords": [
          [
            "regret",
            0.034096356039979465
          ],
          [
            "bandit",
            0.029687305515540455
          ],
          [
            "bandits",
            0.02178683527113403
          ],
          [
            "arm",
            0.02172391657735416
          ],
          [
            "algorithm",
            0.018151252053835415
          ],
          [
            "Bandits",
            0.01704661586709175
          ],
          [
            "reward",
            0.016182052006353254
          ],
          [
            "problem",
            0.015320187223822614
          ],
          [
            "armed",
            0.014726627177592615
          ],
          [
            "contextual",
            0.013918606431417862
          ]
        ],
        "count": 1225
      },
      "8": {
        "name": "8_language_LLMs_models_attention",
        "keywords": [
          [
            "language",
            0.02105503837634711
          ],
          [
            "LLMs",
            0.01571569065677583
          ],
          [
            "models",
            0.014450230199068652
          ],
          [
            "attention",
            0.013742440914356686
          ],
          [
            "model",
            0.012568426299793822
          ],
          [
            "language models",
            0.01117511579372509
          ],
          [
            "LLM",
            0.010530253127405052
          ],
          [
            "tasks",
            0.010109173071370907
          ],
          [
            "Language",
            0.009839634395835016
          ],
          [
            "text",
            0.009702178110021768
          ]
        ],
        "count": 1222
      },
      "9": {
        "name": "9_matrix_rank_tensor_low rank",
        "keywords": [
          [
            "matrix",
            0.02367704579217479
          ],
          [
            "rank",
            0.022271079915103752
          ],
          [
            "tensor",
            0.022226517046669024
          ],
          [
            "low rank",
            0.013255889631594332
          ],
          [
            "low",
            0.01278610146622871
          ],
          [
            "algorithm",
            0.011590856498549094
          ],
          [
            "sparse",
            0.010786261116599947
          ],
          [
            "problem",
            0.010469103378430758
          ],
          [
            "recovery",
            0.009471580513225086
          ],
          [
            "signal",
            0.009079382805024926
          ]
        ],
        "count": 900
      },
      "10": {
        "name": "10_adversarial_attacks_robustness_training",
        "keywords": [
          [
            "adversarial",
            0.05066802466829731
          ],
          [
            "attacks",
            0.029055527513142155
          ],
          [
            "robustness",
            0.02631542580131591
          ],
          [
            "training",
            0.017066101333012406
          ],
          [
            "attack",
            0.016617367119762645
          ],
          [
            "Adversarial",
            0.016131544274201055
          ],
          [
            "robust",
            0.015904319237029944
          ],
          [
            "perturbations",
            0.014339775599269923
          ],
          [
            "adversarial attacks",
            0.013266923317215377
          ],
          [
            "adversarial training",
            0.013240893176791993
          ]
        ],
        "count": 884
      },
      "11": {
        "name": "11_federated_Federated_FL_communication",
        "keywords": [
          [
            "federated",
            0.02977977498969082
          ],
          [
            "Federated",
            0.0285961825586746
          ],
          [
            "FL",
            0.02532022102235619
          ],
          [
            "communication",
            0.025171884892425323
          ],
          [
            "clients",
            0.018301431597519627
          ],
          [
            "learning",
            0.01768661993286226
          ],
          [
            "local",
            0.015724801028634015
          ],
          [
            "privacy",
            0.015510236547811673
          ],
          [
            "data",
            0.014458238044732497
          ],
          [
            "client",
            0.013016520881010898
          ]
        ],
        "count": 808
      },
      "12": {
        "name": "12_survival_data_missing_imputation",
        "keywords": [
          [
            "survival",
            0.01837862167854458
          ],
          [
            "data",
            0.01799694940111135
          ],
          [
            "missing",
            0.016620206710117685
          ],
          [
            "imputation",
            0.016529204853803357
          ],
          [
            "model",
            0.013589156332135872
          ],
          [
            "models",
            0.013051412454733001
          ],
          [
            "clinical",
            0.011413539252668017
          ],
          [
            "time",
            0.010836954953598604
          ],
          [
            "disease",
            0.010116901618616432
          ],
          [
            "patients",
            0.009922803337682948
          ]
        ],
        "count": 684
      },
      "13": {
        "name": "13_label_labels_supervised_learning",
        "keywords": [
          [
            "label",
            0.02444928942688958
          ],
          [
            "labels",
            0.021752864411420204
          ],
          [
            "supervised",
            0.017150639222308386
          ],
          [
            "learning",
            0.015493692571297053
          ],
          [
            "data",
            0.014459704346705134
          ],
          [
            "training",
            0.012924639482230481
          ],
          [
            "classification",
            0.011896389830554558
          ],
          [
            "class",
            0.011391190154227926
          ],
          [
            "supervised learning",
            0.010220828317559825
          ],
          [
            "loss",
            0.010187148154075967
          ]
        ],
        "count": 683
      },
      "14": {
        "name": "14_Langevin_Monte_Carlo_sampling",
        "keywords": [
          [
            "Langevin",
            0.02582140825439538
          ],
          [
            "Monte",
            0.01908745245512132
          ],
          [
            "Carlo",
            0.01908317533635374
          ],
          [
            "sampling",
            0.01753100825124886
          ],
          [
            "gradient",
            0.014975251671992129
          ],
          [
            "MCMC",
            0.013313336747419701
          ],
          [
            "variational",
            0.013021809924676137
          ],
          [
            "inference",
            0.012382992511287223
          ],
          [
            "distribution",
            0.011617197489239043
          ],
          [
            "convergence",
            0.011115853666906088
          ]
        ],
        "count": 571
      },
      "15": {
        "name": "15_privacy_private_DP_differential privacy",
        "keywords": [
          [
            "privacy",
            0.059724648443502386
          ],
          [
            "private",
            0.03929531052631493
          ],
          [
            "DP",
            0.036666679532186865
          ],
          [
            "differential privacy",
            0.022296183722977862
          ],
          [
            "Private",
            0.018498083117017092
          ],
          [
            "differential",
            0.018163259529161903
          ],
          [
            "Privacy",
            0.013780908645306397
          ],
          [
            "Differentially",
            0.013563140370674426
          ],
          [
            "data",
            0.012123150109577839
          ],
          [
            "algorithm",
            0.009764562163358945
          ]
        ],
        "count": 521
      },
      "16": {
        "name": "16_domain_target_source_shift",
        "keywords": [
          [
            "domain",
            0.04184152423478148
          ],
          [
            "target",
            0.027811620123675485
          ],
          [
            "source",
            0.025308474850004863
          ],
          [
            "shift",
            0.023091973632875004
          ],
          [
            "adaptation",
            0.020224599253342506
          ],
          [
            "transfer",
            0.0174176837146391
          ],
          [
            "domain adaptation",
            0.0170225628228356
          ],
          [
            "domains",
            0.01654877185152933
          ],
          [
            "distribution",
            0.015937853136305954
          ],
          [
            "Domain",
            0.015713887089049947
          ]
        ],
        "count": 510
      },
      "17": {
        "name": "17_Gaussian_GP_GPs_Gaussian process",
        "keywords": [
          [
            "Gaussian",
            0.044820026948972795
          ],
          [
            "GP",
            0.02659061864402669
          ],
          [
            "GPs",
            0.019788502704772507
          ],
          [
            "Gaussian process",
            0.01918165279504707
          ],
          [
            "processes",
            0.018837002505618882
          ],
          [
            "Processes",
            0.01764141394636032
          ],
          [
            "Gaussian processes",
            0.017275380481170968
          ],
          [
            "process",
            0.017233402228025444
          ],
          [
            "kernel",
            0.01605387859784969
          ],
          [
            "Process",
            0.012674659325394846
          ]
        ],
        "count": 504
      },
      "18": {
        "name": "18_fairness_fair_Fairness_group",
        "keywords": [
          [
            "fairness",
            0.0728692530198806
          ],
          [
            "fair",
            0.030099198526807522
          ],
          [
            "Fairness",
            0.017850469556966148
          ],
          [
            "group",
            0.014793843308662315
          ],
          [
            "sensitive",
            0.014588648549186437
          ],
          [
            "groups",
            0.012972541442415665
          ],
          [
            "Fair",
            0.012762081428866562
          ],
          [
            "attributes",
            0.012429745385929322
          ],
          [
            "bias",
            0.011518635318885147
          ],
          [
            "algorithmic",
            0.010855414896001083
          ]
        ],
        "count": 483
      },
      "19": {
        "name": "19_calibration_prediction_conformal_coverage",
        "keywords": [
          [
            "calibration",
            0.04284229830205412
          ],
          [
            "prediction",
            0.03827143579410903
          ],
          [
            "conformal",
            0.037437588508942826
          ],
          [
            "coverage",
            0.02582933170289341
          ],
          [
            "Conformal",
            0.025602791740827333
          ],
          [
            "conformal prediction",
            0.023775807605061
          ],
          [
            "sets",
            0.016558252878548785
          ],
          [
            "uncertainty",
            0.016273581408260233
          ],
          [
            "prediction sets",
            0.014269819557914766
          ],
          [
            "Prediction",
            0.013387375996131544
          ]
        ],
        "count": 451
      },
      "20": {
        "name": "20_meta_learning_tasks_task",
        "keywords": [
          [
            "meta",
            0.03423523734101446
          ],
          [
            "learning",
            0.02926561871724417
          ],
          [
            "tasks",
            0.028660338853690428
          ],
          [
            "task",
            0.02801002279361191
          ],
          [
            "meta learning",
            0.025300528902308054
          ],
          [
            "shot",
            0.02237246813330095
          ],
          [
            "forgetting",
            0.01669630490435857
          ],
          [
            "continual",
            0.01584821286303238
          ],
          [
            "Learning",
            0.014747639973359407
          ],
          [
            "Meta",
            0.014270951170353102
          ]
        ],
        "count": 451
      },
      "21": {
        "name": "21_Wasserstein_distance_transport_OT",
        "keywords": [
          [
            "Wasserstein",
            0.034237916968846496
          ],
          [
            "distance",
            0.02219087091257702
          ],
          [
            "transport",
            0.01916979696908306
          ],
          [
            "OT",
            0.01804605024842058
          ],
          [
            "manifold",
            0.015670084778443227
          ],
          [
            "optimal transport",
            0.014741805157514798
          ],
          [
            "data",
            0.013087902897879953
          ],
          [
            "dimensional",
            0.01276904663345838
          ],
          [
            "measures",
            0.012210828430354887
          ],
          [
            "optimal",
            0.0119036386081767
          ]
        ],
        "count": 439
      },
      "22": {
        "name": "22_explanations_explanation_models_model",
        "keywords": [
          [
            "explanations",
            0.029923596853011478
          ],
          [
            "explanation",
            0.020182128807944746
          ],
          [
            "models",
            0.015404813591638944
          ],
          [
            "model",
            0.01536247125787429
          ],
          [
            "AI",
            0.014151252393960477
          ],
          [
            "methods",
            0.012582584955996378
          ],
          [
            "machine",
            0.012402439003937036
          ],
          [
            "machine learning",
            0.011895684594657325
          ],
          [
            "explainability",
            0.010594333853877496
          ],
          [
            "data",
            0.010566224619119118
          ]
        ],
        "count": 424
      },
      "23": {
        "name": "23_segmentation_EEG_classification_images",
        "keywords": [
          [
            "segmentation",
            0.018736241678582587
          ],
          [
            "EEG",
            0.017788175096617762
          ],
          [
            "classification",
            0.013255041177223936
          ],
          [
            "images",
            0.012764846377823248
          ],
          [
            "ECG",
            0.01242278391630602
          ],
          [
            "image",
            0.01183848324732003
          ],
          [
            "medical",
            0.01051501942611109
          ],
          [
            "deep",
            0.010506688778451569
          ],
          [
            "data",
            0.010346298262446617
          ],
          [
            "model",
            0.010223902384994349
          ]
        ],
        "count": 394
      },
      "24": {
        "name": "24_uncertainty_Bayesian_neural_posterior",
        "keywords": [
          [
            "uncertainty",
            0.03468105747059583
          ],
          [
            "Bayesian",
            0.032415431655021386
          ],
          [
            "neural",
            0.02069730854381672
          ],
          [
            "posterior",
            0.018197102489255972
          ],
          [
            "networks",
            0.017066731748865627
          ],
          [
            "neural networks",
            0.01664708394331927
          ],
          [
            "deep",
            0.015320760036857516
          ],
          [
            "BNNs",
            0.014208188775119479
          ],
          [
            "Bayesian neural",
            0.013949198969016526
          ],
          [
            "inference",
            0.013447873693477469
          ]
        ],
        "count": 383
      },
      "25": {
        "name": "25_PAC_bounds_learning_bound",
        "keywords": [
          [
            "PAC",
            0.03135258992235332
          ],
          [
            "bounds",
            0.025518765406379713
          ],
          [
            "learning",
            0.020883848624543847
          ],
          [
            "bound",
            0.01585602396200819
          ],
          [
            "class",
            0.012634286219680995
          ],
          [
            "setting",
            0.011875676206390089
          ],
          [
            "Bayes",
            0.011846264447876878
          ],
          [
            "dimension",
            0.01150875251567879
          ],
          [
            "generalization",
            0.011356961051754216
          ],
          [
            "complexity",
            0.011164917104801496
          ]
        ],
        "count": 360
      },
      "26": {
        "name": "26_selection_regression_Lasso_lasso",
        "keywords": [
          [
            "selection",
            0.020289136106422617
          ],
          [
            "regression",
            0.01843193974778162
          ],
          [
            "Lasso",
            0.01724821850586435
          ],
          [
            "lasso",
            0.012072115150244053
          ],
          [
            "sparse",
            0.010812306223039591
          ],
          [
            "dimensional",
            0.010459403263707177
          ],
          [
            "high",
            0.010329241105742409
          ],
          [
            "method",
            0.00995010538314329
          ],
          [
            "data",
            0.009769874001994954
          ],
          [
            "screening",
            0.008704702325964504
          ]
        ],
        "count": 356
      },
      "27": {
        "name": "27_clustering_clusters_cluster_means",
        "keywords": [
          [
            "clustering",
            0.06772443825665127
          ],
          [
            "clusters",
            0.026236552398010336
          ],
          [
            "cluster",
            0.026186624494895637
          ],
          [
            "means",
            0.02517625734771774
          ],
          [
            "Clustering",
            0.022207178031273027
          ],
          [
            "data",
            0.018418082027503512
          ],
          [
            "algorithm",
            0.014555442634662335
          ],
          [
            "mixture",
            0.010842539064898144
          ],
          [
            "method",
            0.010179106787466734
          ],
          [
            "number",
            0.009747266569943038
          ]
        ],
        "count": 328
      },
      "28": {
        "name": "28_tree_trees_boosting_forest",
        "keywords": [
          [
            "tree",
            0.025768441396027388
          ],
          [
            "trees",
            0.02575369084023618
          ],
          [
            "boosting",
            0.020070720902300842
          ],
          [
            "forest",
            0.014125145049052296
          ],
          [
            "decision",
            0.01412065334560577
          ],
          [
            "feature",
            0.013782169983576184
          ],
          [
            "importance",
            0.013695674595857911
          ],
          [
            "forests",
            0.01319031101797804
          ],
          [
            "random",
            0.013177924498457625
          ],
          [
            "regression",
            0.01231205450047586
          ]
        ],
        "count": 324
      },
      "29": {
        "name": "29_user_recommendation_users_recommender",
        "keywords": [
          [
            "user",
            0.03558717495507034
          ],
          [
            "recommendation",
            0.029927102832067422
          ],
          [
            "users",
            0.023469629868488467
          ],
          [
            "recommender",
            0.022910314462370254
          ],
          [
            "items",
            0.02083535418659019
          ],
          [
            "item",
            0.019289711707191307
          ],
          [
            "systems",
            0.015068207962619632
          ],
          [
            "recommender systems",
            0.015026476814161142
          ],
          [
            "recommendations",
            0.013899580703109966
          ],
          [
            "model",
            0.011725849973243429
          ]
        ],
        "count": 282
      },
      "30": {
        "name": "30_speech_audio_speaker_ASR",
        "keywords": [
          [
            "speech",
            0.0445334766218663
          ],
          [
            "audio",
            0.03192387468778578
          ],
          [
            "speaker",
            0.02836800494479537
          ],
          [
            "ASR",
            0.016756356840593668
          ],
          [
            "model",
            0.015990586513826066
          ],
          [
            "recognition",
            0.015038226989972982
          ],
          [
            "acoustic",
            0.011919068468163607
          ],
          [
            "Speech",
            0.011771106233860584
          ],
          [
            "training",
            0.011453692420473987
          ],
          [
            "models",
            0.011130675214353597
          ]
        ],
        "count": 264
      },
      "31": {
        "name": "31_quantum_Quantum_classical_learning",
        "keywords": [
          [
            "quantum",
            0.13251683613196238
          ],
          [
            "Quantum",
            0.03796162362336898
          ],
          [
            "classical",
            0.02639789443912435
          ],
          [
            "learning",
            0.019243089907545488
          ],
          [
            "machine",
            0.018311698054099386
          ],
          [
            "machine learning",
            0.0179071744703797
          ],
          [
            "circuits",
            0.015483635686711673
          ],
          [
            "quantum machine",
            0.01389494808623194
          ],
          [
            "quantum machine learning",
            0.013754544733465187
          ],
          [
            "states",
            0.012093879483449414
          ]
        ],
        "count": 257
      },
      "32": {
        "name": "32_traffic_road_travel_prediction",
        "keywords": [
          [
            "traffic",
            0.046169312431281376
          ],
          [
            "road",
            0.017818927671877493
          ],
          [
            "travel",
            0.01634871129708164
          ],
          [
            "prediction",
            0.015752739614305736
          ],
          [
            "temporal",
            0.014005583177936496
          ],
          [
            "time",
            0.01381387948076225
          ],
          [
            "trajectory",
            0.013083777436034897
          ],
          [
            "data",
            0.012823181671749888
          ],
          [
            "model",
            0.012731744125703987
          ],
          [
            "transportation",
            0.012520253666305044
          ]
        ],
        "count": 211
      },
      "33": {
        "name": "33_active_active learning_Active_learning",
        "keywords": [
          [
            "active",
            0.05596684775410681
          ],
          [
            "active learning",
            0.05064333654260229
          ],
          [
            "Active",
            0.03919375915221229
          ],
          [
            "learning",
            0.025110951313636592
          ],
          [
            "AL",
            0.014686066601537021
          ],
          [
            "data",
            0.014573257814292319
          ],
          [
            "label",
            0.01440345789431298
          ],
          [
            "Learning",
            0.0134532984878445
          ],
          [
            "labeling",
            0.012367164131902298
          ],
          [
            "labels",
            0.010186091578827682
          ]
        ],
        "count": 201
      }
    },
    "correlations": [
      [
        1.0,
        -0.7287319959516478,
        -0.7517103374902783,
        -0.7002489998877162,
        -0.7308930486447982,
        -0.7203266171493554,
        -0.7096155871926757,
        -0.7286219091588846,
        -0.7186922655992039,
        -0.7166762511460737,
        -0.6713786885964164,
        -0.7253831914016471,
        -0.7155071141345677,
        -0.6866174749981033,
        -0.6917476259377568,
        -0.748896795542763,
        -0.7132700784138137,
        -0.7143433171919531,
        -0.7586663226977381,
        -0.725188895598598,
        -0.6942384834249746,
        -0.7282838093123326,
        -0.6731211232120219,
        -0.70709271360193,
        -0.08101589569599113,
        -0.7026858984867304,
        -0.7172796279489458,
        -0.7237444847238946,
        -0.7398287931609955,
        -0.7454865446670664,
        -0.7527329469340011,
        -0.722886290786469,
        -0.756357977489434,
        -0.7349924923305624
      ],
      [
        -0.7287319959516478,
        1.0,
        -0.7416837390150943,
        -0.7341237649648751,
        -0.7540946156900341,
        -0.7507250588992622,
        -0.7324646309018823,
        -0.6458198339757526,
        -0.7267302913277991,
        -0.7488578363681875,
        -0.7266742602006744,
        -0.676600179302781,
        -0.7407523286593611,
        -0.6819819557262616,
        -0.7444330173414297,
        -0.7564394500834768,
        -0.7177462232387147,
        -0.7458903825926123,
        -0.759072510879029,
        -0.7473429924740129,
        -0.6249875125027818,
        -0.7527549302807494,
        -0.7240443445984185,
        -0.7492255318831678,
        -0.7166810809620233,
        -0.6538584461352412,
        -0.7514178401090698,
        -0.7457728370304892,
        -0.7588446476251942,
        -0.735302443992178,
        -0.7609421790299706,
        -0.7436016893450732,
        -0.7478025080087334,
        -0.6525672908040532
      ],
      [
        -0.7517103374902783,
        -0.7416837390150943,
        1.0,
        -0.7153348524808916,
        -0.760455347160507,
        -0.7126703401106297,
        -0.742207163638783,
        -0.7410310710079768,
        -0.7429103618940903,
        -0.7575497725989103,
        -0.7526717438737577,
        -0.7589484372268225,
        -0.7145179423956115,
        -0.7322164146195582,
        -0.7574208182918899,
        -0.7575849050373604,
        -0.7227427814214213,
        -0.7442880893631715,
        -0.7423957566020603,
        -0.7394921819600166,
        -0.740456212887991,
        -0.754585776641691,
        -0.7223698896515991,
        -0.7589410621210779,
        -0.7360160113008457,
        -0.7471509826670959,
        -0.7406186675790865,
        -0.7392206001530383,
        -0.7412643552712912,
        -0.7463641870263031,
        -0.7622226111106455,
        -0.7470837473587569,
        -0.7617247871554125,
        -0.7489509972210697
      ],
      [
        -0.7002489998877162,
        -0.7341237649648751,
        -0.7153348524808916,
        1.0,
        -0.731545422037207,
        -0.7238534544909586,
        -0.6979071597033102,
        -0.7199273774093786,
        -0.7110965834121088,
        -0.73069426760482,
        -0.7197700870662256,
        -0.7379475485278805,
        -0.4292648206564994,
        -0.5190790910751051,
        -0.7350608844447173,
        -0.7438957585910699,
        -0.7081871915633197,
        -0.706751972643048,
        -0.7525085275511489,
        -0.7139049176169883,
        -0.7032849021352707,
        -0.7382964690024423,
        -0.6560189216889173,
        -0.7145776501776964,
        -0.6849894191472199,
        -0.7356951699266099,
        -0.7166420256921122,
        -0.5333619792487652,
        -0.728091786012536,
        -0.7334134460030298,
        -0.7512158101177253,
        -0.7270801286217339,
        -0.7335159428271006,
        -0.7373935598266929
      ],
      [
        -0.7308930486447982,
        -0.7540946156900341,
        -0.760455347160507,
        -0.731545422037207,
        1.0,
        -0.7566840231136231,
        -0.7434302190735568,
        -0.7479559044991743,
        -0.7489490615185699,
        -0.754159125548092,
        -0.7525408964968157,
        -0.7596505418094446,
        -0.739158092872225,
        -0.748121166311369,
        -0.7324163745142023,
        -0.7632702400584985,
        -0.7379394900109733,
        -0.6781453814001781,
        -0.764936612525224,
        -0.7551823773750106,
        -0.7429305576410088,
        -0.7520491946812324,
        -0.7239355489846686,
        -0.7477226922386622,
        -0.6863898992271009,
        -0.7544668821637197,
        -0.7311128058975674,
        -0.7512917274277371,
        -0.757564371787389,
        -0.7521953513542291,
        -0.7572473924870367,
        -0.7488598025572484,
        -0.7612140254671378,
        -0.7444073190845901
      ],
      [
        -0.7203266171493554,
        -0.7507250588992622,
        -0.7126703401106297,
        -0.7238534544909586,
        -0.7566840231136231,
        1.0,
        -0.7311498828170723,
        -0.7387364860525345,
        -0.7332460174253177,
        -0.7365668512485515,
        -0.7376734525286712,
        -0.7470856242233443,
        -0.7349857230697807,
        -0.7084413986562268,
        -0.753750613997844,
        -0.7570004640063042,
        -0.7377736655873168,
        -0.7420020259989158,
        -0.755749532611157,
        -0.740817824847421,
        -0.7141673265380599,
        -0.7398439447617167,
        -0.7129100746695027,
        -0.74882620226266,
        -0.7143778353103263,
        -0.7467083680599464,
        -0.7437230370855663,
        -0.6970571897870943,
        -0.7438225439076436,
        -0.734473243020077,
        -0.7620875445734092,
        -0.7440207697921685,
        -0.749259061565166,
        -0.7473023050538435
      ],
      [
        -0.7096155871926757,
        -0.7324646309018823,
        -0.742207163638783,
        -0.6979071597033102,
        -0.7434302190735568,
        -0.7311498828170723,
        1.0,
        -0.7432558988440101,
        -0.48807033129546584,
        -0.7497081980000122,
        -0.6925858702744063,
        -0.7505187678233034,
        -0.7105443648811828,
        -0.7073191860406738,
        -0.7019817454512391,
        -0.7468434022188789,
        -0.7047059477219423,
        -0.7108538619238403,
        -0.7529696125562823,
        -0.7357717037541216,
        -0.7120267705250629,
        -0.707817287276461,
        -0.5350009016205024,
        -0.742925606420892,
        -0.6896924471832381,
        -0.7408577758785219,
        -0.7278037805359316,
        -0.7258444038034999,
        -0.7450331645187976,
        -0.7440982717767002,
        -0.7399540526939017,
        -0.7390903971187541,
        -0.759508339647166,
        -0.7403240517816836
      ],
      [
        -0.7286219091588846,
        -0.6458198339757526,
        -0.7410310710079768,
        -0.7199273774093786,
        -0.7479559044991743,
        -0.7387364860525345,
        -0.7432558988440101,
        1.0,
        -0.7431078982153511,
        -0.7269955054316425,
        -0.7086988175677633,
        -0.7236066296700184,
        -0.7374951654463462,
        -0.7223820581967796,
        -0.7347845496625061,
        -0.7406396739771239,
        -0.7341593233232292,
        -0.7217651835505597,
        -0.7507718958014484,
        -0.7458105499637977,
        -0.719595372809475,
        -0.7460234489087529,
        -0.7278014500978632,
        -0.7568221457069977,
        -0.7265247710488945,
        -0.672494863743802,
        -0.7313989321284069,
        -0.727489401447937,
        -0.7480515425943346,
        -0.7036155392708242,
        -0.7638433906508666,
        -0.7461833484845859,
        -0.7570649384363046,
        -0.7331739312865849
      ],
      [
        -0.7186922655992039,
        -0.7267302913277991,
        -0.7429103618940903,
        -0.7110965834121088,
        -0.7489490615185699,
        -0.7332460174253177,
        -0.48807033129546584,
        -0.7431078982153511,
        1.0,
        -0.7417434726438589,
        -0.7156879162303,
        -0.7455106945479656,
        -0.7181761690803328,
        -0.7149700279408553,
        -0.7433548988385383,
        -0.7501286085128198,
        -0.7183335113998305,
        -0.735005593897313,
        -0.7509014276755331,
        -0.7262653997416924,
        -0.6971206878871845,
        -0.751611390792608,
        -0.522851526260969,
        -0.7368303129723031,
        -0.6999466666822012,
        -0.7421944782795827,
        -0.7382847005783126,
        -0.732143204117419,
        -0.74599017983229,
        -0.7247577391078819,
        -0.7360727524683177,
        -0.7432366390191874,
        -0.7572971333183176,
        -0.7441795176163579
      ],
      [
        -0.7166762511460737,
        -0.7488578363681875,
        -0.7575497725989103,
        -0.73069426760482,
        -0.754159125548092,
        -0.7365668512485515,
        -0.7497081980000122,
        -0.7269955054316425,
        -0.7417434726438589,
        1.0,
        -0.7503038750077997,
        -0.7548896175556283,
        -0.7261212022191486,
        -0.7284025707710762,
        -0.7483452976222686,
        -0.7578524655960748,
        -0.7494527324640075,
        -0.7294425262285877,
        -0.7614692338716065,
        -0.7555089523553069,
        -0.7398158689997616,
        -0.7474421424013584,
        -0.7301463604198364,
        -0.753186240355028,
        -0.7261468410772137,
        -0.7412640747961744,
        -0.7298289175751624,
        -0.7207190588065813,
        -0.7576040596344764,
        -0.7374087566032358,
        -0.7630707601501816,
        -0.7401500083457625,
        -0.757283165576694,
        -0.7522154486583037
      ],
      [
        -0.6713786885964164,
        -0.7266742602006744,
        -0.7526717438737577,
        -0.7197700870662256,
        -0.7525408964968157,
        -0.7376734525286712,
        -0.6925858702744063,
        -0.7086988175677633,
        -0.7156879162303,
        -0.7503038750077997,
        1.0,
        -0.7323226941982768,
        -0.7299521660689889,
        -0.6972065404154377,
        -0.748040648732775,
        -0.7365366664416262,
        -0.7046993027908921,
        -0.7397086922219205,
        -0.7463187943656885,
        -0.7386977381640902,
        -0.7180560347651664,
        -0.7389175726831061,
        -0.6893808579982472,
        -0.7351472666300667,
        -0.6736408854297331,
        -0.7284023823653247,
        -0.7367671227898531,
        -0.7305666463891185,
        -0.7503759825663712,
        -0.7467405276672388,
        -0.7443497856144257,
        -0.7446376210354541,
        -0.7596429238260856,
        -0.7459268096446002
      ],
      [
        -0.7253831914016471,
        -0.676600179302781,
        -0.7589484372268225,
        -0.7379475485278805,
        -0.7596505418094446,
        -0.7470856242233443,
        -0.7505187678233034,
        -0.7236066296700184,
        -0.7455106945479656,
        -0.7548896175556283,
        -0.7323226941982768,
        1.0,
        -0.7276687574036846,
        -0.6653779415480765,
        -0.7550254746210017,
        -0.6089601673678183,
        -0.7462206583490616,
        -0.7482484935749228,
        -0.7453154768504251,
        -0.7562829502409023,
        -0.6478506977287455,
        -0.7527728230711186,
        -0.7260431296169189,
        -0.7481589042163199,
        -0.7326485341283693,
        -0.6508650139741741,
        -0.7558067104199598,
        -0.7334144094343887,
        -0.7605653807116106,
        -0.7342484766397204,
        -0.7573654987677712,
        -0.7418803861218444,
        -0.7549096701218212,
        -0.6406163749396663
      ],
      [
        -0.7155071141345677,
        -0.7407523286593611,
        -0.7145179423956115,
        -0.4292648206564994,
        -0.739158092872225,
        -0.7349857230697807,
        -0.7105443648811828,
        -0.7374951654463462,
        -0.7181761690803328,
        -0.7261212022191486,
        -0.7299521660689889,
        -0.7276687574036846,
        1.0,
        -0.36509294196376846,
        -0.7422001713516884,
        -0.7322224487327281,
        -0.7134876521217529,
        -0.7199457177119106,
        -0.7413253791715961,
        -0.7146220543603655,
        -0.7162916935345822,
        -0.7443877265221357,
        -0.6643344947054862,
        -0.7281420302106639,
        -0.6964020861003444,
        -0.7391317239054231,
        -0.7080029532725282,
        -0.418693956810233,
        -0.7235397926197383,
        -0.731032234492613,
        -0.7511098402066965,
        -0.7313878929608182,
        -0.7464227880394103,
        -0.7381286522172077
      ],
      [
        -0.6866174749981033,
        -0.6819819557262616,
        -0.7322164146195582,
        -0.5190790910751051,
        -0.748121166311369,
        -0.7084413986562268,
        -0.7073191860406738,
        -0.7223820581967796,
        -0.7149700279408553,
        -0.7284025707710762,
        -0.6972065404154377,
        -0.6653779415480765,
        -0.36509294196376846,
        1.0,
        -0.7425582460535989,
        -0.7346916832175506,
        -0.6869162682773262,
        -0.7200556844620277,
        -0.7440325189730648,
        -0.7161088150712669,
        -0.6174023312976255,
        -0.7373738587477747,
        -0.6660199103359667,
        -0.722089503628211,
        -0.6820330456868908,
        -0.6502495528372696,
        -0.7050401423676262,
        -0.4971058559649852,
        -0.7324036708302137,
        -0.7367029556777619,
        -0.749070345355846,
        -0.721705573366981,
        -0.7544772519526152,
        -0.6381868384628563
      ],
      [
        -0.6917476259377568,
        -0.7444330173414297,
        -0.7574208182918899,
        -0.7350608844447173,
        -0.7324163745142023,
        -0.753750613997844,
        -0.7019817454512391,
        -0.7347845496625061,
        -0.7433548988385383,
        -0.7483452976222686,
        -0.748040648732775,
        -0.7550254746210017,
        -0.7422001713516884,
        -0.7425582460535989,
        1.0,
        -0.7548627465916342,
        -0.7229220038187613,
        -0.7079908868819148,
        -0.7618376597288222,
        -0.7480950062367518,
        -0.744902486363819,
        -0.712521746525725,
        -0.7287881474341487,
        -0.7554348820508445,
        -0.651137695128803,
        -0.7408638648309802,
        -0.7343502024615522,
        -0.7428756793757924,
        -0.7536108761598006,
        -0.7544244658069971,
        -0.76175289568364,
        -0.7404893677497808,
        -0.7612863957206277,
        -0.747623510172616
      ],
      [
        -0.748896795542763,
        -0.7564394500834768,
        -0.7575849050373604,
        -0.7438957585910699,
        -0.7632702400584985,
        -0.7570004640063042,
        -0.7468434022188789,
        -0.7406396739771239,
        -0.7501286085128198,
        -0.7578524655960748,
        -0.7365366664416262,
        -0.6089601673678183,
        -0.7322224487327281,
        -0.7346916832175506,
        -0.7548627465916342,
        1.0,
        -0.748407037630821,
        -0.7524670273065339,
        -0.7415202298455175,
        -0.7570218243864063,
        -0.7481583391866704,
        -0.7534709914884195,
        -0.740367375058701,
        -0.7604938967111736,
        -0.750379622129624,
        -0.7372609688182918,
        -0.7525982462039416,
        -0.7416279311838656,
        -0.7580142634600759,
        -0.7272161720445548,
        -0.7568409975989365,
        -0.7518172257111322,
        -0.7625359745488443,
        -0.7546302745376914
      ],
      [
        -0.7132700784138137,
        -0.7177462232387147,
        -0.7227427814214213,
        -0.7081871915633197,
        -0.7379394900109733,
        -0.7377736655873168,
        -0.7047059477219423,
        -0.7341593233232292,
        -0.7183335113998305,
        -0.7494527324640075,
        -0.7046993027908921,
        -0.7462206583490616,
        -0.7134876521217529,
        -0.6869162682773262,
        -0.7229220038187613,
        -0.748407037630821,
        1.0,
        -0.7268243673344443,
        -0.7502259277967136,
        -0.7279425146370239,
        -0.6919511699923815,
        -0.707396260502881,
        -0.6984054548590133,
        -0.7394435174962908,
        -0.70086641789331,
        -0.7301721665770253,
        -0.7321053185859353,
        -0.7273580801980106,
        -0.7471970534722552,
        -0.7412106167702399,
        -0.7451882906971579,
        -0.739855055632795,
        -0.7578502144048248,
        -0.7381207221539179
      ],
      [
        -0.7143433171919531,
        -0.7458903825926123,
        -0.7442880893631715,
        -0.706751972643048,
        -0.6781453814001781,
        -0.7420020259989158,
        -0.7108538619238403,
        -0.7217651835505597,
        -0.735005593897313,
        -0.7294425262285877,
        -0.7397086922219205,
        -0.7482484935749228,
        -0.7199457177119106,
        -0.7200556844620277,
        -0.7079908868819148,
        -0.7524670273065339,
        -0.7268243673344443,
        1.0,
        -0.7630970913617958,
        -0.7387190920478784,
        -0.7281684215225961,
        -0.7299428917455382,
        -0.7004697302957245,
        -0.750320586544541,
        -0.6583126373977249,
        -0.73254380186908,
        -0.7167915896189372,
        -0.7195559687268149,
        -0.7489786858889341,
        -0.7527928689666048,
        -0.7588840040279508,
        -0.7417273503301278,
        -0.756488742359322,
        -0.7245853836853777
      ],
      [
        -0.7586663226977381,
        -0.759072510879029,
        -0.7423957566020603,
        -0.7525085275511489,
        -0.764936612525224,
        -0.755749532611157,
        -0.7529696125562823,
        -0.7507718958014484,
        -0.7509014276755331,
        -0.7614692338716065,
        -0.7463187943656885,
        -0.7453154768504251,
        -0.7413253791715961,
        -0.7440325189730648,
        -0.7618376597288222,
        -0.7415202298455175,
        -0.7502259277967136,
        -0.7630970913617958,
        1.0,
        -0.7464005346893414,
        -0.7524782595208324,
        -0.7524396867147223,
        -0.7317904292297284,
        -0.7607584558505378,
        -0.7548809192691356,
        -0.7556775930525612,
        -0.7590533408190283,
        -0.7405646665814432,
        -0.7565481905214699,
        -0.7406921289316403,
        -0.7619917357114161,
        -0.7502235461965295,
        -0.7639073590745826,
        -0.7517105266475828
      ],
      [
        -0.725188895598598,
        -0.7473429924740129,
        -0.7394921819600166,
        -0.7139049176169883,
        -0.7551823773750106,
        -0.740817824847421,
        -0.7357717037541216,
        -0.7458105499637977,
        -0.7262653997416924,
        -0.7555089523553069,
        -0.7386977381640902,
        -0.7562829502409023,
        -0.7146220543603655,
        -0.7161088150712669,
        -0.7480950062367518,
        -0.7570218243864063,
        -0.7279425146370239,
        -0.7387190920478784,
        -0.7464005346893414,
        1.0,
        -0.7339202788789244,
        -0.755030912725563,
        -0.6976047488128603,
        -0.7502377960625457,
        -0.6705114797983613,
        -0.7445690623797309,
        -0.7377525923841958,
        -0.7396048227189628,
        -0.7332168170034341,
        -0.7433246575553701,
        -0.7589391766487106,
        -0.7453276922519054,
        -0.7463176184716804,
        -0.7489957780286921
      ],
      [
        -0.6942384834249746,
        -0.6249875125027818,
        -0.740456212887991,
        -0.7032849021352707,
        -0.7429305576410088,
        -0.7141673265380599,
        -0.7120267705250629,
        -0.719595372809475,
        -0.6971206878871845,
        -0.7398158689997616,
        -0.7180560347651664,
        -0.6478506977287455,
        -0.7162916935345822,
        -0.6174023312976255,
        -0.744902486363819,
        -0.7481583391866704,
        -0.6919511699923815,
        -0.7281684215225961,
        -0.7524782595208324,
        -0.7339202788789244,
        1.0,
        -0.7475698235646986,
        -0.6791200261862285,
        -0.7253586848494026,
        -0.6854490898530803,
        -0.6289078534698107,
        -0.7341867133352828,
        -0.7216712511540361,
        -0.7475324425743559,
        -0.737765626550088,
        -0.7414224972694986,
        -0.7244298588376634,
        -0.7559172450646027,
        -0.6284229230010916
      ],
      [
        -0.7282838093123326,
        -0.7527549302807494,
        -0.754585776641691,
        -0.7382964690024423,
        -0.7520491946812324,
        -0.7398439447617167,
        -0.707817287276461,
        -0.7460234489087529,
        -0.751611390792608,
        -0.7474421424013584,
        -0.7389175726831061,
        -0.7527728230711186,
        -0.7443877265221357,
        -0.7373738587477747,
        -0.712521746525725,
        -0.7534709914884195,
        -0.707396260502881,
        -0.7299428917455382,
        -0.7524396867147223,
        -0.755030912725563,
        -0.7475698235646986,
        1.0,
        -0.7318495387162065,
        -0.7566573790300293,
        -0.7305347873527939,
        -0.7420979087703755,
        -0.7486027988729328,
        -0.7314456711657196,
        -0.7553500223959166,
        -0.760251660643996,
        -0.7633454611042284,
        -0.741612553708293,
        -0.7623421440003608,
        -0.754613144150858
      ],
      [
        -0.6731211232120219,
        -0.7240443445984185,
        -0.7223698896515991,
        -0.6560189216889173,
        -0.7239355489846686,
        -0.7129100746695027,
        -0.5350009016205024,
        -0.7278014500978632,
        -0.522851526260969,
        -0.7301463604198364,
        -0.6893808579982472,
        -0.7260431296169189,
        -0.6643344947054862,
        -0.6660199103359667,
        -0.7287881474341487,
        -0.740367375058701,
        -0.6984054548590133,
        -0.7004697302957245,
        -0.7317904292297284,
        -0.6976047488128603,
        -0.6791200261862285,
        -0.7318495387162065,
        1.0,
        -0.7250479712220528,
        -0.6452150993087842,
        -0.724235572175592,
        -0.7008079580598238,
        -0.6966243135450623,
        -0.7050999530577375,
        -0.7096338569481937,
        -0.7410267942560989,
        -0.48192018547220544,
        -0.7503783079854995,
        -0.7213308661447873
      ],
      [
        -0.70709271360193,
        -0.7492255318831678,
        -0.7589410621210779,
        -0.7145776501776964,
        -0.7477226922386622,
        -0.74882620226266,
        -0.742925606420892,
        -0.7568221457069977,
        -0.7368303129723031,
        -0.753186240355028,
        -0.7351472666300667,
        -0.7481589042163199,
        -0.7281420302106639,
        -0.722089503628211,
        -0.7554348820508445,
        -0.7604938967111736,
        -0.7394435174962908,
        -0.750320586544541,
        -0.7607584558505378,
        -0.7502377960625457,
        -0.7253586848494026,
        -0.7566573790300293,
        -0.7250479712220528,
        1.0,
        -0.5893066626409439,
        -0.745759576385048,
        -0.7477684626389884,
        -0.7416104769681264,
        -0.7485363172270751,
        -0.7549970532984082,
        -0.742677641351425,
        -0.7497205184211011,
        -0.7541267426702021,
        -0.7427663646923137
      ],
      [
        -0.08101589569599113,
        -0.7166810809620233,
        -0.7360160113008457,
        -0.6849894191472199,
        -0.6863898992271009,
        -0.7143778353103263,
        -0.6896924471832381,
        -0.7265247710488945,
        -0.6999466666822012,
        -0.7261468410772137,
        -0.6736408854297331,
        -0.7326485341283693,
        -0.6964020861003444,
        -0.6820330456868908,
        -0.651137695128803,
        -0.750379622129624,
        -0.70086641789331,
        -0.6583126373977249,
        -0.7548809192691356,
        -0.6705114797983613,
        -0.6854490898530803,
        -0.7305347873527939,
        -0.6452150993087842,
        -0.5893066626409439,
        1.0,
        -0.7147790339542214,
        -0.7113945254983887,
        -0.7163026227014477,
        -0.7320439280505311,
        -0.7368074978300708,
        -0.7528002265468536,
        -0.722052690453177,
        -0.7519295821391694,
        -0.7152062751331836
      ],
      [
        -0.7026858984867304,
        -0.6538584461352412,
        -0.7471509826670959,
        -0.7356951699266099,
        -0.7544668821637197,
        -0.7467083680599464,
        -0.7408577758785219,
        -0.672494863743802,
        -0.7421944782795827,
        -0.7412640747961744,
        -0.7284023823653247,
        -0.6508650139741741,
        -0.7391317239054231,
        -0.6502495528372696,
        -0.7408638648309802,
        -0.7372609688182918,
        -0.7301721665770253,
        -0.73254380186908,
        -0.7556775930525612,
        -0.7445690623797309,
        -0.6289078534698107,
        -0.7420979087703755,
        -0.724235572175592,
        -0.745759576385048,
        -0.7147790339542214,
        1.0,
        -0.7427000919734714,
        -0.7412330667565423,
        -0.7505666300103591,
        -0.7470838595338458,
        -0.7593344337464396,
        -0.7339409705941808,
        -0.7642908196240938,
        -0.6265101306231975
      ],
      [
        -0.7172796279489458,
        -0.7514178401090698,
        -0.7406186675790865,
        -0.7166420256921122,
        -0.7311128058975674,
        -0.7437230370855663,
        -0.7278037805359316,
        -0.7313989321284069,
        -0.7382847005783126,
        -0.7298289175751624,
        -0.7367671227898531,
        -0.7558067104199598,
        -0.7080029532725282,
        -0.7050401423676262,
        -0.7343502024615522,
        -0.7525982462039416,
        -0.7321053185859353,
        -0.7167915896189372,
        -0.7590533408190283,
        -0.7377525923841958,
        -0.7341867133352828,
        -0.7486027988729328,
        -0.7008079580598238,
        -0.7477684626389884,
        -0.7113945254983887,
        -0.7427000919734714,
        1.0,
        -0.7120822012179393,
        -0.7231273104307248,
        -0.7501782909278693,
        -0.7623420871810618,
        -0.738144907649082,
        -0.7604173307447567,
        -0.7456504524548293
      ],
      [
        -0.7237444847238946,
        -0.7457728370304892,
        -0.7392206001530383,
        -0.5333619792487652,
        -0.7512917274277371,
        -0.6970571897870943,
        -0.7258444038034999,
        -0.727489401447937,
        -0.732143204117419,
        -0.7207190588065813,
        -0.7305666463891185,
        -0.7334144094343887,
        -0.418693956810233,
        -0.4971058559649852,
        -0.7428756793757924,
        -0.7416279311838656,
        -0.7273580801980106,
        -0.7195559687268149,
        -0.7405646665814432,
        -0.7396048227189628,
        -0.7216712511540361,
        -0.7314456711657196,
        -0.6966243135450623,
        -0.7416104769681264,
        -0.7163026227014477,
        -0.7412330667565423,
        -0.7120822012179393,
        1.0,
        -0.7319963826539246,
        -0.7389000974468936,
        -0.7530428223666308,
        -0.7377826018041045,
        -0.752801524227599,
        -0.7411524387280246
      ],
      [
        -0.7398287931609955,
        -0.7588446476251942,
        -0.7412643552712912,
        -0.728091786012536,
        -0.757564371787389,
        -0.7438225439076436,
        -0.7450331645187976,
        -0.7480515425943346,
        -0.74599017983229,
        -0.7576040596344764,
        -0.7503759825663712,
        -0.7605653807116106,
        -0.7235397926197383,
        -0.7324036708302137,
        -0.7536108761598006,
        -0.7580142634600759,
        -0.7471970534722552,
        -0.7489786858889341,
        -0.7565481905214699,
        -0.7332168170034341,
        -0.7475324425743559,
        -0.7553500223959166,
        -0.7050999530577375,
        -0.7485363172270751,
        -0.7320439280505311,
        -0.7505666300103591,
        -0.7231273104307248,
        -0.7319963826539246,
        1.0,
        -0.7528265707668346,
        -0.7633673774542511,
        -0.7452374000567497,
        -0.7610925668896908,
        -0.7533325453804167
      ],
      [
        -0.7454865446670664,
        -0.735302443992178,
        -0.7463641870263031,
        -0.7334134460030298,
        -0.7521953513542291,
        -0.734473243020077,
        -0.7440982717767002,
        -0.7036155392708242,
        -0.7247577391078819,
        -0.7374087566032358,
        -0.7467405276672388,
        -0.7342484766397204,
        -0.731032234492613,
        -0.7367029556777619,
        -0.7544244658069971,
        -0.7272161720445548,
        -0.7412106167702399,
        -0.7527928689666048,
        -0.7406921289316403,
        -0.7433246575553701,
        -0.737765626550088,
        -0.760251660643996,
        -0.7096338569481937,
        -0.7549970532984082,
        -0.7368074978300708,
        -0.7470838595338458,
        -0.7501782909278693,
        -0.7389000974468936,
        -0.7528265707668346,
        1.0,
        -0.7547143204107869,
        -0.7515281243816705,
        -0.743626913814715,
        -0.7433641814199683
      ],
      [
        -0.7527329469340011,
        -0.7609421790299706,
        -0.7622226111106455,
        -0.7512158101177253,
        -0.7572473924870367,
        -0.7620875445734092,
        -0.7399540526939017,
        -0.7638433906508666,
        -0.7360727524683177,
        -0.7630707601501816,
        -0.7443497856144257,
        -0.7573654987677712,
        -0.7511098402066965,
        -0.749070345355846,
        -0.76175289568364,
        -0.7568409975989365,
        -0.7451882906971579,
        -0.7588840040279508,
        -0.7619917357114161,
        -0.7589391766487106,
        -0.7414224972694986,
        -0.7633454611042284,
        -0.7410267942560989,
        -0.742677641351425,
        -0.7528002265468536,
        -0.7593344337464396,
        -0.7623420871810618,
        -0.7530428223666308,
        -0.7633673774542511,
        -0.7547143204107869,
        1.0,
        -0.7586985247283924,
        -0.7631980216020903,
        -0.752421714972356
      ],
      [
        -0.722886290786469,
        -0.7436016893450732,
        -0.7470837473587569,
        -0.7270801286217339,
        -0.7488598025572484,
        -0.7440207697921685,
        -0.7390903971187541,
        -0.7461833484845859,
        -0.7432366390191874,
        -0.7401500083457625,
        -0.7446376210354541,
        -0.7418803861218444,
        -0.7313878929608182,
        -0.721705573366981,
        -0.7404893677497808,
        -0.7518172257111322,
        -0.739855055632795,
        -0.7417273503301278,
        -0.7502235461965295,
        -0.7453276922519054,
        -0.7244298588376634,
        -0.741612553708293,
        -0.48192018547220544,
        -0.7497205184211011,
        -0.722052690453177,
        -0.7339409705941808,
        -0.738144907649082,
        -0.7377826018041045,
        -0.7452374000567497,
        -0.7515281243816705,
        -0.7586985247283924,
        1.0,
        -0.7615942203612601,
        -0.7338704049952816
      ],
      [
        -0.756357977489434,
        -0.7478025080087334,
        -0.7617247871554125,
        -0.7335159428271006,
        -0.7612140254671378,
        -0.749259061565166,
        -0.759508339647166,
        -0.7570649384363046,
        -0.7572971333183176,
        -0.757283165576694,
        -0.7596429238260856,
        -0.7549096701218212,
        -0.7464227880394103,
        -0.7544772519526152,
        -0.7612863957206277,
        -0.7625359745488443,
        -0.7578502144048248,
        -0.756488742359322,
        -0.7639073590745826,
        -0.7463176184716804,
        -0.7559172450646027,
        -0.7623421440003608,
        -0.7503783079854995,
        -0.7541267426702021,
        -0.7519295821391694,
        -0.7642908196240938,
        -0.7604173307447567,
        -0.752801524227599,
        -0.7610925668896908,
        -0.743626913814715,
        -0.7631980216020903,
        -0.7615942203612601,
        1.0,
        -0.7576124228664479
      ],
      [
        -0.7349924923305624,
        -0.6525672908040532,
        -0.7489509972210697,
        -0.7373935598266929,
        -0.7444073190845901,
        -0.7473023050538435,
        -0.7403240517816836,
        -0.7331739312865849,
        -0.7441795176163579,
        -0.7522154486583037,
        -0.7459268096446002,
        -0.6406163749396663,
        -0.7381286522172077,
        -0.6381868384628563,
        -0.747623510172616,
        -0.7546302745376914,
        -0.7381207221539179,
        -0.7245853836853777,
        -0.7517105266475828,
        -0.7489957780286921,
        -0.6284229230010916,
        -0.754613144150858,
        -0.7213308661447873,
        -0.7427663646923137,
        -0.7152062751331836,
        -0.6265101306231975,
        -0.7456504524548293,
        -0.7411524387280246,
        -0.7533325453804167,
        -0.7433641814199683,
        -0.752421714972356,
        -0.7338704049952816,
        -0.7576124228664479,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        138,
        49,
        18,
        55,
        5,
        40,
        20,
        31,
        16,
        17,
        43,
        17,
        17,
        28,
        17,
        14,
        27,
        29,
        10,
        15,
        36,
        10,
        45,
        8,
        28,
        5,
        15,
        36,
        16,
        18,
        20,
        8,
        8,
        16
      ],
      "2020-02": [
        255,
        103,
        19,
        69,
        9,
        78,
        40,
        72,
        17,
        30,
        117,
        41,
        28,
        45,
        32,
        21,
        64,
        45,
        26,
        26,
        74,
        28,
        53,
        13,
        58,
        14,
        12,
        54,
        20,
        36,
        33,
        8,
        9,
        24
      ],
      "2020-03": [
        185,
        69,
        15,
        65,
        5,
        63,
        33,
        42,
        16,
        14,
        67,
        21,
        37,
        25,
        13,
        32,
        44,
        36,
        8,
        25,
        52,
        6,
        36,
        11,
        45,
        3,
        7,
        38,
        13,
        22,
        10,
        7,
        22,
        12
      ],
      "2020-04": [
        148,
        62,
        18,
        72,
        5,
        32,
        31,
        23,
        29,
        15,
        64,
        18,
        54,
        27,
        12,
        17,
        34,
        26,
        4,
        20,
        65,
        8,
        39,
        19,
        41,
        9,
        19,
        34,
        13,
        18,
        21,
        5,
        8,
        14
      ],
      "2020-05": [
        139,
        57,
        15,
        53,
        6,
        44,
        22,
        29,
        7,
        15,
        59,
        16,
        39,
        31,
        17,
        21,
        38,
        29,
        19,
        18,
        43,
        7,
        40,
        7,
        40,
        5,
        18,
        40,
        8,
        20,
        19,
        12,
        13,
        17
      ],
      "2020-06": [
        333,
        141,
        33,
        85,
        6,
        140,
        64,
        98,
        21,
        37,
        121,
        40,
        69,
        63,
        42,
        43,
        80,
        62,
        46,
        38,
        123,
        39,
        77,
        8,
        96,
        29,
        29,
        81,
        29,
        45,
        22,
        9,
        18,
        27
      ],
      "2020-07": [
        194,
        106,
        24,
        53,
        5,
        76,
        37,
        53,
        9,
        21,
        81,
        31,
        43,
        45,
        23,
        34,
        53,
        30,
        16,
        29,
        83,
        9,
        51,
        6,
        66,
        7,
        18,
        41,
        22,
        40,
        19,
        15,
        15,
        24
      ],
      "2020-08": [
        110,
        60,
        21,
        45,
        2,
        41,
        18,
        22,
        10,
        28,
        39,
        28,
        33,
        27,
        13,
        24,
        22,
        20,
        7,
        15,
        37,
        12,
        33,
        6,
        31,
        5,
        17,
        35,
        11,
        40,
        18,
        7,
        6,
        9
      ],
      "2020-09": [
        131,
        51,
        17,
        52,
        2,
        68,
        21,
        39,
        12,
        14,
        55,
        22,
        39,
        26,
        11,
        26,
        26,
        26,
        13,
        12,
        53,
        5,
        35,
        5,
        30,
        5,
        17,
        46,
        12,
        33,
        13,
        6,
        14,
        9
      ],
      "2020-10": [
        118,
        44,
        30,
        41,
        4,
        45,
        22,
        48,
        6,
        30,
        55,
        10,
        32,
        31,
        25,
        16,
        20,
        42,
        16,
        16,
        38,
        17,
        30,
        1,
        50,
        11,
        22,
        25,
        17,
        9,
        11,
        8,
        2,
        11
      ],
      "2020-11": [
        80,
        27,
        15,
        17,
        8,
        17,
        10,
        29,
        3,
        12,
        13,
        6,
        27,
        16,
        10,
        11,
        10,
        24,
        9,
        7,
        13,
        9,
        19,
        2,
        27,
        3,
        12,
        22,
        8,
        4,
        2,
        5,
        1,
        5
      ],
      "2020-12": [
        62,
        16,
        16,
        25,
        5,
        12,
        8,
        22,
        3,
        18,
        16,
        11,
        15,
        8,
        13,
        6,
        20,
        16,
        13,
        11,
        16,
        12,
        18,
        1,
        22,
        11,
        11,
        23,
        10,
        9,
        4,
        4,
        2,
        15
      ],
      "2021-01": [
        51,
        8,
        10,
        31,
        4,
        14,
        7,
        14,
        3,
        13,
        15,
        4,
        9,
        9,
        9,
        5,
        12,
        16,
        5,
        7,
        11,
        8,
        12,
        0,
        20,
        4,
        5,
        10,
        5,
        3,
        1,
        7,
        4,
        3
      ],
      "2021-02": [
        92,
        29,
        19,
        26,
        3,
        22,
        13,
        44,
        3,
        12,
        29,
        7,
        14,
        18,
        11,
        18,
        20,
        26,
        9,
        13,
        21,
        18,
        27,
        1,
        31,
        7,
        9,
        19,
        8,
        5,
        3,
        8,
        2,
        11
      ],
      "2021-03": [
        70,
        24,
        26,
        16,
        2,
        20,
        12,
        20,
        3,
        10,
        14,
        7,
        19,
        13,
        10,
        10,
        10,
        19,
        8,
        12,
        12,
        9,
        15,
        0,
        19,
        5,
        6,
        15,
        8,
        6,
        4,
        6,
        0,
        8
      ],
      "2021-04": [
        44,
        12,
        14,
        21,
        3,
        9,
        6,
        14,
        3,
        10,
        21,
        5,
        14,
        9,
        9,
        7,
        8,
        18,
        6,
        9,
        9,
        6,
        12,
        1,
        16,
        6,
        10,
        13,
        8,
        6,
        4,
        3,
        4,
        10
      ],
      "2021-05": [
        78,
        12,
        12,
        12,
        1,
        17,
        11,
        26,
        2,
        16,
        15,
        4,
        12,
        11,
        12,
        3,
        11,
        27,
        8,
        9,
        13,
        8,
        9,
        1,
        32,
        5,
        7,
        18,
        9,
        9,
        3,
        3,
        2,
        6
      ],
      "2021-06": [
        102,
        33,
        23,
        30,
        2,
        30,
        21,
        39,
        3,
        24,
        22,
        17,
        24,
        24,
        11,
        18,
        31,
        40,
        13,
        10,
        31,
        14,
        22,
        1,
        38,
        11,
        20,
        24,
        7,
        8,
        2,
        15,
        0,
        19
      ],
      "2021-07": [
        49,
        17,
        17,
        23,
        2,
        19,
        9,
        23,
        3,
        10,
        10,
        2,
        10,
        14,
        13,
        7,
        14,
        26,
        3,
        11,
        9,
        9,
        19,
        2,
        29,
        6,
        10,
        12,
        4,
        5,
        2,
        6,
        0,
        7
      ],
      "2021-08": [
        25,
        8,
        12,
        9,
        2,
        5,
        8,
        11,
        0,
        8,
        6,
        2,
        7,
        4,
        11,
        4,
        7,
        10,
        1,
        3,
        4,
        7,
        11,
        0,
        12,
        4,
        2,
        9,
        8,
        2,
        2,
        8,
        3,
        3
      ],
      "2021-09": [
        44,
        16,
        17,
        23,
        3,
        12,
        8,
        17,
        2,
        15,
        7,
        7,
        11,
        7,
        7,
        3,
        7,
        18,
        10,
        4,
        11,
        10,
        9,
        0,
        11,
        3,
        9,
        12,
        8,
        7,
        0,
        5,
        3,
        9
      ],
      "2021-10": [
        72,
        29,
        22,
        28,
        3,
        28,
        16,
        41,
        6,
        14,
        19,
        10,
        22,
        18,
        19,
        17,
        14,
        26,
        11,
        11,
        20,
        16,
        11,
        1,
        30,
        9,
        13,
        18,
        9,
        13,
        3,
        7,
        1,
        10
      ],
      "2021-11": [
        57,
        11,
        28,
        23,
        1,
        22,
        13,
        25,
        2,
        6,
        19,
        6,
        9,
        8,
        14,
        13,
        18,
        31,
        6,
        7,
        8,
        7,
        15,
        2,
        18,
        8,
        11,
        12,
        9,
        5,
        2,
        7,
        0,
        7
      ],
      "2021-12": [
        52,
        20,
        18,
        16,
        2,
        18,
        11,
        21,
        2,
        4,
        15,
        4,
        7,
        6,
        6,
        3,
        12,
        23,
        4,
        5,
        10,
        9,
        22,
        3,
        17,
        5,
        8,
        14,
        6,
        2,
        1,
        3,
        1,
        4
      ],
      "2022-01": [
        42,
        18,
        14,
        14,
        2,
        10,
        8,
        18,
        4,
        10,
        14,
        8,
        13,
        8,
        11,
        3,
        8,
        10,
        6,
        14,
        9,
        9,
        8,
        1,
        17,
        5,
        6,
        12,
        4,
        5,
        3,
        2,
        0,
        7
      ],
      "2022-02": [
        78,
        31,
        24,
        26,
        4,
        19,
        19,
        44,
        5,
        14,
        12,
        13,
        7,
        16,
        14,
        18,
        12,
        20,
        16,
        7,
        9,
        9,
        18,
        1,
        27,
        14,
        8,
        18,
        4,
        8,
        4,
        2,
        2,
        10
      ],
      "2022-03": [
        56,
        25,
        27,
        20,
        4,
        16,
        12,
        13,
        5,
        13,
        15,
        1,
        18,
        10,
        11,
        8,
        11,
        25,
        6,
        7,
        16,
        15,
        18,
        0,
        25,
        8,
        8,
        26,
        6,
        4,
        4,
        5,
        3,
        8
      ],
      "2022-04": [
        32,
        8,
        14,
        20,
        2,
        14,
        8,
        9,
        0,
        11,
        12,
        7,
        13,
        7,
        8,
        6,
        14,
        21,
        0,
        4,
        7,
        6,
        10,
        0,
        12,
        4,
        10,
        10,
        4,
        4,
        3,
        5,
        1,
        4
      ],
      "2022-05": [
        83,
        15,
        24,
        19,
        3,
        12,
        13,
        42,
        1,
        10,
        15,
        5,
        10,
        11,
        15,
        10,
        12,
        34,
        11,
        15,
        13,
        12,
        18,
        0,
        27,
        12,
        8,
        15,
        9,
        7,
        1,
        4,
        2,
        8
      ],
      "2022-06": [
        86,
        36,
        33,
        13,
        2,
        23,
        26,
        49,
        3,
        11,
        24,
        15,
        19,
        12,
        14,
        17,
        25,
        29,
        9,
        15,
        17,
        13,
        27,
        1,
        30,
        12,
        14,
        15,
        9,
        6,
        2,
        7,
        2,
        7
      ],
      "2022-07": [
        40,
        13,
        14,
        12,
        1,
        13,
        8,
        16,
        4,
        9,
        8,
        6,
        9,
        5,
        7,
        13,
        13,
        10,
        9,
        9,
        7,
        8,
        12,
        0,
        23,
        5,
        4,
        9,
        13,
        7,
        2,
        6,
        2,
        6
      ],
      "2022-08": [
        43,
        14,
        15,
        16,
        2,
        13,
        13,
        10,
        0,
        9,
        13,
        2,
        15,
        4,
        6,
        5,
        14,
        17,
        2,
        4,
        8,
        1,
        14,
        3,
        12,
        2,
        9,
        12,
        6,
        5,
        1,
        6,
        4,
        4
      ],
      "2022-09": [
        37,
        16,
        20,
        20,
        2,
        17,
        16,
        15,
        2,
        12,
        3,
        6,
        13,
        11,
        16,
        10,
        9,
        23,
        6,
        8,
        7,
        9,
        19,
        1,
        22,
        5,
        6,
        16,
        6,
        3,
        2,
        2,
        0,
        4
      ],
      "2022-10": [
        74,
        22,
        20,
        28,
        8,
        24,
        10,
        38,
        9,
        8,
        16,
        14,
        14,
        14,
        15,
        20,
        22,
        19,
        9,
        30,
        15,
        13,
        14,
        2,
        32,
        14,
        7,
        18,
        7,
        5,
        5,
        10,
        3,
        8
      ],
      "2022-11": [
        67,
        20,
        25,
        6,
        3,
        19,
        12,
        24,
        2,
        14,
        13,
        7,
        13,
        8,
        11,
        7,
        12,
        26,
        13,
        9,
        10,
        6,
        11,
        2,
        18,
        11,
        7,
        17,
        8,
        8,
        2,
        3,
        1,
        8
      ],
      "2022-12": [
        39,
        16,
        15,
        14,
        1,
        8,
        10,
        17,
        5,
        12,
        7,
        4,
        5,
        2,
        6,
        8,
        14,
        14,
        5,
        4,
        9,
        5,
        16,
        1,
        16,
        5,
        5,
        9,
        3,
        4,
        3,
        4,
        1,
        6
      ],
      "2023-01": [
        50,
        26,
        23,
        17,
        1,
        14,
        14,
        21,
        0,
        7,
        6,
        7,
        6,
        7,
        6,
        12,
        14,
        14,
        7,
        11,
        8,
        11,
        14,
        0,
        16,
        3,
        9,
        6,
        5,
        7,
        2,
        3,
        2,
        7
      ],
      "2023-02": [
        79,
        26,
        20,
        17,
        7,
        15,
        22,
        39,
        6,
        17,
        20,
        8,
        14,
        13,
        15,
        18,
        20,
        29,
        12,
        17,
        17,
        12,
        9,
        1,
        28,
        10,
        5,
        13,
        8,
        10,
        1,
        6,
        1,
        11
      ],
      "2023-03": [
        65,
        15,
        12,
        20,
        1,
        9,
        19,
        33,
        4,
        16,
        12,
        4,
        11,
        10,
        15,
        8,
        13,
        12,
        3,
        16,
        11,
        10,
        16,
        2,
        19,
        10,
        14,
        15,
        7,
        6,
        2,
        5,
        1,
        4
      ],
      "2023-04": [
        57,
        12,
        17,
        16,
        3,
        4,
        12,
        15,
        4,
        8,
        11,
        7,
        6,
        7,
        11,
        8,
        12,
        18,
        3,
        3,
        5,
        12,
        12,
        1,
        16,
        6,
        6,
        13,
        3,
        3,
        0,
        3,
        2,
        3
      ],
      "2023-05": [
        85,
        28,
        25,
        23,
        2,
        24,
        29,
        27,
        10,
        26,
        14,
        10,
        16,
        10,
        27,
        9,
        26,
        25,
        10,
        10,
        16,
        9,
        25,
        1,
        31,
        8,
        9,
        19,
        6,
        5,
        1,
        11,
        2,
        5
      ],
      "2023-06": [
        94,
        26,
        44,
        22,
        5,
        23,
        25,
        32,
        11,
        13,
        30,
        6,
        10,
        13,
        8,
        19,
        17,
        23,
        9,
        17,
        17,
        14,
        16,
        0,
        31,
        13,
        7,
        12,
        8,
        6,
        5,
        5,
        6,
        5
      ],
      "2023-07": [
        52,
        14,
        19,
        20,
        2,
        13,
        12,
        23,
        8,
        6,
        9,
        3,
        9,
        7,
        13,
        7,
        21,
        16,
        3,
        16,
        10,
        14,
        13,
        1,
        17,
        7,
        10,
        12,
        10,
        4,
        0,
        5,
        6,
        3
      ],
      "2023-08": [
        43,
        4,
        16,
        19,
        1,
        7,
        5,
        14,
        7,
        7,
        4,
        4,
        11,
        5,
        7,
        4,
        8,
        14,
        2,
        4,
        11,
        9,
        14,
        0,
        15,
        4,
        7,
        15,
        2,
        9,
        3,
        2,
        3,
        0
      ],
      "2023-09": [
        59,
        13,
        10,
        16,
        2,
        12,
        15,
        15,
        9,
        8,
        12,
        5,
        7,
        4,
        9,
        9,
        15,
        18,
        5,
        7,
        7,
        9,
        11,
        1,
        26,
        3,
        12,
        9,
        4,
        6,
        8,
        7,
        0,
        2
      ],
      "2023-10": [
        83,
        32,
        38,
        17,
        4,
        22,
        33,
        32,
        18,
        18,
        23,
        6,
        9,
        10,
        20,
        21,
        21,
        36,
        17,
        17,
        23,
        27,
        25,
        5,
        35,
        10,
        6,
        16,
        9,
        3,
        2,
        7,
        2,
        9
      ],
      "2023-11": [
        61,
        14,
        14,
        21,
        2,
        17,
        23,
        18,
        6,
        12,
        1,
        4,
        14,
        8,
        11,
        7,
        11,
        26,
        6,
        10,
        10,
        14,
        15,
        1,
        17,
        6,
        7,
        17,
        7,
        2,
        2,
        9,
        2,
        5
      ],
      "2023-12": [
        66,
        16,
        17,
        24,
        1,
        8,
        13,
        28,
        11,
        10,
        8,
        8,
        8,
        11,
        7,
        11,
        18,
        16,
        7,
        15,
        14,
        5,
        22,
        1,
        21,
        8,
        6,
        8,
        5,
        9,
        1,
        6,
        1,
        8
      ],
      "2024-01": [
        47,
        13,
        25,
        12,
        2,
        9,
        8,
        13,
        2,
        10,
        15,
        3,
        6,
        5,
        12,
        9,
        9,
        13,
        7,
        8,
        4,
        12,
        14,
        0,
        9,
        3,
        7,
        9,
        8,
        2,
        1,
        7,
        1,
        3
      ],
      "2024-02": [
        109,
        30,
        32,
        22,
        8,
        26,
        28,
        36,
        17,
        24,
        11,
        8,
        16,
        12,
        23,
        15,
        30,
        34,
        10,
        24,
        11,
        14,
        21,
        0,
        27,
        13,
        9,
        25,
        12,
        8,
        1,
        3,
        2,
        11
      ],
      "2024-03": [
        46,
        27,
        24,
        16,
        1,
        9,
        14,
        23,
        7,
        9,
        12,
        3,
        15,
        9,
        14,
        15,
        24,
        24,
        11,
        19,
        12,
        13,
        10,
        0,
        24,
        6,
        9,
        10,
        7,
        4,
        1,
        2,
        0,
        5
      ],
      "2024-04": [
        40,
        10,
        15,
        18,
        2,
        13,
        15,
        13,
        8,
        6,
        11,
        4,
        11,
        4,
        6,
        9,
        15,
        12,
        7,
        14,
        6,
        6,
        18,
        1,
        21,
        6,
        6,
        3,
        4,
        3,
        1,
        4,
        0,
        5
      ],
      "2024-05": [
        88,
        23,
        34,
        22,
        2,
        17,
        27,
        41,
        20,
        15,
        19,
        10,
        14,
        10,
        24,
        14,
        19,
        16,
        13,
        34,
        14,
        18,
        20,
        1,
        43,
        10,
        9,
        15,
        8,
        8,
        0,
        9,
        0,
        7
      ],
      "2024-06": [
        84,
        23,
        32,
        29,
        7,
        21,
        27,
        24,
        31,
        16,
        18,
        4,
        13,
        10,
        23,
        18,
        19,
        20,
        6,
        22,
        14,
        14,
        15,
        0,
        34,
        10,
        14,
        14,
        14,
        4,
        2,
        7,
        1,
        5
      ],
      "2024-07": [
        50,
        21,
        20,
        17,
        2,
        10,
        15,
        15,
        12,
        10,
        9,
        9,
        9,
        6,
        13,
        9,
        12,
        22,
        9,
        11,
        11,
        9,
        8,
        0,
        19,
        6,
        5,
        19,
        7,
        6,
        3,
        4,
        3,
        3
      ],
      "2024-08": [
        48,
        16,
        19,
        25,
        5,
        10,
        8,
        10,
        11,
        9,
        6,
        3,
        9,
        4,
        7,
        11,
        13,
        18,
        7,
        11,
        8,
        7,
        16,
        1,
        22,
        1,
        6,
        11,
        4,
        7,
        1,
        6,
        1,
        6
      ],
      "2024-09": [
        67,
        13,
        25,
        23,
        3,
        12,
        21,
        14,
        16,
        6,
        12,
        4,
        5,
        4,
        15,
        6,
        14,
        24,
        6,
        15,
        10,
        7,
        15,
        0,
        22,
        4,
        8,
        12,
        5,
        4,
        1,
        8,
        2,
        9
      ],
      "2024-10": [
        117,
        30,
        40,
        43,
        7,
        35,
        52,
        40,
        36,
        31,
        36,
        15,
        15,
        12,
        24,
        12,
        31,
        39,
        10,
        14,
        20,
        15,
        22,
        3,
        27,
        16,
        21,
        14,
        16,
        6,
        1,
        1,
        2,
        5
      ],
      "2024-11": [
        66,
        22,
        41,
        26,
        2,
        20,
        19,
        17,
        19,
        12,
        7,
        10,
        12,
        12,
        13,
        8,
        19,
        25,
        10,
        22,
        8,
        9,
        20,
        1,
        10,
        8,
        3,
        14,
        6,
        7,
        1,
        3,
        2,
        11
      ],
      "2024-12": [
        57,
        14,
        28,
        28,
        4,
        24,
        19,
        19,
        20,
        12,
        12,
        7,
        7,
        7,
        21,
        9,
        8,
        26,
        9,
        14,
        5,
        11,
        17,
        3,
        18,
        7,
        12,
        19,
        10,
        5,
        0,
        6,
        0,
        9
      ],
      "2025-01": [
        54,
        17,
        23,
        24,
        5,
        14,
        21,
        28,
        12,
        6,
        10,
        10,
        10,
        5,
        10,
        10,
        12,
        19,
        6,
        23,
        12,
        12,
        14,
        0,
        18,
        9,
        9,
        16,
        4,
        3,
        2,
        8,
        1,
        2
      ],
      "2025-02": [
        95,
        26,
        34,
        36,
        4,
        21,
        41,
        33,
        36,
        17,
        20,
        4,
        15,
        11,
        18,
        19,
        27,
        35,
        10,
        35,
        13,
        13,
        17,
        1,
        25,
        15,
        13,
        18,
        5,
        5,
        2,
        9,
        0,
        9
      ],
      "2025-03": [
        57,
        18,
        26,
        14,
        2,
        17,
        29,
        25,
        16,
        13,
        10,
        8,
        12,
        7,
        13,
        13,
        20,
        24,
        11,
        23,
        11,
        9,
        10,
        0,
        25,
        7,
        8,
        17,
        7,
        4,
        0,
        5,
        0,
        7
      ],
      "2025-04": [
        52,
        18,
        19,
        17,
        3,
        11,
        12,
        6,
        13,
        9,
        9,
        5,
        14,
        8,
        14,
        11,
        16,
        14,
        9,
        15,
        13,
        9,
        14,
        2,
        15,
        6,
        11,
        17,
        4,
        6,
        0,
        4,
        0,
        6
      ],
      "2025-05": [
        93,
        32,
        45,
        36,
        3,
        26,
        41,
        36,
        46,
        17,
        14,
        12,
        13,
        15,
        27,
        13,
        23,
        40,
        10,
        36,
        15,
        24,
        25,
        2,
        45,
        13,
        13,
        22,
        6,
        5,
        0,
        5,
        3,
        10
      ],
      "2025-06": [
        78,
        30,
        39,
        26,
        2,
        18,
        32,
        21,
        37,
        11,
        21,
        8,
        13,
        6,
        18,
        16,
        19,
        27,
        8,
        30,
        14,
        19,
        17,
        2,
        25,
        8,
        8,
        20,
        9,
        4,
        1,
        9,
        3,
        10
      ],
      "2025-07": [
        43,
        23,
        29,
        27,
        4,
        11,
        17,
        13,
        14,
        13,
        8,
        4,
        17,
        5,
        10,
        15,
        19,
        21,
        9,
        29,
        7,
        14,
        12,
        1,
        26,
        10,
        7,
        19,
        5,
        3,
        0,
        9,
        2,
        4
      ],
      "2025-08": [
        46,
        8,
        16,
        14,
        4,
        7,
        19,
        18,
        15,
        7,
        7,
        5,
        9,
        4,
        16,
        11,
        9,
        21,
        9,
        18,
        6,
        6,
        6,
        1,
        11,
        4,
        8,
        11,
        5,
        4,
        2,
        9,
        2,
        4
      ],
      "2025-09": [
        23,
        4,
        9,
        5,
        6,
        8,
        12,
        7,
        7,
        5,
        5,
        2,
        4,
        3,
        5,
        5,
        7,
        7,
        7,
        7,
        3,
        9,
        6,
        0,
        19,
        2,
        3,
        3,
        2,
        1,
        0,
        2,
        0,
        2
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Spectral Evolution and Invariance in Linear-width Neural Networks",
          "year": "2022-11",
          "abstract": "We investigate the spectral properties of linear-width feed-forward neural\nnetworks, where the sample size is asymptotically proportional to network\nwidth. Empirically, we show that the spectra of weight in this high dimensional\nregime are invariant when trained by gradient descent for small constant\nlearning rates; we provide a theoretical justification for this observation and\nprove the invariance of the bulk spectra for both conjugate and neural tangent\nkernels. We demonstrate similar characteristics when training with stochastic\ngradient descent with small learning rates. When the learning rate is large, we\nexhibit the emergence of an outlier whose corresponding eigenvector is aligned\nwith the training data structure. We also show that after adaptive gradient\ntraining, where a lower test error and feature learning emerge, both weight and\nkernel matrices exhibit heavy tail behavior. Simple examples are provided to\nexplain when heavy tails can have better generalizations. We exhibit different\nspectral properties such as invariant bulk, spike, and heavy-tailed\ndistribution from a two-layer neural network using different training\nstrategies, and then correlate them to the feature learning. Analogous\nphenomena also appear when we train conventional neural networks with\nreal-world data. We conclude that monitoring the evolution of the spectra\nduring training is an essential step toward understanding the training dynamics\nand feature learning.",
          "arxiv_id": "2211.06506v2"
        },
        {
          "title": "A Dynamical View on Optimization Algorithms of Overparameterized Neural Networks",
          "year": "2020-10",
          "abstract": "When equipped with efficient optimization algorithms, the over-parameterized\nneural networks have demonstrated high level of performance even though the\nloss function is non-convex and non-smooth. While many works have been focusing\non understanding the loss dynamics by training neural networks with the\ngradient descent (GD), in this work, we consider a broad class of optimization\nalgorithms that are commonly used in practice. For example, we show from a\ndynamical system perspective that the Heavy Ball (HB) method can converge to\nglobal minimum on mean squared error (MSE) at a linear rate (similar to GD);\nhowever, the Nesterov accelerated gradient descent (NAG) may only converges to\nglobal minimum sublinearly.\n  Our results rely on the connection between neural tangent kernel (NTK) and\nfinite over-parameterized neural networks with ReLU activation, which leads to\nanalyzing the limiting ordinary differential equations (ODE) for optimization\nalgorithms. We show that, optimizing the non-convex loss over the weights\ncorresponds to optimizing some strongly convex loss over the prediction error.\nAs a consequence, we can leverage the classical convex optimization theory to\nunderstand the convergence behavior of neural networks. We believe our approach\ncan also be extended to other optimization algorithms and network\narchitectures.",
          "arxiv_id": "2010.13165v2"
        },
        {
          "title": "Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime",
          "year": "2020-06",
          "abstract": "We analyze the convergence of the averaged stochastic gradient descent for\noverparameterized two-layer neural networks for regression problems. It was\nrecently found that a neural tangent kernel (NTK) plays an important role in\nshowing the global convergence of gradient-based methods under the NTK regime,\nwhere the learning dynamics for overparameterized neural networks can be almost\ncharacterized by that for the associated reproducing kernel Hilbert space\n(RKHS). However, there is still room for a convergence rate analysis in the NTK\nregime. In this study, we show that the averaged stochastic gradient descent\ncan achieve the minimax optimal convergence rate, with the global convergence\nguarantee, by exploiting the complexities of the target function and the RKHS\nassociated with the NTK. Moreover, we show that the target function specified\nby the NTK of a ReLU network can be learned at the optimal convergence rate\nthrough a smooth approximation of a ReLU network under certain conditions.",
          "arxiv_id": "2006.12297v2"
        }
      ],
      "1": [
        {
          "title": "Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL",
          "year": "2021-06",
          "abstract": "The success of deep reinforcement learning (DRL) lies in its ability to learn\na representation that is well-suited for the exploration and exploitation task.\nTo understand how the choice of representation can improve the efficiency of\nreinforcement learning (RL), we study representation selection for a class of\nlow-rank Markov Decision Processes (MDPs) where the transition kernel can be\nrepresented in a bilinear form. We propose an efficient algorithm, called\nReLEX, for representation learning in both online and offline RL. Specifically,\nwe show that the online version of ReLEX, called ReLEX-UCB, always performs no\nworse than the state-of-the-art algorithm without representation selection, and\nachieves a strictly better constant regret if the representation function class\nhas a \"coverage\" property over the entire state-action space. For the offline\ncounterpart, ReLEX-LCB, we show that the algorithm can find the optimal policy\nif the representation class can cover the state-action space and achieves\ngap-dependent sample complexity. This is the first result with constant sample\ncomplexity for representation learning in offline RL.",
          "arxiv_id": "2106.11935v2"
        },
        {
          "title": "FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization",
          "year": "2020-10",
          "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm\nwhich enables reinforcement learning (RL) algorithms to quickly adapt to unseen\ntasks without any interactions with the environments, making RL truly practical\nin many real-world applications. This problem is still not fully understood,\nfor which two major challenges need to be addressed. First, offline RL usually\nsuffers from bootstrapping errors of out-of-distribution state-actions which\nleads to divergence of value functions. Second, meta-RL requires efficient and\nrobust task inference learned jointly with control policy. In this work, we\nenforce behavior regularization on learned policy as a general approach to\noffline RL, combined with a deterministic context encoder for efficient task\ninference. We propose a novel negative-power distance metric on bounded context\nembedding space, whose gradients propagation is detached from the Bellman\nbackup. We provide analysis and insight showing that some simple design choices\ncan yield substantial improvements over recent approaches involving meta-RL and\ndistance metric learning. To the best of our knowledge, our method is the first\nmodel-free and end-to-end OMRL algorithm, which is computationally efficient\nand demonstrated to outperform prior algorithms on several meta-RL benchmarks.",
          "arxiv_id": "2010.01112v4"
        },
        {
          "title": "Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs",
          "year": "2024-08",
          "abstract": "Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.",
          "arxiv_id": "2408.04526v1"
        }
      ],
      "2": [
        {
          "title": "Interpretable Neural Causal Models with TRAM-DAGs",
          "year": "2025-03",
          "abstract": "The ultimate goal of most scientific studies is to understand the underlying\ncausal mechanism between the involved variables. Structural causal models\n(SCMs) are widely used to represent such causal mechanisms. Given an SCM,\ncausal queries on all three levels of Pearl's causal hierarchy can be answered:\n$L_1$ observational, $L_2$ interventional, and $L_3$ counterfactual. An\nessential aspect of modeling the SCM is to model the dependency of each\nvariable on its causal parents. Traditionally this is done by parametric\nstatistical models, such as linear or logistic regression models. This allows\nto handle all kinds of data types and fit interpretable models but bears the\nrisk of introducing a bias. More recently neural causal models came up using\nneural networks (NNs) to model the causal relationships, allowing the\nestimation of nearly any underlying functional form without bias. However,\ncurrent neural causal models are generally restricted to continuous variables\nand do not yield an interpretable form of the causal relationships.\nTransformation models range from simple statistical regressions to complex\nnetworks and can handle continuous, ordinal, and binary data. Here, we propose\nto use TRAMs to model the functional relationships in SCMs allowing us to\nbridge the gap between interpretability and flexibility in causal modeling. We\ncall this method TRAM-DAG and assume currently that the underlying directed\nacyclic graph is known. For the fully observed case, we benchmark TRAM-DAGs\nagainst state-of-the-art statistical and NN-based causal models. We show that\nTRAM-DAGs are interpretable but also achieve equal or superior performance in\nqueries ranging from $L_1$ to $L_3$ in the causal hierarchy. For the continuous\ncase, TRAM-DAGs allow for counterfactual queries for three common causal\nstructures, including unobserved confounding.",
          "arxiv_id": "2503.16206v1"
        },
        {
          "title": "Dynamic Causal Structure Discovery and Causal Effect Estimation",
          "year": "2025-01",
          "abstract": "To represent the causal relationships between variables, a directed acyclic\ngraph (DAG) is widely utilized in many areas, such as social sciences,\nepidemics, and genetics. Many causal structure learning approaches are\ndeveloped to learn the hidden causal structure utilizing deep-learning\napproaches. However, these approaches have a hidden assumption that the causal\nrelationship remains unchanged over time, which may not hold in real life. In\nthis paper, we develop a new framework to model the dynamic causal graph where\nthe causal relations are allowed to be time-varying. We incorporate the basis\napproximation method into the score-based causal discovery approach to capture\nthe dynamic pattern of the causal graphs. Utilizing the autoregressive model\nstructure, we could capture both contemporaneous and time-lagged causal\nrelationships while allowing them to vary with time. We propose an algorithm\nthat could provide both past-time estimates and future-time predictions on the\ncausal graphs, and conduct simulations to demonstrate the usefulness of the\nproposed method. We also apply the proposed method for the covid-data analysis,\nand provide causal estimates on how policy restriction's effect changes.",
          "arxiv_id": "2501.06534v1"
        },
        {
          "title": "Black Box Causal Inference: Effect Estimation via Meta Prediction",
          "year": "2025-03",
          "abstract": "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
          "arxiv_id": "2503.05985v1"
        }
      ],
      "3": [
        {
          "title": "EasyTime: Time Series Forecasting Made Easy",
          "year": "2024-12",
          "abstract": "Time series forecasting has important applications across diverse domains.\nEasyTime, the system we demonstrate, facilitates easy use of time-series\nforecasting methods by researchers and practitioners alike. First, EasyTime\nenables one-click evaluation, enabling researchers to evaluate new forecasting\nmethods using the suite of diverse time series datasets collected in the\npreexisting time series forecasting benchmark (TFB). This is achieved by\nleveraging TFB's flexible and consistent evaluation pipeline. Second, when\npractitioners must perform forecasting on a new dataset, a nontrivial first\nstep is often to find an appropriate forecasting method. EasyTime provides an\nAutomated Ensemble module that combines the promising forecasting methods to\nyield superior forecasting accuracy compared to individual methods. Third,\nEasyTime offers a natural language Q&A module leveraging large language models.\nGiven a question like \"Which method is best for long term forecasting on time\nseries with strong seasonality?\", EasyTime converts the question into SQL\nqueries on the database of results obtained by TFB and then returns an answer\nin natural language and charts. By demonstrating EasyTime, we intend to show\nhow it is possible to simplify the use of time series forecasting and to offer\nbetter support for the development of new generations of time series\nforecasting methods.",
          "arxiv_id": "2412.17603v1"
        },
        {
          "title": "Optimal Latent Space Forecasting for Large Collections of Short Time Series Using Temporal Matrix Factorization",
          "year": "2021-12",
          "abstract": "In the context of time series forecasting, it is a common practice to\nevaluate multiple methods and choose one of these methods or an ensemble for\nproducing the best forecasts. However, choosing among different ensembles over\nmultiple methods remains a challenging task that undergoes a combinatorial\nexplosion as the number of methods increases. In the context of demand\nforecasting or revenue forecasting, this challenge is further exacerbated by a\nlarge number of time series as well as limited historical data points available\ndue to changing business context. Although deep learning forecasting methods\naim to simultaneously forecast large collections of time series, they become\nchallenging to apply in such scenarios due to the limited history available and\nmight not yield desirable results. We propose a framework for forecasting short\nhigh-dimensional time series data by combining low-rank temporal matrix\nfactorization and optimal model selection on latent time series using\ncross-validation. We demonstrate that forecasting the latent factors leads to\nsignificant performance gains as compared to directly applying different\nuni-variate models on time series. Performance has been validated on a\ntruncated version of the M4 monthly dataset which contains time series data\nfrom multiple domains showing the general applicability of the method.\nMoreover, it is amenable to incorporating the analyst view of the future owing\nto the low number of latent factors which is usually impractical when applying\nforecasting methods directly to high dimensional datasets.",
          "arxiv_id": "2112.08052v1"
        },
        {
          "title": "Dive into Time-Series Anomaly Detection: A Decade Review",
          "year": "2024-12",
          "abstract": "Recent advances in data collection technology, accompanied by the ever-rising\nvolume and velocity of streaming data, underscore the vital need for time\nseries analytics. In this regard, time-series anomaly detection has been an\nimportant activity, entailing various applications in fields such as cyber\nsecurity, financial markets, law enforcement, and health care. While\ntraditional literature on anomaly detection is centered on statistical\nmeasures, the increasing number of machine learning algorithms in recent years\ncall for a structured, general characterization of the research methods for\ntime-series anomaly detection. This survey groups and summarizes anomaly\ndetection existing solutions under a process-centric taxonomy in the time\nseries context. In addition to giving an original categorization of anomaly\ndetection methods, we also perform a meta-analysis of the literature and\noutline general trends in time-series anomaly detection research.",
          "arxiv_id": "2412.20512v1"
        }
      ],
      "4": [
        {
          "title": "Composition of kernel and acquisition functions for High Dimensional Bayesian Optimization",
          "year": "2020-03",
          "abstract": "Bayesian Optimization has become the reference method for the global\noptimization of black box, expensive and possibly noisy functions. Bayesian\nOp-timization learns a probabilistic model about the objective function,\nusually a Gaussian Process, and builds, depending on its mean and variance, an\nacquisition function whose optimizer yields the new evaluation point, leading\nto update the probabilistic surrogate model. Despite its sample efficiency,\nBayesian Optimiza-tion does not scale well with the dimensions of the problem.\nThe optimization of the acquisition function has received less attention\nbecause its computational cost is usually considered negligible compared to\nthat of the evaluation of the objec-tive function. Its efficient optimization\nis often inhibited, particularly in high di-mensional problems, by multiple\nextrema. In this paper we leverage the addition-ality of the objective function\ninto mapping both the kernel and the acquisition function of the Bayesian\nOptimization in lower dimensional subspaces. This ap-proach makes more\nefficient the learning/updating of the probabilistic surrogate model and allows\nan efficient optimization of the acquisition function. Experi-mental results\nare presented for real-life application, that is the control of pumps in urban\nwater distribution systems.",
          "arxiv_id": "2003.04207v1"
        },
        {
          "title": "Using Bayesian deep learning approaches for uncertainty-aware building energy surrogate models",
          "year": "2020-10",
          "abstract": "Fast machine learning-based surrogate models are trained to emulate slow,\nhigh-fidelity engineering simulation models to accelerate engineering design\ntasks. This introduces uncertainty as the surrogate is only an approximation of\nthe original model.\n  Bayesian methods can quantify that uncertainty, and deep learning models\nexist that follow the Bayesian paradigm. These models, namely Bayesian neural\nnetworks and Gaussian process models, enable us to give predictions together\nwith an estimate of the model's uncertainty. As a result we can derive\nuncertainty-aware surrogate models that can automatically suspect unseen design\nsamples that cause large emulation errors. For these samples, the high-fidelity\nmodel can be queried instead. This outlines how the Bayesian paradigm allows us\nto hybridize fast, but approximate, and slow, but accurate models.\n  In this paper, we train two types of Bayesian models, dropout neural networks\nand stochastic variational Gaussian Process models, to emulate a complex high\ndimensional building energy performance simulation problem. The surrogate model\nprocesses 35 building design parameters (inputs) to estimate 12 different\nperformance metrics (outputs). We benchmark both approaches, prove their\naccuracy to be competitive, and show that errors can be reduced by up to 30%\nwhen the 10% of samples with the highest uncertainty are transferred to the\nhigh-fidelity model.",
          "arxiv_id": "2010.03029v1"
        },
        {
          "title": "Simulation Based Bayesian Optimization",
          "year": "2024-01",
          "abstract": "Bayesian Optimization (BO) is a powerful method for optimizing black-box\nfunctions by combining prior knowledge with ongoing function evaluations. BO\nconstructs a probabilistic surrogate model of the objective function given the\ncovariates, which is in turn used to inform the selection of future evaluation\npoints through an acquisition function. For smooth continuous search spaces,\nGaussian Processes (GPs) are commonly used as the surrogate model as they offer\nanalytical access to posterior predictive distributions, thus facilitating the\ncomputation and optimization of acquisition functions. However, in complex\nscenarios involving optimization over categorical or mixed covariate spaces,\nGPs may not be ideal. This paper introduces Simulation Based Bayesian\nOptimization (SBBO) as a novel approach to optimizing acquisition functions\nthat only requires sampling-based access to posterior predictive distributions.\nSBBO allows the use of surrogate probabilistic models tailored for\ncombinatorial spaces with discrete variables. Any Bayesian model in which\nposterior inference is carried out through Markov chain Monte Carlo can be\nselected as the surrogate model in SBBO. We demonstrate empirically the\neffectiveness of SBBO using various choices of surrogate models in applications\ninvolving combinatorial optimization.",
          "arxiv_id": "2401.10811v3"
        }
      ],
      "5": [
        {
          "title": "MathNet: Haar-Like Wavelet Multiresolution-Analysis for Graph Representation and Learning",
          "year": "2020-07",
          "abstract": "Graph Neural Networks (GNNs) have recently caught great attention and\nachieved significant progress in graph-level applications. In this paper, we\npropose a framework for graph neural networks with multiresolution Haar-like\nwavelets, or MathNet, with interrelated convolution and pooling strategies. The\nunderlying method takes graphs in different structures as input and assembles\nconsistent graph representations for readout layers, which then accomplishes\nlabel prediction. To achieve this, the multiresolution graph representations\nare first constructed and fed into graph convolutional layers for processing.\nThe hierarchical graph pooling layers are then involved to downsample graph\nresolution while simultaneously remove redundancy within graph signals. The\nwhole workflow could be formed with a multi-level graph analysis, which not\nonly helps embed the intrinsic topological information of each graph into the\nGNN, but also supports fast computation of forward and adjoint graph\ntransforms. We show by extensive experiments that the proposed framework\nobtains notable accuracy gains on graph classification and regression tasks\nwith performance stability. The proposed MathNet outperforms various existing\nGNN models, especially on big data sets.",
          "arxiv_id": "2007.11202v2"
        },
        {
          "title": "Unsupervised Graph Embedding via Adaptive Graph Learning",
          "year": "2020-03",
          "abstract": "Graph autoencoders (GAEs) are powerful tools in representation learning for\ngraph embedding. However, the performance of GAEs is very dependent on the\nquality of the graph structure, i.e., of the adjacency matrix. In other words,\nGAEs would perform poorly when the adjacency matrix is incomplete or be\ndisturbed. In this paper, two novel unsupervised graph embedding methods,\nunsupervised graph embedding via adaptive graph learning (BAGE) and\nunsupervised graph embedding via variational adaptive graph learning (VBAGE)\nare proposed. The proposed methods expand the application range of GAEs on\ngraph embedding, i.e, on the general datasets without graph structure.\nMeanwhile, the adaptive learning mechanism can initialize the adjacency matrix\nwithout be affected by the parameter. Besides that, the latent representations\nare embedded in the laplacian graph structure to preserve the topology\nstructure of the graph in the vector space. Moreover, the adjacency matrix can\nbe self-learned for better embedding performance when the original graph\nstructure is incomplete. With adaptive learning, the proposed method is much\nmore robust to the graph structure. Experimental studies on several datasets\nvalidate our design and demonstrate that our methods outperform baselines by a\nwide margin in node clustering, node classification, and graph visualization\ntasks.",
          "arxiv_id": "2003.04508v3"
        },
        {
          "title": "CAGNN: Cluster-Aware Graph Neural Networks for Unsupervised Graph Representation Learning",
          "year": "2020-09",
          "abstract": "Unsupervised graph representation learning aims to learn low-dimensional node\nembeddings without supervision while preserving graph topological structures\nand node attributive features. Previous graph neural networks (GNN) require a\nlarge number of labeled nodes, which may not be accessible in real-world graph\ndata. In this paper, we present a novel cluster-aware graph neural network\n(CAGNN) model for unsupervised graph representation learning using\nself-supervised techniques. In CAGNN, we perform clustering on the node\nembeddings and update the model parameters by predicting the cluster\nassignments. Moreover, we observe that graphs often contain inter-class edges,\nwhich mislead the GNN model to aggregate noisy information from neighborhood\nnodes. We further refine the graph topology by strengthening intra-class edges\nand reducing node connections between different classes based on cluster\nlabels, which better preserves cluster structures in the embedding space. We\nconduct comprehensive experiments on two benchmark tasks using real-world\ndatasets. The results demonstrate the superior performance of the proposed\nmodel over existing baseline methods. Notably, our model gains over 7%\nimprovements in terms of accuracy on node clustering over state-of-the-arts.",
          "arxiv_id": "2009.01674v1"
        }
      ],
      "6": [
        {
          "title": "How Much is Enough? A Study on Diffusion Times in Score-based Generative Models",
          "year": "2022-06",
          "abstract": "Score-based diffusion models are a class of generative models whose dynamics\nis described by stochastic differential equations that map noise into data.\nWhile recent works have started to lay down a theoretical foundation for these\nmodels, an analytical understanding of the role of the diffusion time T is\nstill lacking. Current best practice advocates for a large T to ensure that the\nforward dynamics brings the diffusion sufficiently close to a known and simple\nnoise distribution; however, a smaller value of T should be preferred for a\nbetter approximation of the score-matching objective and higher computational\nefficiency. Starting from a variational interpretation of diffusion models, in\nthis work we quantify this trade-off, and suggest a new method to improve\nquality and efficiency of both training and sampling, by adopting smaller\ndiffusion times. Indeed, we show how an auxiliary model can be used to bridge\nthe gap between the ideal and the simulated forward dynamics, followed by a\nstandard reverse diffusion process. Empirical results support our analysis; for\nimage data, our method is competitive w.r.t. the state-of-the-art, according to\nstandard sample quality metrics and log-likelihood.",
          "arxiv_id": "2206.05173v1"
        },
        {
          "title": "A Score-Based Density Formula, with Applications in Diffusion Generative Models",
          "year": "2024-08",
          "abstract": "Score-based generative models (SGMs) have revolutionized the field of\ngenerative modeling, achieving unprecedented success in generating realistic\nand diverse content. Despite empirical advances, the theoretical basis for why\noptimizing the evidence lower bound (ELBO) on the log-likelihood is effective\nfor training diffusion generative models, such as DDPMs, remains largely\nunexplored. In this paper, we address this question by establishing a density\nformula for a continuous-time diffusion process, which can be viewed as the\ncontinuous-time limit of the forward process in an SGM. This formula reveals\nthe connection between the target density and the score function associated\nwith each step of the forward process. Building on this, we demonstrate that\nthe minimizer of the optimization objective for training DDPMs nearly coincides\nwith that of the true objective, providing a theoretical foundation for\noptimizing DDPMs using the ELBO. Furthermore, we offer new insights into the\nrole of score-matching regularization in training GANs, the use of ELBO in\ndiffusion classifiers, and the recently proposed diffusion loss.",
          "arxiv_id": "2408.16765v1"
        },
        {
          "title": "DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks",
          "year": "2023-07",
          "abstract": "Generative models can be categorized into two types: explicit generative\nmodels that define explicit density forms and allow exact likelihood inference,\nsuch as score-based diffusion models (SDMs) and normalizing flows; implicit\ngenerative models that directly learn a transformation from the prior to the\ndata distribution, such as generative adversarial nets (GANs). While these two\ntypes of models have shown great success, they suffer from respective\nlimitations that hinder them from achieving fast sampling and high sample\nquality simultaneously. In this paper, we propose a unified theoretic framework\nfor SDMs and GANs. We shown that: i) the learning dynamics of both SDMs and\nGANs can be described as a novel SDE named Discriminator Denoising Diffusion\nFlow (DiffFlow) where the drift can be determined by some weighted combinations\nof scores of the real data and the generated data; ii) By adjusting the\nrelative weights between different score terms, we can obtain a smooth\ntransition between SDMs and GANs while the marginal distribution of the SDE\nremains invariant to the change of the weights; iii) we prove the asymptotic\noptimality and maximal likelihood training scheme of the DiffFlow dynamics; iv)\nunder our unified theoretic framework, we introduce several instantiations of\nthe DiffFLow that provide new algorithms beyond GANs and SDMs with exact\nlikelihood inference and have potential to achieve flexible trade-off between\nhigh sample quality and fast sampling speed.",
          "arxiv_id": "2307.02159v1"
        }
      ],
      "7": [
        {
          "title": "Asymptotically Optimal Bandits under Weighted Information",
          "year": "2021-05",
          "abstract": "We study the problem of regret minimization in a multi-armed bandit setup\nwhere the agent is allowed to play multiple arms at each round by spreading the\nresources usually allocated to only one arm. At each iteration the agent\nselects a normalized power profile and receives a Gaussian vector as outcome,\nwhere the unknown variance of each sample is inversely proportional to the\npower allocated to that arm. The reward corresponds to a linear combination of\nthe power profile and the outcomes, resembling a linear bandit. By spreading\nthe power, the agent can choose to collect information much faster than in a\ntraditional multi-armed bandit at the price of reducing the accuracy of the\nsamples. This setup is fundamentally different from that of a linear bandit --\nthe regret is known to scale as $\\Theta(\\sqrt{T})$ for linear bandits, while in\nthis setup the agent receives a much more detailed feedback, for which we\nderive a tight $\\log(T)$ problem-dependent lower-bound. We propose a\nThompson-Sampling-based strategy, called Weighted Thompson Sampling (\\WTS),\nthat designs the power profile as its posterior belief of each arm being the\nbest arm, and show that its upper bound matches the derived logarithmic lower\nbound. Finally, we apply this strategy to a problem of control and system\nidentification, where the goal is to estimate the maximum gain (also called\n$\\mathcal{H}_\\infty$-norm) of a linear dynamical system based on batches of\ninput-output samples.",
          "arxiv_id": "2105.14114v1"
        },
        {
          "title": "Stochastic Bandits with Linear Constraints",
          "year": "2020-06",
          "abstract": "We study a constrained contextual linear bandit setting, where the goal of\nthe agent is to produce a sequence of policies, whose expected cumulative\nreward over the course of $T$ rounds is maximum, and each has an expected cost\nbelow a certain threshold $\\tau$. We propose an upper-confidence bound\nalgorithm for this problem, called optimistic pessimistic linear bandit (OPLB),\nand prove an $\\widetilde{\\mathcal{O}}(\\frac{d\\sqrt{T}}{\\tau-c_0})$ bound on its\n$T$-round regret, where the denominator is the difference between the\nconstraint threshold and the cost of a known feasible action. We further\nspecialize our results to multi-armed bandits and propose a computationally\nefficient algorithm for this setting. We prove a regret bound of\n$\\widetilde{\\mathcal{O}}(\\frac{\\sqrt{KT}}{\\tau - c_0})$ for this algorithm in\n$K$-armed bandits, which is a $\\sqrt{K}$ improvement over the regret bound we\nobtain by simply casting multi-armed bandits as an instance of contextual\nlinear bandits and using the regret bound of OPLB. We also prove a lower-bound\nfor the problem studied in the paper and provide simulations to validate our\ntheoretical results.",
          "arxiv_id": "2006.10185v1"
        },
        {
          "title": "Contexts can be Cheap: Solving Stochastic Contextual Bandits with Linear Bandit Algorithms",
          "year": "2022-11",
          "abstract": "In this paper, we address the stochastic contextual linear bandit problem,\nwhere a decision maker is provided a context (a random set of actions drawn\nfrom a distribution). The expected reward of each action is specified by the\ninner product of the action and an unknown parameter. The goal is to design an\nalgorithm that learns to play as close as possible to the unknown optimal\npolicy after a number of action plays. This problem is considered more\nchallenging than the linear bandit problem, which can be viewed as a contextual\nbandit problem with a \\emph{fixed} context. Surprisingly, in this paper, we\nshow that the stochastic contextual problem can be solved as if it is a linear\nbandit problem. In particular, we establish a novel reduction framework that\nconverts every stochastic contextual linear bandit instance to a linear bandit\ninstance, when the context distribution is known. When the context distribution\nis unknown, we establish an algorithm that reduces the stochastic contextual\ninstance to a sequence of linear bandit instances with small misspecifications\nand achieves nearly the same worst-case regret bound as the algorithm that\nsolves the misspecified linear bandit instances.\n  As a consequence, our results imply a $O(d\\sqrt{T\\log T})$ high-probability\nregret bound for contextual linear bandits, making progress in resolving an\nopen problem in (Li et al., 2019), (Li et al., 2021).\n  Our reduction framework opens up a new way to approach stochastic contextual\nlinear bandit problems, and enables improved regret bounds in a number of\ninstances including the batch setting, contextual bandits with\nmisspecifications, contextual bandits with sparse unknown parameters, and\ncontextual bandits with adversarial corruption.",
          "arxiv_id": "2211.05632v2"
        }
      ],
      "8": [
        {
          "title": "Investigating the Impact of Model Complexity in Large Language Models",
          "year": "2024-10",
          "abstract": "Large Language Models (LLMs) based on the pre-trained fine-tuning paradigm\nhave become pivotal in solving natural language processing tasks, consistently\nachieving state-of-the-art performance. Nevertheless, the theoretical\nunderstanding of how model complexity influences fine-tuning performance\nremains challenging and has not been well explored yet. In this paper, we focus\non autoregressive LLMs and propose to employ Hidden Markov Models (HMMs) to\nmodel them. Based on the HMM modeling, we investigate the relationship between\nmodel complexity and the generalization capability in downstream tasks.\nSpecifically, we consider a popular tuning paradigm for downstream tasks, head\ntuning, where all pre-trained parameters are frozen and only individual heads\nare trained atop pre-trained LLMs. Our theoretical analysis reveals that the\nrisk initially increases and then decreases with rising model complexity,\nshowcasing a \"double descent\" phenomenon. In this case, the initial \"descent\"\nis degenerate, signifying that the \"sweet spot\" where bias and variance are\nbalanced occurs when the model size is zero. Obtaining the presented in this\nstudy conclusion confronts several challenges, primarily revolving around\neffectively modeling autoregressive LLMs and downstream tasks, as well as\nconducting a comprehensive risk analysis for multivariate regression. Our\nresearch is substantiated by experiments conducted on data generated from HMMs,\nwhich provided empirical support and alignment with our theoretical insights.",
          "arxiv_id": "2410.00699v1"
        },
        {
          "title": "Deep de Finetti: Recovering Topic Distributions from Large Language Models",
          "year": "2023-12",
          "abstract": "Large language models (LLMs) can produce long, coherent passages of text,\nsuggesting that LLMs, although trained on next-word prediction, must represent\nthe latent structure that characterizes a document. Prior work has found that\ninternal representations of LLMs encode one aspect of latent structure, namely\nsyntax; here we investigate a complementary aspect, namely the document's topic\nstructure. We motivate the hypothesis that LLMs capture topic structure by\nconnecting LLM optimization to implicit Bayesian inference. De Finetti's\ntheorem shows that exchangeable probability distributions can be represented as\na mixture with respect to a latent generating distribution. Although text is\nnot exchangeable at the level of syntax, exchangeability is a reasonable\nstarting assumption for topic structure. We thus hypothesize that predicting\nthe next token in text will lead LLMs to recover latent topic distributions. We\nexamine this hypothesis using Latent Dirichlet Allocation (LDA), an\nexchangeable probabilistic topic model, as a target, and we show that the\nrepresentations formed by LLMs encode both the topics used to generate\nsynthetic data and those used to explain natural corpus data.",
          "arxiv_id": "2312.14226v1"
        },
        {
          "title": "Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization",
          "year": "2025-02",
          "abstract": "Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets.",
          "arxiv_id": "2502.17024v1"
        }
      ],
      "9": [
        {
          "title": "Projected Robust PCA with Application to Smooth Image Recovery",
          "year": "2020-07",
          "abstract": "Most high-dimensional matrix recovery problems are studied under the\nassumption that the target matrix has certain intrinsic structures. For image\ndata related matrix recovery problems, approximate low-rankness and smoothness\nare the two most commonly imposed structures. For approximately low-rank matrix\nrecovery, the robust principal component analysis (PCA) is well-studied and\nproved to be effective. For smooth matrix problem, 2d fused Lasso and other\ntotal variation based approaches have played a fundamental role. Although both\nlow-rankness and smoothness are key assumptions for image data analysis, the\ntwo lines of research, however, have very limited interaction. Motivated by\ntaking advantage of both features, we in this paper develop a framework named\nprojected robust PCA (PRPCA), under which the low-rank matrices are projected\nonto a space of smooth matrices. Consequently, a large class of image matrices\ncan be decomposed as a low-rank and smooth component plus a sparse component. A\nkey advantage of this decomposition is that the dimension of the core low-rank\ncomponent can be significantly reduced. Consequently, our framework is able to\naddress a problematic bottleneck of many low-rank matrix problems: singular\nvalue decomposition (SVD) on large matrices. Theoretically, we provide explicit\nstatistical recovery guarantees of PRPCA and include classical robust PCA as a\nspecial case.",
          "arxiv_id": "2009.05478v2"
        },
        {
          "title": "Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent",
          "year": "2020-05",
          "abstract": "Low-rank matrix estimation is a canonical problem that finds numerous\napplications in signal processing, machine learning and imaging science. A\npopular approach in practice is to factorize the matrix into two compact\nlow-rank factors, and then optimize these factors directly via simple iterative\nmethods such as gradient descent and alternating minimization. Despite\nnonconvexity, recent literatures have shown that these simple heuristics in\nfact achieve linear convergence when initialized properly for a growing number\nof problems of interest. However, upon closer examination, existing approaches\ncan still be computationally expensive especially for ill-conditioned matrices:\nthe convergence rate of gradient descent depends linearly on the condition\nnumber of the low-rank matrix, while the per-iteration cost of alternating\nminimization is often prohibitive for large matrices. The goal of this paper is\nto set forth a competitive algorithmic approach dubbed Scaled Gradient Descent\n(ScaledGD) which can be viewed as pre-conditioned or diagonally-scaled gradient\ndescent, where the pre-conditioners are adaptive and iteration-varying with a\nminimal computational overhead. With tailored variants for low-rank matrix\nsensing, robust principal component analysis and matrix completion, we\ntheoretically show that ScaledGD achieves the best of both worlds: it converges\nlinearly at a rate independent of the condition number of the low-rank matrix\nsimilar as alternating minimization, while maintaining the low per-iteration\ncost of gradient descent. Our analysis is also applicable to general loss\nfunctions that are restricted strongly convex and smooth over low-rank\nmatrices. To the best of our knowledge, ScaledGD is the first algorithm that\nprovably has such properties over a wide range of low-rank matrix estimation\ntasks.",
          "arxiv_id": "2005.08898v4"
        },
        {
          "title": "Robust Low-rank Matrix Completion via an Alternating Manifold Proximal Gradient Continuation Method",
          "year": "2020-08",
          "abstract": "Robust low-rank matrix completion (RMC), or robust principal component\nanalysis with partially observed data, has been studied extensively for\ncomputer vision, signal processing and machine learning applications. This\nproblem aims to decompose a partially observed matrix into the superposition of\na low-rank matrix and a sparse matrix, where the sparse matrix captures the\ngrossly corrupted entries of the matrix. A widely used approach to tackle RMC\nis to consider a convex formulation, which minimizes the nuclear norm of the\nlow-rank matrix (to promote low-rankness) and the l1 norm of the sparse matrix\n(to promote sparsity). In this paper, motivated by some recent works on\nlow-rank matrix completion and Riemannian optimization, we formulate this\nproblem as a nonsmooth Riemannian optimization problem over Grassmann manifold.\nThis new formulation is scalable because the low-rank matrix is factorized to\nthe multiplication of two much smaller matrices. We then propose an alternating\nmanifold proximal gradient continuation (AManPGC) method to solve the proposed\nnew formulation. The convergence rate of the proposed algorithm is rigorously\nanalyzed. Numerical results on both synthetic data and real data on background\nextraction from surveillance videos are reported to demonstrate the advantages\nof the proposed new formulation and algorithm over several popular existing\napproaches.",
          "arxiv_id": "2008.07740v1"
        }
      ],
      "10": [
        {
          "title": "Understanding Catastrophic Overfitting in Single-step Adversarial Training",
          "year": "2020-10",
          "abstract": "Although fast adversarial training has demonstrated both robustness and\nefficiency, the problem of \"catastrophic overfitting\" has been observed. This\nis a phenomenon in which, during single-step adversarial training, the robust\naccuracy against projected gradient descent (PGD) suddenly decreases to 0%\nafter a few epochs, whereas the robust accuracy against fast gradient sign\nmethod (FGSM) increases to 100%. In this paper, we demonstrate that\ncatastrophic overfitting is very closely related to the characteristic of\nsingle-step adversarial training which uses only adversarial examples with the\nmaximum perturbation, and not all adversarial examples in the adversarial\ndirection, which leads to decision boundary distortion and a highly curved loss\nsurface. Based on this observation, we propose a simple method that not only\nprevents catastrophic overfitting, but also overrides the belief that it is\ndifficult to prevent multi-step adversarial attacks with single-step\nadversarial training.",
          "arxiv_id": "2010.01799v2"
        },
        {
          "title": "Semantics-Preserving Adversarial Training",
          "year": "2020-09",
          "abstract": "Adversarial training is a defense technique that improves adversarial\nrobustness of a deep neural network (DNN) by including adversarial examples in\nthe training data. In this paper, we identify an overlooked problem of\nadversarial training in that these adversarial examples often have different\nsemantics than the original data, introducing unintended biases into the model.\nWe hypothesize that such non-semantics-preserving (and resultingly ambiguous)\nadversarial data harm the robustness of the target models. To mitigate such\nunintended semantic changes of adversarial examples, we propose\nsemantics-preserving adversarial training (SPAT) which encourages perturbation\non the pixels that are shared among all classes when generating adversarial\nexamples in the training stage. Experiment results show that SPAT improves\nadversarial robustness and achieves state-of-the-art results in CIFAR-10 and\nCIFAR-100.",
          "arxiv_id": "2009.10978v1"
        },
        {
          "title": "Beneficial Perturbations Network for Defending Adversarial Examples",
          "year": "2020-09",
          "abstract": "Deep neural networks can be fooled by adversarial attacks: adding carefully\ncomputed small adversarial perturbations to clean inputs can cause\nmisclassification on state-of-the-art machine learning models. The reason is\nthat neural networks fail to accommodate the distribution drift of the input\ndata caused by adversarial perturbations. Here, we present a new solution -\nBeneficial Perturbation Network (BPN) - to defend against adversarial attacks\nby fixing the distribution drift. During training, BPN generates and leverages\nbeneficial perturbations (somewhat opposite to well-known adversarial\nperturbations) by adding new, out-of-network biasing units. Biasing units\ninfluence the parameter space of the network, to preempt and neutralize future\nadversarial perturbations on input data samples. To achieve this, BPN creates\nreverse adversarial attacks during training, with very little cost, by\nrecycling the training gradients already computed. Reverse attacks are captured\nby the biasing units, and the biases can in turn effectively defend against\nfuture adversarial examples. Reverse attacks are a shortcut, i.e., they affect\nthe network's parameters without requiring instantiation of adversarial\nexamples that could assist training. We provide comprehensive empirical\nevidence showing that 1) BPN is robust to adversarial examples and is much more\nrunning memory and computationally efficient compared to classical adversarial\ntraining. 2) BPN can defend against adversarial examples with negligible\nadditional computation and parameter costs compared to training only on clean\nexamples; 3) BPN hurts the accuracy on clean examples much less than classic\nadversarial training; 4) BPN can improve the generalization of the network 5)\nBPN trained only with Fast Gradient Sign Attack can generalize to defend PGD\nattacks.",
          "arxiv_id": "2009.12724v3"
        }
      ],
      "11": [
        {
          "title": "Semi-Federated Learning",
          "year": "2020-03",
          "abstract": "Federated learning (FL) enables massive distributed Information and\nCommunication Technology (ICT) devices to learn a global consensus model\nwithout any participants revealing their own data to the central server.\nHowever, the practicality, communication expense and non-independent and\nidentical distribution (Non-IID) data challenges in FL still need to be\nconcerned. In this work, we propose the Semi-Federated Learning (Semi-FL) which\ndiffers from the FL in two aspects, local clients clustering and in-cluster\ntraining. A sequential training manner is designed for our in-cluster training\nin this paper which enables the neighboring clients to share their learning\nmodels. The proposed Semi-FL can be easily applied to future mobile\ncommunication networks and require less up-link transmission bandwidth.\nNumerical experiments validate the feasibility, learning performance and the\nrobustness to Non-IID data of the proposed Semi-FL. The Semi-FL extends the\nexisting potentials of FL.",
          "arxiv_id": "2003.12795v1"
        },
        {
          "title": "Shuffled Model of Federated Learning: Privacy, Communication and Accuracy Trade-offs",
          "year": "2020-08",
          "abstract": "We consider a distributed empirical risk minimization (ERM) optimization\nproblem with communication efficiency and privacy requirements, motivated by\nthe federated learning (FL) framework. Unique challenges to the traditional ERM\nproblem in the context of FL include (i) need to provide privacy guarantees on\nclients' data, (ii) compress the communication between clients and the server,\nsince clients might have low-bandwidth links, (iii) work with a dynamic client\npopulation at each round of communication between the server and the clients,\nas a small fraction of clients are sampled at each round. To address these\nchallenges we develop (optimal) communication-efficient schemes for private\nmean estimation for several $\\ell_p$ spaces, enabling efficient gradient\naggregation for each iteration of the optimization solution of the ERM. We also\nprovide lower and upper bounds for mean estimation with privacy and\ncommunication constraints for arbitrary $\\ell_p$ spaces. To get the overall\ncommunication, privacy, and optimization performance operation point, we\ncombine this with privacy amplification opportunities inherent to this setup.\nOur solution takes advantage of the inherent privacy amplification provided by\nclient sampling and data sampling at each client (through Stochastic Gradient\nDescent) as well as the recently developed privacy framework using\nanonymization, which effectively presents to the server responses that are\nrandomly shuffled with respect to the clients. Putting these together, we\ndemonstrate that one can get the same privacy, optimization-performance\noperating point developed in recent methods that use full-precision\ncommunication, but at a much lower communication cost, i.e., effectively\ngetting communication efficiency for \"free\".",
          "arxiv_id": "2008.07180v2"
        },
        {
          "title": "A Framework for Evaluating Gradient Leakage Attacks in Federated Learning",
          "year": "2020-04",
          "abstract": "Federated learning (FL) is an emerging distributed machine learning framework\nfor collaborative model training with a network of clients (edge devices). FL\noffers default client privacy by allowing clients to keep their sensitive data\non local devices and to only share local training parameter updates with the\nfederated server. However, recent studies have shown that even sharing local\nparameter updates from a client to the federated server may be susceptible to\ngradient leakage attacks and intrude the client privacy regarding its training\ndata. In this paper, we present a principled framework for evaluating and\ncomparing different forms of client privacy leakage attacks. We first provide\nformal and experimental analysis to show how adversaries can reconstruct the\nprivate local training data by simply analyzing the shared parameter update\nfrom local training (e.g., local gradient or weight update vector). We then\nanalyze how different hyperparameter configurations in federated learning and\ndifferent settings of the attack algorithm may impact on both attack\neffectiveness and attack cost. Our framework also measures, evaluates, and\nanalyzes the effectiveness of client privacy leakage attacks under different\ngradient compression ratios when using communication efficient FL protocols.\nOur experiments also include some preliminary mitigation strategies to\nhighlight the importance of providing a systematic attack evaluation framework\ntowards an in-depth understanding of the various forms of client privacy\nleakage threats in federated learning and developing theoretical foundations\nfor attack mitigation.",
          "arxiv_id": "2004.10397v2"
        }
      ],
      "12": [
        {
          "title": "Modeling time evolving COVID-19 uncertainties with density dependent asymptomatic infections and social reinforcement",
          "year": "2021-08",
          "abstract": "The COVID-19 pandemic has posed significant challenges in modeling its\ncomplex epidemic transmissions, infection and contagion, which are very\ndifferent from known epidemics. The challenges in quantifying COVID-19\ncomplexities include effectively modeling its process and data uncertainties.\nThe uncertainties are embedded in implicit and high-proportional undocumented\ninfections, asymptomatic contagion, social reinforcement of infections, and\nvarious quality issues in the reported data. These uncertainties become even\nmore apparent in the first two months of the COVID-19 pandemic, when the\nrelevant knowledge, case reporting and testing were all limited. Here we\nintroduce a novel hybrid approach Susceptible-Undocumented infected-Documented\ninfected-Recovered (SUDR) model. First, SUDR (1) characterizes and\ndistinguishes Undocumented (U) and Documented (D) infections commonly seen\nduring COVID-19 incubation periods and asymptomatic infections. Second, SUDR\ncharacterizes the probabilistic density of infections by capturing exogenous\nprocesses. Lastly, SUDR approximates the density likelihood of COVID-19\nprevalence over time by incorporating Bayesian inference into SUDR. Different\nfrom existing COVID-19 models, SUDR characterizes the undocumented infections\nduring unknown transmission processes. To capture the uncertainties of temporal\ntransmission and social reinforcement during COVID-19 contagion, the\ntransmission rate is modeled by a time-varying density function of undocumented\ninfectious cases. By sampling from the mean-field posterior distribution with\nreasonable priors, SUDR handles the randomness, noise and sparsity of COVID-19\nobservations widely seen in the public COVID-19 case data. The results\ndemonstrate a deeper quantitative understanding of the above uncertainties, in\ncomparison with classic SIR, time-dependent SIR, and probabilistic SIR models.",
          "arxiv_id": "2108.10029v2"
        },
        {
          "title": "AICov: An Integrative Deep Learning Framework for COVID-19 Forecasting with Population Covariates",
          "year": "2020-10",
          "abstract": "The COVID-19 pandemic has profound global consequences on health, economic,\nsocial, political, and almost every major aspect of human life. Therefore, it\nis of great importance to model COVID-19 and other pandemics in terms of the\nbroader social contexts in which they take place. We present the architecture\nof AICov, which provides an integrative deep learning framework for COVID-19\nforecasting with population covariates, some of which may serve as putative\nrisk factors. We have integrated multiple different strategies into AICov,\nincluding the ability to use deep learning strategies based on LSTM and even\nmodeling. To demonstrate our approach, we have conducted a pilot that\nintegrates population covariates from multiple sources. Thus, AICov not only\nincludes data on COVID-19 cases and deaths but, more importantly, the\npopulation's socioeconomic, health and behavioral risk factors at a local\nlevel. The compiled data are fed into AICov, and thus we obtain improved\nprediction by integration of the data to our model as compared to one that only\nuses case and death data.",
          "arxiv_id": "2010.03757v1"
        },
        {
          "title": "A spatiotemporal machine learning approach to forecasting COVID-19 incidence at the county level in the USA",
          "year": "2021-09",
          "abstract": "With COVID-19 affecting every country globally and changing everyday life,\nthe ability to forecast the spread of the disease is more important than any\nprevious epidemic. The conventional methods of disease-spread modeling,\ncompartmental models, are based on the assumption of spatiotemporal homogeneity\nof the spread of the virus, which may cause forecasting to underperform,\nespecially at high spatial resolutions. In this paper we approach the\nforecasting task with an alternative technique - spatiotemporal machine\nlearning. We present COVID-LSTM, a data-driven model based on a Long Short-term\nMemory deep learning architecture for forecasting COVID-19 incidence at the\ncounty-level in the US. We use the weekly number of new positive cases as\ntemporal input, and hand-engineered spatial features from Facebook movement and\nconnectedness datasets to capture the spread of the disease in time and space.\nCOVID-LSTM outperforms the COVID-19 Forecast Hub's Ensemble model\n(COVIDhub-ensemble) on our 17-week evaluation period, making it the first model\nto be more accurate than the COVIDhub-ensemble over one or more forecast\nperiods. Over the 4-week forecast horizon, our model is on average 50 cases per\ncounty more accurate than the COVIDhub-ensemble. We highlight that the\nunderutilization of data-driven forecasting of disease spread prior to COVID-19\nis likely due to the lack of sufficient data available for previous diseases,\nin addition to the recent advances in machine learning methods for\nspatiotemporal forecasting. We discuss the impediments to the wider uptake of\ndata-driven forecasting, and whether it is likely that more deep learning-based\nmodels will be used in the future.",
          "arxiv_id": "2109.12094v4"
        }
      ],
      "13": [
        {
          "title": "Identifying noisy labels with a transductive semi-supervised leave-one-out filter",
          "year": "2020-09",
          "abstract": "Obtaining data with meaningful labels is often costly and error-prone. In\nthis situation, semi-supervised learning (SSL) approaches are interesting, as\nthey leverage assumptions about the unlabeled data to make up for the limited\namount of labels. However, in real-world situations, we cannot assume that the\nlabeling process is infallible, and the accuracy of many SSL classifiers\ndecreases significantly in the presence of label noise. In this work, we\nintroduce the LGC_LVOF, a leave-one-out filtering approach based on the Local\nand Global Consistency (LGC) algorithm. Our method aims to detect and remove\nwrong labels, and thus can be used as a preprocessing step to any SSL\nclassifier. Given the propagation matrix, detecting noisy labels takes O(cl)\nper step, with c the number of classes and l the number of labels. Moreover,\none does not need to compute the whole propagation matrix, but only an $l$ by\n$l$ submatrix corresponding to interactions between labeled instances. As a\nresult, our approach is best suited to datasets with a large amount of\nunlabeled data but not many labels. Results are provided for a number of\ndatasets, including MNIST and ISOLET. LGCLVOF appears to be equally or more\nprecise than the adapted gradient-based filter. We show that the best-case\naccuracy of the embedding of LGCLVOF into LGC yields performance comparable to\nthe best-case of $\\ell_1$-based classifiers designed to be robust to label\nnoise. We provide a heuristic to choose the number of removed instances.",
          "arxiv_id": "2009.11811v1"
        },
        {
          "title": "Pseudo-Representation Labeling Semi-Supervised Learning",
          "year": "2020-05",
          "abstract": "In recent years, semi-supervised learning (SSL) has shown tremendous success\nin leveraging unlabeled data to improve the performance of deep learning\nmodels, which significantly reduces the demand for large amounts of labeled\ndata. Many SSL techniques have been proposed and have shown promising\nperformance on famous datasets such as ImageNet and CIFAR-10. However, some\nexiting techniques (especially data augmentation based) are not suitable for\nindustrial applications empirically. Therefore, this work proposes the\npseudo-representation labeling, a simple and flexible framework that utilizes\npseudo-labeling techniques to iteratively label a small amount of unlabeled\ndata and use them as training data. In addition, our framework is integrated\nwith self-supervised representation learning such that the classifier gains\nbenefits from representation learning of both labeled and unlabeled data. This\nframework can be implemented without being limited at the specific model\nstructure, but a general technique to improve the existing model. Compared with\nthe existing approaches, the pseudo-representation labeling is more intuitive\nand can effectively solve practical problems in the real world. Empirically, it\noutperforms the current state-of-the-art semi-supervised learning methods in\nindustrial types of classification problems such as the WM-811K wafer map and\nthe MIT-BIH Arrhythmia dataset.",
          "arxiv_id": "2006.00429v1"
        },
        {
          "title": "Evaluating Multi-label Classifiers with Noisy Labels",
          "year": "2021-02",
          "abstract": "Multi-label classification (MLC) is a generalization of standard\nclassification where multiple labels may be assigned to a given sample. In the\nreal world, it is more common to deal with noisy datasets than clean datasets,\ngiven how modern datasets are labeled by a large group of annotators on\ncrowdsourcing platforms, but little attention has been given to evaluating\nmulti-label classifiers with noisy labels. Exploiting label correlations now\nbecomes a standard component of a multi-label classifier to achieve competitive\nperformance. However, this component makes the classifier more prone to poor\ngeneralization - it overfits labels as well as label dependencies. We identify\nthree common real-world label noise scenarios and show how previous approaches\nper-form poorly with noisy labels. To address this issue, we present a\nContext-Based Multi-LabelClassifier (CbMLC) that effectively handles noisy\nlabels when learning label dependencies, without requiring additional\nsupervision. We compare CbMLC against other domain-specific state-of-the-art\nmodels on a variety of datasets, under both the clean and the noisy settings.\nWe show CbMLC yields substantial improvements over the previous methods in most\ncases.",
          "arxiv_id": "2102.08427v1"
        }
      ],
      "14": [
        {
          "title": "Non-Log-Concave and Nonsmooth Sampling via Langevin Monte Carlo Algorithms",
          "year": "2023-05",
          "abstract": "We study the problem of approximate sampling from non-log-concave\ndistributions, e.g., Gaussian mixtures, which is often challenging even in low\ndimensions due to their multimodality. We focus on performing this task via\nMarkov chain Monte Carlo (MCMC) methods derived from discretizations of the\noverdamped Langevin diffusions, which are commonly known as Langevin Monte\nCarlo algorithms. Furthermore, we are also interested in two nonsmooth cases\nfor which a large class of proximal MCMC methods have been developed: (i) a\nnonsmooth prior is considered with a Gaussian mixture likelihood; (ii) a\nLaplacian mixture distribution. Such nonsmooth and non-log-concave sampling\ntasks arise from a wide range of applications to Bayesian inference and imaging\ninverse problems such as image deconvolution. We perform numerical simulations\nto compare the performance of most commonly used Langevin Monte Carlo\nalgorithms.",
          "arxiv_id": "2305.15988v2"
        },
        {
          "title": "Subspace Langevin Monte Carlo",
          "year": "2024-12",
          "abstract": "Sampling from high-dimensional distributions has wide applications in data\nscience and machine learning but poses significant computational challenges. We\nintroduce Subspace Langevin Monte Carlo (SLMC), a novel and efficient sampling\nmethod that generalizes random-coordinate Langevin Monte Carlo and\npreconditioned Langevin Monte Carlo by projecting the Langevin update onto\nsubsampled eigenblocks of a time-varying preconditioner at each iteration. The\nadvantage of SLMC is its superior adaptability and computational efficiency\ncompared to traditional Langevin Monte Carlo and preconditioned Langevin Monte\nCarlo. Using coupling arguments, we establish error guarantees for SLMC and\ndemonstrate its practical effectiveness through a few experiments on sampling\nfrom ill-conditioned distributions.",
          "arxiv_id": "2412.13928v2"
        },
        {
          "title": "Antithetic Riemannian Manifold And Quantum-Inspired Hamiltonian Monte Carlo",
          "year": "2021-07",
          "abstract": "Markov Chain Monte Carlo inference of target posterior distributions in\nmachine learning is predominately conducted via Hamiltonian Monte Carlo and its\nvariants. This is due to Hamiltonian Monte Carlo based samplers ability to\nsuppress random-walk behaviour. As with other Markov Chain Monte Carlo methods,\nHamiltonian Monte Carlo produces auto-correlated samples which results in high\nvariance in the estimators, and low effective sample size rates in the\ngenerated samples. Adding antithetic sampling to Hamiltonian Monte Carlo has\nbeen previously shown to produce higher effective sample rates compared to\nvanilla Hamiltonian Monte Carlo. In this paper, we present new algorithms which\nare antithetic versions of Riemannian Manifold Hamiltonian Monte Carlo and\nQuantum-Inspired Hamiltonian Monte Carlo. The Riemannian Manifold Hamiltonian\nMonte Carlo algorithm improves on Hamiltonian Monte Carlo by taking into\naccount the local geometry of the target, which is beneficial for target\ndensities that may exhibit strong correlations in the parameters.\nQuantum-Inspired Hamiltonian Monte Carlo is based on quantum particles that can\nhave random mass. Quantum-Inspired Hamiltonian Monte Carlo uses a random mass\nmatrix which results in better sampling than Hamiltonian Monte Carlo on spiky\nand multi-modal distributions such as jump diffusion processes. The analysis is\nperformed on jump diffusion process using real world financial market data, as\nwell as on real world benchmark classification tasks using Bayesian logistic\nregression.",
          "arxiv_id": "2107.02070v1"
        }
      ],
      "15": [
        {
          "title": "Rao Differential Privacy",
          "year": "2025-08",
          "abstract": "Differential privacy (DP) has recently emerged as a definition of privacy to\nrelease private estimates. DP calibrates noise to be on the order of an\nindividuals contribution. Due to the this calibration a private estimate\nobscures any individual while preserving the utility of the estimate. Since the\noriginal definition, many alternate definitions have been proposed. These\nalternates have been proposed for various reasons including improvements on\ncomposition results, relaxations, and formalizations. Nevertheless, thus far\nnearly all definitions of privacy have used a divergence of densities as the\nbasis of the definition. In this paper we take an information geometry\nperspective towards differential privacy. Specifically, rather than define\nprivacy via a divergence, we define privacy via the Rao distance. We show that\nour proposed definition of privacy shares the interpretation of previous\ndefinitions of privacy while improving on sequential composition.",
          "arxiv_id": "2508.17135v1"
        },
        {
          "title": "Individual Privacy Accounting with Gaussian Differential Privacy",
          "year": "2022-09",
          "abstract": "Individual privacy accounting enables bounding differential privacy (DP) loss\nindividually for each participant involved in the analysis. This can be\ninformative as often the individual privacy losses are considerably smaller\nthan those indicated by the DP bounds that are based on considering worst-case\nbounds at each data access. In order to account for the individual privacy\nlosses in a principled manner, we need a privacy accountant for adaptive\ncompositions of randomised mechanisms, where the loss incurred at a given data\naccess is allowed to be smaller than the worst-case loss. This kind of analysis\nhas been carried out for the R\\'enyi differential privacy (RDP) by Feldman and\nZrnic (2021), however not yet for the so-called optimal privacy accountants. We\nmake first steps in this direction by providing a careful analysis using the\nGaussian differential privacy which gives optimal bounds for the Gaussian\nmechanism, one of the most versatile DP mechanisms. This approach is based on\ndetermining a certain supermartingale for the hockey-stick divergence and on\nextending the R\\'enyi divergence-based fully adaptive composition results by\nFeldman and Zrnic. We also consider measuring the individual\n$(\\varepsilon,\\delta)$-privacy losses using the so-called privacy loss\ndistributions. With the help of the Blackwell theorem, we can then make use of\nthe RDP analysis to construct an approximative individual\n$(\\varepsilon,\\delta)$-accountant.",
          "arxiv_id": "2209.15596v2"
        },
        {
          "title": "Practical Privacy Filters and Odometers with Rnyi Differential Privacy and Applications to Differentially Private Deep Learning",
          "year": "2021-03",
          "abstract": "Differential Privacy (DP) is the leading approach to privacy preserving deep\nlearning. As such, there are multiple efforts to provide drop-in integration of\nDP into popular frameworks. These efforts, which add noise to each gradient\ncomputation to make it DP, rely on composition theorems to bound the total\nprivacy loss incurred over this sequence of DP computations.\n  However, existing composition theorems present a tension between efficiency\nand flexibility. Most theorems require all computations in the sequence to have\na predefined DP parameter, called the privacy budget. This prevents the design\nof training algorithms that adapt the privacy budget on the fly, or that\nterminate early to reduce the total privacy loss. Alternatively, the few\nexisting composition results for adaptive privacy budgets provide complex\nbounds on the privacy loss, with constants too large to be practical.\n  In this paper, we study DP composition under adaptive privacy budgets through\nthe lens of R\\'enyi Differential Privacy, proving a simpler composition theorem\nwith smaller constants, making it practical enough to use in algorithm design.\nWe demonstrate two applications of this theorem for DP deep learning: adapting\nthe noise or batch size online to improve a model's accuracy within a fixed\ntotal privacy loss, and stopping early when fine-tuning a model to reduce total\nprivacy loss.",
          "arxiv_id": "2103.01379v2"
        }
      ],
      "16": [
        {
          "title": "Ensemble Multi-Source Domain Adaptation with Pseudolabels",
          "year": "2020-09",
          "abstract": "Given multiple source datasets with labels, how can we train a target model\nwith no labeled data? Multi-source domain adaptation (MSDA) aims to train a\nmodel using multiple source datasets different from a target dataset in the\nabsence of target data labels. MSDA is a crucial problem applicable to many\npractical cases where labels for the target data are unavailable due to privacy\nissues. Existing MSDA frameworks are limited since they align data without\nconsidering conditional distributions p(x|y) of each domain. They also miss a\nlot of target label information by not considering the target label at all and\nrelying on only one feature extractor. In this paper, we propose Ensemble\nMulti-source Domain Adaptation with Pseudolabels (EnMDAP), a novel method for\nmulti-source domain adaptation. EnMDAP exploits label-wise moment matching to\nalign conditional distributions p(x|y), using pseudolabels for the unavailable\ntarget labels, and introduces ensemble learning theme by using multiple feature\nextractors for accurate domain adaptation. Extensive experiments show that\nEnMDAP provides the state-of-the-art performance for multi-source domain\nadaptation tasks in both of image domains and text domains.",
          "arxiv_id": "2009.14248v1"
        },
        {
          "title": "Gradual Domain Adaptation via Normalizing Flows",
          "year": "2022-06",
          "abstract": "Standard domain adaptation methods do not work well when a large gap exists\nbetween the source and target domains. Gradual domain adaptation is one of the\napproaches used to address the problem. It involves leveraging the intermediate\ndomain, which gradually shifts from the source domain to the target domain. In\nprevious work, it is assumed that the number of intermediate domains is large\nand the distance between adjacent domains is small; hence, the gradual domain\nadaptation algorithm, involving self-training with unlabeled datasets, is\napplicable. In practice, however, gradual self-training will fail because the\nnumber of intermediate domains is limited and the distance between adjacent\ndomains is large. We propose the use of normalizing flows to deal with this\nproblem while maintaining the framework of unsupervised domain adaptation. The\nproposed method learns a transformation from the distribution of the target\ndomain to the Gaussian mixture distribution via the source domain. We evaluate\nour proposed method by experiments using real-world datasets and confirm that\nit mitigates the above-explained problem and improves the classification\nperformance.",
          "arxiv_id": "2206.11492v4"
        },
        {
          "title": "Unsupervised Domain Adaptation with Progressive Domain Augmentation",
          "year": "2020-04",
          "abstract": "Domain adaptation aims to exploit a label-rich source domain for learning\nclassifiers in a different label-scarce target domain. It is particularly\nchallenging when there are significant divergences between the two domains. In\nthe paper, we propose a novel unsupervised domain adaptation method based on\nprogressive domain augmentation. The proposed method generates virtual\nintermediate domains via domain interpolation, progressively augments the\nsource domain and bridges the source-target domain divergence by conducting\nmultiple subspace alignment on the Grassmann manifold. We conduct experiments\non multiple domain adaptation tasks and the results shows the proposed method\nachieves the state-of-the-art performance.",
          "arxiv_id": "2004.01735v2"
        }
      ],
      "17": [
        {
          "title": "Sparse Gaussian Process Based On Hat Basis Functions",
          "year": "2020-06",
          "abstract": "Gaussian process is one of the most popular non-parametric Bayesian\nmethodologies for modeling the regression problem. It is completely determined\nby its mean and covariance functions. And its linear property makes it\nrelatively straightforward to solve the prediction problem. Although Gaussian\nprocess has been successfully applied in many fields, it is still not enough to\ndeal with physical systems that satisfy inequality constraints. This issue has\nbeen addressed by the so-called constrained Gaussian process in recent years.\nIn this paper, we extend the core ideas of constrained Gaussian process.\nAccording to the range of training or test data, we redefine the hat basis\nfunctions mentioned in the constrained Gaussian process. Based on hat basis\nfunctions, we propose a new sparse Gaussian process method to solve the\nunconstrained regression problem. Similar to the exact Gaussian process and\nGaussian process with Fully Independent Training Conditional approximation, our\nmethod obtains satisfactory approximate results on open-source datasets or\nanalytical functions. In terms of performance, the proposed method reduces the\noverall computational complexity from $O(n^{3})$ computation in exact Gaussian\nprocess to $O(nm^{2})$ with $m$ hat basis functions and $n$ training data\npoints.",
          "arxiv_id": "2006.08117v1"
        },
        {
          "title": "Matrn Gaussian Processes on Graphs",
          "year": "2020-10",
          "abstract": "Gaussian processes are a versatile framework for learning unknown functions\nin a manner that permits one to utilize prior information about their\nproperties. Although many different Gaussian process models are readily\navailable when the input space is Euclidean, the choice is much more limited\nfor Gaussian processes whose input space is an undirected graph. In this work,\nwe leverage the stochastic partial differential equation characterization of\nMat\\'ern Gaussian processes - a widely-used model class in the Euclidean\nsetting - to study their analog for undirected graphs. We show that the\nresulting Gaussian processes inherit various attractive properties of their\nEuclidean and Riemannian analogs and provide techniques that allow them to be\ntrained using standard methods, such as inducing points. This enables graph\nMat\\'ern Gaussian processes to be employed in mini-batch and non-conjugate\nsettings, thereby making them more accessible to practitioners and easier to\ndeploy within larger learning frameworks.",
          "arxiv_id": "2010.15538v3"
        },
        {
          "title": "Gaussian Process for Trajectories",
          "year": "2021-10",
          "abstract": "The Gaussian process is a powerful and flexible technique for interpolating\nspatiotemporal data, especially with its ability to capture complex trends and\nuncertainty from the input signal. This chapter describes Gaussian processes as\nan interpolation technique for geospatial trajectories. A Gaussian process\nmodels measurements of a trajectory as coming from a multidimensional Gaussian,\nand it produces for each timestamp a Gaussian distribution as a prediction. We\ndiscuss elements that need to be considered when applying Gaussian process to\ntrajectories, common choices for those elements, and provide a concrete example\nof implementing a Gaussian process.",
          "arxiv_id": "2110.03712v1"
        }
      ],
      "18": [
        {
          "title": "FACT: A Diagnostic for Group Fairness Trade-offs",
          "year": "2020-04",
          "abstract": "Group fairness, a class of fairness notions that measure how different groups\nof individuals are treated differently according to their protected attributes,\nhas been shown to conflict with one another, often with a necessary cost in\nloss of model's predictive performance. We propose a general diagnostic that\nenables systematic characterization of these trade-offs in group fairness. We\nobserve that the majority of group fairness notions can be expressed via the\nfairness-confusion tensor, which is the confusion matrix split according to the\nprotected attribute values. We frame several optimization problems that\ndirectly optimize both accuracy and fairness objectives over the elements of\nthis tensor, which yield a general perspective for understanding multiple\ntrade-offs including group fairness incompatibilities. It also suggests an\nalternate post-processing method for designing fair classifiers. On synthetic\nand real datasets, we demonstrate the use cases of our diagnostic, particularly\non understanding the trade-off landscape between accuracy and fairness.",
          "arxiv_id": "2004.03424v3"
        },
        {
          "title": "Within-group fairness: A guidance for more sound between-group fairness",
          "year": "2023-01",
          "abstract": "As they have a vital effect on social decision-making, AI algorithms not only\nshould be accurate and but also should not pose unfairness against certain\nsensitive groups (e.g., non-white, women). Various specially designed AI\nalgorithms to ensure trained AI models to be fair between sensitive groups have\nbeen developed. In this paper, we raise a new issue that between-group fair AI\nmodels could treat individuals in a same sensitive group unfairly. We introduce\na new concept of fairness so-called within-group fairness which requires that\nAI models should be fair for those in a same sensitive group as well as those\nin different sensitive groups. We materialize the concept of within-group\nfairness by proposing corresponding mathematical definitions and developing\nlearning algorithms to control within-group fairness and between-group fairness\nsimultaneously. Numerical studies show that the proposed learning algorithms\nimprove within-group fairness without sacrificing accuracy as well as\nbetween-group fairness.",
          "arxiv_id": "2301.08375v1"
        },
        {
          "title": "Algorithmic Decision Making with Conditional Fairness",
          "year": "2020-06",
          "abstract": "Nowadays fairness issues have raised great concerns in decision-making\nsystems. Various fairness notions have been proposed to measure the degree to\nwhich an algorithm is unfair. In practice, there frequently exist a certain set\nof variables we term as fair variables, which are pre-decision covariates such\nas users' choices. The effects of fair variables are irrelevant in assessing\nthe fairness of the decision support algorithm. We thus define conditional\nfairness as a more sound fairness metric by conditioning on the fairness\nvariables. Given different prior knowledge of fair variables, we demonstrate\nthat traditional fairness notations, such as demographic parity and equalized\nodds, are special cases of our conditional fairness notations. Moreover, we\npropose a Derivable Conditional Fairness Regularizer (DCFR), which can be\nintegrated into any decision-making model, to track the trade-off between\nprecision and fairness of algorithmic decision making. Specifically, an\nadversarial representation based conditional independence loss is proposed in\nour DCFR to measure the degree of unfairness. With extensive experiments on\nthree real-world datasets, we demonstrate the advantages of our conditional\nfairness notation and DCFR.",
          "arxiv_id": "2006.10483v5"
        }
      ],
      "19": [
        {
          "title": "Conformal Prediction Sets with Improved Conditional Coverage using Trust Scores",
          "year": "2025-01",
          "abstract": "Standard conformal prediction offers a marginal guarantee on coverage, but\nfor prediction sets to be truly useful, they should ideally ensure coverage\nconditional on each test point. Unfortunately, it is impossible to achieve\nexact, distribution-free conditional coverage in finite samples. In this work,\nwe propose an alternative conformal prediction algorithm that targets coverage\nwhere it matters most--in instances where a classifier is overconfident in its\nincorrect predictions. We start by dissecting miscoverage events in\nmarginally-valid conformal prediction, and show that miscoverage rates vary\nbased on the classifier's confidence and its deviation from the Bayes optimal\nclassifier. Motivated by this insight, we develop a variant of conformal\nprediction that targets coverage conditional on a reduced set of two variables:\nthe classifier's confidence in a prediction and a nonparametric trust score\nthat measures its deviation from the Bayes classifier. Empirical evaluation on\nmultiple image datasets shows that our method generally improves conditional\ncoverage properties compared to standard conformal prediction, including\nclass-conditional coverage, coverage over arbitrary subgroups, and coverage\nover demographic groups.",
          "arxiv_id": "2501.10139v2"
        },
        {
          "title": "E-Values Expand the Scope of Conformal Prediction",
          "year": "2025-03",
          "abstract": "Conformal prediction is a powerful framework for distribution-free\nuncertainty quantification. The standard approach to conformal prediction\nrelies on comparing the ranks of prediction scores: under exchangeability, the\nrank of a future test point cannot be too extreme relative to a calibration\nset. This rank-based method can be reformulated in terms of p-values. In this\npaper, we explore an alternative approach based on e-values, known as conformal\ne-prediction. E-values offer key advantages that cannot be achieved with\np-values, enabling new theoretical and practical capabilities. In particular,\nwe present three applications that leverage the unique strengths of e-values:\nbatch anytime-valid conformal prediction, fixed-size conformal sets with\ndata-dependent coverage, and conformal prediction under ambiguous ground truth.\nOverall, these examples demonstrate that e-value-based constructions provide a\nflexible expansion of the toolbox of conformal prediction.",
          "arxiv_id": "2503.13050v3"
        },
        {
          "title": "Conformal e-prediction",
          "year": "2020-01",
          "abstract": "This paper discusses a counterpart of conformal prediction for e-values,\nconformal e-prediction. Conformal e-prediction is conceptually simpler and had\nbeen developed in the 1990s as a precursor of conformal prediction. When\nconformal prediction emerged as result of replacing e-values by p-values, it\nseemed to have important advantages over conformal e-prediction without obvious\ndisadvantages. This paper re-examines relations between conformal prediction\nand conformal e-prediction systematically from a modern perspective. Conformal\ne-prediction has advantages of its own, such as the ease of designing\nconditional conformal e-predictors and the guaranteed validity of\ncross-conformal e-predictors (whereas for cross-conformal predictors validity\nis only an empirical fact and can be broken with excessive randomization). Even\nwhere conformal prediction has clear advantages, conformal e-prediction can\noften emulate those advantages, more or less successfully.",
          "arxiv_id": "2001.05989v5"
        }
      ],
      "20": [
        {
          "title": "ST-MAML: A Stochastic-Task based Method for Task-Heterogeneous Meta-Learning",
          "year": "2021-09",
          "abstract": "Optimization-based meta-learning typically assumes tasks are sampled from a\nsingle distribution - an assumption oversimplifies and limits the diversity of\ntasks that meta-learning can model. Handling tasks from multiple different\ndistributions is challenging for meta-learning due to a so-called task\nambiguity issue. This paper proposes a novel method, ST-MAML, that empowers\nmodel-agnostic meta-learning (MAML) to learn from multiple task distributions.\nST-MAML encodes tasks using a stochastic neural network module, that summarizes\nevery task with a stochastic representation. The proposed Stochastic Task (ST)\nstrategy allows a meta-model to get tailored for the current task and enables\nus to learn a distribution of solutions for an ambiguous task. ST-MAML also\npropagates the task representation to revise the encoding of input variables.\nEmpirically, we demonstrate that ST-MAML matches or outperforms the\nstate-of-the-art on two few-shot image classification tasks, one curve\nregression benchmark, one image completion problem, and a real-world\ntemperature prediction application. To the best of authors' knowledge, this is\nthe first time optimization-based meta-learning method being applied on a\nlarge-scale real-world task.",
          "arxiv_id": "2109.13305v1"
        },
        {
          "title": "Task-similarity Aware Meta-learning through Nonparametric Kernel Regression",
          "year": "2020-06",
          "abstract": "This paper investigates the use of nonparametric kernel-regression to obtain\na tasksimilarity aware meta-learning algorithm. Our hypothesis is that the use\nof tasksimilarity helps meta-learning when the available tasks are limited and\nmay contain outlier/ dissimilar tasks. While existing meta-learning approaches\nimplicitly assume the tasks as being similar, it is generally unclear how this\ntask-similarity could be quantified and used in the learning. As a result, most\npopular metalearning approaches do not actively use the\nsimilarity/dissimilarity between the tasks, but rely on availability of huge\nnumber of tasks for their working. Our contribution is a novel framework for\nmeta-learning that explicitly uses task-similarity in the form of kernels and\nan associated meta-learning algorithm. We model the task-specific parameters to\nbelong to a reproducing kernel Hilbert space where the kernel function captures\nthe similarity across tasks. The proposed algorithm iteratively learns a\nmeta-parameter which is used to assign a task-specific descriptor for every\ntask. The task descriptors are then used to quantify the task-similarity\nthrough the kernel function. We show how our approach conceptually generalizes\nthe popular meta-learning approaches of model-agnostic meta-learning (MAML) and\nMeta-stochastic gradient descent (Meta-SGD) approaches. Numerical experiments\nwith regression tasks show that our algorithm outperforms these approaches when\nthe number of tasks is limited, even in the presence of outlier or dissimilar\ntasks. This supports our hypothesis that task-similarity helps improve the\nmetalearning performance in task-limited and adverse settings.",
          "arxiv_id": "2006.07212v2"
        },
        {
          "title": "A Comprehensive Overview and Survey of Recent Advances in Meta-Learning",
          "year": "2020-04",
          "abstract": "This article reviews meta-learning also known as learning-to-learn which\nseeks rapid and accurate model adaptation to unseen tasks with applications in\nhighly automated AI, few-shot learning, natural language processing and\nrobotics. Unlike deep learning, meta-learning can be applied to few-shot\nhigh-dimensional datasets and considers further improving model generalization\nto unseen tasks. Deep learning is focused upon in-sample prediction and\nmeta-learning concerns model adaptation for out-of-sample prediction.\nMeta-learning can continually perform self-improvement to achieve highly\nautonomous AI. Meta-learning may serve as an additional generalization block\ncomplementary for original deep learning model. Meta-learning seeks adaptation\nof machine learning models to unseen tasks which are vastly different from\ntrained tasks. Meta-learning with coevolution between agent and environment\nprovides solutions for complex tasks unsolvable by training from scratch.\nMeta-learning methodology covers a wide range of great minds and thoughts. We\nbriefly introduce meta-learning methodologies in the following categories:\nblack-box meta-learning, metric-based meta-learning, layered meta-learning and\nBayesian meta-learning framework. Recent applications concentrate upon the\nintegration of meta-learning with other machine learning framework to provide\nfeasible integrated problem solutions. We briefly present recent meta-learning\nadvances and discuss potential future research directions.",
          "arxiv_id": "2004.11149v7"
        }
      ],
      "21": [
        {
          "title": "Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds",
          "year": "2024-03",
          "abstract": "While many Machine Learning methods were developed or transposed on\nRiemannian manifolds to tackle data with known non Euclidean geometry, Optimal\nTransport (OT) methods on such spaces have not received much attention. The\nmain OT tool on these spaces is the Wasserstein distance which suffers from a\nheavy computational burden. On Euclidean spaces, a popular alternative is the\nSliced-Wasserstein distance, which leverages a closed-form solution of the\nWasserstein distance in one dimension, but which is not readily available on\nmanifolds. In this work, we derive general constructions of Sliced-Wasserstein\ndistances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive\ncurvature, which include among others Hyperbolic spaces or the space of\nSymmetric Positive Definite matrices. Then, we propose different applications.\nAdditionally, we derive non-parametric schemes to minimize these new distances\nby approximating their Wasserstein gradient flows.",
          "arxiv_id": "2403.06560v1"
        },
        {
          "title": "An Introduction to Sliced Optimal Transport",
          "year": "2025-08",
          "abstract": "Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal\ntransport (OT) that exploits the tractability of one-dimensional OT problems.\nBy combining tools from OT, integral geometry, and computational statistics,\nSOT enables fast and scalable computation of distances, barycenters, and\nkernels for probability measures, while retaining rich geometric structure.\nThis paper provides a comprehensive review of SOT, covering its mathematical\nfoundations, methodological advances, computational methods, and applications.\nWe discuss key concepts of OT and one-dimensional OT, the role of tools from\nintegral geometry such as Radon transform in projecting measures, and\nstatistical techniques for estimating sliced distances. The paper further\nexplores recent methodological advances, including non-linear projections,\nimproved Monte Carlo approximations, statistical estimation techniques for\none-dimensional optimal transport, weighted slicing techniques, and\ntransportation plan estimation methods. Variational problems, such as minimum\nsliced Wasserstein estimation, barycenters, gradient flows, kernel\nconstructions, and embeddings are examined alongside extensions to unbalanced,\npartial, multi-marginal, and Gromov-Wasserstein settings. Applications span\nmachine learning, statistics, computer graphics and computer visions,\nhighlighting SOT's versatility as a practical computational tool. This work\nwill be of interest to researchers and practitioners in machine learning, data\nsciences, and computational disciplines seeking efficient alternatives to\nclassical OT.",
          "arxiv_id": "2508.12519v1"
        },
        {
          "title": "Heterogeneous Wasserstein Discrepancy for Incomparable Distributions",
          "year": "2021-06",
          "abstract": "Optimal Transport (OT) metrics allow for defining discrepancies between two\nprobability measures. Wasserstein distance is for longer the celebrated\nOT-distance frequently-used in the literature, which seeks probability\ndistributions to be supported on the $\\textit{same}$ metric space. Because of\nits high computational complexity, several approximate Wasserstein distances\nhave been proposed based on entropy regularization or on slicing, and\none-dimensional Wassserstein computation. In this paper, we propose a novel\nextension of Wasserstein distance to compare two incomparable distributions,\nthat hinges on the idea of $\\textit{distributional slicing}$, embeddings, and\non computing the closed-form Wassertein distance between the sliced\ndistributions. We provide a theoretical analysis of this new divergence, called\n$\\textit{heterogeneous Wasserstein discrepancy (HWD)}$, and we show that it\npreserves several interesting properties including rotation-invariance. We show\nthat the embeddings involved in HWD can be efficiently learned. Finally, we\nprovide a large set of experiments illustrating the behavior of HWD as a\ndivergence in the context of generative modeling and in query framework.",
          "arxiv_id": "2106.02542v2"
        }
      ],
      "22": [
        {
          "title": "Deceptive AI Explanations: Creation and Detection",
          "year": "2020-01",
          "abstract": "Artificial intelligence (AI) comes with great opportunities but can also pose\nsignificant risks. Automatically generated explanations for decisions can\nincrease transparency and foster trust, especially for systems based on\nautomated predictions by AI models. However, given, e.g., economic incentives\nto create dishonest AI, to what extent can we trust explanations? To address\nthis issue, our work investigates how AI models (i.e., deep learning, and\nexisting instruments to increase transparency regarding AI decisions) can be\nused to create and detect deceptive explanations. As an empirical evaluation,\nwe focus on text classification and alter the explanations generated by\nGradCAM, a well-established explanation technique in neural networks. Then, we\nevaluate the effect of deceptive explanations on users in an experiment with\n200 participants. Our findings confirm that deceptive explanations can indeed\nfool humans. However, one can deploy machine learning (ML) methods to detect\nseemingly minor deception attempts with accuracy exceeding 80% given sufficient\ndomain knowledge. Without domain knowledge, one can still infer inconsistencies\nin the explanations in an unsupervised manner, given basic knowledge of the\npredictive model under scrutiny.",
          "arxiv_id": "2001.07641v3"
        },
        {
          "title": "Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach",
          "year": "2020-01",
          "abstract": "We examine counterfactual explanations for explaining the decisions made by\nmodel-based AI systems. The counterfactual approach we consider defines an\nexplanation as a set of the system's data inputs that causally drives the\ndecision (i.e., changing the inputs in the set changes the decision) and is\nirreducible (i.e., changing any subset of the inputs does not change the\ndecision). We (1) demonstrate how this framework may be used to provide\nexplanations for decisions made by general, data-driven AI systems that may\nincorporate features with arbitrary data types and multiple predictive models,\nand (2) propose a heuristic procedure to find the most useful explanations\ndepending on the context. We then contrast counterfactual explanations with\nmethods that explain model predictions by weighting features according to their\nimportance (e.g., SHAP, LIME) and present two fundamental reasons why we should\ncarefully consider whether importance-weight explanations are well-suited to\nexplain system decisions. Specifically, we show that (i) features that have a\nlarge importance weight for a model prediction may not affect the corresponding\ndecision, and (ii) importance weights are insufficient to communicate whether\nand how features influence decisions. We demonstrate this with several concise\nexamples and three detailed case studies that compare the counterfactual\napproach with SHAP to illustrate various conditions under which counterfactual\nexplanations explain data-driven decisions better than importance weights.",
          "arxiv_id": "2001.07417v5"
        },
        {
          "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models",
          "year": "2020-08",
          "abstract": "With machine learning models being increasingly applied to various\ndecision-making scenarios, people have spent growing efforts to make machine\nlearning models more transparent and explainable. Among various explanation\ntechniques, counterfactual explanations have the advantages of being\nhuman-friendly and actionable -- a counterfactual explanation tells the user\nhow to gain the desired prediction with minimal changes to the input. Besides,\ncounterfactual explanations can also serve as efficient probes to the models'\ndecisions. In this work, we exploit the potential of counterfactual\nexplanations to understand and explore the behavior of machine learning models.\nWe design DECE, an interactive visualization system that helps understand and\nexplore a model's decisions on individual instances and data subsets,\nsupporting users ranging from decision-subjects to model developers. DECE\nsupports exploratory analysis of model decisions by combining the strengths of\ncounterfactual explanations at instance- and subgroup-levels. We also introduce\na set of interactions that enable users to customize the generation of\ncounterfactual explanations to find more actionable ones that can suit their\nneeds. Through three use cases and an expert interview, we demonstrate the\neffectiveness of DECE in supporting decision exploration tasks and instance\nexplanations.",
          "arxiv_id": "2008.08353v1"
        }
      ],
      "23": [
        {
          "title": "Uncovering the structure of clinical EEG signals with self-supervised learning",
          "year": "2020-07",
          "abstract": "Objective. Supervised learning paradigms are often limited by the amount of\nlabeled data that is available. This phenomenon is particularly problematic in\nclinically-relevant data, such as electroencephalography (EEG), where labeling\ncan be costly in terms of specialized expertise and human processing time.\nConsequently, deep learning architectures designed to learn on EEG data have\nyielded relatively shallow models and performances at best similar to those of\ntraditional feature-based approaches. However, in most situations, unlabeled\ndata is available in abundance. By extracting information from this unlabeled\ndata, it might be possible to reach competitive performance with deep neural\nnetworks despite limited access to labels. Approach. We investigated\nself-supervised learning (SSL), a promising technique for discovering structure\nin unlabeled data, to learn representations of EEG signals. Specifically, we\nexplored two tasks based on temporal context prediction as well as contrastive\npredictive coding on two clinically-relevant problems: EEG-based sleep staging\nand pathology detection. We conducted experiments on two large public datasets\nwith thousands of recordings and performed baseline comparisons with purely\nsupervised and hand-engineered approaches. Main results. Linear classifiers\ntrained on SSL-learned features consistently outperformed purely supervised\ndeep neural networks in low-labeled data regimes while reaching competitive\nperformance when all labels were available. Additionally, the embeddings\nlearned with each method revealed clear latent structures related to\nphysiological and clinical phenomena, such as age effects. Significance. We\ndemonstrate the benefit of self-supervised learning approaches on EEG data. Our\nresults suggest that SSL may pave the way to a wider use of deep learning\nmodels on EEG data.",
          "arxiv_id": "2007.16104v1"
        },
        {
          "title": "Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network",
          "year": "2020-03",
          "abstract": "Chest X-ray is the first imaging technique that plays an important role in\nthe diagnosis of COVID-19 disease. Due to the high availability of large-scale\nannotated image datasets, great success has been achieved using convolutional\nneural networks (CNNs) for image recognition and classification. However, due\nto the limited availability of annotated medical images, the classification of\nmedical images remains the biggest challenge in medical diagnosis. Thanks to\ntransfer learning, an effective mechanism that can provide a promising solution\nby transferring knowledge from generic object recognition tasks to\ndomain-specific tasks. In this paper, we validate and adapt our previously\ndeveloped CNN, called Decompose, Transfer, and Compose (DeTraC), for the\nclassification of COVID-19 chest X-ray images. DeTraC can deal with any\nirregularities in the image dataset by investigating its class boundaries using\na class decomposition mechanism. The experimental results showed the capability\nof DeTraC in the detection of COVID-19 cases from a comprehensive image dataset\ncollected from several hospitals around the world. High accuracy of 95.12%\n(with a sensitivity of 97.91%, a specificity of 91.87%, and a precision of\n93.36%) was achieved by DeTraC in the detection of COVID-19 X-ray images from\nnormal, and severe acute respiratory syndrome cases.",
          "arxiv_id": "2003.13815v3"
        },
        {
          "title": "A review: Deep learning for medical image segmentation using multi-modality fusion",
          "year": "2020-04",
          "abstract": "Multi-modality is widely used in medical imaging, because it can provide\nmultiinformation about a target (tumor, organ or tissue). Segmentation using\nmultimodality consists of fusing multi-information to improve the segmentation.\nRecently, deep learning-based approaches have presented the state-of-the-art\nperformance in image classification, segmentation, object detection and\ntracking tasks. Due to their self-learning and generalization ability over\nlarge amounts of data, deep learning recently has also gained great interest in\nmulti-modal medical image segmentation. In this paper, we give an overview of\ndeep learning-based approaches for multi-modal medical image segmentation task.\nFirstly, we introduce the general principle of deep learning and multi-modal\nmedical image segmentation. Secondly, we present different deep learning\nnetwork architectures, then analyze their fusion strategies and compare their\nresults. The earlier fusion is commonly used, since it's simple and it focuses\non the subsequent segmentation network architecture. However, the later fusion\ngives more attention on fusion strategy to learn the complex relationship\nbetween different modalities. In general, compared to the earlier fusion, the\nlater fusion can give more accurate result if the fusion method is effective\nenough. We also discuss some common problems in medical image segmentation.\nFinally, we summarize and provide some perspectives on the future research.",
          "arxiv_id": "2004.10664v2"
        }
      ],
      "24": [
        {
          "title": "Tractable Function-Space Variational Inference in Bayesian Neural Networks",
          "year": "2023-12",
          "abstract": "Reliable predictive uncertainty estimation plays an important role in\nenabling the deployment of neural networks to safety-critical settings. A\npopular approach for estimating the predictive uncertainty of neural networks\nis to define a prior distribution over the network parameters, infer an\napproximate posterior distribution, and use it to make stochastic predictions.\nHowever, explicit inference over neural network parameters makes it difficult\nto incorporate meaningful prior information about the data-generating process\ninto the model. In this paper, we pursue an alternative approach. Recognizing\nthat the primary object of interest in most settings is the distribution over\nfunctions induced by the posterior distribution over neural network parameters,\nwe frame Bayesian inference in neural networks explicitly as inferring a\nposterior distribution over functions and propose a scalable function-space\nvariational inference method that allows incorporating prior information and\nresults in reliable predictive uncertainty estimates. We show that the proposed\nmethod leads to state-of-the-art uncertainty estimation and predictive\nperformance on a range of prediction tasks and demonstrate that it performs\nwell on a challenging safety-critical medical diagnosis task in which reliable\nuncertainty estimation is essential.",
          "arxiv_id": "2312.17199v1"
        },
        {
          "title": "A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness",
          "year": "2022-05",
          "abstract": "Accurate uncertainty quantification is a major challenge in deep learning, as\nneural networks can make overconfident errors and assign high confidence\npredictions to out-of-distribution (OOD) inputs. The most popular approaches to\nestimate predictive uncertainty in deep learning are methods that combine\npredictions from multiple neural networks, such as Bayesian neural networks\n(BNNs) and deep ensembles. However their practicality in real-time,\nindustrial-scale applications are limited due to the high memory and\ncomputational cost. Furthermore, ensembles and BNNs do not necessarily fix all\nthe issues with the underlying member networks. In this work, we study\nprincipled approaches to improve uncertainty property of a single network,\nbased on a single, deterministic representation. By formalizing the uncertainty\nquantification as a minimax learning problem, we first identify distance\nawareness, i.e., the model's ability to quantify the distance of a testing\nexample from the training data, as a necessary condition for a DNN to achieve\nhigh-quality (i.e., minimax optimal) uncertainty estimation. We then propose\nSpectral-normalized Neural Gaussian Process (SNGP), a simple method that\nimproves the distance-awareness ability of modern DNNs with two simple changes:\n(1) applying spectral normalization to hidden weights to enforce bi-Lipschitz\nsmoothness in representations and (2) replacing the last output layer with a\nGaussian process layer. On a suite of vision and language understanding\nbenchmarks, SNGP outperforms other single-model approaches in prediction,\ncalibration and out-of-domain detection. Furthermore, SNGP provides\ncomplementary benefits to popular techniques such as deep ensembles and data\naugmentation, making it a simple and scalable building block for probabilistic\ndeep learning. Code is open-sourced at\nhttps://github.com/google/uncertainty-baselines",
          "arxiv_id": "2205.00403v2"
        },
        {
          "title": "Hybrid Bayesian Neural Networks with Functional Probabilistic Layers",
          "year": "2021-07",
          "abstract": "Bayesian neural networks provide a direct and natural way to extend standard\ndeep neural networks to support probabilistic deep learning through the use of\nprobabilistic layers that, traditionally, encode weight (and bias) uncertainty.\nIn particular, hybrid Bayesian neural networks utilize standard deterministic\nlayers together with few probabilistic layers judicially positioned in the\nnetworks for uncertainty estimation. A major aspect and benefit of Bayesian\ninference is that priors, in principle, provide the means to encode prior\nknowledge for use in inference and prediction. However, it is difficult to\nspecify priors on weights since the weights have no intuitive interpretation.\nFurther, the relationships of priors on weights to the functions computed by\nnetworks are difficult to characterize. In contrast, functions are intuitive to\ninterpret and are direct since they map inputs to outputs. Therefore, it is\nnatural to specify priors on functions to encode prior knowledge, and to use\nthem in inference and prediction based on functions. To support this, we\npropose hybrid Bayesian neural networks with functional probabilistic layers\nthat encode function (and activation) uncertainty. We discuss their foundations\nin functional Bayesian inference, functional variational inference, sparse\nGaussian processes, and sparse variational Gaussian processes. We further\nperform few proof-of-concept experiments using GPflus, a new library that\nprovides Gaussian process layers and supports their use with deterministic\nKeras layers to form hybrid neural network and Gaussian process models.",
          "arxiv_id": "2107.07014v1"
        }
      ],
      "25": [
        {
          "title": "A Limitation of the PAC-Bayes Framework",
          "year": "2020-06",
          "abstract": "PAC-Bayes is a useful framework for deriving generalization bounds which was\nintroduced by McAllester ('98). This framework has the flexibility of deriving\ndistribution- and algorithm-dependent bounds, which are often tighter than\nVC-related uniform convergence bounds. In this manuscript we present a\nlimitation for the PAC-Bayes framework. We demonstrate an easy learning task\nthat is not amenable to a PAC-Bayes analysis.\n  Specifically, we consider the task of linear classification in 1D; it is\nwell-known that this task is learnable using just $O(\\log(1/\\delta)/\\epsilon)$\nexamples. On the other hand, we show that this fact can not be proved using a\nPAC-Bayes analysis: for any algorithm that learns 1-dimensional linear\nclassifiers there exists a (realizable) distribution for which the PAC-Bayes\nbound is arbitrarily large.",
          "arxiv_id": "2006.13508v3"
        },
        {
          "title": "How Tight Can PAC-Bayes be in the Small Data Regime?",
          "year": "2021-06",
          "abstract": "In this paper, we investigate the question: Given a small number of\ndatapoints, for example N = 30, how tight can PAC-Bayes and test set bounds be\nmade? For such small datasets, test set bounds adversely affect generalisation\nperformance by withholding data from the training procedure. In this setting,\nPAC-Bayes bounds are especially attractive, due to their ability to use all the\ndata to simultaneously learn a posterior and bound its generalisation risk. We\nfocus on the case of i.i.d. data with a bounded loss and consider the generic\nPAC-Bayes theorem of Germain et al. While their theorem is known to recover\nmany existing PAC-Bayes bounds, it is unclear what the tightest bound derivable\nfrom their framework is. For a fixed learning algorithm and dataset, we show\nthat the tightest possible bound coincides with a bound considered by Catoni;\nand, in the more natural case of distributions over datasets, we establish a\nlower bound on the best bound achievable in expectation. Interestingly, this\nlower bound recovers the Chernoff test set bound if the posterior is equal to\nthe prior. Moreover, to illustrate how tight these bounds can be, we study\nsynthetic one-dimensional classification tasks in which it is feasible to\nmeta-learn both the prior and the form of the bound to numerically optimise for\nthe tightest bounds possible. We find that in this simple, controlled scenario,\nPAC-Bayes bounds are competitive with comparable, commonly used Chernoff test\nset bounds. However, the sharpest test set bounds still lead to better\nguarantees on the generalisation error than the PAC-Bayes bounds we consider.",
          "arxiv_id": "2106.03542v4"
        },
        {
          "title": "PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes",
          "year": "2021-06",
          "abstract": "We give a novel, unified derivation of conditional PAC-Bayesian and mutual\ninformation (MI) generalization bounds. We derive conditional MI bounds as an\ninstance, with special choice of prior, of conditional MAC-Bayesian (Mean\nApproximately Correct) bounds, itself derived from conditional PAC-Bayesian\nbounds, where `conditional' means that one can use priors conditioned on a\njoint training and ghost sample. This allows us to get nontrivial PAC-Bayes and\nMI-style bounds for general VC classes, something recently shown to be\nimpossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get\nfaster rates of order $O \\left(({\\text{KL}}/n)^{\\gamma}\\right)$ for $\\gamma >\n1/2$ if a Bernstein condition holds and for exp-concave losses (with\n$\\gamma=1$), which is impossible with both standard PAC-Bayes generalization\nand MI bounds. Our work extends the recent work by Steinke and Zakynthinou\n[2020] who handle MI with VC but neither PAC-Bayes nor fast rates, the recent\nwork of Hellstr\\\"om and Durisi [2020] who extend the latter to the PAC-Bayes\nsetting via a unifying exponential inequality, and Mhammedi et al. [2019] who\ninitiated fast rate PAC-Bayes generalization error bounds but handle neither MI\nnor general VC classes.",
          "arxiv_id": "2106.09683v1"
        }
      ],
      "26": [
        {
          "title": "Solar: $L_0$ solution path averaging for fast and accurate variable selection in high-dimensional data",
          "year": "2020-07",
          "abstract": "We propose a new variable selection algorithm, subsample-ordered least-angle\nregression (solar), and its coordinate descent generalization, solar-cd. Solar\nre-constructs lasso paths using the $L_0$ norm and averages the resulting\nsolution paths across subsamples. Path averaging retains the ranking\ninformation of the informative variables while averaging out sensitivity to\nhigh dimensionality, improving variable selection stability, efficiency, and\naccuracy. We prove that: (i) with a high probability, path averaging perfectly\nseparates informative variables from redundant variables on the average $L_0$\npath; (ii) solar variable selection is consistent and accurate; and (iii) the\nprobability that solar omits weak signals is controllable for finite sample\nsize. We also demonstrate that: (i) solar yields, with less than $1/3$ of the\nlasso computation load, substantial improvements over lasso in terms of the\nsparsity (64-84\\% reduction in redundant variable selection) and accuracy of\nvariable selection; (ii) compared with the lasso safe/strong rule and variable\nscreening, solar largely avoids selection of redundant variables and rejection\nof informative variables in the presence of complicated dependence structures;\n(iii) the sparsity and stability of solar conserves residual degrees of freedom\nfor data-splitting hypothesis testing, improving the accuracy of post-selection\ninference on weak signals with limited $n$; (iv) replacing lasso with solar in\nbootstrap selection (e.g., bolasso or stability selection) produces a\nmulti-layer variable ranking scheme that improves selection sparsity and\nranking accuracy with the computation load of only one lasso realization; and\n(v) given the computation resources, solar bootstrap selection is substantially\nfaster (98\\% lower computation time) than the theoretical maximum speedup for\nparallelized bootstrap lasso (confirmed by Amdahl's law).",
          "arxiv_id": "2007.15707v3"
        },
        {
          "title": "Stability Selection via Variable Decorrelation",
          "year": "2025-05",
          "abstract": "The Lasso is a prominent algorithm for variable selection. However, its\ninstability in the presence of correlated variables in the high-dimensional\nsetting is well-documented. Although previous research has attempted to address\nthis issue by modifying the Lasso loss function, this paper introduces an\napproach that simplifies the data processed by Lasso. We propose that\ndecorrelating variables before applying the Lasso improves the stability of\nvariable selection regardless of the direction of correlation among predictors.\nFurthermore, we highlight that the irrepresentable condition, which ensures\nconsistency for the Lasso, is satisfied after variable decorrelation under two\nassumptions. In addition, by noting that the instability of the Lasso is not\nlimited to high-dimensional settings, we demonstrate the effectiveness of the\nproposed approach for low-dimensional data. Finally, we present empirical\nresults that indicate the efficacy of the proposed method across different\nvariable selection techniques, highlighting its potential for broader\napplication. The DVS R package is developed to facilitate the implementation of\nthe methodology proposed in this paper.",
          "arxiv_id": "2505.20864v1"
        },
        {
          "title": "The Adaptive $$-Lasso: Robustness and Oracle Properties",
          "year": "2023-04",
          "abstract": "This paper introduces a new regularized version of the robust\n$\\tau$-regression estimator for analyzing high-dimensional datasets subject to\ngross contamination in the response variables and covariates. The resulting\nestimator, termed adaptive $\\tau$-Lasso, is robust to outliers and\nhigh-leverage points. It also incorporates an adaptive $\\ell_1$-norm penalty\nterm, which enables the selection of relevant variables and reduces the bias\nassociated with large true regression coefficients. More specifically, this\nadaptive $\\ell_1$-norm penalty term assigns a weight to each regression\ncoefficient. For a fixed number of predictors $p$, we show that the adaptive\n$\\tau$-Lasso has the oracle property, ensuring both variable-selection\nconsistency and asymptotic normality. Asymptotic normality applies only to the\nentries of the regression vector corresponding to the true support, assuming\nknowledge of the true regression vector support. We characterize its robustness\nby establishing the finite-sample breakdown point and the influence function.\nWe carry out extensive simulations and observe that the class of $\\tau$-Lasso\nestimators exhibits robustness and reliable performance in both contaminated\nand uncontaminated data settings. We also validate our theoretical findings on\nrobustness properties through simulations. In the face of outliers and\nhigh-leverage points, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators\nachieve the best performance or match the best performances of competing\nregularized estimators, with minimal or no loss in terms of prediction and\nvariable selection accuracy for almost all scenarios considered in this study.\nTherefore, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators provide\nattractive tools for a variety of sparse linear regression problems,\nparticularly in high-dimensional settings and when the data is contaminated by\noutliers and high-leverage points.",
          "arxiv_id": "2304.09310v5"
        }
      ],
      "27": [
        {
          "title": "Distributed Clustering based on Distributional Kernel",
          "year": "2024-09",
          "abstract": "This paper introduces a new framework for clustering in a distributed network\ncalled Distributed Clustering based on Distributional Kernel (K) or KDC that\nproduces the final clusters based on the similarity with respect to the\ndistributions of initial clusters, as measured by K. It is the only framework\nthat satisfies all three of the following properties. First, KDC guarantees\nthat the combined clustering outcome from all sites is equivalent to the\nclustering outcome of its centralized counterpart from the combined dataset\nfrom all sites. Second, the maximum runtime cost of any site in distributed\nmode is smaller than the runtime cost in centralized mode. Third, it is\ndesigned to discover clusters of arbitrary shapes, sizes and densities. To the\nbest of our knowledge, this is the first distributed clustering framework that\nemploys a distributional kernel. The distribution-based clustering leads\ndirectly to significantly better clustering outcomes than existing methods of\ndistributed clustering. In addition, we introduce a new clustering algorithm\ncalled Kernel Bounded Cluster Cores, which is the best clustering algorithm\napplied to KDC among existing clustering algorithms. We also show that KDC is a\ngeneric framework that enables a quadratic time clustering algorithm to deal\nwith large datasets that would otherwise be impossible.",
          "arxiv_id": "2409.09418v1"
        },
        {
          "title": "Convex Clustering through MM: An Efficient Algorithm to Perform Hierarchical Clustering",
          "year": "2022-11",
          "abstract": "Convex clustering is a modern method with both hierarchical and $k$-means\nclustering characteristics. Although convex clustering can capture complex\nclustering structures hidden in data, the existing convex clustering algorithms\nare not scalable to large data sets with sample sizes greater than several\nthousands. Moreover, it is known that convex clustering sometimes fails to\nproduce a complete hierarchical clustering structure. This issue arises if\nclusters split up or the minimum number of possible clusters is larger than the\ndesired number of clusters. In this paper, we propose convex clustering through\nmajorization-minimization (CCMM) -- an iterative algorithm that uses cluster\nfusions and a highly efficient updating scheme derived using diagonal\nmajorization. Additionally, we explore different strategies to ensure that the\nhierarchical clustering structure terminates in a single cluster. With a\ncurrent desktop computer, CCMM efficiently solves convex clustering problems\nfeaturing over one million objects in seven-dimensional space, achieving a\nsolution time of 51 seconds on average.",
          "arxiv_id": "2211.01877v2"
        },
        {
          "title": "K-expectiles clustering",
          "year": "2021-03",
          "abstract": "$K$-means clustering is one of the most widely-used partitioning algorithm in\ncluster analysis due to its simplicity and computational efficiency. However,\n$K$-means does not provide an appropriate clustering result when applying to\ndata with non-spherically shaped clusters. We propose a novel partitioning\nclustering algorithm based on expectiles. The cluster centers are defined as\nmultivariate expectiles and clusters are searched via a greedy algorithm by\nminimizing the within cluster '$\\tau$ -variance'. We suggest two schemes: fixed\n$\\tau$ clustering, and adaptive $\\tau$ clustering. Validated by simulation\nresults, this method beats both $K$-means and spectral clustering on data with\nasymmetric shaped clusters, or clusters with a complicated structure, including\nasymmetric normal, beta, skewed $t$ and $F$ distributed clusters. Applications\nof adaptive $\\tau$ clustering on crypto-currency (CC) market data are provided.\nOne finds that the expectiles clusters of CC markets show the phenomena of an\ninstitutional investors dominated market. The second application is on image\nsegmentation. compared to other center based clustering methods, the adaptive\n$\\tau$ cluster centers of pixel data can better capture and describe the\nfeatures of an image. The fixed $\\tau$ clustering brings more flexibility on\nsegmentation with a decent accuracy.",
          "arxiv_id": "2103.09329v1"
        }
      ],
      "28": [
        {
          "title": "Trees, forests, and impurity-based variable importance",
          "year": "2020-01",
          "abstract": "Tree ensemble methods such as random forests [Breiman, 2001] are very popular\nto handle high-dimensional tabular data sets, notably because of their good\npredictive accuracy. However, when machine learning is used for decision-making\nproblems, settling for the best predictive procedures may not be reasonable\nsince enlightened decisions require an in-depth comprehension of the algorithm\nprediction process. Unfortunately, random forests are not intrinsically\ninterpretable since their prediction results from averaging several hundreds of\ndecision trees. A classic approach to gain knowledge on this so-called\nblack-box algorithm is to compute variable importances, that are employed to\nassess the predictive impact of each input variable. Variable importances are\nthen used to rank or select variables and thus play a great role in data\nanalysis. Nevertheless, there is no justification to use random forest variable\nimportances in such way: we do not even know what these quantities estimate. In\nthis paper, we analyze one of the two well-known random forest variable\nimportances, the Mean Decrease Impurity (MDI). We prove that if input variables\nare independent and in absence of interactions, MDI provides a variance\ndecomposition of the output, where the contribution of each variable is clearly\nidentified. We also study models exhibiting dependence between input variables\nor interaction, for which the variable importance is intrinsically ill-defined.\nOur analysis shows that there may exist some benefits to use a forest compared\nto a single tree.",
          "arxiv_id": "2001.04295v3"
        },
        {
          "title": "Binary Classification: Is Boosting stronger than Bagging?",
          "year": "2024-10",
          "abstract": "Random Forests have been one of the most popular bagging methods in the past\nfew decades, especially due to their success at handling tabular datasets. They\nhave been extensively studied and compared to boosting models, like XGBoost,\nwhich are generally considered more performant. Random Forests adopt several\nsimplistic assumptions, such that all samples and all trees that form the\nforest are equally important for building the final model. We introduce\nEnhanced Random Forests, an extension of vanilla Random Forests with extra\nfunctionalities and adaptive sample and model weighting. We develop an\niterative algorithm for adapting the training sample weights, by favoring the\nhardest examples, and an approach for finding personalized tree weighting\nschemes for each new sample. Our method significantly improves upon regular\nRandom Forests across 15 different binary classification datasets and\nconsiderably outperforms other tree methods, including XGBoost, when run with\ndefault hyperparameters, which indicates the robustness of our approach across\ndatasets, without the need for extensive hyperparameter tuning. Our\ntree-weighting methodology results in enhanced or comparable performance to the\nuniformly weighted ensemble, and is, more importantly, leveraged to define\nimportance scores for trees based on their contributions to classifying each\nnew sample. This enables us to only focus on a small number of trees as the\nmain models that define the outcome of a new sample and, thus, to partially\nrecover interpretability, which is critically missing from both bagging and\nboosting methods. In binary classification problems, the proposed extensions\nand the corresponding results suggest the equivalence of bagging and boosting\nmethods in performance, and the edge of bagging in interpretability by\nleveraging a few learners of the ensemble, which is not an option in the less\nexplainable boosting methods.",
          "arxiv_id": "2410.19200v1"
        },
        {
          "title": "Boosting-Based Sequential Meta-Tree Ensemble Construction for Improved Decision Trees",
          "year": "2024-02",
          "abstract": "A decision tree is one of the most popular approaches in machine learning\nfields. However, it suffers from the problem of overfitting caused by overly\ndeepened trees. Then, a meta-tree is recently proposed. It solves the problem\nof overfitting caused by overly deepened trees. Moreover, the meta-tree\nguarantees statistical optimality based on Bayes decision theory. Therefore,\nthe meta-tree is expected to perform better than the decision tree. In contrast\nto a single decision tree, it is known that ensembles of decision trees, which\nare typically constructed boosting algorithms, are more effective in improving\npredictive performance. Thus, it is expected that ensembles of meta-trees are\nmore effective in improving predictive performance than a single meta-tree, and\nthere are no previous studies that construct multiple meta-trees in boosting.\nTherefore, in this study, we propose a method to construct multiple meta-trees\nusing a boosting approach. Through experiments with synthetic and benchmark\ndatasets, we conduct a performance comparison between the proposed methods and\nthe conventional methods using ensembles of decision trees. Furthermore, while\nensembles of decision trees can cause overfitting as well as a single decision\ntree, experiments confirmed that ensembles of meta-trees can prevent\noverfitting due to the tree depth.",
          "arxiv_id": "2402.06386v1"
        }
      ],
      "29": [
        {
          "title": "DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation",
          "year": "2020-01",
          "abstract": "Social recommendation has emerged to leverage social connections among users\nfor predicting users' unknown preferences, which could alleviate the data\nsparsity issue in collaborative filtering based recommendation. Early\napproaches relied on utilizing each user's first-order social neighbors'\ninterests for better user modeling and failed to model the social influence\ndiffusion process from the global social network structure. Recently, we\npropose a preliminary work of a neural influence diffusion network (i.e.,\nDiffNet) for social recommendation (Diffnet), which models the recursive social\ndiffusion process to capture the higher-order relationships for each user.\nHowever, we argue that, as users play a central role in both user-user social\nnetwork and user-item interest network, only modeling the influence diffusion\nprocess in the social network would neglect the users' latent collaborative\ninterests in the user-item interest network. In this paper, we propose\nDiffNet++, an improved algorithm of DiffNet that models the neural influence\ndiffusion and interest diffusion in a unified framework. By reformulating the\nsocial recommendation as a heterogeneous graph with social network and interest\nnetwork as input, DiffNet++ advances DiffNet by injecting these two network\ninformation for user embedding learning at the same time. This is achieved by\niteratively aggregating each user's embedding from three aspects: the user's\nprevious embedding, the influence aggregation of social neighbors from the\nsocial network, and the interest aggregation of item neighbors from the\nuser-item interest network. Furthermore, we design a multi-level attention\nnetwork that learns how to attentively aggregate user embeddings from these\nthree aspects. Finally, extensive experimental results on two real-world\ndatasets clearly show the effectiveness of our proposed model.",
          "arxiv_id": "2002.00844v4"
        },
        {
          "title": "Controllable Multi-Interest Framework for Recommendation",
          "year": "2020-05",
          "abstract": "Recently, neural networks have been widely used in e-commerce recommender\nsystems, owing to the rapid development of deep learning. We formalize the\nrecommender system as a sequential recommendation problem, intending to predict\nthe next items that the user might be interacted with. Recent works usually\ngive an overall embedding from a user's behavior sequence. However, a unified\nuser embedding cannot reflect the user's multiple interests during a period. In\nthis paper, we propose a novel controllable multi-interest framework for the\nsequential recommendation, called ComiRec. Our multi-interest module captures\nmultiple interests from user behavior sequences, which can be exploited for\nretrieving candidate items from the large-scale item pool. These items are then\nfed into an aggregation module to obtain the overall recommendation. The\naggregation module leverages a controllable factor to balance the\nrecommendation accuracy and diversity. We conduct experiments for the\nsequential recommendation on two real-world datasets, Amazon and Taobao.\nExperimental results demonstrate that our framework achieves significant\nimprovements over state-of-the-art models. Our framework has also been\nsuccessfully deployed on the offline Alibaba distributed cloud platform.",
          "arxiv_id": "2005.09347v2"
        },
        {
          "title": "FINN.no Slates Dataset: A new Sequential Dataset Logging Interactions, allViewed Items and Click Responses/No-Click for Recommender Systems Research",
          "year": "2021-11",
          "abstract": "We present a novel recommender systems dataset that records the sequential\ninteractions between users and an online marketplace. The users are\nsequentially presented with both recommendations and search results in the form\nof ranked lists of items, called slates, from the marketplace. The dataset\nincludes the presented slates at each round, whether the user clicked on any of\nthese items and which item the user clicked on. Although the usage of exposure\ndata in recommender systems is growing, to our knowledge there is no open\nlarge-scale recommender systems dataset that includes the slates of items\npresented to the users at each interaction. As a result, most articles on\nrecommender systems do not utilize this exposure information. Instead, the\nproposed models only depend on the user's click responses, and assume that the\nuser is exposed to all the items in the item universe at each step, often\ncalled uniform candidate sampling. This is an incomplete assumption, as it\ntakes into account items the user might not have been exposed to. This way\nitems might be incorrectly considered as not of interest to the user. Taking\ninto account the actually shown slates allows the models to use a more natural\nlikelihood, based on the click probability given the exposure set of items, as\nis prevalent in the bandit and reinforcement learning literature.\n\\cite{Eide2021DynamicSampling} shows that likelihoods based on uniform\ncandidate sampling (and similar assumptions) are implicitly assuming that the\nplatform only shows the most relevant items to the user. This causes the\nrecommender system to implicitly reinforce feedback loops and to be biased\ntowards previously exposed items to the user.",
          "arxiv_id": "2111.03340v1"
        }
      ],
      "30": [
        {
          "title": "AudioPaLM: A Large Language Model That Can Speak and Listen",
          "year": "2023-06",
          "abstract": "We introduce AudioPaLM, a large language model for speech understanding and\ngeneration. AudioPaLM fuses text-based and speech-based language models, PaLM-2\n[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified\nmultimodal architecture that can process and generate text and speech with\napplications including speech recognition and speech-to-speech translation.\nAudioPaLM inherits the capability to preserve paralinguistic information such\nas speaker identity and intonation from AudioLM and the linguistic knowledge\npresent only in text large language models such as PaLM-2. We demonstrate that\ninitializing AudioPaLM with the weights of a text-only large language model\nimproves speech processing, successfully leveraging the larger quantity of text\ntraining data used in pretraining to assist with the speech tasks. The\nresulting model significantly outperforms existing systems for speech\ntranslation tasks and has the ability to perform zero-shot speech-to-text\ntranslation for many languages for which input/target language combinations\nwere not seen in training. AudioPaLM also demonstrates features of audio\nlanguage models, such as transferring a voice across languages based on a short\nspoken prompt. We release examples of our method at\nhttps://google-research.github.io/seanet/audiopalm/examples",
          "arxiv_id": "2306.12925v1"
        },
        {
          "title": "Speech Enhancement using Self-Adaptation and Multi-Head Self-Attention",
          "year": "2020-02",
          "abstract": "This paper investigates a self-adaptation method for speech enhancement using\nauxiliary speaker-aware features; we extract a speaker representation used for\nadaptation directly from the test utterance. Conventional studies of deep\nneural network (DNN)--based speech enhancement mainly focus on building a\nspeaker independent model. Meanwhile, in speech applications including speech\nrecognition and synthesis, it is known that model adaptation to the target\nspeaker improves the accuracy. Our research question is whether a DNN for\nspeech enhancement can be adopted to unknown speakers without any auxiliary\nguidance signal in test-phase. To achieve this, we adopt multi-task learning of\nspeech enhancement and speaker identification, and use the output of the final\nhidden layer of speaker identification branch as an auxiliary feature. In\naddition, we use multi-head self-attention for capturing long-term dependencies\nin the speech and noise. Experimental results on a public dataset show that our\nstrategy achieves the state-of-the-art performance and also outperform\nconventional methods in terms of subjective quality.",
          "arxiv_id": "2002.05873v1"
        },
        {
          "title": "AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition",
          "year": "2023-09",
          "abstract": "Audio-visual speech contains synchronized audio and visual information that\nprovides cross-modal supervision to learn representations for both automatic\nspeech recognition (ASR) and visual speech recognition (VSR). We introduce\ncontinuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a\nsemi-supervised method to train an audio-visual speech recognition (AVSR) model\non a combination of labeled and unlabeled videos with continuously regenerated\npseudo-labels. Our models are trained for speech recognition from audio-visual\ninputs and can perform speech recognition using both audio and visual\nmodalities, or only one modality. Our method uses the same audio-visual model\nfor both supervised training and pseudo-label generation, mitigating the need\nfor external speech recognition models to generate pseudo-labels. AV-CPL\nobtains significant improvements in VSR performance on the LRS3 dataset while\nmaintaining practical ASR and AVSR performance. Finally, using visual-only\nspeech data, our method is able to leverage unlabeled visual speech to improve\nVSR.",
          "arxiv_id": "2309.17395v1"
        }
      ],
      "31": [
        {
          "title": "Enhancing variational quantum algorithms by balancing training on classical and quantum hardware",
          "year": "2025-03",
          "abstract": "Quantum computers offer a promising route to tackling problems that are\nclassically intractable such as in prime-factorization, solving large-scale\nlinear algebra and simulating complex quantum systems, but potentially require\nfault-tolerant quantum hardware. On the other hand, variational quantum\nalgorithms (VQAs) are a promising approach for leveraging near-term quantum\ncomputers to solve complex problems. However, there remain major challenges in\ntheir trainability and resource costs on quantum hardware. Here we address\nthese challenges by adopting Hardware Efficient and dynamical LIe algebra\nsupported Ansatz (HELIA), and propose two training methods that combine an\nexisting classical-enhanced g-sim method and the quantum-based Parameter-Shift\nRule (PSR). Our improvement comes from distributing the resources required for\ngradient estimation and training to both classical and quantum hardware. We\nnumerically evaluate our approach for ground-state estimation of 6 to 18-qubit\nHamiltonians using the Variational Quantum Eigensolver (VQE) and quantum phase\nclassification for up to 12-qubit Hamiltonians using quantum neural networks.\nFor VQE, our method achieves higher accuracy and success rates, with an average\nreduction in quantum hardware calls of up to 60% compared to purely\nquantum-based PSR. For classification, we observe test accuracy improvements of\nup to 2.8%. We also numerically demonstrate the capability of HELIA in\nmitigating barren plateaus, paving the way for training large-scale quantum\nmodels.",
          "arxiv_id": "2503.16361v2"
        },
        {
          "title": "Universal Approximation Property of Quantum Machine Learning Models in Quantum-Enhanced Feature Spaces",
          "year": "2020-09",
          "abstract": "Encoding classical data into quantum states is considered a quantum feature\nmap to map classical data into a quantum Hilbert space. This feature map\nprovides opportunities to incorporate quantum advantages into machine learning\nalgorithms to be performed on near-term intermediate-scale quantum computers.\nThe crucial idea is using the quantum Hilbert space as a quantum-enhanced\nfeature space in machine learning models. While the quantum feature map has\ndemonstrated its capability when combined with linear classification models in\nsome specific applications, its expressive power from the theoretical\nperspective remains unknown. We prove that the machine learning models induced\nfrom the quantum-enhanced feature space are universal approximators of\ncontinuous functions under typical quantum feature maps. We also study the\ncapability of quantum feature maps in the classification of disjoint regions.\nOur work enables an important theoretical analysis to ensure that machine\nlearning algorithms based on quantum feature maps can handle a broad class of\nmachine learning tasks. In light of this, one can design a quantum machine\nlearning model with more powerful expressivity.",
          "arxiv_id": "2009.00298v3"
        },
        {
          "title": "Supervised quantum machine learning models are kernel methods",
          "year": "2021-01",
          "abstract": "With near-term quantum devices available and the race for fault-tolerant\nquantum computers in full swing, researchers became interested in the question\nof what happens if we replace a supervised machine learning model with a\nquantum circuit. While such \"quantum models\" are sometimes called \"quantum\nneural networks\", it has been repeatedly noted that their mathematical\nstructure is actually much more closely related to kernel methods: they analyse\ndata in high-dimensional Hilbert spaces to which we only have access through\ninner products revealed by measurements. This technical manuscript summarises\nand extends the idea of systematically rephrasing supervised quantum models as\na kernel method. With this, a lot of near-term and fault-tolerant quantum\nmodels can be replaced by a general support vector machine whose kernel\ncomputes distances between data-encoding quantum states. Kernel-based training\nis then guaranteed to find better or equally good quantum models than\nvariational circuit training. Overall, the kernel perspective of quantum\nmachine learning tells us that the way that data is encoded into quantum states\nis the main ingredient that can potentially set quantum models apart from\nclassical machine learning models.",
          "arxiv_id": "2101.11020v2"
        }
      ],
      "32": [
        {
          "title": "Space Meets Time: Local Spacetime Neural Network For Traffic Flow Forecasting",
          "year": "2021-09",
          "abstract": "Traffic flow forecasting is a crucial task in urban computing. The challenge\narises as traffic flows often exhibit intrinsic and latent spatio-temporal\ncorrelations that cannot be identified by extracting the spatial and temporal\npatterns of traffic data separately. We argue that such correlations are\nuniversal and play a pivotal role in traffic flow. We put forward {spacetime\ninterval learning} as a paradigm to explicitly capture these correlations\nthrough a unified analysis of both spatial and temporal features. Unlike the\nstate-of-the-art methods, which are restricted to a particular road network, we\nmodel the universal spatio-temporal correlations that are transferable from\ncities to cities. To this end, we propose a new spacetime interval learning\nframework that constructs a local-spacetime context of a traffic sensor\ncomprising the data from its neighbors within close time points. Based on this\nidea, we introduce local spacetime neural network (STNN), which employs novel\nspacetime convolution and attention mechanism to learn the universal\nspatio-temporal correlations. The proposed STNN captures local traffic\npatterns, which does not depend on a specific network structure. As a result, a\ntrained STNN model can be applied on any unseen traffic networks. We evaluate\nthe proposed STNN on two public real-world traffic datasets and a simulated\ndataset on dynamic networks. The experiment results show that STNN not only\nimproves prediction accuracy by 4% over state-of-the-art methods, but is also\neffective in handling the case when the traffic network undergoes dynamic\nchanges as well as the superior generalization capability.",
          "arxiv_id": "2109.05225v2"
        },
        {
          "title": "PSTN: Periodic Spatial-temporal Deep Neural Network for Traffic Condition Prediction",
          "year": "2021-08",
          "abstract": "Accurate forecasting of traffic conditions is critical for improving safety,\nstability, and efficiency of a city transportation system. In reality, it is\nchallenging to produce accurate traffic forecasts due to the complex and\ndynamic spatiotemporal correlations. Most existing works only consider partial\ncharacteristics and features of traffic data, and result in unsatisfactory\nperformances on modeling and forecasting. In this paper, we propose a periodic\nspatial-temporal deep neural network (PSTN) with three pivotal modules to\nimprove the forecasting performance of traffic conditions through a novel\nintegration of three types of information. First, the historical traffic\ninformation is folded and fed into a module consisting of a graph convolutional\nnetwork and a temporal convolutional network. Second, the recent traffic\ninformation together with the historical output passes through the second\nmodule consisting of a graph convolutional network and a gated recurrent unit\nframework. Finally, a multi-layer perceptron is applied to process the\nauxiliary road attributes and output the final predictions. Experimental\nresults on two publicly accessible real-world urban traffic data sets show that\nthe proposed PSTN outperforms the state-of-the-art benchmarks by significant\nmargins for short-term traffic conditions forecasting",
          "arxiv_id": "2108.02424v1"
        },
        {
          "title": "Newell's theory based feature transformations for spatio-temporal traffic prediction",
          "year": "2023-07",
          "abstract": "Deep learning (DL) models for spatio-temporal traffic flow forecasting employ\nconvolutional or graph-convolutional filters along with recurrent neural\nnetworks to capture spatial and temporal dependencies in traffic data. These\nmodels, such as CNN-LSTM, utilize traffic flows from neighboring detector\nstations to predict flows at a specific location of interest. However, these\nmodels are limited in their ability to capture the broader dynamics of the\ntraffic system, as they primarily learn features specific to the detector\nconfiguration and traffic characteristics at the target location. Hence, the\ntransferability of these models to different locations becomes challenging,\nparticularly when data is unavailable at the new location for model training.\nTo address this limitation, we propose a traffic flow physics-based feature\ntransformation for spatio-temporal DL models. This transformation incorporates\nNewell's uncongested and congested-state estimators of traffic flows at the\ntarget locations, enabling the models to learn broader dynamics of the system.\nOur methodology is empirically validated using traffic data from two different\nlocations. The results demonstrate that the proposed feature transformation\nimproves the models' performance in predicting traffic flows over different\nprediction horizons, as indicated by better goodness-of-fit statistics. An\nimportant advantage of our framework is its ability to be transferred to new\nlocations where data is unavailable. This is achieved by appropriately\naccounting for spatial dependencies based on station distances and various\ntraffic parameters. In contrast, regular DL models are not easily transferable\nas their inputs remain fixed. It should be noted that due to data limitations,\nwe were unable to perform spatial sensitivity analysis, which calls for further\nresearch using simulated data.",
          "arxiv_id": "2307.05949v2"
        }
      ],
      "33": [
        {
          "title": "Depth Uncertainty Networks for Active Learning",
          "year": "2021-12",
          "abstract": "In active learning, the size and complexity of the training dataset changes\nover time. Simple models that are well specified by the amount of data\navailable at the start of active learning might suffer from bias as more points\nare actively sampled. Flexible models that might be well suited to the full\ndataset can suffer from overfitting towards the start of active learning. We\ntackle this problem using Depth Uncertainty Networks (DUNs), a BNN variant in\nwhich the depth of the network, and thus its complexity, is inferred. We find\nthat DUNs outperform other BNN variants on several active learning tasks.\nImportantly, we show that on the tasks in which DUNs perform best they present\nnotably less overfitting than baselines.",
          "arxiv_id": "2112.06796v2"
        },
        {
          "title": "Bayesian active learning for production, a systematic study and a reusable library",
          "year": "2020-06",
          "abstract": "Active learning is able to reduce the amount of labelling effort by using a\nmachine learning model to query the user for specific inputs.\n  While there are many papers on new active learning techniques, these\ntechniques rarely satisfy the constraints of a real-world project. In this\npaper, we analyse the main drawbacks of current active learning techniques and\nwe present approaches to alleviate them. We do a systematic study on the\neffects of the most common issues of real-world datasets on the deep active\nlearning process: model convergence, annotation error, and dataset imbalance.\nWe derive two techniques that can speed up the active learning loop such as\npartial uncertainty sampling and larger query size. Finally, we present our\nopen-source Bayesian active learning library, BaaL.",
          "arxiv_id": "2006.09916v1"
        },
        {
          "title": "Efficient Human-in-the-Loop Active Learning: A Novel Framework for Data Labeling in AI Systems",
          "year": "2024-12",
          "abstract": "Modern AI algorithms require labeled data. In real world, majority of data\nare unlabeled. Labeling the data are costly. this is particularly true for some\nareas requiring special skills, such as reading radiology images by physicians.\nTo most efficiently use expert's time for the data labeling, one promising\napproach is human-in-the-loop active learning algorithm. In this work, we\npropose a novel active learning framework with significant potential for\napplication in modern AI systems. Unlike the traditional active learning\nmethods, which only focus on determining which data point should be labeled,\nour framework also introduces an innovative perspective on incorporating\ndifferent query scheme. We propose a model to integrate the information from\ndifferent types of queries. Based on this model, our active learning frame can\nautomatically determine how the next question is queried. We further developed\na data driven exploration and exploitation framework into our active learning\nmethod. This method can be embedded in numerous active learning algorithms.\nThrough simulations on five real-world datasets, including a highly complex\nreal image task, our proposed active learning framework exhibits higher\naccuracy and lower loss compared to other methods.",
          "arxiv_id": "2501.00277v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T20:08:56Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}