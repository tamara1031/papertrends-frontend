{
  "topics": {
    "data": {
      "0": {
        "name": "0_type_theory_categories_category",
        "keywords": [
          [
            "type",
            0.032590196278740595
          ],
          [
            "theory",
            0.02833286629258582
          ],
          [
            "categories",
            0.023844143299859517
          ],
          [
            "category",
            0.020459244016696212
          ],
          [
            "types",
            0.017663502185460807
          ],
          [
            "type theory",
            0.017358021152398467
          ],
          [
            "lambda",
            0.013593326883798232
          ],
          [
            "calculus",
            0.013134706792763377
          ],
          [
            "categorical",
            0.01031595657331626
          ],
          [
            "semantics",
            0.010310959631056492
          ]
        ],
        "count": 980
      },
      "1": {
        "name": "1_model_systems_temporal_LTL",
        "keywords": [
          [
            "model",
            0.016807806217835924
          ],
          [
            "systems",
            0.01623809363280346
          ],
          [
            "temporal",
            0.014304101369979788
          ],
          [
            "LTL",
            0.013892463689057432
          ],
          [
            "checking",
            0.01314245792800095
          ],
          [
            "synthesis",
            0.013080185027821385
          ],
          [
            "safety",
            0.012698123138078119
          ],
          [
            "verification",
            0.011769668551450278
          ],
          [
            "problem",
            0.011599394871305567
          ],
          [
            "specification",
            0.011489173756100077
          ]
        ],
        "count": 850
      },
      "2": {
        "name": "2_logic_modal_proof_intuitionistic",
        "keywords": [
          [
            "logic",
            0.041965481910776634
          ],
          [
            "modal",
            0.03505922785696191
          ],
          [
            "proof",
            0.03275307126296278
          ],
          [
            "intuitionistic",
            0.028992486748904183
          ],
          [
            "logics",
            0.028563791676656344
          ],
          [
            "semantics",
            0.022005895369048754
          ],
          [
            "sequent",
            0.019080696775628892
          ],
          [
            "classical",
            0.0180987306517008
          ],
          [
            "cut",
            0.01792067455698954
          ],
          [
            "modal logic",
            0.016635577406215
          ]
        ],
        "count": 353
      },
      "3": {
        "name": "3_ASP_SAT_solvers_SMT",
        "keywords": [
          [
            "ASP",
            0.02646499833353212
          ],
          [
            "SAT",
            0.024980161692700308
          ],
          [
            "solvers",
            0.021049780524030746
          ],
          [
            "SMT",
            0.018275520249943052
          ],
          [
            "problems",
            0.015277782107575424
          ],
          [
            "solving",
            0.01519348352391
          ],
          [
            "solver",
            0.014999138918037504
          ],
          [
            "problem",
            0.014641338221053034
          ],
          [
            "Answer",
            0.013260620517662414
          ],
          [
            "Programming",
            0.0130040495303771
          ]
        ],
        "count": 332
      },
      "4": {
        "name": "4_programs_program_verification_memory",
        "keywords": [
          [
            "programs",
            0.027881848170820227
          ],
          [
            "program",
            0.025655532566171303
          ],
          [
            "verification",
            0.022237078270797593
          ],
          [
            "memory",
            0.015149487273213368
          ],
          [
            "analysis",
            0.013500225811282676
          ],
          [
            "data",
            0.013405388237884426
          ],
          [
            "code",
            0.013385298476345876
          ],
          [
            "correctness",
            0.012468061703251133
          ],
          [
            "logic",
            0.010593414218458292
          ],
          [
            "concurrent",
            0.010462926618938672
          ]
        ],
        "count": 314
      },
      "5": {
        "name": "5_reasoning_learning_models_explanations",
        "keywords": [
          [
            "reasoning",
            0.02560252246153945
          ],
          [
            "learning",
            0.02086377197415127
          ],
          [
            "models",
            0.02042157774888352
          ],
          [
            "explanations",
            0.01883863126778011
          ],
          [
            "neural",
            0.017324989458350357
          ],
          [
            "logical",
            0.015255413385706176
          ],
          [
            "knowledge",
            0.014238405961550401
          ],
          [
            "AI",
            0.014190798287305864
          ],
          [
            "data",
            0.013552786483058542
          ],
          [
            "language",
            0.012265118791787794
          ]
        ],
        "count": 296
      },
      "6": {
        "name": "6_theory_computable_complexity_numbers",
        "keywords": [
          [
            "theory",
            0.01981452683296923
          ],
          [
            "computable",
            0.018406317593436167
          ],
          [
            "complexity",
            0.018307629630759166
          ],
          [
            "numbers",
            0.014334937125875026
          ],
          [
            "functions",
            0.014051981873928606
          ],
          [
            "theorem",
            0.013969272030582245
          ],
          [
            "order",
            0.012448155300632422
          ],
          [
            "machines",
            0.012328759897490263
          ],
          [
            "proof",
            0.012004574244347527
          ],
          [
            "real",
            0.011462442309080471
          ]
        ],
        "count": 278
      },
      "7": {
        "name": "7_graphs_graph_classes_bounded",
        "keywords": [
          [
            "graphs",
            0.044902084080700316
          ],
          [
            "graph",
            0.038474921558664575
          ],
          [
            "classes",
            0.033398185151328454
          ],
          [
            "bounded",
            0.03141626563125594
          ],
          [
            "width",
            0.028511140035191982
          ],
          [
            "class",
            0.022802075495631974
          ],
          [
            "CSP",
            0.01931359568705458
          ],
          [
            "CSPs",
            0.018405369559969693
          ],
          [
            "order",
            0.018175708206836196
          ],
          [
            "problem",
            0.016305196348345517
          ]
        ],
        "count": 258
      },
      "8": {
        "name": "8_query_queries_ontology_ontologies",
        "keywords": [
          [
            "query",
            0.032119971418782416
          ],
          [
            "queries",
            0.029966475253683643
          ],
          [
            "ontology",
            0.02541215477244708
          ],
          [
            "ontologies",
            0.01952069897779478
          ],
          [
            "data",
            0.01823694165948093
          ],
          [
            "knowledge",
            0.018057470003068326
          ],
          [
            "DL",
            0.01365030926825884
          ],
          [
            "answering",
            0.01299822759065783
          ],
          [
            "database",
            0.012689399176953576
          ],
          [
            "conjunctive",
            0.011988459850828541
          ]
        ],
        "count": 251
      },
      "9": {
        "name": "9_epistemic_knowledge_logic_agents",
        "keywords": [
          [
            "epistemic",
            0.0412698658623488
          ],
          [
            "knowledge",
            0.033358765613061905
          ],
          [
            "logic",
            0.029072440547869404
          ],
          [
            "agents",
            0.025919796127227182
          ],
          [
            "agent",
            0.02082515644064773
          ],
          [
            "belief",
            0.019415772440633385
          ],
          [
            "model",
            0.016435509017167796
          ],
          [
            "epistemic logic",
            0.015690125136679663
          ],
          [
            "Epistemic",
            0.014240720524674249
          ],
          [
            "paper",
            0.013627053904194993
          ]
        ],
        "count": 222
      },
      "10": {
        "name": "10_quantum_Quantum_circuits_classical",
        "keywords": [
          [
            "quantum",
            0.11990744020871222
          ],
          [
            "Quantum",
            0.03904704460577053
          ],
          [
            "circuits",
            0.0256973508913592
          ],
          [
            "classical",
            0.020017857229777843
          ],
          [
            "quantum circuits",
            0.01683584235591289
          ],
          [
            "language",
            0.016207796808421614
          ],
          [
            "quantum programs",
            0.016145943082656326
          ],
          [
            "logic",
            0.01584310816477367
          ],
          [
            "reversible",
            0.015480066192466016
          ],
          [
            "circuit",
            0.014583225370932538
          ]
        ],
        "count": 198
      },
      "11": {
        "name": "11_proof_theorem_proofs_Isabelle",
        "keywords": [
          [
            "proof",
            0.036691565236535856
          ],
          [
            "theorem",
            0.026146249829672306
          ],
          [
            "proofs",
            0.02591093339794334
          ],
          [
            "Isabelle",
            0.023261054260477105
          ],
          [
            "proving",
            0.020969076362484254
          ],
          [
            "theorem proving",
            0.019632900431314264
          ],
          [
            "formal",
            0.018160215010919266
          ],
          [
            "learning",
            0.01766629782346573
          ],
          [
            "problems",
            0.01550807281487313
          ],
          [
            "language",
            0.013273227602485652
          ]
        ],
        "count": 182
      },
      "12": {
        "name": "12_automata_automaton_deterministic_words",
        "keywords": [
          [
            "automata",
            0.06392717807056393
          ],
          [
            "automaton",
            0.024854324814675647
          ],
          [
            "deterministic",
            0.022920280281851547
          ],
          [
            "words",
            0.02161867721807251
          ],
          [
            "regular",
            0.02062761151724171
          ],
          [
            "languages",
            0.019595703595443797
          ],
          [
            "finite",
            0.01910282609575975
          ],
          [
            "problem",
            0.01623248178127021
          ],
          [
            "weighted",
            0.016063217267205793
          ],
          [
            "infinite",
            0.015739771760420233
          ]
        ],
        "count": 171
      },
      "13": {
        "name": "13_neural_verification_networks_network",
        "keywords": [
          [
            "neural",
            0.049557944798129354
          ],
          [
            "verification",
            0.04274493114153191
          ],
          [
            "networks",
            0.03954298285627587
          ],
          [
            "network",
            0.033704053897647475
          ],
          [
            "neural networks",
            0.032191214738311706
          ],
          [
            "Neural",
            0.03199248146675929
          ],
          [
            "DNN",
            0.027208217433896717
          ],
          [
            "neural network",
            0.023633976334152762
          ],
          [
            "robustness",
            0.020447672490237812
          ],
          [
            "Networks",
            0.01956645010819747
          ]
        ],
        "count": 111
      },
      "14": {
        "name": "14_bisimilarity_process_Petri_nets",
        "keywords": [
          [
            "bisimilarity",
            0.06872299544540862
          ],
          [
            "process",
            0.03968663950683935
          ],
          [
            "Petri",
            0.03314804811073544
          ],
          [
            "nets",
            0.028562590444407034
          ],
          [
            "process algebra",
            0.0278944407538021
          ],
          [
            "concurrent",
            0.026959843730668384
          ],
          [
            "Petri nets",
            0.022904321548453514
          ],
          [
            "algebra",
            0.022679234673660207
          ],
          [
            "processes",
            0.020702705178188955
          ],
          [
            "reversibility",
            0.018949983884536097
          ]
        ],
        "count": 108
      },
      "15": {
        "name": "15_smart_contracts_blockchain_contract",
        "keywords": [
          [
            "smart",
            0.04414743971060983
          ],
          [
            "contracts",
            0.042920178119634264
          ],
          [
            "blockchain",
            0.037307931690245794
          ],
          [
            "contract",
            0.03209147004710827
          ],
          [
            "smart contracts",
            0.030167235412987752
          ],
          [
            "protocol",
            0.021402777886473354
          ],
          [
            "smart contract",
            0.020195637956440235
          ],
          [
            "verification",
            0.01847278830618135
          ],
          [
            "consensus",
            0.018171842063629363
          ],
          [
            "Ethereum",
            0.017951704302239305
          ]
        ],
        "count": 88
      }
    },
    "correlations": [
      [
        1.0,
        -0.4393436080226498,
        -0.6121649622102028,
        -0.7402650658848103,
        -0.703874510108139,
        -0.7045989358643789,
        -0.22243655053543598,
        -0.6854446616065492,
        -0.7412840392226474,
        -0.7153736831619115,
        -0.7339428073746199,
        -0.663966991189632,
        -0.6883779625461489,
        -0.7296670387326583,
        -0.7173230072106178,
        -0.7519628999092043
      ],
      [
        -0.4393436080226498,
        1.0,
        -0.6136545774347899,
        -0.7221255873078085,
        -0.6874940204193507,
        -0.6779942663863263,
        -0.2986484584962388,
        -0.6847645758690959,
        -0.7209892423878772,
        -0.6552657359894296,
        -0.741096017983977,
        -0.7018377223439438,
        -0.6425558448491183,
        -0.6699421206003526,
        -0.7070892301402594,
        -0.743131496621922
      ],
      [
        -0.6121649622102028,
        -0.6136545774347899,
        1.0,
        -0.7378842496652352,
        -0.6967525839738008,
        -0.6985100982499661,
        -0.6187820858973812,
        -0.6894736694844148,
        -0.7240561121150102,
        -0.47141487992450215,
        -0.7405743530497133,
        -0.5736751489273357,
        -0.7008133903172172,
        -0.7311503306273719,
        -0.7306559108385905,
        -0.7572849746152126
      ],
      [
        -0.7402650658848103,
        -0.7221255873078085,
        -0.7378842496652352,
        1.0,
        -0.7186095213098257,
        -0.7124313301029037,
        -0.7247829781763624,
        -0.7433193376657519,
        -0.7437861733670641,
        -0.717018026511484,
        -0.7590457942242961,
        -0.7433765547088722,
        -0.7481920651032177,
        -0.726064944293946,
        -0.7537671331324749,
        -0.7624748770134978
      ],
      [
        -0.703874510108139,
        -0.6874940204193507,
        -0.6967525839738008,
        -0.7186095213098257,
        1.0,
        -0.7051254387358241,
        -0.699270517682685,
        -0.734569026264253,
        -0.7403592745400354,
        -0.7202979630673761,
        -0.7500458826445628,
        -0.7037739762866961,
        -0.7358096523653156,
        -0.5748714984305892,
        -0.7405071735649135,
        -0.7328364759854396
      ],
      [
        -0.7045989358643789,
        -0.6779942663863263,
        -0.6985100982499661,
        -0.7124313301029037,
        -0.7051254387358241,
        1.0,
        -0.6980497964769049,
        -0.7383250393522593,
        -0.7094294494594356,
        -0.6798699737427583,
        -0.7586091293798385,
        -0.703508821713045,
        -0.7398694643336527,
        -0.6484183752319304,
        -0.7410792703159979,
        -0.7586372652995248
      ],
      [
        -0.22243655053543598,
        -0.2986484584962388,
        -0.6187820858973812,
        -0.7247829781763624,
        -0.699270517682685,
        -0.6980497964769049,
        1.0,
        -0.674854885985823,
        -0.7278981276165464,
        -0.6953684126585844,
        -0.7356031687651182,
        -0.6747764435947388,
        -0.6502084661129433,
        -0.7097241135996264,
        -0.708895629318482,
        -0.7428019792653222
      ],
      [
        -0.6854446616065492,
        -0.6847645758690959,
        -0.6894736694844148,
        -0.7433193376657519,
        -0.734569026264253,
        -0.7383250393522593,
        -0.674854885985823,
        1.0,
        -0.723445513926583,
        -0.7354875639674128,
        -0.7487590259726999,
        -0.7384064802635262,
        -0.7209522323084925,
        -0.7325806233559125,
        -0.7365242450591172,
        -0.7584915767548024
      ],
      [
        -0.7412840392226474,
        -0.7209892423878772,
        -0.7240561121150102,
        -0.7437861733670641,
        -0.7403592745400354,
        -0.7094294494594356,
        -0.7278981276165464,
        -0.723445513926583,
        1.0,
        -0.7170559972538837,
        -0.7655764556207767,
        -0.7557526471340802,
        -0.7442080800017559,
        -0.7442458524059734,
        -0.7548925304951885,
        -0.7577257224265156
      ],
      [
        -0.7153736831619115,
        -0.6552657359894296,
        -0.47141487992450215,
        -0.717018026511484,
        -0.7202979630673761,
        -0.6798699737427583,
        -0.6953684126585844,
        -0.7354875639674128,
        -0.7170559972538837,
        1.0,
        -0.7533136128948326,
        -0.7194087166904441,
        -0.7381334149442539,
        -0.7261201131116553,
        -0.7473731933581471,
        -0.7558585829583586
      ],
      [
        -0.7339428073746199,
        -0.741096017983977,
        -0.7405743530497133,
        -0.7590457942242961,
        -0.7500458826445628,
        -0.7586091293798385,
        -0.7356031687651182,
        -0.7487590259726999,
        -0.7655764556207767,
        -0.7533136128948326,
        1.0,
        -0.7507212299123013,
        -0.7599333884482964,
        -0.7543189255939747,
        -0.7521939799936106,
        -0.7655764556207767
      ],
      [
        -0.663966991189632,
        -0.7018377223439438,
        -0.5736751489273357,
        -0.7433765547088722,
        -0.7037739762866961,
        -0.703508821713045,
        -0.6747764435947388,
        -0.7384064802635262,
        -0.7557526471340802,
        -0.7194087166904441,
        -0.7507212299123013,
        1.0,
        -0.7415128911604505,
        -0.7089704608668527,
        -0.7387872866809967,
        -0.7443398998874047
      ],
      [
        -0.6883779625461489,
        -0.6425558448491183,
        -0.7008133903172172,
        -0.7481920651032177,
        -0.7358096523653156,
        -0.7398694643336527,
        -0.6502084661129433,
        -0.7209522323084925,
        -0.7442080800017559,
        -0.7381334149442539,
        -0.7599333884482964,
        -0.7415128911604505,
        1.0,
        -0.7339676865642475,
        -0.7381408515795809,
        -0.7588550031318062
      ],
      [
        -0.7296670387326583,
        -0.6699421206003526,
        -0.7311503306273719,
        -0.726064944293946,
        -0.5748714984305892,
        -0.6484183752319304,
        -0.7097241135996264,
        -0.7325806233559125,
        -0.7442458524059734,
        -0.7261201131116553,
        -0.7543189255939747,
        -0.7089704608668527,
        -0.7339676865642475,
        1.0,
        -0.7372481958536905,
        -0.7413630504128308
      ],
      [
        -0.7173230072106178,
        -0.7070892301402594,
        -0.7306559108385905,
        -0.7537671331324749,
        -0.7405071735649135,
        -0.7410792703159979,
        -0.708895629318482,
        -0.7365242450591172,
        -0.7548925304951885,
        -0.7473731933581471,
        -0.7521939799936106,
        -0.7387872866809967,
        -0.7381408515795809,
        -0.7372481958536905,
        1.0,
        -0.7590460354771723
      ],
      [
        -0.7519628999092043,
        -0.743131496621922,
        -0.7572849746152126,
        -0.7624748770134978,
        -0.7328364759854396,
        -0.7586372652995248,
        -0.7428019792653222,
        -0.7584915767548024,
        -0.7577257224265156,
        -0.7558585829583586,
        -0.7655764556207767,
        -0.7443398998874047,
        -0.7588550031318062,
        -0.7413630504128308,
        -0.7590460354771723,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        32,
        4,
        12,
        2,
        7,
        0,
        2,
        6,
        5,
        5,
        3,
        10,
        3,
        4,
        6,
        1
      ],
      "2020-02": [
        36,
        2,
        14,
        3,
        5,
        1,
        3,
        1,
        3,
        1,
        2,
        13,
        8,
        5,
        5,
        3
      ],
      "2020-03": [
        32,
        5,
        10,
        3,
        0,
        2,
        5,
        7,
        7,
        2,
        3,
        3,
        1,
        5,
        7,
        2
      ],
      "2020-04": [
        36,
        9,
        8,
        5,
        2,
        2,
        1,
        4,
        2,
        7,
        3,
        5,
        5,
        8,
        7,
        1
      ],
      "2020-05": [
        42,
        6,
        10,
        7,
        5,
        2,
        3,
        6,
        4,
        2,
        6,
        11,
        6,
        3,
        10,
        3
      ],
      "2020-06": [
        29,
        4,
        11,
        2,
        5,
        5,
        3,
        9,
        2,
        5,
        3,
        4,
        2,
        7,
        2,
        0
      ],
      "2020-07": [
        52,
        6,
        20,
        8,
        5,
        4,
        5,
        5,
        2,
        4,
        3,
        2,
        14,
        7,
        6,
        2
      ],
      "2020-08": [
        44,
        5,
        15,
        11,
        7,
        0,
        5,
        4,
        3,
        2,
        3,
        3,
        4,
        1,
        7,
        1
      ],
      "2020-09": [
        40,
        6,
        11,
        11,
        5,
        3,
        3,
        3,
        1,
        3,
        1,
        5,
        4,
        6,
        2,
        0
      ],
      "2020-10": [
        38,
        6,
        5,
        1,
        6,
        5,
        3,
        7,
        5,
        6,
        2,
        7,
        12,
        6,
        7,
        3
      ],
      "2020-11": [
        15,
        4,
        5,
        2,
        4,
        2,
        2,
        6,
        2,
        1,
        3,
        2,
        4,
        4,
        2,
        1
      ],
      "2020-12": [
        19,
        11,
        7,
        4,
        6,
        4,
        1,
        4,
        2,
        2,
        6,
        6,
        6,
        4,
        5,
        3
      ],
      "2021-01": [
        30,
        12,
        12,
        1,
        3,
        4,
        5,
        5,
        1,
        1,
        4,
        9,
        7,
        4,
        7,
        1
      ],
      "2021-02": [
        38,
        4,
        14,
        3,
        2,
        2,
        2,
        8,
        3,
        3,
        2,
        10,
        6,
        6,
        4,
        1
      ],
      "2021-03": [
        22,
        4,
        15,
        3,
        2,
        2,
        3,
        3,
        0,
        4,
        6,
        5,
        2,
        2,
        8,
        0
      ],
      "2021-04": [
        41,
        8,
        20,
        1,
        6,
        2,
        5,
        8,
        1,
        4,
        3,
        15,
        2,
        7,
        8,
        0
      ],
      "2021-05": [
        40,
        6,
        17,
        4,
        7,
        7,
        4,
        11,
        1,
        9,
        6,
        10,
        6,
        4,
        6,
        0
      ],
      "2021-06": [
        32,
        4,
        16,
        5,
        3,
        2,
        2,
        10,
        2,
        10,
        3,
        7,
        4,
        7,
        4,
        3
      ],
      "2021-07": [
        36,
        9,
        17,
        5,
        2,
        3,
        1,
        6,
        5,
        2,
        7,
        9,
        6,
        4,
        5,
        3
      ],
      "2021-08": [
        34,
        6,
        14,
        6,
        7,
        2,
        2,
        3,
        3,
        8,
        0,
        8,
        2,
        6,
        6,
        2
      ],
      "2021-09": [
        33,
        7,
        14,
        16,
        6,
        5,
        4,
        5,
        1,
        8,
        8,
        2,
        4,
        4,
        4,
        3
      ],
      "2021-10": [
        27,
        9,
        15,
        2,
        3,
        4,
        1,
        6,
        2,
        3,
        6,
        8,
        4,
        8,
        4,
        1
      ],
      "2021-11": [
        23,
        6,
        8,
        3,
        1,
        2,
        3,
        5,
        1,
        2,
        3,
        3,
        3,
        7,
        3,
        2
      ],
      "2021-12": [
        36,
        2,
        17,
        5,
        3,
        9,
        2,
        3,
        4,
        0,
        4,
        9,
        4,
        6,
        6,
        0
      ],
      "2022-01": [
        28,
        6,
        9,
        3,
        2,
        5,
        3,
        2,
        1,
        2,
        3,
        7,
        5,
        5,
        1,
        1
      ],
      "2022-02": [
        33,
        5,
        14,
        1,
        4,
        1,
        3,
        10,
        3,
        2,
        2,
        7,
        3,
        8,
        3,
        1
      ],
      "2022-03": [
        30,
        3,
        8,
        2,
        1,
        3,
        2,
        9,
        3,
        7,
        2,
        6,
        12,
        4,
        4,
        2
      ],
      "2022-04": [
        42,
        10,
        25,
        0,
        0,
        3,
        1,
        2,
        2,
        0,
        8,
        7,
        3,
        5,
        1,
        0
      ],
      "2022-05": [
        47,
        9,
        19,
        5,
        10,
        4,
        1,
        5,
        6,
        11,
        6,
        15,
        9,
        7,
        2,
        6
      ],
      "2022-06": [
        22,
        6,
        13,
        1,
        3,
        5,
        8,
        8,
        3,
        3,
        7,
        3,
        6,
        8,
        3,
        0
      ],
      "2022-07": [
        38,
        8,
        15,
        2,
        1,
        3,
        2,
        4,
        4,
        3,
        2,
        11,
        10,
        8,
        4,
        3
      ],
      "2022-08": [
        29,
        6,
        9,
        6,
        2,
        10,
        4,
        2,
        6,
        2,
        1,
        9,
        5,
        3,
        1,
        5
      ],
      "2022-09": [
        32,
        8,
        16,
        4,
        4,
        4,
        4,
        7,
        6,
        2,
        1,
        6,
        5,
        2,
        3,
        3
      ],
      "2022-10": [
        29,
        1,
        18,
        1,
        1,
        1,
        1,
        6,
        7,
        1,
        2,
        8,
        2,
        8,
        3,
        2
      ],
      "2022-11": [
        25,
        4,
        4,
        6,
        3,
        3,
        4,
        7,
        3,
        7,
        0,
        4,
        2,
        0,
        4,
        1
      ],
      "2022-12": [
        22,
        6,
        8,
        1,
        1,
        0,
        4,
        2,
        5,
        2,
        4,
        5,
        6,
        6,
        4,
        1
      ],
      "2023-01": [
        32,
        6,
        14,
        2,
        4,
        6,
        4,
        5,
        5,
        2,
        5,
        4,
        6,
        3,
        6,
        0
      ],
      "2023-02": [
        38,
        1,
        12,
        1,
        9,
        1,
        4,
        10,
        1,
        3,
        4,
        12,
        4,
        3,
        4,
        0
      ],
      "2023-03": [
        45,
        8,
        18,
        8,
        5,
        0,
        5,
        8,
        5,
        4,
        6,
        14,
        2,
        6,
        6,
        2
      ],
      "2023-04": [
        37,
        6,
        19,
        2,
        1,
        4,
        9,
        3,
        4,
        4,
        2,
        8,
        4,
        6,
        3,
        0
      ],
      "2023-05": [
        66,
        9,
        19,
        5,
        11,
        5,
        5,
        7,
        5,
        8,
        0,
        10,
        5,
        13,
        3,
        0
      ],
      "2023-06": [
        31,
        4,
        18,
        6,
        2,
        1,
        2,
        5,
        10,
        8,
        2,
        6,
        4,
        8,
        5,
        0
      ],
      "2023-07": [
        57,
        12,
        24,
        6,
        4,
        4,
        6,
        7,
        8,
        15,
        6,
        6,
        11,
        7,
        4,
        1
      ],
      "2023-08": [
        37,
        7,
        19,
        8,
        4,
        4,
        3,
        6,
        6,
        7,
        3,
        2,
        1,
        7,
        4,
        1
      ],
      "2023-09": [
        37,
        1,
        15,
        3,
        5,
        0,
        2,
        4,
        3,
        4,
        2,
        5,
        3,
        6,
        4,
        2
      ],
      "2023-10": [
        45,
        7,
        20,
        2,
        3,
        8,
        2,
        4,
        6,
        3,
        2,
        8,
        6,
        6,
        8,
        2
      ],
      "2023-11": [
        33,
        7,
        8,
        4,
        1,
        5,
        3,
        5,
        2,
        3,
        9,
        10,
        4,
        8,
        3,
        1
      ],
      "2023-12": [
        32,
        5,
        11,
        4,
        3,
        6,
        5,
        4,
        1,
        3,
        2,
        5,
        2,
        3,
        2,
        1
      ],
      "2024-01": [
        41,
        8,
        10,
        4,
        2,
        7,
        4,
        6,
        2,
        4,
        3,
        12,
        11,
        10,
        6,
        3
      ],
      "2024-02": [
        43,
        5,
        19,
        4,
        6,
        5,
        3,
        7,
        4,
        6,
        4,
        8,
        4,
        9,
        2,
        5
      ],
      "2024-03": [
        28,
        3,
        17,
        6,
        8,
        9,
        1,
        6,
        5,
        4,
        4,
        7,
        4,
        12,
        2,
        2
      ],
      "2024-04": [
        52,
        2,
        23,
        6,
        5,
        1,
        2,
        6,
        5,
        7,
        1,
        12,
        7,
        7,
        3,
        2
      ],
      "2024-05": [
        53,
        9,
        21,
        4,
        6,
        8,
        5,
        8,
        8,
        6,
        3,
        6,
        9,
        10,
        4,
        1
      ],
      "2024-06": [
        26,
        7,
        19,
        6,
        1,
        5,
        1,
        6,
        1,
        5,
        6,
        9,
        4,
        8,
        3,
        2
      ],
      "2024-07": [
        40,
        9,
        11,
        6,
        6,
        2,
        3,
        4,
        4,
        1,
        0,
        8,
        2,
        8,
        3,
        0
      ],
      "2024-08": [
        26,
        7,
        8,
        9,
        4,
        7,
        2,
        4,
        3,
        3,
        5,
        3,
        6,
        6,
        2,
        2
      ],
      "2024-09": [
        27,
        3,
        6,
        3,
        2,
        1,
        1,
        0,
        2,
        3,
        1,
        5,
        5,
        2,
        3,
        0
      ],
      "2024-10": [
        43,
        9,
        13,
        5,
        2,
        6,
        3,
        2,
        3,
        5,
        4,
        14,
        7,
        5,
        7,
        3
      ],
      "2024-11": [
        34,
        2,
        13,
        1,
        7,
        4,
        3,
        9,
        0,
        3,
        3,
        7,
        7,
        5,
        6,
        0
      ],
      "2024-12": [
        33,
        7,
        23,
        5,
        1,
        1,
        5,
        4,
        7,
        8,
        4,
        14,
        3,
        1,
        3,
        1
      ],
      "2025-01": [
        30,
        9,
        15,
        7,
        3,
        5,
        2,
        9,
        2,
        6,
        4,
        16,
        6,
        8,
        3,
        4
      ],
      "2025-02": [
        35,
        6,
        17,
        12,
        3,
        6,
        1,
        8,
        5,
        5,
        2,
        12,
        7,
        7,
        5,
        2
      ],
      "2025-03": [
        28,
        3,
        12,
        6,
        3,
        3,
        1,
        6,
        3,
        3,
        3,
        11,
        9,
        8,
        2,
        2
      ],
      "2025-04": [
        42,
        3,
        13,
        2,
        3,
        3,
        9,
        7,
        3,
        6,
        7,
        7,
        8,
        13,
        3,
        1
      ],
      "2025-05": [
        45,
        7,
        18,
        12,
        5,
        7,
        7,
        14,
        1,
        9,
        1,
        21,
        9,
        12,
        3,
        1
      ],
      "2025-06": [
        29,
        5,
        20,
        10,
        3,
        3,
        2,
        7,
        2,
        5,
        7,
        13,
        5,
        12,
        5,
        1
      ],
      "2025-07": [
        61,
        7,
        23,
        14,
        3,
        7,
        4,
        5,
        6,
        9,
        6,
        15,
        4,
        6,
        1,
        3
      ],
      "2025-08": [
        26,
        7,
        19,
        6,
        4,
        8,
        1,
        4,
        5,
        5,
        6,
        13,
        9,
        13,
        3,
        0
      ],
      "2025-09": [
        22,
        4,
        6,
        1,
        3,
        1,
        2,
        3,
        3,
        3,
        3,
        2,
        2,
        4,
        0,
        2
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Synthetic 1-Categories in Directed Type Theory",
          "year": "2024-10",
          "abstract": "The field of directed type theory seeks to design type theories capable of\nreasoning synthetically about (higher) categories, by generalizing the\nsymmetric identity types of Martin-L\\\"of Type Theory to asymmetric hom-types.\nWe articulate the directed type theory of the category model, with appropriate\nmodalities for keeping track of variances and a powerful directed-J rule\ncapable of proving results about arbitrary terms of hom-types; we put this rule\nto use in making several constructions in synthetic 1-category theory. Because\nthis theory is expressed entirely in terms of generalized algebraic theories,\nwe know automatically that this directed type theory admits a syntax model and\nis the first step towards directed higher observational type theory.",
          "arxiv_id": "2410.19520v1"
        },
        {
          "title": "An interpretation of dependent type theory in a model category of locally cartesian closed categories",
          "year": "2020-07",
          "abstract": "Locally cartesian closed (lcc) categories are natural categorical models of\nextensional dependent type theory. This paper introduces the \"gros\" semantics\nin the category of lcc categories: Instead of constructing an interpretation in\na given individual lcc category, we show that also the category of all lcc\ncategories can be endowed with the structure of a model of dependent type\ntheory. The original interpretation in an individual lcc category can then be\nrecovered by slicing. As in the original interpretation, we face the issue of\ncoherence: Categorical structure is usually preserved by functors only up to\nisomorphism, whereas syntactic substitution commutes strictly with all type\ntheoretic structure. Our solution involves a suitable presentation of the\nhigher category of lcc categories as model category. To that end, we construct\na model category of lcc sketches, from which we obtain by the formalism of\nalgebraically (co)fibrant objects model categories of strict lcc categories and\nthen algebraically cofibrant strict lcc categories. The latter is our model of\ndependent type theory.",
          "arxiv_id": "2007.02900v2"
        },
        {
          "title": "Internal $\\infty$-Categorical Models of Dependent Type Theory: Towards 2LTT Eating HoTT",
          "year": "2020-09",
          "abstract": "Using dependent type theory to formalise the syntax of dependent type theory\nis a very active topic of study and goes under the name of \"type theory eating\nitself\" or \"type theory in type theory.\" Most approaches are at least loosely\nbased on Dybjer's categories with families (CwF's) and come with a type CON of\ncontexts, a type family TY indexed over it modelling types, and so on. This\nworks well in versions of type theory where the principle of unique identity\nproofs (UIP) holds. In homotopy type theory (HoTT) however, it is a\nlong-standing and frequently discussed open problem whether the type theory\n\"eats itself\" and can serve as its own interpreter. The fundamental underlying\ndifficulty seems to be that categories are not suitable to capture a type\ntheory in the absence of UIP.\n  In this paper, we develop a notion of $\\infty$-categories with families\n($\\infty$-CwF's). The approach to higher categories used relies on the\npreviously suggested semi-Segal types, with a new construction of identity\nsubstitutions that allow for both univalent and non-univalent variations. The\ntype-theoretic universe as well as the internalised syntax are models, although\nit remains a conjecture that the latter is initial. To circumvent the known\nunsolved problem of constructing semisimplicial types, the definition is\npresented in two-level type theory (2LTT).\n  Apart from introducing $\\infty$-CwF's, the paper explains the shortcomings of\n1-categories in type theory without UIP as well as the difficulties of and\napproaches to internal higher-dimensional categories.",
          "arxiv_id": "2009.01883v2"
        }
      ],
      "1": [
        {
          "title": "Learning Optimal Strategies for Temporal Tasks in Stochastic Games",
          "year": "2021-02",
          "abstract": "Synthesis from linear temporal logic (LTL) specifications provides assured\ncontrollers for systems operating in stochastic and potentially adversarial\nenvironments. Automatic synthesis tools, however, require a model of the\nenvironment to construct controllers. In this work, we introduce a model-free\nreinforcement learning (RL) approach to derive controllers from given LTL\nspecifications even when the environment is completely unknown. We model the\nproblem as a stochastic game (SG) between the controller and the adversarial\nenvironment; we then learn optimal control strategies that maximize the\nprobability of satisfying the LTL specifications against the worst-case\nenvironment behavior. We first construct a product game using the deterministic\nparity automaton (DPA) translated from the given LTL specification. By deriving\ndistinct rewards and discount factors from the acceptance condition of the DPA,\nwe reduce the maximization of the worst-case probability of satisfying the LTL\nspecification into the maximization of a discounted reward objective in the\nproduct game; this enables the use of model-free RL algorithms to learn an\noptimal controller strategy. To deal with the common scalability problems when\nthe number of sets defining the acceptance condition of the DPA (usually\nreferred as colors), is large, we propose a lazy color generation method where\ndistinct rewards and discount factors are utilized only when needed, and an\napproximate method where the controller eventually focuses on only one color.\nIn several case studies, we show that our approach is scalable to a wide range\nof LTL formulas, significantly outperforming existing methods for learning\ncontrollers from LTL specifications in SGs.",
          "arxiv_id": "2102.04307v3"
        },
        {
          "title": "Policy Synthesis for Factored MDPs with Graph Temporal Logic Specifications",
          "year": "2020-01",
          "abstract": "We study the synthesis of policies for multi-agent systems to implement\nspatial-temporal tasks. We formalize the problem as a factored Markov decision\nprocess subject to so-called graph temporal logic specifications. The\ntransition function and the spatial-temporal task of each agent depend on the\nagent itself and its neighboring agents. The structure in the model and the\nspecifications enable to develop a distributed algorithm that, given a factored\nMarkov decision process and a graph temporal logic formula, decomposes the\nsynthesis problem into a set of smaller synthesis problems, one for each agent.\nWe prove that the algorithm runs in time linear in the total number of agents.\nThe size of the synthesis problem for each agent is exponential only in the\nnumber of neighboring agents, which is typically much smaller than the number\nof agents. We demonstrate the algorithm in case studies on disease control and\nurban security. The numerical examples show that the algorithm can scale to\nhundreds of agents.",
          "arxiv_id": "2001.09066v1"
        },
        {
          "title": "LTLf Synthesis on Probabilistic Systems",
          "year": "2020-09",
          "abstract": "Many systems are naturally modeled as Markov Decision Processes (MDPs),\ncombining probabilities and strategic actions. Given a model of a system as an\nMDP and some logical specification of system behavior, the goal of synthesis is\nto find a policy that maximizes the probability of achieving this behavior. A\npopular choice for defining behaviors is Linear Temporal Logic (LTL). Policy\nsynthesis on MDPs for properties specified in LTL has been well studied. LTL,\nhowever, is defined over infinite traces, while many properties of interest are\ninherently finite. Linear Temporal Logic over finite traces (LTLf) has been\nused to express such properties, but no tools exist to solve policy synthesis\nfor MDP behaviors given finite-trace properties. We present two algorithms for\nsolving this synthesis problem: the first via reduction of LTLf to LTL and the\nsecond using native tools for LTLf. We compare the scalability of these two\napproaches for synthesis and show that the native approach offers better\nscalability compared to existing automaton generation tools for LTL.",
          "arxiv_id": "2009.10883v1"
        }
      ],
      "2": [
        {
          "title": "Minimal modal logics, constructive modal logics and their relations",
          "year": "2023-09",
          "abstract": "We present a family of minimal modal logics (namely, modal logics based on\nminimal propositional logic) corresponding each to a different classical modal\nlogic. The minimal modal logics are defined based on their classical\ncounterparts in two distinct ways: (1) via embedding into fusions of classical\nmodal logics through a natural extension of the G\\\"odel-Johansson translation\nof minimal logic into modal logic S4; (2) via extension to modal logics of the\nmulti- vs. single-succedent correspondence of sequent calculi for classical and\nminimal logic. We show that, despite being mutually independent, the two\nmethods turn out to be equivalent for a wide class of modal systems. Moreover,\nwe compare the resulting minimal version of K with the constructive modal logic\nCK studied in the literature, displaying tight relations among the two systems.\nBased on these relations, we also define a constructive correspondent for each\nminimal system, thus obtaining a family of constructive modal logics which\nincludes CK as well as other constructive modal logics studied in the\nliterature.",
          "arxiv_id": "2309.02367v1"
        },
        {
          "title": "Nested Sequents for Intuitionistic Grammar Logics via Structural Refinement",
          "year": "2022-10",
          "abstract": "Intuitionistic grammar logics fuse constructive and multi-modal reasoning\nwhile permitting the use of converse modalities, serving as a generalization of\nstandard intuitionistic modal logics. In this paper, we provide definitions of\nthese logics as well as establish a suitable proof theory thereof. In\nparticular, we show how to apply the structural refinement methodology to\nextract cut-free nested sequent calculi for intuitionistic grammar logics from\ntheir semantics. This method proceeds by first transforming the semantics of\nthese logics into sound and complete labeled sequent systems, which we prove\nhave favorable proof-theoretic properties such as syntactic cut-elimination. We\nthen transform these labeled systems into nested sequent systems via the\nintroduction of propagation rules and the elimination of structural rules. Our\nderived proof systems are then put to use, whereby we prove the conservativity\nof intuitionistic grammar logics over their modal counterparts, establish the\ngeneral undecidability of these logics, and recognize a decidable subclass,\nreferred to as \"simple\" intuitionistic grammar logics.",
          "arxiv_id": "2210.17139v1"
        },
        {
          "title": "Defining Logical Systems via Algebraic Constraints on Proofs",
          "year": "2023-01",
          "abstract": "We present a comprehensive programme analysing the decomposition of proof\nsystems for non-classical logics into proof systems for other logics,\nespecially classical logic, using an algebra of constraints. That is, one\nrecovers a proof system for a target logic by enriching a proof system for\nanother, typically simpler, logic with an algebra of constraints that act as\ncorrectness conditions on the latter to capture the former; for example, one\nmay use Boolean algebra to give constraints in a sequent calculus for classical\npropositional logic to produce a sequent calculus for intuitionistic\npropositional logic. The idea behind such forms of reduction is to obtain a\ntool for uniform and modular treatment of proof theory and provide a bridge\nbetween semantics logics and their proof theory. The article discusses the\ntheoretical background of the project and provides several illustrations of its\nwork in the field of intuitionistic and modal logics. The results include the\nfollowing: a uniform treatment of modular and cut-free proof systems for a\nlarge class of propositional logics; a general criterion for a novel approach\nto soundness and completeness of a logic with respect to a model-theoretic\nsemantics; and, a case study deriving a model-theoretic semantics from a\nproof-theoretic specification of a logic.",
          "arxiv_id": "2301.02125v3"
        }
      ],
      "3": [
        {
          "title": "LP2PB: Translating Answer Set Programs into Pseudo-Boolean Theories",
          "year": "2020-09",
          "abstract": "Answer set programming (ASP) is a well-established knowledge representation\nformalism. Most ASP solvers are based on (extensions of) technology from\nBoolean satisfiability solving. While these solvers have shown to be very\nsuccessful in many practical applications, their strength is limited by their\nunderlying proof system, resolution. In this paper, we present a new tool LP2PB\nthat translates ASP programs into pseudo-Boolean theories, for which solvers\nbased on the (stronger) cutting plane proof system exist. We evaluate our tool,\nand the potential of cutting-plane-based solving for ASP on traditional ASP\nbenchmarks as well as benchmarks from pseudo-Boolean solving. Our results are\nmixed: overall, traditional ASP solvers still outperform our translational\napproach, but several benchmark families are identified where the balance\nshifts the other way, thereby suggesting that further investigation into a\nstronger proof system for ASP is valuable.",
          "arxiv_id": "2009.10248v1"
        },
        {
          "title": "Exact ASP Counting with Compact Encodings",
          "year": "2023-12",
          "abstract": "Answer Set Programming (ASP) has emerged as a promising paradigm in knowledge\nrepresentation and automated reasoning owing to its ability to model hard\ncombinatorial problems from diverse domains in a natural way. Building on\nadvances in propositional SAT solving, the past two decades have witnessed the\nemergence of well-engineered systems for solving the answer set satisfiability\nproblem, i.e., finding models or answer sets for a given answer set program. In\nrecent years, there has been growing interest in problems beyond\nsatisfiability, such as model counting, in the context of ASP. Akin to the\nearly days of propositional model counting, state-of-the-art exact answer set\ncounters do not scale well beyond small instances. Exact ASP counters struggle\nwith handling larger input formulas. The primary contribution of this paper is\na new ASP counting framework, called sharpASP, which counts answer sets\navoiding larger input formulas. This relies on an alternative way of defining\nanswer sets that allows for the lifting of key techniques developed in the\ncontext of propositional model counting. Our extensive empirical analysis over\n1470 benchmarks demonstrates significant performance gain over current\nstate-of-the-art exact answer set counters. Specifically, by using sharpASP, we\nwere able to solve 1062 benchmarks with PAR2 score of 3082 whereas using prior\nstate-of-the-art, we could only solve 895 benchmarks with a PAR2 score of 4205,\nall other experimental conditions being the same.",
          "arxiv_id": "2312.11936v1"
        },
        {
          "title": "diff-SAT -- A Software for Sampling and Probabilistic Reasoning for SAT and Answer Set Programming",
          "year": "2021-01",
          "abstract": "This paper describes diff-SAT, an Answer Set and SAT solver which combines\nregular solving with the capability to use probabilistic clauses, facts and\nrules, and to sample an optimal world-view (multiset of satisfying Boolean\nvariable assignments or answer sets) subject to user-provided probabilistic\nconstraints. The sampling process minimizes a user-defined differentiable\nobjective function using a gradient descent based optimization method called\nDifferentiable Satisfiability Solving ($\\partial\\mathrm{SAT}$) respectively\nDifferentiable Answer Set Programming ($\\partial\\mathrm{ASP}$). Use cases are\ni.a. probabilistic logic programming (in form of Probabilistic Answer Set\nProgramming), Probabilistic Boolean Satisfiability solving (PSAT), and\ndistribution-aware sampling of model multisets (answer sets or Boolean\ninterpretations).",
          "arxiv_id": "2101.00589v1"
        }
      ],
      "4": [
        {
          "title": "An Abstract Domain for Heap Commutativity (Extended Version)",
          "year": "2024-11",
          "abstract": "Commutativity of program code (i.e. the equivalence of two code fragments\ncomposed in alternate orders) is of ongoing interest in many settings such as\nprogram verification, scalable concurrency, and security analysis. While some\nhave explored static analysis for code commutativity, few have specifically\ncatered to heap-manipulating programs. We introduce an abstract domain in which\ncommutativity synthesis or verification techniques can safely be performed on\nabstract mathematical models and, from those results, one can directly obtain\ncommutativity conditions for concrete heap programs. This approach offloads\nchallenges of concrete heap reasoning into the simpler abstract space. We show\nthis reasoning supports framing and composition, and conclude with\ncommutativity analysis of programs operating on example heap data structures.\nOur work has been mechanized in Coq and is available in the supplement.",
          "arxiv_id": "2411.12857v2"
        },
        {
          "title": "Semi-Automated Modular Formal Verification of Critical Software: Liveness and Completeness Thresholds",
          "year": "2024-03",
          "abstract": "In this dissertation we describe two contributions to the state of the art in\nreasoning about liveness and safety, respectively.\n  Programs for multiprocessor machines commonly perform busy waiting for\nsynchronization. We propose the first separation logic for modularly verifying\ntermination of such programs under fair scheduling. Our logic requires the\nproof author to associate a ghost signal with each busy-waiting loop and allows\nsuch loops to iterate while their corresponding signal $s$ is not set. The\nproof author further has to define a well-founded order on signals and to prove\nthat if the looping thread holds an obligation to set a signal $s'$, then $s'$\nis ordered above $s$. By using conventional shared state invariants to\nassociate the state of ghost signals with the state of data structures,\nprograms busy-waiting for arbitrary conditions over arbitrary data structures\ncan be verified.\n  Moreover, we present the first study of completeness thresholds for bounded\nmemory safety proofs. Specifically, we consider heap-manipulating programs that\niterate over arrays without allocating or freeing memory. In this setting, we\npresent the first notion of completeness thresholds for program verification\nwhich reduce unbounded memory safety proofs to bounded ones. Furthermore, we\ndemonstrate that we can characterise completeness thresholds for simple classes\nof array traversing programs. Finally, we suggest avenues of research to scale\nthis technique theoretically, i.e., to larger classes of programs (heap\nmanipulation, tree-like data structures), and practically by highlighting\nautomation opportunities.",
          "arxiv_id": "2403.00934v2"
        },
        {
          "title": "Verification-Preserving Inlining in Automatic Separation Logic Verifiers (extended version)",
          "year": "2022-08",
          "abstract": "Bounded verification has proved useful to detect bugs and to increase\nconfidence in the correctness of a program. In contrast to unbounded\nverification, reasoning about calls via (bounded) inlining and about loops via\n(bounded) unrolling does not require method specifications and loop invariants\nand, therefore, reduces the annotation overhead to the bare minimum, namely\nspecifications of the properties to be verified. For verifiers based on\ntraditional program logics, verification is preserved by inlining (and\nunrolling): successful unbounded verification of a program w.r.t. some\nannotation implies successful verification of the inlined program. That is, any\nerror detected in the inlined program reveals a true error in the original\nprogram. However, this essential property might not hold for automatic\nseparation logic verifiers such as Caper, GRASShopper, RefinedC, Steel,\nVeriFast, and verifiers based on Viper. In this setting, inlining generally\nchanges the resources owned by method executions, which may affect automatic\nproof search algorithms and introduce spurious errors.\n  In this paper, we present the first technique for verification-preserving\ninlining in automatic separation logic verifiers. We identify a semantic\ncondition on programs and prove in Isabelle/HOL that it ensures\nverification-preserving inlining for state-of-the-art automatic separation\nlogic verifiers. We also prove a dual result: successful verification of the\ninlined program ensures that there are method and loop annotations that enable\nthe verification of the original program for bounded executions. To check our\nsemantic condition automatically, we present two approximations that can be\nchecked syntactically and with a program verifier, respectively. We implement\nthese checks in Viper and demonstrate that they are effective for non-trivial\nexamples from different verifiers.",
          "arxiv_id": "2208.10456v2"
        }
      ],
      "5": [
        {
          "title": "Bridging Logic Programming and Deep Learning for Explainability through ILASP",
          "year": "2025-02",
          "abstract": "My research explores integrating deep learning and logic programming to set\nthe basis for a new generation of AI systems. By combining neural networks with\nInductive Logic Programming (ILP), the goal is to construct systems that make\naccurate predictions and generate comprehensible rules to validate these\npredictions. Deep learning models process and analyze complex data, while ILP\ntechniques derive logical rules to prove the network's conclusions. Explainable\nAI methods, like eXplainable Answer Set Programming (XASP), elucidate the\nreasoning behind these rules and decisions. The focus is on applying ILP\nframeworks, specifically ILASP and FastLAS, to enhance explainability in\nvarious domains. My test cases span weather prediction, the legal field, and\nimage recognition. In weather forecasting, the system will predict events and\nprovides explanations using FastLAS, with plans to integrate recurrent neural\nnetworks in the future. In the legal domain, the research focuses on\ninterpreting vague decisions and assisting legal professionals by encoding\nItalian legal articles and learning reasoning patterns from Court of Cassation\ndecisions using ILASP. For biological laboratories, we will collaborate with a\nresearch group to automate spermatozoa morphology classification for Bull\nBreeding Soundness Evaluation using YOLO networks and ILP to explain\nclassification outcomes. This hybrid approach aims to bridge the gap between\nthe high performance of deep learning models and the transparency of symbolic\nreasoning, advancing AI by providing interpretable and trustworthy\napplications.",
          "arxiv_id": "2502.09227v1"
        },
        {
          "title": "Neural Logic Reasoning",
          "year": "2020-08",
          "abstract": "Recent years have witnessed the success of deep neural networks in many\nresearch areas. The fundamental idea behind the design of most neural networks\nis to learn similarity patterns from data for prediction and inference, which\nlacks the ability of cognitive reasoning. However, the concrete ability of\nreasoning is critical to many theoretical and practical problems. On the other\nhand, traditional symbolic reasoning methods do well in making logical\ninference, but they are mostly hard rule-based reasoning, which limits their\ngeneralization ability to different tasks since difference tasks may require\ndifferent rules. Both reasoning and generalization ability are important for\nprediction tasks such as recommender systems, where reasoning provides strong\nconnection between user history and target items for accurate prediction, and\ngeneralization helps the model to draw a robust user portrait over noisy\ninputs.\n  In this paper, we propose Logic-Integrated Neural Network (LINN) to integrate\nthe power of deep learning and logic reasoning. LINN is a dynamic neural\narchitecture that builds the computational graph according to input logical\nexpressions. It learns basic logical operations such as AND, OR, NOT as neural\nmodules, and conducts propositional logical reasoning through the network for\ninference. Experiments on theoretical task show that LINN achieves significant\nperformance on solving logical equations and variables. Furthermore, we test\nour approach on the practical task of recommendation by formulating the task\ninto a logical inference problem. Experiments show that LINN significantly\noutperforms state-of-the-art recommendation models in Top-K recommendation,\nwhich verifies the potential of LINN in practice.",
          "arxiv_id": "2008.09514v1"
        },
        {
          "title": "Reasoning in Neurosymbolic AI",
          "year": "2025-05",
          "abstract": "Knowledge representation and reasoning in neural networks have been a\nlong-standing endeavor which has attracted much attention recently. The\nprincipled integration of reasoning and learning in neural networks is a main\nobjective of the area of neurosymbolic Artificial Intelligence (AI). In this\nchapter, a simple energy-based neurosymbolic AI system is described that can\nrepresent and reason formally about any propositional logic formula. This\ncreates a powerful combination of learning from data and knowledge and logical\nreasoning. We start by positioning neurosymbolic AI in the context of the\ncurrent AI landscape that is unsurprisingly dominated by Large Language Models\n(LLMs). We identify important challenges of data efficiency, fairness and\nsafety of LLMs that might be addressed by neurosymbolic reasoning systems with\nformal reasoning capabilities. We then discuss the representation of logic by\nthe specific energy-based system, including illustrative examples and empirical\nevaluation of the correspondence between logical reasoning and energy\nminimization using Restricted Boltzmann Machines (RBM). Learning from data and\nknowledge is also evaluated empirically and compared with a symbolic, neural\nand a neurosymbolic system. Results reported in this chapter in an accessible\nway are expected to reignite the research on the use of neural networks as\nmassively-parallel models for logical reasoning and promote the principled\nintegration of reasoning and learning in deep networks. We conclude the chapter\nwith a discussion of the importance of positioning neurosymbolic AI within a\nbroader framework of formal reasoning and accountability in AI, discussing the\nchallenges for neurosynbolic AI to tackle the various known problems of\nreliability of deep learning.",
          "arxiv_id": "2505.20313v1"
        }
      ],
      "6": [
        {
          "title": "On the computability of ordered fields",
          "year": "2020-07",
          "abstract": "In this paper we develop general techniques for classes of computable real\nnumbers generated by subsets of total computable (recursive functions) with\nspecial restrictions on basic operations in order to investigate the following\nproblems: whether a generated class is a real closed field and whether there\nexists a computable presentation of a generated class. We prove a series of\ntheorems that lead to the result that there are no computable presentations\nneither for polynomial time computable no even for $E_n$-computable real\nnumbers, where $E_n$ is a level in Grzegorczyk hierarchy, $n \\geq 2$. We also\npropose a criterion of computable presentability of an archimedean ordered\nfield.",
          "arxiv_id": "2007.14801v3"
        },
        {
          "title": "How Much Partiality Is Needed for a Theory of Computability?",
          "year": "2023-05",
          "abstract": "Partiality is a natural phenomenon in computability that we cannot get\naround. So, the question is whether we can give the areas where partiality\noccurs, that is, where non-termination happens, more structure. In this paper\nwe consider function classes which besides the total functions only contain\nfinite functions whose domain of definition is an initial segment of the\nnatural numbers. Such functions appear naturally in computation. We show that a\nrich computability theory can be developed for these functions classes which\nembraces the central results of classical computability theory, in which all\npartial (computable) functions are considered. To do so, the concept of a\nG\\\"odel number is generalised, resulting in a broader class of numberings. The\ncentral algorithmic idea in this approach is to search in enumerated lists. In\nthis way, function computability is reduced to set listability. Besides the\ndevelopment of a computability theory for the functions classes, the new\nnumberings -- called quasi-G\\\"odel numberings -- are studied from a\nnumbering-theoretic perspective: they are complete, and each of the function\nclasses numbered in this way is a retract of the G\\\"odel numbered set of all\npartial computable functions. Moreover, the Rogers semi-lattice of all\ncomputable numberings of the considered function classes is studied and results\nas in the case of the computable numberings of the partial computable functions\nare obtained. The function classes are shown to be effectively given algebraic\ndomains in the sense of Scott-Ershov. The quasi-G\\\"odel numberings are exactly\nthe admissible numberings of the computable elements of the domain. Moreover,\nthe domain can be computably mapped onto every other effectively given one so\nthat every admissible numbering of the computable domain elements is generated\nby a quasi-G\\\"odel numbering via this mapping.",
          "arxiv_id": "2305.06982v2"
        },
        {
          "title": "Epi-constructivism: Decidable sets of computable numbers as foundational objects for mathematics",
          "year": "2022-07",
          "abstract": "It is well known that the R, the set of real numbers, is an abstract set,\nwhere almost all its elements cannot be described in any finite language.\n  We investigate possible approaches to what might be called an\nepi-constructionist approach to mathematics. While most constructive\nmathematics is concerned with constructive proofs, the agenda here is that the\nobjects that we study, specifically the class of numbers that we study, should\nbe an enumerable set of finite symbol strings. These might also be called\ndecidable constructive real numbers, that is our class of numbers should be a\ncomputable sets of explicitly represented computable numbers.\n  There have been various investigations of the computable numbers going back\nto Turing. Most are however not expressed constructively, rather computable is\na property assigned to some of the abstract real numbers. Other definitions\ndefine constructive real numbers without reference to the abstract R, but the\nconstruction is undecidable, i.e., we cannot determine if a given construction\nrepresents a computable real number or not. For example, we may define a real\nas a computable convergent sequence of rationals, but cannot in general decide\nif a given computable sequence is convergent.\n  This paper explores several specific classes of decidable constructive real\nnumbers that could form foundational objects for what we might call an\nepi-constructionist mathematics.",
          "arxiv_id": "2207.04647v1"
        }
      ],
      "7": [
        {
          "title": "Merge-width and First-Order Model Checking",
          "year": "2025-02",
          "abstract": "We introduce merge-width, a family of graph parameters that unifies several\nstructural graph measures, including treewidth, degeneracy, twin-width,\nclique-width, and generalized coloring numbers. Our parameters are based on new\ndecompositions called construction sequences. These are sequences of ever\ncoarser partitions of the vertex set, where each pair of parts has a specified\ndefault connection, and all vertex pairs of the graph that differ from the\ndefault are marked as resolved. The radius-$r$ merge-width is the maximum\nnumber of parts reached from a vertex by following a path of at most $r$\nresolved edges. Graph classes of bounded merge-width -- for which the\nradius-$r$ merge-width parameter can be bounded by a constant, for each fixed\n$r=1,2,3,\\ldots$ -- include all classes of bounded expansion or of bounded\ntwin-width, thus unifying two central notions from the Sparsity and Twin-width\nframeworks. Furthermore, they are preserved under first-order transductions,\nwhich attests to their robustness. We conjecture that classes of bounded\nmerge-width are equivalent to the previously introduced classes of bounded\nflip-width.\n  As our main result, we show that the model checking problem for first-order\nlogic is fixed-parameter tractable on graph classes of bounded merge-width,\nassuming the input includes a witnessing construction sequence. This unites and\nextends two previous model checking results: the result of Dvo\\v{r}\\'{a}k,\nKr\\'{a}l, and Thomas for classes of bounded expansion, and the result of\nBonnet, Kim, Thomass\\'e, and Watrigant for classes of bounded twin-width.\n  Finally, we suggest future research directions that could impact the study of\nstructural and algorithmic graph theory, in particular of monadically dependent\ngraph classes, which we conjecture to coincide with classes of almost bounded\nmerge-width.",
          "arxiv_id": "2502.18065v1"
        },
        {
          "title": "Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes",
          "year": "2024-03",
          "abstract": "A conjecture in algorithmic model theory predicts that the model-checking\nproblem for first-order logic is fixed-parameter tractable on a hereditary\ngraph class if and only if the class is monadically dependent. Originating in\nmodel theory, this notion is defined in terms of logic, and encompasses nowhere\ndense classes, monadically stable classes, and classes of bounded twin-width.\nWorking towards this conjecture, we provide the first two combinatorial\ncharacterizations of monadically dependent graph classes. This yields the\nfollowing dichotomy.\n  On the structure side, we characterize monadic dependence by a\nRamsey-theoretic property called flip-breakability. This notion generalizes the\nnotions of uniform quasi-wideness, flip-flatness, and bounded grid rank, which\ncharacterize nowhere denseness, monadic stability, and bounded twin-width,\nrespectively, and played a key role in their respective model checking\nalgorithms. Natural restrictions of flip-breakability additionally characterize\nbounded treewidth and cliquewidth and bounded treedepth and shrubdepth.\n  On the non-structure side, we characterize monadic dependence by explicitly\nlisting few families of forbidden induced subgraphs. This result is analogous\nto the characterization of nowhere denseness via forbidden subdivided cliques,\nand allows us to resolve one half of the motivating conjecture: First-order\nmodel checking is AW[$*$]-hard on every hereditary graph class that is\nmonadically independent. The result moreover implies that hereditary graph\nclasses which are small, have almost bounded twin-width, or have almost bounded\nflip-width, are monadically dependent.\n  Lastly, we lift our result to also obtain a combinatorial dichotomy in the\nmore general setting of monadically dependent classes of binary structures.",
          "arxiv_id": "2403.15201v2"
        },
        {
          "title": "Flip-width: Cops and Robber on dense graphs",
          "year": "2023-02",
          "abstract": "We define new graph parameters, called flip-width, that generalize treewidth,\ndegeneracy, and generalized coloring numbers for sparse graphs, and\nclique-width and twin-width for dense graphs. The flip-width parameters are\ndefined using variants of the Cops and Robber game, in which the robber has\nspeed bounded by a fixed constant $r\\in\\mathbb N\\cup\\{\\infty\\}$, and the cops\nperform flips (or perturbations) of the considered graph. We then propose a new\nnotion of tameness of a graph class, called bounded flip-width, which is a\ndense counterpart of classes of bounded expansion of Ne\\v{s}etril and Ossona de\nMendez, and includes classes of bounded twin-width of Bonnet, Kim,\nThomass{\\'e}, and Watrigant. This unifies Sparsity Theory and Twin-width\nTheory, providing a common language for studying the central notions of the two\ntheories, such as weak coloring numbers and twin-width -- corresponding to\nwinning strategies of one player -- or dense shallow minors, rich divisions, or\nwell-linked sets, corresponding to winning strategies of the other player. We\nprove that boundedness of flip-width is preserved by first-order\ninterpretations, or transductions, generalizing previous results concerning\nclasses of bounded expansion and bounded twin-width. We provide an algorithm\napproximating the flip-width of a given graph, which runs in slicewise\npolynomial time (XP) in the size of the graph. Finally, we propose a more\ngeneral notion of tameness, called almost bounded flip-width, which is a dense\ncounterpart of nowhere dense classes. We conjecture, and provide evidence, that\nclasses with almost bounded flip-width coincide with monadically dependent (or\nmonadically NIP) classes, introduced by Shelah in model theory. We also provide\nevidence that classes of almost bounded flip-width characterise the hereditary\ngraph classes for which the model-checking problem is fixed-parameter\ntractable.",
          "arxiv_id": "2302.00352v3"
        }
      ],
      "8": [
        {
          "title": "Answering Counting Queries over DL-Lite Ontologies",
          "year": "2020-09",
          "abstract": "Ontology-mediated query answering (OMQA) is a promising approach to data\naccess and integration that has been actively studied in the knowledge\nrepresentation and database communities for more than a decade. The vast\nmajority of work on OMQA focuses on conjunctive queries, whereas more\nexpressive queries that feature counting or other forms of aggregation remain\nlargely unex-plored. In this paper, we introduce a general form of counting\nquery, relate it to previous proposals, and study the complexity of answering\nsuch queries in the presence of DL-Lite ontologies. As it follows from existing\nwork that query answering is intractable and often of high complexity, we\nconsider some practically relevant restrictions, for which we establish\nimproved complexity bounds.",
          "arxiv_id": "2009.09801v1"
        },
        {
          "title": "Finding Good Proofs for Answers to Conjunctive Queries Mediated by Lightweight Ontologies (Technical Report)",
          "year": "2022-06",
          "abstract": "In ontology-mediated query answering, access to incomplete data sources is\nmediated by a conceptual layer constituted by an ontology. To correctly compute\nanswers to queries, it is necessary to perform complex reasoning over the\nconstraints expressed by the ontology. In the literature, there exists a\nmultitude of techniques incorporating the ontological knowledge into queries.\nHowever, few of these approaches were designed for comprehensibility of the\nquery answers. In this article, we try to bridge these two qualities by\nadapting a proof framework originally applied to axiom entailment for\nconjunctive query answering. We investigate the data and combined complexity of\ndetermining the existence of a proof below a given quality threshold, which can\nbe measured in different ways. By distinguishing various parameters such as the\nshape of a query, we obtain an overview of the complexity of this problem for\nthe lightweight ontology languages DL-Lite_R and EL, and also have a brief look\nat temporal query answering.",
          "arxiv_id": "2206.09758v2"
        },
        {
          "title": "Temporal Conjunctive Query Answering in the Extended DL-Lite Family",
          "year": "2020-03",
          "abstract": "Ontology-based query answering (OBQA) augments classical query answering in\ndatabases by domain knowledge encoded in an ontology. Systems for OBQA use the\nontological knowledge to infer new information that is not explicitly given in\nthe data. Moreover, they usually employ the open-world assumption, which means\nthat knowledge that is not stated explicitly in the data and that is not\ninferred is not assumed to be true or false. Classical OBQA however considers\nonly a snapshot of the data, which means that information about the temporal\nevolution of the data is not used for reasoning and hence lost. We investigate\ntemporal conjunctive queries (TCQs) that allow to access temporal data through\nclassical ontologies. In particular, we study combined and data complexity of\nTCQ entailment for ontologies written in description logics from the extended\nDL-Lite family. Many of these logics allow for efficient reasoning in the\natemporal setting and are successfully applied in practice. We show\ncomprehensive complexity results for temporal reasoning with these logics.",
          "arxiv_id": "2003.09508v1"
        }
      ],
      "9": [
        {
          "title": "Revisiting Epistemic Logic with Names",
          "year": "2021-06",
          "abstract": "This paper revisits the multi-agent epistemic logic presented in [10], where\nagents and sets of agents are replaced by abstract, intensional \"names\". We\nmake three contributions. First, we study its model theory, providing adequate\nnotions of bisimulation and frame morphisms, and use them to study the logic's\nexpressive power and definability. Second, we show that the logic has a natural\nneighborhood semantics, which in turn allows to show that the axiomatization in\n[10] does not rely on possibly controversial introspective properties of\nknowledge. Finally, we extend the logic with common and distributed knowledge\noperators, and provide a sound and complete axiomatization for each of these\nextensions. These results together put the original epistemic logic with names\nin a more modern context and opens the door for a logical analysis of epistemic\nphenomena where group membership is uncertain or variable.",
          "arxiv_id": "2106.11493v1"
        },
        {
          "title": "Learning What Others Know",
          "year": "2021-09",
          "abstract": "We propose a number of powerful dynamic-epistemic logics for multi-agent\ninformation sharing and acts of publicly or privately accessing other agents'\ninformation databases. The static base of our logics is obtained by adding to\nstandard epistemic logic comparative epistemic assertions, that can express\nepistemic superiority between groups or individuals, as well as a common\ndistributed knowledge operator (that combines features of both common knowledge\nand distributed knowledge). On the dynamic side, we introduce actions by which\nepistemic superiority can be acquired: \"sharing all one knows\" (by e.g. giving\naccess to one's information database to all or some of the other agents), as\nwell as more complex informational events, such as hacking. We completely\naxiomatize several such logics and prove their decidability.",
          "arxiv_id": "2109.07255v1"
        },
        {
          "title": "Epistemic Logic over Similarity Graphs: Common, Distributed and Mutual Knowledge",
          "year": "2023-09",
          "abstract": "In this paper, we delve into the study of epistemic logics, interpreted\nthrough similarity models based on weighted graphs. We explore eight languages\nthat extend the traditional epistemic language by incorporating modalities of\ncommon, distributed, and mutual knowledge. The concept of individual knowledge\nis redefined under these similarity models. It is no longer just a matter of\npersonal knowledge, but is now enriched and understood as knowledge under the\nindividual's epistemic ability. Common knowledge is presented as higher-order\nknowledge that is universally known to any degree, a definition that aligns\nwith existing literature. We reframe distributed knowledge as a form of\nknowledge acquired by collectively leveraging the abilities of a group of\nagents. In contrast, mutual knowledge is defined as the knowledge obtained\nthrough the shared abilities of a group. We then focus on the resulting logics,\nexamining their relative expressivity, semantic correspondence to the classical\nepistemic logic, proof systems and the computational complexity associated with\nthe model checking problem and the satisfiability/validity problem. This paper\noffers significant insights into the logical analysis and understanding of\nthese enriched forms of knowledge, contributing to the broader discourse on\nepistemic logic.",
          "arxiv_id": "2310.00264v1"
        }
      ],
      "10": [
        {
          "title": "Reasoning about Recursive Quantum Programs",
          "year": "2021-07",
          "abstract": "Most modern (classical) programming languages support recursion. Recursion\nhas also been successfully applied to the design of several quantum algorithms\nand introduced in a couple of quantum programming languages. So, it can be\nexpected that recursion will become one of the fundamental paradigms of quantum\nprogramming. Several program logics have been developed for verification of\nquantum while-programs. However, there are as yet no general methods for\nreasoning about (mutual) recursive procedures and ancilla quantum data\nstructure in quantum computing (with measurement). We fill the gap in this\npaper by proposing a parameterized quantum assertion logic and, based on which,\ndesigning a quantum Hoare logic for verifying parameterized recursive quantum\nprograms with ancilla data and probabilistic control. The quantum Hoare logic\ncan be used to prove partial, total, and even probabilistic correctness (by\nreducing to total correctness) of those quantum programs. In particular, two\ncounterexamples for illustrating incompleteness of non-parameterized assertions\nin verifying recursive procedures, and, one counterexample for showing the\nfailure of reasoning with exact probabilities based on partial correctness, are\nconstructed. The effectiveness of our logic is shown by three main examples --\nrecursive quantum Markov chain (with probabilistic control), fixed-point\nGrover's search, and recursive quantum Fourier sampling.",
          "arxiv_id": "2107.11679v1"
        },
        {
          "title": "Quantum circuits are just a phase",
          "year": "2025-07",
          "abstract": "Quantum programs today are written at a low level of abstraction - quantum\ncircuits akin to assembly languages - and even advanced quantum programming\nlanguages essentially function as circuit description languages. This state of\naffairs impedes scalability, clarity, and support for higher-level reasoning.\nMore abstract and expressive quantum programming constructs are needed.\n  To this end, we introduce a novel yet simple quantum programming language for\ngenerating unitaries from \"just a phase\"; we combine a (global) phase operation\nthat captures phase shifts with a quantum analogue of the \"if let\" construct\nthat captures subspace selection via pattern matching. This minimal language\nlifts the focus from quantum gates to eigendecomposition, conjugation, and\ncontrolled unitaries; common building blocks in quantum algorithm design.\n  We demonstrate several aspects of the expressive power of our language in\nseveral ways. Firstly, we establish that our representation is universal by\nderiving a universal quantum gate set. Secondly, we show that important quantum\nalgorithms can be expressed naturally and concisely, including Grover's search\nalgorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal\nProcessing, and the Quantum Eigenvalue Transformation. Furthermore, we give\nclean denotational semantics grounded in categorical quantum mechanics.\nFinally, we implement a prototype compiler that efficiently translates terms of\nour language to quantum circuits, and prove that it is sound with respect to\nthese semantics. Collectively, these contributions show that this construct\noffers a principled and practical step toward more abstract and structured\nquantum programming.",
          "arxiv_id": "2507.11676v1"
        },
        {
          "title": "Quantum First-Order Logics That Capture Logarithmic-Time/Space Quantum Computability",
          "year": "2025-01",
          "abstract": "We introduce a quantum analogue of classical first-order logic (FO) and\ndevelop a theory of quantum first-order logic as a basis of the productive\ndiscussions on the power of logical expressiveness toward quantum computing.\nThe purpose of this work is to logically express \"quantum computation\" by\nintroducing specially-featured quantum connectives and quantum quantifiers that\nquantify fixed-dimensional quantum states. Our approach is founded on the\nrecently introduced recursion-theoretical schematic definitions of time-bounded\nquantum functions, which map finite-dimensional Hilbert spaces to themselves.\nThe quantum first-order logic (QFO) in this work therefore looks quite\ndifferent from the well-known old concept of quantum logic based on lattice\ntheory. We demonstrate that quantum first-order logics possess an ability of\nexpressing bounded-error quantum logarithmic-time computability by the use of\nnew \"functional\" quantum variables. In contrast, an extra inclusion of quantum\ntransitive closure operator helps us characterize quantum logarithmic-space\ncomputability. The same computability can be achieved by the use of different\n\"functional\" quantum variables.",
          "arxiv_id": "2501.12007v1"
        }
      ],
      "11": [
        {
          "title": "SubgoalXL: Subgoal-based Expert Learning for Theorem Proving",
          "year": "2024-08",
          "abstract": "Formal theorem proving, a field at the intersection of mathematics and\ncomputer science, has seen renewed interest with advancements in large language\nmodels (LLMs). This paper introduces SubgoalXL, a novel approach that\nsynergizes subgoal-based proofs with expert learning to enhance LLMs'\ncapabilities in formal theorem proving within the Isabelle environment.\nSubgoalXL addresses two critical challenges: the scarcity of specialized\nmathematics and theorem-proving data, and the need for improved multi-step\nreasoning abilities in LLMs. By optimizing data efficiency and employing\nsubgoal-level supervision, SubgoalXL extracts richer information from limited\nhuman-generated proofs. The framework integrates subgoal-oriented proof\nstrategies with an expert learning system, iteratively refining formal\nstatement, proof, and subgoal generators. Leveraging the Isabelle environment's\nadvantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art\nperformance of 56.1\\% in Isabelle on the standard miniF2F dataset, marking an\nabsolute improvement of 4.9\\%. Notably, SubgoalXL successfully solves 41 AMC12,\n9 AIME, and 3 IMO problems from miniF2F. These results underscore the\neffectiveness of maximizing limited data utility and employing targeted\nguidance for complex reasoning in formal theorem proving, contributing to the\nongoing advancement of AI reasoning capabilities. The implementation is\navailable at \\url{https://github.com/zhaoxlpku/SubgoalXL}.",
          "arxiv_id": "2408.11172v1"
        },
        {
          "title": "Generating Millions Of Lean Theorems With Proofs By Exploring State Transition Graphs",
          "year": "2025-02",
          "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\ngenerating mathematical proofs. However, a persistent challenge is that LLMs\noccasionally make mistakes, while even a minor mistake can invalidate an entire\nproof. Proof assistants like Lean offer a great remedy. They are designed for\nverifying each step of a proof in a formal language, and in recent years\nresearchers have created AI models to generate proofs in their languages.\nHowever, the scarcity of large-scale datasets of Lean proofs restrict the\nperformance of such Automated Theorem Proving (ATP) models.\n  We developed LeanNavigator, a novel method for generating a large-scale\ndataset of Lean theorems and proofs by finding new ways to prove existing Lean\ntheorems. By leveraging an interactive Lean client and an efficient method for\nproof step generation, LeanNavigator efficiently produces new theorems with\ncorresponding proofs. Applying this approach to Mathlib4, we generated 4.7\nmillion theorems totaling 1 billion tokens, surpassing previous datasets by\nmore than an order of magnitude. Using this extensive dataset, we trained an AI\nmodel that outperforms the state-of-the-art ReProver model in theorem-proving\ntasks. These results confirm our hypothesis and demonstrate the critical role\nof large datasets in improving the performance of automated theorem provers.",
          "arxiv_id": "2503.04772v1"
        },
        {
          "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean",
          "year": "2024-04",
          "abstract": "Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, a general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research.",
          "arxiv_id": "2404.12534v3"
        }
      ],
      "12": [
        {
          "title": "A Hierarchy of Nondeterminism",
          "year": "2022-09",
          "abstract": "We study three levels in a hierarchy of nondeterminism: A nondeterministic\nautomaton $\\mathcal{A}$ is determinizable by pruning (DBP) if we can obtain a\ndeterministic automaton equivalent to $\\mathcal{A}$ by removing some of its\ntransitions. Then, $\\mathcal{A}$ is history deterministic (HD) if its\nnondeterministic choices can be resolved in a way that only depends on the\npast. Finally, $\\mathcal{A}$ is semantically deterministic (SD) if different\nnondeterministic choices in $\\mathcal{A}$ lead to equivalent states. Some\napplications of automata in formal methods require deterministic automata, yet\nin fact can use automata with some level of nondeterminism. For example, DBP\nautomata are useful in the analysis of online algorithms, and HD automata are\nuseful in synthesis and control. For automata on finite words, the three levels\nin the hierarchy coincide. We study the hierarchy for B\\\"uchi, co-B\\\"uchi, and\nweak automata on infinite words. We show that the hierarchy is strict, study\nthe expressive power of the different levels in it, as well as the complexity\nof deciding the membership of a language in a given level. Finally, we describe\na probability-based analysis of the hierarchy, which relates the level of\nnondeterminism with the probability that a random run on a word in the language\nis accepting. We relate the latter to nondeterministic automata that can be\nused when reasoning about probabilistic systems.",
          "arxiv_id": "2209.09866v5"
        },
        {
          "title": "Minimization and Canonization of GFG Transition-Based Automata",
          "year": "2021-06",
          "abstract": "While many applications of automata in formal methods can use\nnondeterministic automata, some applications, most notably synthesis, need\ndeterministic or good-for-games (GFG) automata. The latter are nondeterministic\nautomata that can resolve their nondeterministic choices in a way that only\ndepends on the past. The minimization problem for deterministic B\\\"uchi and\nco-B\\\"uchi word automata is NP-complete. In particular, no canonical minimal\ndeterministic automaton exists, and a language may have different minimal\ndeterministic automata. We describe a polynomial minimization algorithm for GFG\nco-B\\\"uchi word automata with transition-based acceptance. Thus, a run is\naccepting if it traverses a set $\\alpha$ of designated transitions only\nfinitely often. Our algorithm is based on a sequence of transformations we\napply to the automaton, on top of which a minimal quotient automaton is\ndefined. We use our minimization algorithm to show canonicity for\ntransition-based GFG co-B\\\"uchi word automata: all minimal automata have\nisomorphic safe components (namely components obtained by restricting the\ntransitions to these not in $\\alpha$) and once we saturate the automata with\n$\\alpha$-transitions, we get full isomorphism.",
          "arxiv_id": "2106.06745v5"
        },
        {
          "title": "Rerailing Automata",
          "year": "2025-03",
          "abstract": "In this paper, we introduce rerailing automata for $\\omega$-regular\nlanguages. They generalize both deterministic parity (DPW) and minimized\nhistory-deterministic co-B\\\"uchi automata (with transition based acceptance,\nHdTbcBW) while combining their favorable properties. In particular, rerailing\nautomata can represent arbitrary $\\omega$-regular languages while allowing for\npolynomial-time minimization, just as HdTbcBW do. Since DPW are a special case\nof rerailing automata, a minimized rerailing automaton is never larger than the\nsmallest deterministic parity automaton for the same language. We also show\nthat rerailing automata can be used as a replacement for deterministic parity\nautomata for the realizability check of open systems.\n  The price to be paid to obtain the useful properties of rerailing automata is\nthat the acceptance condition in such automata refers to the dominating colors\nalong all runs for a given word, where just as in parity automata, the\ndominating color along a run is the lowest one occurring infinitely often along\nit. A rerailing automaton accepts those words for which the greatest of the\ndominating colors along the runs is even. Additionally, rerailing automata\nguarantee that every prefix of a run for a word can be extended to eventually\nreach a point from which all runs for the word extending the prefix have the\nsame dominating color, and it is even if and only if the word is in the\nlanguage of the automaton. We show that these properties together allow\ncharacterizing the role of each state in such an automaton in a way that\nrelates it to state combinations in a sequence of co-B\\\"uchi automata for the\nrepresented language. This characterization forms the basis of the\npolynomial-time minimization approach in this paper.",
          "arxiv_id": "2503.08438v1"
        }
      ],
      "13": [
        {
          "title": "Automated Design of Linear Bounding Functions for Sigmoidal Nonlinearities in Neural Networks",
          "year": "2024-06",
          "abstract": "The ubiquity of deep learning algorithms in various applications has\namplified the need for assuring their robustness against small input\nperturbations such as those occurring in adversarial attacks. Existing complete\nverification techniques offer provable guarantees for all robustness queries\nbut struggle to scale beyond small neural networks. To overcome this\ncomputational intractability, incomplete verification methods often rely on\nconvex relaxation to over-approximate the nonlinearities in neural networks.\nProgress in tighter approximations has been achieved for piecewise linear\nfunctions. However, robustness verification of neural networks for general\nactivation functions (e.g., Sigmoid, Tanh) remains under-explored and poses new\nchallenges. Typically, these networks are verified using convex relaxation\ntechniques, which involve computing linear upper and lower bounds of the\nnonlinear activation functions. In this work, we propose a novel parameter\nsearch method to improve the quality of these linear approximations.\nSpecifically, we show that using a simple search method, carefully adapted to\nthe given verification problem through state-of-the-art algorithm configuration\ntechniques, improves the average global lower bound by 25% on average over the\ncurrent state of the art on several commonly used local robustness verification\nbenchmarks.",
          "arxiv_id": "2406.10154v1"
        },
        {
          "title": "CheckINN: Wide Range Neural Network Verification in Imandra (Extended)",
          "year": "2022-07",
          "abstract": "Neural networks are increasingly relied upon as components of complex\nsafety-critical systems such as autonomous vehicles. There is high demand for\ntools and methods that embed neural network verification in a larger\nverification cycle. However, neural network verification is difficult due to a\nwide range of verification properties of interest, each typically only amenable\nto verification in specialised solvers. In this paper, we show how Imandra, a\nfunctional programming language and a theorem prover originally designed for\nverification, validation and simulation of financial infrastructure can offer a\nholistic infrastructure for neural network verification. We develop a novel\nlibrary CheckINN that formalises neural networks in Imandra, and covers\ndifferent important facets of neural network verification.",
          "arxiv_id": "2207.10562v2"
        },
        {
          "title": "Set-Based Training for Neural Network Verification",
          "year": "2024-01",
          "abstract": "Neural networks are vulnerable to adversarial attacks, i.e., small input\nperturbations can significantly affect the outputs of a neural network.\nTherefore, to ensure safety of neural networks in safety-critical environments,\nthe robustness of a neural network must be formally verified against input\nperturbations, e.g., from noisy sensors. To improve the robustness of neural\nnetworks and thus simplify the formal verification, we present a novel\nset-based training procedure in which we compute the set of possible outputs\ngiven the set of possible inputs and compute for the first time a gradient set,\ni.e., each possible output has a different gradient. Therefore, we can directly\nreduce the size of the output enclosure by choosing gradients toward its\ncenter. Small output enclosures increase the robustness of a neural network\nand, at the same time, simplify its formal verification. The latter benefit is\ndue to the fact that a larger size of propagated sets increases the\nconservatism of most verification methods. Our extensive evaluation\ndemonstrates that set-based training produces robust neural networks with\ncompetitive performance, which can be verified using fast (polynomial-time)\nverification algorithms due to the reduced output set.",
          "arxiv_id": "2401.14961v4"
        }
      ],
      "14": [
        {
          "title": "Truly Concurrent Calculi with Reversibility, Probabilism and Guards",
          "year": "2021-08",
          "abstract": "The well-known process algebras, such as CCS, ACP and $\\pi$-calculus, capture\nthe interleaving concurrency based on bisimilarity semantics. We did some work\non truly concurrent process algebras, such as CTC, APTC and $\\pi_{tc}$, capture\nthe true concurrency based on truly concurrent bisimilarities, such as pomset\nbisimilarity, step bisimilarity, history-preserving (hp-) bisimilarity and\nhereditary history-preserving (hhp-) bisimilarity. Truly concurrent process\nalgebras are generalizations of the corresponding traditional process algebras.\nIn this book, we introduce reversibility, probabilism, and guards into truly\nconcurrent calculus CTC.",
          "arxiv_id": "2108.10156v1"
        },
        {
          "title": "Truly Concurrent Process Algebra with Localities",
          "year": "2021-09",
          "abstract": "The well-known process algebras, such as CCS, ACP and $\\pi$-calculus, capture\nthe interleaving concurrency based on bisimilarity semantics. We did some work\non truly concurrent process algebras, such as CTC, APTC and $\\pi_{tc}$, capture\nthe true concurrency based on truly concurrent bisimilarities, such as pomset\nbisimilarity, step bisimilarity, history-preserving (hp-) bisimilarity and\nhereditary history-preserving (hhp-) bisimilarity. Truly concurrent process\nalgebras are generalizations of the corresponding traditional process algebras.\nIn this book, we introduce localities into truly concurrent process algebras,\nbased on the work on process algebra with localities.",
          "arxiv_id": "2109.05936v1"
        },
        {
          "title": "Probabilistic Process Algebra for True Concurrency",
          "year": "2021-07",
          "abstract": "The well-known process algebras, such as CCS, ACP and $\\pi$-calculus, capture\nthe interleaving concurrency based on bisimilarity semantics. We did some work\non truly concurrent process algebras, such as CTC, APTC and $\\pi_{tc}$ ,\ncapture the true concurrency based on truly concurrent bisimilarities, such as\npomset bisimilarity, step bisimilarity, history-preserving (hp-) bisimilarity\nand hereditary history-preserving (hhp-) bisimilarity. Truly concurrent process\nalgebras are generalizations of the corresponding traditional process algebras.\nIn this book, we introduce probabilism into truly concurrent process algebras,\nbased on the work on probabilistic process algebra.",
          "arxiv_id": "2107.08453v2"
        }
      ],
      "15": [
        {
          "title": "Multi: a Formal Playground for Multi-Smart Contract Interaction",
          "year": "2022-07",
          "abstract": "Blockchains are maintained by a network of participants that run algorithms\ndesigned to maintain collectively a distributed machine tolerant to Byzantine\nattacks. From the point of view of users, blockchains provide the illusion of\ncentralized computers that perform trustable verifiable computations, where all\ncomputations are deterministic and the results cannot be manipulated or undone.\nSmart-contracts are written in a special-purpose programming language with\ndeterministic semantics. Each transaction begins with an invocation from an\nexternal user to a smart contract. Contracts have local storage and can call\nother contracts, and more importantly, they store, send and receive\ncryptocurrency. It is very important to guarantee that contracts are correct\nbefore deployment since their code cannot be modified afterward deployment.\nHowever, the resulting ecosystem makes it very difficult to reason about\nprogram correctness, since contracts can be executed by malicious users or\nmalicious contracts can be designed to exploit other contracts that call them.\nMany attacks and bugs are caused by unexpected interactions between multiple\ncontracts, the attacked contract and unknown code that performs the exploit.\nMoreover, there is a very aggressive competition between different blockchains\nto expand their user base. Ideas are implemented fast and blockchains compete\nto offer and adopt new features quickly. In this paper, we propose a formal\nextensible playground that allows reasoning about multi-contract interactions\nto ultimately prove properties before features are incorporated into the real\nblockchain. We implemented a model of computation that models the execution\nplatform, abstracts the internal code of each individual contract and focuses\non contract interactions. Moreover, we show how many features, existing or\nproposed, can be used to reason about multi-contract interactions.",
          "arxiv_id": "2207.06681v1"
        },
        {
          "title": "Transaction Monitoring of Smart Contracts",
          "year": "2022-07",
          "abstract": "Blockchains are modern distributed systems that provide decentralized\nfinancial capabilities with trustable guarantees. Smart contracts are programs\nwritten in specialized programming languages running on a blockchain and govern\nhow tokens and cryptocurrency are sent and received. Smart contracts can invoke\nother contracts during the execution of transactions initiated by external\nusers.\n  Once deployed, smart contracts cannot be modified and their pitfalls can\ncause malfunctions and losses, for example by attacks from malicious users.\nRuntime verification is a very appealing technique to improve the reliability\nof smart contracts. One approach consists of specifying undesired executions\n(never claims) and detecting violations of the specification on the fly. This\ncan be done by extending smart contracts with additional instructions\ncorresponding to monitor specified properties, resulting in an onchain\nmonitoring approach.\n  In this paper, we study transaction monitoring that consists of detecting\nviolations of complete transaction executions and not of individual operations\nwithin transactions. Our main contributions are to show that transaction\nmonitoring is not possible in most blockchains and propose different execution\nmechanisms that would enable transaction monitoring.",
          "arxiv_id": "2207.02517v1"
        },
        {
          "title": "Extracting Smart Contracts Tested and Verified in Coq",
          "year": "2020-12",
          "abstract": "We implement extraction of Coq programs to functional languages based on\nMetaCoq's certified erasure. As part of this, we implement an optimisation pass\nremoving unused arguments. We prove the pass correct wrt. a conventional\ncall-by-value operational semantics of functional languages. We apply this to\ntwo functional smart contract languages, Liquidity and Midlang, and to the\nfunctional language Elm. Our development is done in the context of the ConCert\nframework that enables smart contract verification. We contribute a verified\nboardroom voting smart contract featuring maximum voter privacy such that each\nvote is kept private except under collusion of all other parties. We also\nintegrate property-based testing into ConCert using QuickChick and our\ndevelopment is the first to support testing properties of interacting smart\ncontracts. We test several complex contracts such as a DAO-like contract, an\nescrow contract, an implementation of a Decentralized Finance (DeFi) contract\nwhich includes a custom token standard (Tezos FA2), and more. In total, this\ngives us a way to write dependent programs in Coq, test them\nsemi-automatically, verify, and then extract to functional smart contract\nlanguages, while retaining a small trusted computing base of only MetaCoq and\nthe pretty-printers into these languages.",
          "arxiv_id": "2012.09138v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T19:40:16Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}