{
  "topics": {
    "data": {
      "0": {
        "name": "0_policy_RL_reinforcement_reinforcement learning",
        "keywords": [
          [
            "policy",
            0.018401262846497166
          ],
          [
            "RL",
            0.017798549134530203
          ],
          [
            "reinforcement",
            0.013881401955476328
          ],
          [
            "reinforcement learning",
            0.01382254104193402
          ],
          [
            "agent",
            0.013119663062883608
          ],
          [
            "Reinforcement",
            0.011684723685868901
          ],
          [
            "agents",
            0.010756354981855768
          ],
          [
            "reward",
            0.009913742704206535
          ],
          [
            "control",
            0.009885643059848584
          ],
          [
            "Learning",
            0.009768112814189961
          ]
        ],
        "count": 15402
      },
      "1": {
        "name": "1_privacy_FL_Federated_adversarial",
        "keywords": [
          [
            "privacy",
            0.019847500767975156
          ],
          [
            "FL",
            0.017561899576242265
          ],
          [
            "Federated",
            0.015373946459121556
          ],
          [
            "adversarial",
            0.013767723189280441
          ],
          [
            "attacks",
            0.013283775264565674
          ],
          [
            "clients",
            0.011327358083845674
          ],
          [
            "training",
            0.009641168847975877
          ],
          [
            "data",
            0.009341047160910979
          ],
          [
            "communication",
            0.008900480408939421
          ],
          [
            "attack",
            0.00862640622244321
          ]
        ],
        "count": 13153
      },
      "2": {
        "name": "2_LLMs_language_LLM_language models",
        "keywords": [
          [
            "LLMs",
            0.01744371131948144
          ],
          [
            "language",
            0.016086009969495857
          ],
          [
            "LLM",
            0.01146807716640945
          ],
          [
            "language models",
            0.01103720858180701
          ],
          [
            "models",
            0.010473011129756397
          ],
          [
            "Language",
            0.010157776168439363
          ],
          [
            "tasks",
            0.009104051207685442
          ],
          [
            "Large",
            0.008392510950738296
          ],
          [
            "training",
            0.007755137011490353
          ],
          [
            "tuning",
            0.007728361500581834
          ]
        ],
        "count": 10054
      },
      "3": {
        "name": "3_equations_neural_PDEs_physics",
        "keywords": [
          [
            "equations",
            0.015451908828828227
          ],
          [
            "neural",
            0.012955444269637234
          ],
          [
            "PDEs",
            0.01192335060978759
          ],
          [
            "physics",
            0.011580717319502594
          ],
          [
            "differential",
            0.011254530253179108
          ],
          [
            "systems",
            0.010485322858012581
          ],
          [
            "differential equations",
            0.010429842169077875
          ],
          [
            "dynamics",
            0.010153188380303614
          ],
          [
            "PDE",
            0.009665976239235314
          ],
          [
            "physical",
            0.009419841730948756
          ]
        ],
        "count": 5859
      },
      "4": {
        "name": "4_molecular_protein_drug_molecules",
        "keywords": [
          [
            "molecular",
            0.021860113527578228
          ],
          [
            "protein",
            0.019293045781361963
          ],
          [
            "drug",
            0.017368812500522565
          ],
          [
            "molecules",
            0.013329826998199047
          ],
          [
            "chemical",
            0.010784923016296712
          ],
          [
            "prediction",
            0.009929633719163894
          ],
          [
            "materials",
            0.009784285233930984
          ],
          [
            "discovery",
            0.009149068857706598
          ],
          [
            "structure",
            0.008056307135359519
          ],
          [
            "structures",
            0.007579908307353579
          ]
        ],
        "count": 5363
      },
      "5": {
        "name": "5_gradient_networks_neural_convergence",
        "keywords": [
          [
            "gradient",
            0.018178686029111317
          ],
          [
            "networks",
            0.016686011064377896
          ],
          [
            "neural",
            0.0148930503999103
          ],
          [
            "convergence",
            0.013938004042112814
          ],
          [
            "neural networks",
            0.013296998147343456
          ],
          [
            "SGD",
            0.012501697828300788
          ],
          [
            "optimization",
            0.012167438023346917
          ],
          [
            "descent",
            0.011544083670893593
          ],
          [
            "convex",
            0.01127255554212556
          ],
          [
            "functions",
            0.010959132952352553
          ]
        ],
        "count": 5345
      },
      "6": {
        "name": "6_graph_node_Graph_graphs",
        "keywords": [
          [
            "graph",
            0.04403291191086039
          ],
          [
            "node",
            0.02618699283755723
          ],
          [
            "Graph",
            0.025569660448038777
          ],
          [
            "graphs",
            0.023394984033048724
          ],
          [
            "GNNs",
            0.02064994073052324
          ],
          [
            "nodes",
            0.016091573813913573
          ],
          [
            "GNN",
            0.013734945398817056
          ],
          [
            "Networks",
            0.009890820107193533
          ],
          [
            "networks",
            0.00976759625117451
          ],
          [
            "Neural",
            0.008852220439154849
          ]
        ],
        "count": 5018
      },
      "7": {
        "name": "7_segmentation_images_medical_image",
        "keywords": [
          [
            "segmentation",
            0.022624159863872534
          ],
          [
            "images",
            0.01943143357872963
          ],
          [
            "medical",
            0.016832475323160543
          ],
          [
            "image",
            0.016543811683214157
          ],
          [
            "MRI",
            0.013163766709803446
          ],
          [
            "CT",
            0.012898784726164271
          ],
          [
            "imaging",
            0.012744935586618697
          ],
          [
            "cancer",
            0.009257730405238427
          ],
          [
            "clinical",
            0.009093901765721613
          ],
          [
            "deep",
            0.008947931600616509
          ]
        ],
        "count": 4948
      },
      "8": {
        "name": "8_speech_audio_speaker_music",
        "keywords": [
          [
            "speech",
            0.0423938183163348
          ],
          [
            "audio",
            0.03140547380277376
          ],
          [
            "speaker",
            0.018689963363437905
          ],
          [
            "music",
            0.016385337517359027
          ],
          [
            "Speech",
            0.015318482067618934
          ],
          [
            "ASR",
            0.01497951396328271
          ],
          [
            "recognition",
            0.010244221749119896
          ],
          [
            "Audio",
            0.010177466194591646
          ],
          [
            "acoustic",
            0.009347536534110712
          ],
          [
            "voice",
            0.009068139303036595
          ]
        ],
        "count": 4481
      },
      "9": {
        "name": "9_diffusion_image_generative_Diffusion",
        "keywords": [
          [
            "diffusion",
            0.022305539039490903
          ],
          [
            "image",
            0.021733717144474446
          ],
          [
            "generative",
            0.017059454787551342
          ],
          [
            "Diffusion",
            0.01414536121575126
          ],
          [
            "diffusion models",
            0.014127793063528167
          ],
          [
            "generation",
            0.013884838517239241
          ],
          [
            "models",
            0.011421132958794553
          ],
          [
            "images",
            0.011234583027202505
          ],
          [
            "GANs",
            0.010252636417231682
          ],
          [
            "latent",
            0.009406939296002479
          ]
        ],
        "count": 4286
      },
      "10": {
        "name": "10_channel_wireless_communication_network",
        "keywords": [
          [
            "channel",
            0.01765997963692169
          ],
          [
            "wireless",
            0.01546295100861663
          ],
          [
            "communication",
            0.013152890422814437
          ],
          [
            "network",
            0.012771820748316712
          ],
          [
            "networks",
            0.008398241391807604
          ],
          [
            "MIMO",
            0.0076347789872370595
          ],
          [
            "Learning",
            0.0070552384487853875
          ],
          [
            "radio",
            0.006924227363303306
          ],
          [
            "signal",
            0.006856772985829944
          ],
          [
            "resource",
            0.006769578868785519
          ]
        ],
        "count": 3125
      },
      "11": {
        "name": "11_pruning_hardware_quantization_memory",
        "keywords": [
          [
            "pruning",
            0.020356850662624756
          ],
          [
            "hardware",
            0.014983590619123077
          ],
          [
            "quantization",
            0.013725686275661161
          ],
          [
            "memory",
            0.012514342683993315
          ],
          [
            "DNN",
            0.012317545207200258
          ],
          [
            "training",
            0.011309635466633532
          ],
          [
            "accuracy",
            0.011240515228322034
          ],
          [
            "devices",
            0.01019519912379456
          ],
          [
            "network",
            0.010171260408725543
          ],
          [
            "networks",
            0.010074639811724614
          ]
        ],
        "count": 2932
      },
      "12": {
        "name": "12_quantum_Quantum_classical_machine",
        "keywords": [
          [
            "quantum",
            0.09318212150892359
          ],
          [
            "Quantum",
            0.033303589851782535
          ],
          [
            "classical",
            0.023463390988573293
          ],
          [
            "machine",
            0.012238297715855195
          ],
          [
            "machine learning",
            0.011831587063069615
          ],
          [
            "circuit",
            0.010978960753714186
          ],
          [
            "circuits",
            0.010532211357354472
          ],
          [
            "states",
            0.008660064805514381
          ],
          [
            "neural",
            0.008330744732670706
          ],
          [
            "learning",
            0.00825484029943249
          ]
        ],
        "count": 2746
      },
      "13": {
        "name": "13_regret_bandit_bandits_algorithm",
        "keywords": [
          [
            "regret",
            0.04476904415574399
          ],
          [
            "bandit",
            0.03020025485328765
          ],
          [
            "bandits",
            0.02161924696890863
          ],
          [
            "algorithm",
            0.020668684731745615
          ],
          [
            "arm",
            0.019090080886646675
          ],
          [
            "bound",
            0.01794738175999495
          ],
          [
            "Bandits",
            0.016909912359803658
          ],
          [
            "online",
            0.01625383063555297
          ],
          [
            "problem",
            0.016084618016384028
          ],
          [
            "algorithms",
            0.014990897588097612
          ]
        ],
        "count": 2709
      },
      "14": {
        "name": "14_recommendation_user_item_items",
        "keywords": [
          [
            "recommendation",
            0.031153741724123975
          ],
          [
            "user",
            0.029048419451312032
          ],
          [
            "item",
            0.02076083468927437
          ],
          [
            "items",
            0.018270660095141875
          ],
          [
            "recommender",
            0.01751794661141972
          ],
          [
            "users",
            0.016901925476376315
          ],
          [
            "recommender systems",
            0.012762277883207584
          ],
          [
            "Recommendation",
            0.012333736711458456
          ],
          [
            "systems",
            0.011078854412847646
          ],
          [
            "ranking",
            0.01078472593839599
          ]
        ],
        "count": 2612
      },
      "15": {
        "name": "15_causal_treatment_Causal_variables",
        "keywords": [
          [
            "causal",
            0.056910007133005314
          ],
          [
            "treatment",
            0.024079481064276532
          ],
          [
            "Causal",
            0.020745183306739683
          ],
          [
            "variables",
            0.01481978023449894
          ],
          [
            "effect",
            0.01390247715311463
          ],
          [
            "observational",
            0.013232685516845366
          ],
          [
            "effects",
            0.011789069116766615
          ],
          [
            "causal inference",
            0.010650778189622878
          ],
          [
            "causal discovery",
            0.010173971814722857
          ],
          [
            "discovery",
            0.009890349126320806
          ]
        ],
        "count": 2578
      },
      "16": {
        "name": "16_series_time series_time_forecasting",
        "keywords": [
          [
            "series",
            0.052385074768842695
          ],
          [
            "time series",
            0.049668181095506796
          ],
          [
            "time",
            0.032492600592272096
          ],
          [
            "forecasting",
            0.02564005735956888
          ],
          [
            "Time",
            0.023487472375260738
          ],
          [
            "Series",
            0.02075104920002273
          ],
          [
            "series forecasting",
            0.014399767091701153
          ],
          [
            "time series forecasting",
            0.01284136872350995
          ],
          [
            "series data",
            0.011740002474118454
          ],
          [
            "anomaly",
            0.01169088101934445
          ]
        ],
        "count": 2453
      },
      "17": {
        "name": "17_visual_video_image_multimodal",
        "keywords": [
          [
            "visual",
            0.021174578697865754
          ],
          [
            "video",
            0.018693945709175566
          ],
          [
            "image",
            0.015936576112072665
          ],
          [
            "multimodal",
            0.015430536816953092
          ],
          [
            "language",
            0.014399239808083655
          ],
          [
            "text",
            0.013885870310246694
          ],
          [
            "vision",
            0.012726145606376136
          ],
          [
            "modal",
            0.012656049258510736
          ],
          [
            "CLIP",
            0.012139845261293795
          ],
          [
            "modalities",
            0.010455208145343366
          ]
        ],
        "count": 2356
      },
      "18": {
        "name": "18_label_labels_supervised_class",
        "keywords": [
          [
            "label",
            0.025767916454998614
          ],
          [
            "labels",
            0.02038463585405512
          ],
          [
            "supervised",
            0.014575026065795946
          ],
          [
            "class",
            0.013377982772843697
          ],
          [
            "SSL",
            0.011098999321501287
          ],
          [
            "supervised learning",
            0.010015656511392903
          ],
          [
            "noisy",
            0.009821086112789038
          ],
          [
            "samples",
            0.009461174226033672
          ],
          [
            "learning",
            0.009371785755281933
          ],
          [
            "loss",
            0.00933320516518952
          ]
        ],
        "count": 2309
      },
      "19": {
        "name": "19_clinical_patient_medical_patients",
        "keywords": [
          [
            "clinical",
            0.026385738466113275
          ],
          [
            "patient",
            0.020372469722859343
          ],
          [
            "medical",
            0.019054494716532956
          ],
          [
            "patients",
            0.01767897838415749
          ],
          [
            "healthcare",
            0.01458146792495956
          ],
          [
            "health",
            0.014248276701081551
          ],
          [
            "prediction",
            0.009727034960864646
          ],
          [
            "risk",
            0.00912458437919013
          ],
          [
            "models",
            0.008770607286443062
          ],
          [
            "care",
            0.008291135517006849
          ]
        ],
        "count": 2295
      },
      "20": {
        "name": "20_3D_point_object_scene",
        "keywords": [
          [
            "3D",
            0.042521735646088514
          ],
          [
            "point",
            0.01757695750875589
          ],
          [
            "object",
            0.0169870775856118
          ],
          [
            "scene",
            0.015257351192380258
          ],
          [
            "depth",
            0.012469475742809337
          ],
          [
            "point cloud",
            0.011546912621729981
          ],
          [
            "view",
            0.01110450010085329
          ],
          [
            "shape",
            0.010479859555704532
          ],
          [
            "clouds",
            0.010422246531963136
          ],
          [
            "objects",
            0.010409081480410073
          ]
        ],
        "count": 1937
      },
      "21": {
        "name": "21_explanations_explanation_XAI_interpretability",
        "keywords": [
          [
            "explanations",
            0.03636788273160341
          ],
          [
            "explanation",
            0.021025129439576094
          ],
          [
            "XAI",
            0.016924297914534824
          ],
          [
            "interpretability",
            0.012093035490507517
          ],
          [
            "concept",
            0.011423695212374678
          ],
          [
            "methods",
            0.011010024629851853
          ],
          [
            "Explanations",
            0.010798494336804226
          ],
          [
            "interpretable",
            0.010450300464051735
          ],
          [
            "counterfactual",
            0.01041548698508295
          ],
          [
            "explainability",
            0.010224119041616945
          ]
        ],
        "count": 1896
      },
      "22": {
        "name": "22_news_social_media_social media",
        "keywords": [
          [
            "news",
            0.021311049492635122
          ],
          [
            "social",
            0.020928609641207425
          ],
          [
            "media",
            0.02050322371466182
          ],
          [
            "social media",
            0.01739716055811872
          ],
          [
            "sentiment",
            0.01465201097659653
          ],
          [
            "fake",
            0.013932519422621172
          ],
          [
            "Twitter",
            0.012162449340084959
          ],
          [
            "content",
            0.011387419698087773
          ],
          [
            "detection",
            0.011281289050181581
          ],
          [
            "tweets",
            0.010780106076348323
          ]
        ],
        "count": 1884
      },
      "23": {
        "name": "23_fairness_fair_Fairness_Shapley",
        "keywords": [
          [
            "fairness",
            0.07000143849381904
          ],
          [
            "fair",
            0.02287511354932065
          ],
          [
            "Fairness",
            0.019923809044842603
          ],
          [
            "Shapley",
            0.019738880131989996
          ],
          [
            "bias",
            0.01610475299057767
          ],
          [
            "groups",
            0.012237066955106575
          ],
          [
            "group",
            0.01206982513701202
          ],
          [
            "sensitive",
            0.011172931353621294
          ],
          [
            "Fair",
            0.010291884954205145
          ],
          [
            "machine learning",
            0.010070760668272472
          ]
        ],
        "count": 1798
      },
      "24": {
        "name": "24_energy_power_forecasting_grid",
        "keywords": [
          [
            "energy",
            0.029512415840904874
          ],
          [
            "power",
            0.02754516670216917
          ],
          [
            "forecasting",
            0.016463253090517934
          ],
          [
            "grid",
            0.01605839367603751
          ],
          [
            "load",
            0.01542901118233926
          ],
          [
            "electricity",
            0.015068964811103817
          ],
          [
            "wind",
            0.013701533641455444
          ],
          [
            "renewable",
            0.011282241684330736
          ],
          [
            "consumption",
            0.010903209754644516
          ],
          [
            "solar",
            0.00998748891054926
          ]
        ],
        "count": 1795
      },
      "25": {
        "name": "25_traffic_temporal_spatial_Traffic",
        "keywords": [
          [
            "traffic",
            0.047412233741231054
          ],
          [
            "temporal",
            0.01949778015128308
          ],
          [
            "spatial",
            0.017688494565448253
          ],
          [
            "Traffic",
            0.0156103419625668
          ],
          [
            "mobility",
            0.015302714167553433
          ],
          [
            "prediction",
            0.015162009575190698
          ],
          [
            "transportation",
            0.013650559218717617
          ],
          [
            "road",
            0.013261213209230316
          ],
          [
            "travel",
            0.01300137962295682
          ],
          [
            "urban",
            0.012544283880063827
          ]
        ],
        "count": 1390
      },
      "26": {
        "name": "26_matrix_tensor_rank_low rank",
        "keywords": [
          [
            "matrix",
            0.029025784243673847
          ],
          [
            "tensor",
            0.02857381634691722
          ],
          [
            "rank",
            0.027190971328458588
          ],
          [
            "low rank",
            0.01620257726829116
          ],
          [
            "algorithm",
            0.013810934289661228
          ],
          [
            "low",
            0.013035484385269665
          ],
          [
            "sparse",
            0.012765173674538324
          ],
          [
            "completion",
            0.011835091893498696
          ],
          [
            "problem",
            0.011713987761860065
          ],
          [
            "matrices",
            0.01151661250769931
          ]
        ],
        "count": 1311
      },
      "27": {
        "name": "27_driving_autonomous_traffic_vehicles",
        "keywords": [
          [
            "driving",
            0.04470329827680219
          ],
          [
            "autonomous",
            0.023985272008572037
          ],
          [
            "traffic",
            0.019947182052959183
          ],
          [
            "vehicles",
            0.018956536584000733
          ],
          [
            "vehicle",
            0.01880761993746239
          ],
          [
            "trajectory",
            0.0176084095502191
          ],
          [
            "autonomous driving",
            0.015512857754272118
          ],
          [
            "scenarios",
            0.014018818984300888
          ],
          [
            "Driving",
            0.013982569805399468
          ],
          [
            "Autonomous",
            0.013496730934748019
          ]
        ],
        "count": 1265
      },
      "28": {
        "name": "28_continual_forgetting_continual learning_Continual",
        "keywords": [
          [
            "continual",
            0.03646161163139378
          ],
          [
            "forgetting",
            0.03617079736778332
          ],
          [
            "continual learning",
            0.034168855227524574
          ],
          [
            "Continual",
            0.027820075984623643
          ],
          [
            "catastrophic forgetting",
            0.021833828485778507
          ],
          [
            "catastrophic",
            0.021578070119186426
          ],
          [
            "incremental",
            0.01679168529265379
          ],
          [
            "tasks",
            0.016561427297958096
          ],
          [
            "new",
            0.01552510549962388
          ],
          [
            "task",
            0.01510246175323837
          ]
        ],
        "count": 1225
      },
      "29": {
        "name": "29_market_stock_financial_trading",
        "keywords": [
          [
            "market",
            0.040915425923463825
          ],
          [
            "stock",
            0.038238182221606964
          ],
          [
            "financial",
            0.03207719831031417
          ],
          [
            "trading",
            0.024869167271315343
          ],
          [
            "price",
            0.02267177553820656
          ],
          [
            "portfolio",
            0.017287450478449994
          ],
          [
            "prices",
            0.013831981153879841
          ],
          [
            "markets",
            0.013319524321465022
          ],
          [
            "volatility",
            0.012768236419832924
          ],
          [
            "risk",
            0.011178472736216357
          ]
        ],
        "count": 1197
      },
      "30": {
        "name": "30_detection_IoT_traffic_attacks",
        "keywords": [
          [
            "detection",
            0.025547335555098902
          ],
          [
            "IoT",
            0.02143526516059214
          ],
          [
            "traffic",
            0.01998146560297465
          ],
          [
            "attacks",
            0.018263342444564068
          ],
          [
            "intrusion",
            0.015330325521119074
          ],
          [
            "security",
            0.014973079124743131
          ],
          [
            "network",
            0.013965674309392924
          ],
          [
            "Detection",
            0.01336501345597515
          ],
          [
            "intrusion detection",
            0.012836217641898127
          ],
          [
            "attack",
            0.012093326969202591
          ]
        ],
        "count": 1193
      },
      "31": {
        "name": "31_clustering_clusters_means_Clustering",
        "keywords": [
          [
            "clustering",
            0.05229890826631131
          ],
          [
            "clusters",
            0.021341916884315276
          ],
          [
            "means",
            0.01987541385372429
          ],
          [
            "Clustering",
            0.018878848539543233
          ],
          [
            "cluster",
            0.018495420497303412
          ],
          [
            "algorithm",
            0.016190653528325473
          ],
          [
            "dimensional",
            0.013320088334739244
          ],
          [
            "data",
            0.012499345098287044
          ],
          [
            "points",
            0.012047592007876038
          ],
          [
            "SNE",
            0.010987684598623836
          ]
        ],
        "count": 1075
      },
      "32": {
        "name": "32_uncertainty_calibration_conformal_prediction",
        "keywords": [
          [
            "uncertainty",
            0.03751466201192717
          ],
          [
            "calibration",
            0.03359663969774591
          ],
          [
            "conformal",
            0.02628317966115185
          ],
          [
            "prediction",
            0.022870778112455796
          ],
          [
            "Conformal",
            0.019019294885187676
          ],
          [
            "conformal prediction",
            0.016930783559623314
          ],
          [
            "coverage",
            0.016460365551127263
          ],
          [
            "Uncertainty",
            0.012297303595785243
          ],
          [
            "quantification",
            0.012008271028026344
          ],
          [
            "distribution",
            0.011785450756116768
          ]
        ],
        "count": 1060
      },
      "33": {
        "name": "33_EEG_brain_signals_subject",
        "keywords": [
          [
            "EEG",
            0.09187787712037696
          ],
          [
            "brain",
            0.0232178405516176
          ],
          [
            "signals",
            0.019342918720313265
          ],
          [
            "subject",
            0.013525934306933197
          ],
          [
            "decoding",
            0.01266987109302067
          ],
          [
            "classification",
            0.012035924784515283
          ],
          [
            "subjects",
            0.009790835083534356
          ],
          [
            "Brain",
            0.00944687097425585
          ],
          [
            "emotion",
            0.008876248101861162
          ],
          [
            "signal",
            0.008462562871552231
          ]
        ],
        "count": 1029
      },
      "34": {
        "name": "34_weather_climate_precipitation_forecasting",
        "keywords": [
          [
            "weather",
            0.035027582248643045
          ],
          [
            "climate",
            0.03091719312613445
          ],
          [
            "precipitation",
            0.0242335253993831
          ],
          [
            "forecasting",
            0.019653408193488025
          ],
          [
            "forecasts",
            0.01624522178900917
          ],
          [
            "resolution",
            0.014467155002049157
          ],
          [
            "forecast",
            0.012974805442770554
          ],
          [
            "Weather",
            0.012185043635932062
          ],
          [
            "prediction",
            0.011309798065822328
          ],
          [
            "weather forecasting",
            0.011036095853042195
          ]
        ],
        "count": 1008
      },
      "35": {
        "name": "35_LLMs_attacks_safety_attack",
        "keywords": [
          [
            "LLMs",
            0.028813743665649233
          ],
          [
            "attacks",
            0.026598460362290757
          ],
          [
            "safety",
            0.024527659195530037
          ],
          [
            "attack",
            0.022064175532487282
          ],
          [
            "adversarial",
            0.02111803617047956
          ],
          [
            "LLM",
            0.018226882531148573
          ],
          [
            "harmful",
            0.0179975819461124
          ],
          [
            "Language",
            0.013812363313739124
          ],
          [
            "language",
            0.013157606767122446
          ],
          [
            "Large",
            0.012602539168911396
          ]
        ],
        "count": 994
      },
      "36": {
        "name": "36_entities_entity_Knowledge_knowledge",
        "keywords": [
          [
            "entities",
            0.029732462931332814
          ],
          [
            "entity",
            0.02972243198240185
          ],
          [
            "Knowledge",
            0.026622841094236772
          ],
          [
            "knowledge",
            0.02547576184780914
          ],
          [
            "KG",
            0.024216318986827832
          ],
          [
            "knowledge graph",
            0.019511509363498894
          ],
          [
            "knowledge graphs",
            0.016491304638969497
          ],
          [
            "graph",
            0.016111727068065976
          ],
          [
            "Entity",
            0.015006820277062643
          ],
          [
            "graphs",
            0.013430777412377429
          ]
        ],
        "count": 926
      },
      "37": {
        "name": "37_segmentation_object_image_semantic segmentation",
        "keywords": [
          [
            "segmentation",
            0.03081006448367374
          ],
          [
            "object",
            0.01756589571223257
          ],
          [
            "image",
            0.015186818826211167
          ],
          [
            "semantic segmentation",
            0.014866045989754686
          ],
          [
            "attention",
            0.014574388615828856
          ],
          [
            "semantic",
            0.014008161147143207
          ],
          [
            "vision",
            0.013329476799501647
          ],
          [
            "Segmentation",
            0.012821909164038777
          ],
          [
            "Vision",
            0.012067708393141644
          ],
          [
            "object detection",
            0.008950647421060248
          ]
        ],
        "count": 917
      },
      "38": {
        "name": "38_code_Code_software_code generation",
        "keywords": [
          [
            "code",
            0.07031261530170146
          ],
          [
            "Code",
            0.020469532584339763
          ],
          [
            "software",
            0.018920299642893712
          ],
          [
            "code generation",
            0.016580264541131554
          ],
          [
            "program",
            0.016042835919941916
          ],
          [
            "language",
            0.01492201375378482
          ],
          [
            "generation",
            0.013369697945083966
          ],
          [
            "source code",
            0.012403197604225718
          ],
          [
            "source",
            0.012210111508879108
          ],
          [
            "programs",
            0.011922415313317494
          ]
        ],
        "count": 888
      },
      "39": {
        "name": "39_crop_remote sensing_satellite_remote",
        "keywords": [
          [
            "crop",
            0.020362465315145732
          ],
          [
            "remote sensing",
            0.01677926390149777
          ],
          [
            "satellite",
            0.016269699887572122
          ],
          [
            "remote",
            0.015912459273854616
          ],
          [
            "imagery",
            0.015803142954179658
          ],
          [
            "sensing",
            0.014871073017194203
          ],
          [
            "land",
            0.012679284093355635
          ],
          [
            "images",
            0.012037628248972795
          ],
          [
            "agricultural",
            0.010742096576206148
          ],
          [
            "SAR",
            0.009796063831607582
          ]
        ],
        "count": 883
      },
      "40": {
        "name": "40_Gaussian_GP_variational_inference",
        "keywords": [
          [
            "Gaussian",
            0.03970144591046077
          ],
          [
            "GP",
            0.02305250114537233
          ],
          [
            "variational",
            0.020063241380022245
          ],
          [
            "inference",
            0.017186862781241024
          ],
          [
            "GPs",
            0.01685801959866009
          ],
          [
            "posterior",
            0.016654938873132238
          ],
          [
            "Processes",
            0.016298672977633044
          ],
          [
            "Bayesian",
            0.015223071521277306
          ],
          [
            "Gaussian processes",
            0.01460935886650172
          ],
          [
            "Gaussian process",
            0.014579254493421005
          ]
        ],
        "count": 871
      },
      "41": {
        "name": "41_domain_Domain_source_target",
        "keywords": [
          [
            "domain",
            0.06215081987934222
          ],
          [
            "Domain",
            0.034685800500846936
          ],
          [
            "source",
            0.03284550742745474
          ],
          [
            "target",
            0.032638414936068394
          ],
          [
            "adaptation",
            0.030946154728538094
          ],
          [
            "domain adaptation",
            0.02716247114018633
          ],
          [
            "domains",
            0.025883438419037407
          ],
          [
            "target domain",
            0.022114465366933
          ],
          [
            "Adaptation",
            0.020456769268471803
          ],
          [
            "UDA",
            0.016715506116424684
          ]
        ],
        "count": 862
      },
      "42": {
        "name": "42_problems_instances_combinatorial_problem",
        "keywords": [
          [
            "problems",
            0.020713697554844254
          ],
          [
            "instances",
            0.017167155990902034
          ],
          [
            "combinatorial",
            0.016334458847136746
          ],
          [
            "problem",
            0.016162715399700628
          ],
          [
            "optimization",
            0.013834202278363338
          ],
          [
            "Problem",
            0.013664571402035211
          ],
          [
            "combinatorial optimization",
            0.013609813438312838
          ],
          [
            "solutions",
            0.012892288873260107
          ],
          [
            "solution",
            0.012599697676467325
          ],
          [
            "solvers",
            0.011920705393730427
          ]
        ],
        "count": 826
      },
      "43": {
        "name": "43_spectra_simulations_mass_surveys",
        "keywords": [
          [
            "spectra",
            0.01022939764908584
          ],
          [
            "simulations",
            0.009997280581485501
          ],
          [
            "mass",
            0.009950528058850128
          ],
          [
            "surveys",
            0.008902842527505478
          ],
          [
            "data",
            0.00817107137576701
          ],
          [
            "neural",
            0.00773118758385661
          ],
          [
            "light",
            0.007589271071556106
          ],
          [
            "machine",
            0.007488401239764745
          ],
          [
            "parameters",
            0.00741304120764029
          ],
          [
            "images",
            0.007377239015195083
          ]
        ],
        "count": 822
      },
      "44": {
        "name": "44_PAC_bounds_dimension_risk",
        "keywords": [
          [
            "PAC",
            0.02979930377344181
          ],
          [
            "bounds",
            0.025513629181276157
          ],
          [
            "dimension",
            0.014891341812933015
          ],
          [
            "risk",
            0.014522690944947063
          ],
          [
            "sample",
            0.014284676789516327
          ],
          [
            "distribution",
            0.013714395512987719
          ],
          [
            "class",
            0.013029870656440049
          ],
          [
            "algorithm",
            0.01240701306486746
          ],
          [
            "complexity",
            0.012046506589897955
          ],
          [
            "learning",
            0.011893513405790725
          ]
        ],
        "count": 662
      },
      "45": {
        "name": "45_SNN_spiking_neuromorphic_spike",
        "keywords": [
          [
            "SNN",
            0.03768919604923718
          ],
          [
            "spiking",
            0.03609933951341025
          ],
          [
            "neuromorphic",
            0.0288052956426749
          ],
          [
            "spike",
            0.022228769226012832
          ],
          [
            "energy",
            0.019363174084855692
          ],
          [
            "Neural",
            0.015844091675933838
          ],
          [
            "Networks",
            0.01429627639996768
          ],
          [
            "neurons",
            0.014032821073149527
          ],
          [
            "neural",
            0.013995400644380382
          ],
          [
            "event",
            0.01346561580920193
          ]
        ],
        "count": 660
      },
      "46": {
        "name": "46_HAR_activity_Activity_activity recognition",
        "keywords": [
          [
            "HAR",
            0.03863925362323731
          ],
          [
            "activity",
            0.026323835278655457
          ],
          [
            "Activity",
            0.022226531954932863
          ],
          [
            "activity recognition",
            0.01926558537830114
          ],
          [
            "sensor",
            0.018641658310838288
          ],
          [
            "activities",
            0.018559078501529536
          ],
          [
            "recognition",
            0.018049236829343843
          ],
          [
            "Human",
            0.017272719939198884
          ],
          [
            "Recognition",
            0.017143431749737603
          ],
          [
            "human",
            0.014755341711484578
          ]
        ],
        "count": 642
      },
      "47": {
        "name": "47_ECG_signals_heart_signal",
        "keywords": [
          [
            "ECG",
            0.09209847435229003
          ],
          [
            "signals",
            0.022027305651547858
          ],
          [
            "heart",
            0.020564847535820514
          ],
          [
            "signal",
            0.014379267591052838
          ],
          [
            "lead",
            0.011315589637466724
          ],
          [
            "classification",
            0.01119310954055997
          ],
          [
            "clinical",
            0.010772720332277245
          ],
          [
            "monitoring",
            0.010047657399656856
          ],
          [
            "detection",
            0.008294938253235115
          ],
          [
            "AF",
            0.0076193770031904645
          ]
        ],
        "count": 638
      },
      "48": {
        "name": "48_shot_meta_shot learning_meta learning",
        "keywords": [
          [
            "shot",
            0.04995622511552249
          ],
          [
            "meta",
            0.03938224196039466
          ],
          [
            "shot learning",
            0.0308047564224274
          ],
          [
            "meta learning",
            0.02961671900181127
          ],
          [
            "Few",
            0.021846714546596608
          ],
          [
            "Shot",
            0.01832680727048033
          ],
          [
            "Meta",
            0.01754157754011351
          ],
          [
            "learning",
            0.016191227526094226
          ],
          [
            "classes",
            0.01456397124263661
          ],
          [
            "MAML",
            0.0134374968044213
          ]
        ],
        "count": 620
      },
      "49": {
        "name": "49_brain_fMRI_Alzheimer_AD",
        "keywords": [
          [
            "brain",
            0.0606209154205299
          ],
          [
            "fMRI",
            0.027216252357312835
          ],
          [
            "Alzheimer",
            0.027150203441895948
          ],
          [
            "AD",
            0.024820459048166463
          ],
          [
            "functional",
            0.022327204820051273
          ],
          [
            "disease",
            0.021410275948985435
          ],
          [
            "connectivity",
            0.01647566867483146
          ],
          [
            "MRI",
            0.01635750699924478
          ],
          [
            "Brain",
            0.014811121894951327
          ],
          [
            "neuroimaging",
            0.013801449010944275
          ]
        ],
        "count": 610
      },
      "50": {
        "name": "50_active learning_active_Active_AL",
        "keywords": [
          [
            "active learning",
            0.05906485396259214
          ],
          [
            "active",
            0.05792854359727616
          ],
          [
            "Active",
            0.04635960311198353
          ],
          [
            "AL",
            0.03118068029691596
          ],
          [
            "learning",
            0.015741522698191988
          ],
          [
            "labeling",
            0.014139437408227519
          ],
          [
            "Learning",
            0.013327753258334437
          ],
          [
            "Active learning",
            0.012698553650218034
          ],
          [
            "label",
            0.012371330647712054
          ],
          [
            "unlabeled",
            0.012337080781253408
          ]
        ],
        "count": 603
      },
      "51": {
        "name": "51_fault_maintenance_diagnosis_Fault",
        "keywords": [
          [
            "fault",
            0.04146807780659901
          ],
          [
            "maintenance",
            0.018241637617385315
          ],
          [
            "diagnosis",
            0.0165158097462845
          ],
          [
            "Fault",
            0.01403306806611817
          ],
          [
            "industrial",
            0.013951888923733667
          ],
          [
            "faults",
            0.01392269062299948
          ],
          [
            "monitoring",
            0.01064033958207324
          ],
          [
            "detection",
            0.010521504245465597
          ],
          [
            "failure",
            0.01034542161351091
          ],
          [
            "data",
            0.009939302938943467
          ]
        ],
        "count": 600
      },
      "52": {
        "name": "52_BO_Bayesian_optimization_Bayesian optimization",
        "keywords": [
          [
            "BO",
            0.04617609214948535
          ],
          [
            "Bayesian",
            0.0460233663334191
          ],
          [
            "optimization",
            0.03785042611010124
          ],
          [
            "Bayesian optimization",
            0.03101660787447166
          ],
          [
            "Optimization",
            0.023067093734360375
          ],
          [
            "function",
            0.021997483801340208
          ],
          [
            "acquisition",
            0.021744558777864114
          ],
          [
            "box",
            0.019187300841386173
          ],
          [
            "black",
            0.018725534747685738
          ],
          [
            "black box",
            0.01872223576503327
          ]
        ],
        "count": 586
      },
      "53": {
        "name": "53_AI_systems_ML_Artificial",
        "keywords": [
          [
            "AI",
            0.0834838727274691
          ],
          [
            "systems",
            0.017563978614507345
          ],
          [
            "ML",
            0.017548477029163224
          ],
          [
            "Artificial",
            0.013129762634686766
          ],
          [
            "risks",
            0.01301242748733371
          ],
          [
            "research",
            0.012612546889394775
          ],
          [
            "Intelligence",
            0.012105004153082638
          ],
          [
            "intelligence",
            0.01190154164166866
          ],
          [
            "development",
            0.011710866682533598
          ],
          [
            "ethical",
            0.011274445827946625
          ]
        ],
        "count": 584
      },
      "54": {
        "name": "54_anomaly_detection_Anomaly_anomalies",
        "keywords": [
          [
            "anomaly",
            0.06135556137149393
          ],
          [
            "detection",
            0.04541331528922281
          ],
          [
            "Anomaly",
            0.029473799912927003
          ],
          [
            "anomalies",
            0.02663018080568934
          ],
          [
            "outlier",
            0.02162157704170919
          ],
          [
            "Detection",
            0.020090540503450258
          ],
          [
            "normal",
            0.01766415385570047
          ],
          [
            "outlier detection",
            0.015868193321468115
          ],
          [
            "outliers",
            0.01332993447262209
          ],
          [
            "Outlier",
            0.01301316120595501
          ]
        ],
        "count": 530
      },
      "55": {
        "name": "55_trees_tree_boosting_decision",
        "keywords": [
          [
            "trees",
            0.037030703533777416
          ],
          [
            "tree",
            0.02831964262546267
          ],
          [
            "boosting",
            0.02204325023322369
          ],
          [
            "decision",
            0.020694013148214098
          ],
          [
            "decision trees",
            0.018139666673734112
          ],
          [
            "regression",
            0.014281437271323603
          ],
          [
            "Trees",
            0.013300383714599746
          ],
          [
            "forest",
            0.013224436692856908
          ],
          [
            "ensemble",
            0.013109031085363236
          ],
          [
            "forests",
            0.012627576623731998
          ]
        ],
        "count": 515
      },
      "56": {
        "name": "56_COVID_epidemic_pandemic_spread",
        "keywords": [
          [
            "COVID",
            0.048791737548735864
          ],
          [
            "epidemic",
            0.023697055595805257
          ],
          [
            "pandemic",
            0.021439261714152582
          ],
          [
            "spread",
            0.01895423309980791
          ],
          [
            "disease",
            0.018300533608061584
          ],
          [
            "forecasting",
            0.015494494552906442
          ],
          [
            "cases",
            0.014213550485754864
          ],
          [
            "infection",
            0.012436367609372416
          ],
          [
            "countries",
            0.012091878924924318
          ],
          [
            "infectious",
            0.01145888566761224
          ]
        ],
        "count": 510
      },
      "57": {
        "name": "57_optimization_hyperparameter_Pareto_Optimization",
        "keywords": [
          [
            "optimization",
            0.02899329110907504
          ],
          [
            "hyperparameter",
            0.018965649165657895
          ],
          [
            "Pareto",
            0.016526708778163635
          ],
          [
            "Optimization",
            0.015813130207513373
          ],
          [
            "objective",
            0.015385667702648048
          ],
          [
            "Hyperparameter",
            0.014154147600609063
          ],
          [
            "problems",
            0.013867883160250935
          ],
          [
            "algorithm",
            0.013784577428417231
          ],
          [
            "algorithms",
            0.013598909984766172
          ],
          [
            "search",
            0.012912015112341973
          ]
        ],
        "count": 501
      },
      "58": {
        "name": "58_emotion_facial_Emotion_emotions",
        "keywords": [
          [
            "emotion",
            0.039517754642969326
          ],
          [
            "facial",
            0.023359197579970365
          ],
          [
            "Emotion",
            0.023012776752269166
          ],
          [
            "emotions",
            0.0227207787888696
          ],
          [
            "recognition",
            0.01985783443203639
          ],
          [
            "emotion recognition",
            0.017663775184525828
          ],
          [
            "multimodal",
            0.01596629438953374
          ],
          [
            "emotional",
            0.01418304650568211
          ],
          [
            "affective",
            0.013445351445605887
          ],
          [
            "Recognition",
            0.012895481010785168
          ]
        ],
        "count": 490
      },
      "59": {
        "name": "59_graph_GNNs_Graph_attacks",
        "keywords": [
          [
            "graph",
            0.04485410745838464
          ],
          [
            "GNNs",
            0.0335821872809992
          ],
          [
            "Graph",
            0.029099814054626146
          ],
          [
            "attacks",
            0.028591265260036268
          ],
          [
            "attack",
            0.027538759299288038
          ],
          [
            "node",
            0.024797575669171148
          ],
          [
            "GNN",
            0.022244838559272603
          ],
          [
            "privacy",
            0.019489718357814486
          ],
          [
            "adversarial",
            0.019220683551026584
          ],
          [
            "nodes",
            0.015029866304673935
          ]
        ],
        "count": 449
      },
      "60": {
        "name": "60_fraud_fraud detection_transactions_financial",
        "keywords": [
          [
            "fraud",
            0.05207574402287744
          ],
          [
            "fraud detection",
            0.029743945553394067
          ],
          [
            "transactions",
            0.024816110968602766
          ],
          [
            "financial",
            0.02411118059961221
          ],
          [
            "detection",
            0.022352097246929204
          ],
          [
            "credit",
            0.018856077173420638
          ],
          [
            "fraudulent",
            0.012099263332251933
          ],
          [
            "graph",
            0.010885515826898512
          ],
          [
            "card",
            0.010245580765771842
          ],
          [
            "money",
            0.009640733273499295
          ]
        ],
        "count": 442
      },
      "61": {
        "name": "61_NAS_search_Architecture_architecture",
        "keywords": [
          [
            "NAS",
            0.08477877247116145
          ],
          [
            "search",
            0.05939962689581854
          ],
          [
            "Architecture",
            0.03800191548037739
          ],
          [
            "architecture",
            0.03520591945752215
          ],
          [
            "Search",
            0.0324732456794325
          ],
          [
            "architectures",
            0.02913131043226153
          ],
          [
            "architecture search",
            0.0258481195786174
          ],
          [
            "Neural",
            0.021755752875151944
          ],
          [
            "search space",
            0.021238809010364408
          ],
          [
            "neural architecture",
            0.018107355578991197
          ]
        ],
        "count": 440
      },
      "62": {
        "name": "62_OOD_detection_Out_distribution",
        "keywords": [
          [
            "OOD",
            0.11392573672780687
          ],
          [
            "detection",
            0.04249732882999431
          ],
          [
            "Out",
            0.03283739486824907
          ],
          [
            "distribution",
            0.03228659609773587
          ],
          [
            "ID",
            0.023407855513570785
          ],
          [
            "Distribution",
            0.022078258335746805
          ],
          [
            "samples",
            0.01817146492748191
          ],
          [
            "OOD samples",
            0.017683334421901478
          ],
          [
            "Detection",
            0.01643767051611723
          ],
          [
            "OoD",
            0.014373142279278626
          ]
        ],
        "count": 430
      }
    },
    "correlations": [
      [
        1.0,
        -0.7505002193030139,
        -0.7111368928349939,
        -0.7424209358097772,
        -0.7568668211864469,
        -0.7194259890323518,
        -0.7442309720613256,
        -0.7559039067390634,
        -0.7624410771300881,
        -0.736512465472474,
        -0.7239992215081041,
        -0.7412508394960755,
        -0.7517027021250698,
        -0.659603570956075,
        -0.7302115718923871,
        -0.7427332312896835,
        -0.7301875914069877,
        -0.7210007493937369,
        -0.7394114695613678,
        -0.749457463701072,
        -0.7376859772833098,
        -0.7516560018103974,
        -0.7533194107382375,
        -0.7559298701487032,
        -0.7186936720393173,
        -0.7310912003061609,
        -0.7494851770491188,
        -0.6771002333419165,
        -0.7444216683438276,
        -0.7358644709196509,
        -0.744387800408285,
        -0.721838889760614,
        -0.7197004118169547,
        -0.7605370982727153,
        -0.7577878673899336,
        -0.6932376424475961,
        -0.7265610036872362,
        -0.7480175328277825,
        -0.70754583639552,
        -0.7546113840012988,
        -0.7420263944390819,
        -0.6962523752783026,
        -0.6534945278977999,
        -0.7623266592651814,
        -0.705201598859701,
        -0.7443879117287978,
        -0.7470431536148648,
        -0.758584219246286,
        -0.7139616165987281,
        -0.7559999645284105,
        -0.7385284680212711,
        -0.75688275359419,
        -0.6978475099011835,
        -0.7310736892244291,
        -0.7590398706449911,
        -0.741895126989955,
        -0.75768859605403,
        -0.6766428319272964,
        -0.7617430783459412,
        -0.7398049812013268,
        -0.7535672709924157,
        -0.7190941121975267,
        -0.7344395987948842
      ],
      [
        -0.7505002193030139,
        1.0,
        -0.7380533590833795,
        -0.7518853338895549,
        -0.76278309128693,
        -0.7008779535104959,
        -0.7407671010432364,
        -0.7371962607224158,
        -0.7532403613650243,
        -0.7413162301927699,
        -0.6834462965516424,
        -0.7248047013014299,
        -0.7561370947921853,
        -0.7316254464051326,
        -0.7122069583447062,
        -0.7589336857171979,
        -0.7431489737831907,
        -0.7471789522855143,
        -0.7259597846724799,
        -0.7326801652703998,
        -0.7570721412210752,
        -0.7549566258849505,
        -0.755092694582164,
        -0.7222275642606288,
        -0.7225385980910091,
        -0.7471393514900166,
        -0.7480060828121924,
        -0.7459091075703173,
        -0.7417058926843962,
        -0.7520964661481403,
        -0.6257378649278402,
        -0.7232818631245541,
        -0.7498034784291612,
        -0.7618786308835209,
        -0.7619468451201703,
        -0.5962233793962364,
        -0.7296933336347764,
        -0.7468838486896908,
        -0.7303538557450033,
        -0.7550914003082028,
        -0.7402691624815034,
        -0.7280268052610681,
        -0.7345756162691097,
        -0.7631254174222245,
        -0.7314439346155809,
        -0.7399547350996566,
        -0.7485798797233647,
        -0.7593665121755859,
        -0.7457901400947926,
        -0.7568118084572885,
        -0.7519984512295064,
        -0.7540135024424037,
        -0.7219716376585772,
        -0.734433567238703,
        -0.7418961450627461,
        -0.7518137364897136,
        -0.7545906897972674,
        -0.7175014669830482,
        -0.7570075548923885,
        -0.5598712531448224,
        -0.7460829410932296,
        -0.7488610211747795,
        -0.7459480093210761
      ],
      [
        -0.7111368928349939,
        -0.7380533590833795,
        1.0,
        -0.7548432614162037,
        -0.7355888492930751,
        -0.7388656044435208,
        -0.7432633665030393,
        -0.7442312815506619,
        -0.7323153851716606,
        -0.719813990075955,
        -0.7487834149338548,
        -0.7083499947120573,
        -0.7600402651418958,
        -0.7532188133011933,
        -0.7201321843747763,
        -0.7441347149158957,
        -0.7294958851404523,
        -0.5499006992786175,
        -0.7342416072688307,
        -0.7353605304239712,
        -0.7504337858106379,
        -0.7385422923764001,
        -0.7239809116425324,
        -0.7512651615917865,
        -0.7413195284375661,
        -0.7524067396325614,
        -0.7231697260413643,
        -0.7486634605835404,
        -0.7386042472826398,
        -0.7494581511078311,
        -0.7378100929745579,
        -0.7461883814452517,
        -0.735075300217667,
        -0.7604403044787957,
        -0.7593830683591272,
        0.05923531204684884,
        -0.6994914496307378,
        -0.7331749773248637,
        -0.4388384160640052,
        -0.7603488896263197,
        -0.7514744446557933,
        -0.6822440103056653,
        -0.7320345433778261,
        -0.7643223207014724,
        -0.7569232553371624,
        -0.7509423087789531,
        -0.7490258745254357,
        -0.7594788598051296,
        -0.6772921816344762,
        -0.7544526872467433,
        -0.7512734367693895,
        -0.7563372065557226,
        -0.7290831136781877,
        -0.684379441578413,
        -0.742887800500452,
        -0.7477717730903577,
        -0.7532979609217431,
        -0.7279705125998435,
        -0.7485749347832082,
        -0.732194068796414,
        -0.7491775238381835,
        -0.7205530193734564,
        -0.7408101846529374
      ],
      [
        -0.7424209358097772,
        -0.7518853338895549,
        -0.7548432614162037,
        1.0,
        -0.7485561370914418,
        -0.39196967117388415,
        -0.7233095813231302,
        -0.7401190186842187,
        -0.7541095201831636,
        -0.7188810209658301,
        -0.7094199859120375,
        -0.44212227002640997,
        -0.7372709375544897,
        -0.7482376071311236,
        -0.7552862680551675,
        -0.7511678058115383,
        -0.7069039562531195,
        -0.7497242002142984,
        -0.7396995587387398,
        -0.7520783551855019,
        -0.732197461024367,
        -0.7481705058273407,
        -0.7518834431412562,
        -0.7611797299104472,
        -0.7063957233404999,
        -0.7326857172107734,
        -0.7296077745998191,
        -0.7390990623075904,
        -0.754973835391416,
        -0.7551187634529133,
        -0.7468163241337252,
        -0.7351845050202337,
        -0.7113937632037182,
        -0.7566629353064416,
        -0.7326493695691781,
        -0.7459056639890569,
        -0.7405765573211309,
        -0.7464292360940954,
        -0.736012206576337,
        -0.7573307443063558,
        -0.7070689701130424,
        -0.7082690609474637,
        -0.698241663563836,
        -0.7441034803936635,
        -0.7377191567092831,
        -0.46402848921734785,
        -0.7505409596074906,
        -0.7564231652472064,
        -0.7456809151036272,
        -0.7423374582448168,
        -0.7502427676619333,
        -0.7558314192913518,
        -0.6993780646195139,
        -0.7479810941311116,
        -0.754331108560374,
        -0.748642583857619,
        -0.7544408078071352,
        -0.7158306956739571,
        -0.7605396304516765,
        -0.7163889385872093,
        -0.7568768926044047,
        -0.7251448713805777,
        -0.7436556494377821
      ],
      [
        -0.7568668211864469,
        -0.76278309128693,
        -0.7355888492930751,
        -0.7485561370914418,
        1.0,
        -0.74920672281712,
        -0.6862220561010572,
        -0.7522033580298835,
        -0.7647173664918434,
        -0.7123615241282186,
        -0.7582231617426916,
        -0.7540422504288526,
        -0.7313224293542111,
        -0.7613489184387608,
        -0.758328260734759,
        -0.7488347933284978,
        -0.7532987639297737,
        -0.7547857362797445,
        -0.7501228535569953,
        -0.7359770383353229,
        -0.7036601042950887,
        -0.7562143867700757,
        -0.7580653768499476,
        -0.7627797694953138,
        -0.7307600584599425,
        -0.7594092486579151,
        -0.7540719539246604,
        -0.7593152916517829,
        -0.7640538094275258,
        -0.7624072351513965,
        -0.7613343515913015,
        -0.7501136573714109,
        -0.733816775480871,
        -0.7624365379083391,
        -0.7606016210819443,
        -0.7472315650307106,
        -0.7392274718168979,
        -0.7575305107521897,
        -0.7367105805981873,
        -0.7619916694170406,
        -0.750504704719696,
        -0.7346106635156204,
        -0.7508196239567113,
        -0.7479682021777894,
        -0.763750249796141,
        -0.7458540011354508,
        -0.7440617409235326,
        -0.7615505012517942,
        -0.7472648444318779,
        -0.744527326424408,
        -0.7456318233482155,
        -0.762712760394699,
        -0.738727556976376,
        -0.73993582857462,
        -0.7593535198404171,
        -0.754456064808947,
        -0.7516485568738531,
        -0.7409641191672427,
        -0.761990151517549,
        -0.7051434039464001,
        -0.761383480744154,
        -0.7411263283234834,
        -0.7528380963021246
      ],
      [
        -0.7194259890323518,
        -0.7008779535104959,
        -0.7388656044435208,
        -0.39196967117388415,
        -0.74920672281712,
        1.0,
        -0.7037846722597071,
        -0.7239333626056148,
        -0.7476100405759487,
        -0.7207055869773438,
        -0.671493558064714,
        -0.2665988262262622,
        -0.7351119023040542,
        -0.7085649314885883,
        -0.7429367324867758,
        -0.7512758968360431,
        -0.7119573235217127,
        -0.7346002004568757,
        -0.7052075348979896,
        -0.748526465518218,
        -0.7283337248950377,
        -0.731234341140909,
        -0.7532383146792564,
        -0.754399799989788,
        -0.7029918322363211,
        -0.7442112834942588,
        -0.6913658410026494,
        -0.7368406806284452,
        -0.7415330050442323,
        -0.7523786318362866,
        -0.7304986394238857,
        -0.7040404781855296,
        -0.7128308010335079,
        -0.7543909019654892,
        -0.7512795414800344,
        -0.7205464028065507,
        -0.7232872889398663,
        -0.7256394897025098,
        -0.7228464916130858,
        -0.752145465303522,
        -0.6874310093844989,
        -0.7072311544300964,
        -0.6711234524116471,
        -0.7531127668651079,
        -0.6828595033969156,
        -0.37434685142660407,
        -0.745035382209874,
        -0.7533475223094308,
        -0.7384008328735436,
        -0.7361924071008688,
        -0.7491685612875872,
        -0.754047567037005,
        -0.637215819524907,
        -0.743817155046157,
        -0.7462767066213414,
        -0.7329856866713496,
        -0.7550787614745493,
        -0.6369284083861503,
        -0.7556414269410833,
        -0.6836992768993673,
        -0.7532619406147736,
        -0.7051450692776349,
        -0.7298551470214636
      ],
      [
        -0.7442309720613256,
        -0.7407671010432364,
        -0.7432633665030393,
        -0.7233095813231302,
        -0.6862220561010572,
        -0.7037846722597071,
        1.0,
        -0.7479656552007721,
        -0.7602953338155318,
        -0.737842170422386,
        -0.7244545745890736,
        -0.7210194504386596,
        -0.7483699377243738,
        -0.7421676963526818,
        -0.7208865264398923,
        -0.7324472056186007,
        -0.7262354096179415,
        -0.7469272994509784,
        -0.7114962438794475,
        -0.7497309028703287,
        -0.7326774667629241,
        -0.7397795898635778,
        -0.7381400424442259,
        -0.7496401814674518,
        -0.7173708839989812,
        -0.7045608688588003,
        -0.7224433113530195,
        -0.7449861619891651,
        -0.7535840079216571,
        -0.7497406786705298,
        -0.7265693593762917,
        -0.6924445410749467,
        -0.7388575949918813,
        -0.7527804223224228,
        -0.7510883606974044,
        -0.7347657582204862,
        -0.6492266926883778,
        -0.7426091491281124,
        -0.7123608667235721,
        -0.7594287431189342,
        -0.7421984192916637,
        -0.718857278594067,
        -0.7032480366578777,
        -0.7622138593473107,
        -0.747295631019901,
        -0.6539168061463327,
        -0.7479109141735134,
        -0.7530376559728613,
        -0.737015726431868,
        -0.734931796060853,
        -0.7522068487074507,
        -0.754832675129222,
        -0.7316896136810752,
        -0.7514408183037798,
        -0.7298539183618891,
        -0.7326689240341127,
        -0.7538050371640945,
        -0.7315478381300516,
        -0.7581464678626787,
        0.2395609866099565,
        -0.7332624024643166,
        -0.7212009031480084,
        -0.7397981457983164
      ],
      [
        -0.7559039067390634,
        -0.7371962607224158,
        -0.7442312815506619,
        -0.7401190186842187,
        -0.7522033580298835,
        -0.7239333626056148,
        -0.7479656552007721,
        1.0,
        -0.7512245058835249,
        -0.5626227965862691,
        -0.7353685619817518,
        -0.7291186988601549,
        -0.7572081469710543,
        -0.756284502130452,
        -0.7489219015129271,
        -0.7398696386918412,
        -0.7289936806864361,
        -0.60564219091385,
        -0.6898029914828446,
        -0.544968323335406,
        -0.6430502325054632,
        -0.7443554713862617,
        -0.7109928969491967,
        -0.7571177051246596,
        -0.7426274350572875,
        -0.7462605806764082,
        -0.7488560650323879,
        -0.7427621222058756,
        -0.7548919606137409,
        -0.7585813284391014,
        -0.7123638130581953,
        -0.7328963130711545,
        -0.728301020177391,
        -0.7406664627962788,
        -0.7499307110896962,
        -0.7368170027234893,
        -0.739026864735645,
        -0.2571384146717612,
        -0.7092989063532474,
        -0.6812671753767594,
        -0.7448755855890108,
        -0.6888714670032143,
        -0.7361943700538921,
        -0.756689060100786,
        -0.7603428164294443,
        -0.7332314721802669,
        -0.7370225351875269,
        -0.747936212016439,
        -0.7290194168873794,
        -0.6466716471632744,
        -0.749199333137887,
        -0.7209839704626806,
        -0.7384876487644401,
        -0.7237627294447759,
        -0.6977189039865336,
        -0.7474964608857395,
        -0.6003478584190824,
        -0.7479292047562822,
        -0.7362381508344372,
        -0.7367936675500708,
        -0.7344910716754749,
        -0.7341377629136782,
        -0.7206531650198051
      ],
      [
        -0.7624410771300881,
        -0.7532403613650243,
        -0.7323153851716606,
        -0.7541095201831636,
        -0.7647173664918434,
        -0.7476100405759487,
        -0.7602953338155318,
        -0.7512245058835249,
        1.0,
        -0.7300838562417209,
        -0.7472473357598122,
        -0.7414326207534432,
        -0.7623417476610576,
        -0.7625986426749285,
        -0.7452867688483222,
        -0.7577916454672737,
        -0.7478079192259897,
        -0.6940953709985743,
        -0.7285214234151883,
        -0.7515242532373774,
        -0.7527128381008807,
        -0.7592495332481336,
        -0.729447792734438,
        -0.7611572654648756,
        -0.7537171251904609,
        -0.7543934901309486,
        -0.7561062242551642,
        -0.7603685588136551,
        -0.7584658019367081,
        -0.7616668278461107,
        -0.7368264555440976,
        -0.7517260760949953,
        -0.7547415169501577,
        -0.7481902695773085,
        -0.7634399195722339,
        -0.7416497201674996,
        -0.7470854264117288,
        -0.7430536648630965,
        -0.7336344045041907,
        -0.7619219366289329,
        -0.7547407328199647,
        -0.7101234022711715,
        -0.7545264638849047,
        -0.7633751666906623,
        -0.7635213041424266,
        -0.7475982308734833,
        -0.7312615968125309,
        -0.7546158846801312,
        -0.7383126602062576,
        -0.7501028467700257,
        -0.7542747727692654,
        -0.7608708054276798,
        -0.7554781182524899,
        -0.7481197477030312,
        -0.7365445199722569,
        -0.7587395241407595,
        -0.7524462258117164,
        -0.7550405417779447,
        -0.6495174686090163,
        -0.7517258455548466,
        -0.7504502046903175,
        -0.7459592916830733,
        -0.7494810739837472
      ],
      [
        -0.736512465472474,
        -0.7413162301927699,
        -0.719813990075955,
        -0.7188810209658301,
        -0.7123615241282186,
        -0.7207055869773438,
        -0.737842170422386,
        -0.5626227965862691,
        -0.7300838562417209,
        1.0,
        -0.7368436513268831,
        -0.7330791393402585,
        -0.7535374438121945,
        -0.7526460177076097,
        -0.7407311944971968,
        -0.7507483222906142,
        -0.7281702174539473,
        -0.5603670048435682,
        -0.7234741017930943,
        -0.7403380025475802,
        -0.6928451436696361,
        -0.7458749307973349,
        -0.7467536184307475,
        -0.7565000826899021,
        -0.7296390347497935,
        -0.7449586983270574,
        -0.7447943826173493,
        -0.742982612391458,
        -0.7538862975159978,
        -0.7570453856362951,
        -0.7397869528202929,
        -0.7335944167353086,
        -0.7378759543590333,
        -0.7572736878830535,
        -0.7463076920021081,
        -0.720341344101334,
        -0.7383700621879828,
        -0.6399765226892397,
        -0.6957825944448863,
        -0.7402112479798412,
        -0.7055124805605568,
        -0.699789832690221,
        -0.7276557365457879,
        -0.7621698352812682,
        -0.7503270067342831,
        -0.7412429537771736,
        -0.7492719872525382,
        -0.7566955641732971,
        -0.7268687946653185,
        -0.7418831821132057,
        -0.7553710989935546,
        -0.7585886607180476,
        -0.723154674070181,
        -0.7279715617988025,
        -0.7411056840806637,
        -0.7531321422508223,
        -0.757576411134079,
        -0.7297831292974863,
        -0.7449975224992033,
        -0.7300740585097032,
        -0.7550650265710604,
        -0.7366592862849233,
        -0.7264399933074843
      ],
      [
        -0.7239992215081041,
        -0.6834462965516424,
        -0.7487834149338548,
        -0.7094199859120375,
        -0.7582231617426916,
        -0.671493558064714,
        -0.7244545745890736,
        -0.7353685619817518,
        -0.7472473357598122,
        -0.7368436513268831,
        1.0,
        -0.6742969145620702,
        -0.7521822479801674,
        -0.7355376009594446,
        -0.7245428335926818,
        -0.7575152460739086,
        -0.726618476325356,
        -0.7418464535959968,
        -0.743633399492855,
        -0.7534776687137386,
        -0.7413608465904379,
        -0.7568738755068514,
        -0.7508722492031974,
        -0.7586381614628321,
        -0.6977486793943279,
        -0.7239692625247067,
        -0.7372353897211303,
        -0.7333217632013653,
        -0.7550898499881493,
        -0.7574576083938037,
        -0.6971831670990614,
        -0.7344122117690238,
        -0.7427430949797196,
        -0.7479354908603716,
        -0.7568794408570413,
        -0.741089697514732,
        -0.738622169145543,
        -0.7345351047067823,
        -0.7413931776604877,
        -0.7391071767750405,
        -0.7426918008124932,
        -0.7301759459219015,
        -0.7268442513549322,
        -0.7599256457907437,
        -0.748430859348934,
        -0.7049935581502826,
        -0.7401114058901321,
        -0.7477433676667254,
        -0.7485776444396779,
        -0.7508507014290544,
        -0.7522637186342636,
        -0.7540853011122897,
        -0.7308474968217286,
        -0.7385383336791209,
        -0.7398008737722033,
        -0.7529578029155122,
        -0.758292762789526,
        -0.7244272894095889,
        -0.7527135877533142,
        -0.7213343029014752,
        -0.7520047607961442,
        -0.7328297855674366,
        -0.7492156784468885
      ],
      [
        -0.7412508394960755,
        -0.7248047013014299,
        -0.7083499947120573,
        -0.44212227002640997,
        -0.7540422504288526,
        -0.2665988262262622,
        -0.7210194504386596,
        -0.7291186988601549,
        -0.7414326207534432,
        -0.7330791393402585,
        -0.6742969145620702,
        1.0,
        -0.7441833904448892,
        -0.7464945151180804,
        -0.744959667339705,
        -0.7542522496275932,
        -0.7224152668480639,
        -0.7317426420959422,
        -0.7285955200342951,
        -0.7502193342341461,
        -0.739475087555713,
        -0.7400653841533631,
        -0.7561207204785851,
        -0.755239578499816,
        -0.6696388823935233,
        -0.7487858248396606,
        -0.7114484802022284,
        -0.7392308319614989,
        -0.7454859792213886,
        -0.7537010007933316,
        -0.7230139076187625,
        -0.7315414942947702,
        -0.7206975773114077,
        -0.7544440930783083,
        -0.7533059516245493,
        -0.6981392909715658,
        -0.7371065943835815,
        -0.722285192568128,
        -0.7064693063763778,
        -0.7533448761206354,
        -0.7223377578977257,
        -0.7189063526334808,
        -0.7179219256071847,
        -0.7527043677793784,
        -0.7382766815840816,
        -0.4121717377791376,
        -0.745900016415667,
        -0.7551741082594159,
        -0.7382376060758402,
        -0.7428566223695332,
        -0.7504191713021466,
        -0.7497334625069922,
        -0.7071927251419547,
        -0.7325812891770822,
        -0.745809415578731,
        -0.7369114543888969,
        -0.752830641929884,
        -0.7101556064922716,
        -0.7565114206828125,
        -0.699109467411465,
        -0.7524705660542115,
        -0.691961281584007,
        -0.7380949496432134
      ],
      [
        -0.7517027021250698,
        -0.7561370947921853,
        -0.7600402651418958,
        -0.7372709375544897,
        -0.7313224293542111,
        -0.7351119023040542,
        -0.7483699377243738,
        -0.7572081469710543,
        -0.7623417476610576,
        -0.7535374438121945,
        -0.7521822479801674,
        -0.7441833904448892,
        1.0,
        -0.7475235627394645,
        -0.7615378917084621,
        -0.7608100466855178,
        -0.7508626850408857,
        -0.7626464603830352,
        -0.756419319528918,
        -0.7611420283942201,
        -0.7562862914783203,
        -0.7608976148318906,
        -0.7616757447486693,
        -0.7632766592352842,
        -0.7427710698270571,
        -0.7605526987399293,
        -0.7353739411619378,
        -0.758846650665973,
        -0.7627386851635736,
        -0.7551758362129177,
        -0.7553088020301211,
        -0.7492430508545374,
        -0.75415734809935,
        -0.7626335665893565,
        -0.7603228948563587,
        -0.7585448064038018,
        -0.7578632717218361,
        -0.7605832344637438,
        -0.7568116251120898,
        -0.760363931688528,
        -0.7443107690229754,
        -0.7541874376519492,
        -0.7349916249938437,
        -0.7450127161945175,
        -0.7441097522938089,
        -0.7329157230642012,
        -0.7617910738977296,
        -0.7625950335665299,
        -0.7592828585150297,
        -0.7588493980326413,
        -0.7587580650197907,
        -0.7569955785761788,
        -0.7438131428430255,
        -0.7568201871080452,
        -0.7560319081352902,
        -0.7569323384213051,
        -0.7608760273111981,
        -0.7390864630914147,
        -0.7626819210762914,
        -0.7473600531424125,
        -0.7555291072145798,
        -0.7493614246435947,
        -0.7585907285769294
      ],
      [
        -0.659603570956075,
        -0.7316254464051326,
        -0.7532188133011933,
        -0.7482376071311236,
        -0.7613489184387608,
        -0.7085649314885883,
        -0.7421676963526818,
        -0.756284502130452,
        -0.7625986426749285,
        -0.7526460177076097,
        -0.7355376009594446,
        -0.7464945151180804,
        -0.7475235627394645,
        1.0,
        -0.7129655507727196,
        -0.7430656318711037,
        -0.7344405765798294,
        -0.75661134583012,
        -0.7398088547741716,
        -0.7542335814918246,
        -0.7530766998695688,
        -0.7569427070838202,
        -0.7574944264493032,
        -0.7467488133310605,
        -0.7448771605686522,
        -0.7555274833899421,
        -0.7228259478770624,
        -0.7503472480863801,
        -0.7569332872548622,
        -0.7342979341152944,
        -0.7526745722249946,
        -0.546889316044812,
        -0.7309088964101604,
        -0.7623510850457959,
        -0.7618769948068771,
        -0.7435814508169715,
        -0.7346973930601532,
        -0.7559852202918793,
        -0.7509342695481624,
        -0.7613105127531794,
        -0.7181939805769119,
        -0.7386619993936994,
        -0.6704761657245772,
        -0.7640108197993731,
        -0.5924042142604334,
        -0.7494135345370769,
        -0.7593946878997812,
        -0.7587804819520598,
        -0.7460903524682707,
        -0.7603312061258919,
        -0.7473386825496127,
        -0.7615655967059396,
        -0.6841255740937822,
        -0.7578488731190107,
        -0.7559677472066287,
        -0.7379374105738585,
        -0.7596518005642771,
        -0.6847981576105547,
        -0.7631692773633236,
        -0.7402706663783389,
        -0.7580534383488989,
        -0.7427160863447257,
        -0.7479428014106126
      ],
      [
        -0.7302115718923871,
        -0.7122069583447062,
        -0.7201321843747763,
        -0.7552862680551675,
        -0.758328260734759,
        -0.7429367324867758,
        -0.7208865264398923,
        -0.7489219015129271,
        -0.7452867688483222,
        -0.7407311944971968,
        -0.7245428335926818,
        -0.744959667339705,
        -0.7615378917084621,
        -0.7129655507727196,
        1.0,
        -0.7457768968523328,
        -0.7373765200340996,
        -0.733253140582787,
        -0.7403135572354933,
        -0.7461422431957714,
        -0.7515151136810341,
        -0.7287403622763653,
        -0.7019135951403843,
        -0.7352426563362924,
        -0.7403335452574349,
        -0.7370422718656451,
        -0.7280101124156796,
        -0.745690842868634,
        -0.755933831160423,
        -0.7438291142627068,
        -0.7319973433829937,
        -0.7321774421939664,
        -0.7371214316621718,
        -0.7577101964438854,
        -0.7590003520638431,
        -0.71888279411916,
        -0.7196660342639574,
        -0.7468959804926578,
        -0.7241827876494087,
        -0.7579255731766033,
        -0.7524166699760603,
        -0.7228065052341005,
        -0.7308812349156787,
        -0.7621838107207388,
        -0.7536241756729235,
        -0.7506590537177897,
        -0.7322220474946,
        -0.7560650666277168,
        -0.7419945794083296,
        -0.7594496351101607,
        -0.7434139354002407,
        -0.7535294848707077,
        -0.7315412482224481,
        -0.7284696947552609,
        -0.7447208283136928,
        -0.7453282390303686,
        -0.7545292951968818,
        -0.7283461635345359,
        -0.7503174915013231,
        -0.7188369207022883,
        -0.7434604491038188,
        -0.720089134886224,
        -0.7504821770957081
      ],
      [
        -0.7427332312896835,
        -0.7589336857171979,
        -0.7441347149158957,
        -0.7511678058115383,
        -0.7488347933284978,
        -0.7512758968360431,
        -0.7324472056186007,
        -0.7398696386918412,
        -0.7577916454672737,
        -0.7507483222906142,
        -0.7575152460739086,
        -0.7542522496275932,
        -0.7608100466855178,
        -0.7430656318711037,
        -0.7457768968523328,
        1.0,
        -0.7270859100452971,
        -0.7499195984483149,
        -0.7501306859001995,
        -0.6919933762145345,
        -0.7574433547601909,
        -0.7394335549210987,
        -0.7511690945667027,
        -0.7309178215891523,
        -0.7530808130158692,
        -0.7509957214281835,
        -0.7550564839803052,
        -0.754665695288701,
        -0.7624050920972634,
        -0.7540775867261567,
        -0.7559171252596051,
        -0.749044512833229,
        -0.740290623760697,
        -0.7580821834334825,
        -0.7492013021158741,
        -0.7464883158785731,
        -0.7490609693366124,
        -0.7501293640392921,
        -0.7470678698355933,
        -0.7575868191665119,
        -0.7430295667863585,
        -0.7331473930424376,
        -0.7462542048465188,
        -0.7596006783428957,
        -0.750718246458852,
        -0.7531847467904674,
        -0.7548850776453031,
        -0.7594952460699761,
        -0.7522316578503359,
        -0.7361905184833301,
        -0.7544386283467053,
        -0.7516803146514894,
        -0.7391922843255391,
        -0.746145974187535,
        -0.7525467997317821,
        -0.7457589151188027,
        -0.7539573428948814,
        -0.7518197456148683,
        -0.7599207429305073,
        -0.7414650923115563,
        -0.7541495693847495,
        -0.750949493756909,
        -0.7448121453625574
      ],
      [
        -0.7301875914069877,
        -0.7431489737831907,
        -0.7294958851404523,
        -0.7069039562531195,
        -0.7532987639297737,
        -0.7119573235217127,
        -0.7262354096179415,
        -0.7289936806864361,
        -0.7478079192259897,
        -0.7281702174539473,
        -0.726618476325356,
        -0.7224152668480639,
        -0.7508626850408857,
        -0.7344405765798294,
        -0.7373765200340996,
        -0.7270859100452971,
        1.0,
        -0.7352744704558813,
        -0.7319964419934754,
        -0.7183386464218052,
        -0.742340417561043,
        -0.7400016472928418,
        -0.7437512641514727,
        -0.757086141473238,
        -0.6647707399103868,
        -0.6956734293389146,
        -0.7338701657432407,
        -0.73064056576349,
        -0.7507630959538694,
        -0.6978634868142879,
        -0.7067862383976323,
        -0.7166793089126828,
        -0.7151411131382783,
        -0.7437010265814198,
        -0.638478985262902,
        -0.7273700916326962,
        -0.7309243757608324,
        -0.7330100280424477,
        -0.7186705775590054,
        -0.7369583701107929,
        -0.727546214696426,
        -0.7092400940920283,
        -0.7202606982816752,
        -0.7566587414836381,
        -0.7464922748691439,
        -0.7169964657470362,
        -0.7186981837574637,
        -0.738245394933909,
        -0.7339819063475898,
        -0.7382371577467226,
        -0.7486589779386134,
        -0.737249860087885,
        -0.7245720871163215,
        -0.7378126589417559,
        -0.6846974096997025,
        -0.7375295413876306,
        -0.7403440485420905,
        -0.7288105770207763,
        -0.7524474021254332,
        -0.7254678879598772,
        -0.7264482159937582,
        -0.7306196236692062,
        -0.7395859874920708
      ],
      [
        -0.7210007493937369,
        -0.7471789522855143,
        -0.5499006992786175,
        -0.7497242002142984,
        -0.7547857362797445,
        -0.7346002004568757,
        -0.7469272994509784,
        -0.60564219091385,
        -0.6940953709985743,
        -0.5603670048435682,
        -0.7418464535959968,
        -0.7317426420959422,
        -0.7626464603830352,
        -0.75661134583012,
        -0.733253140582787,
        -0.7499195984483149,
        -0.7352744704558813,
        1.0,
        -0.7138976870051046,
        -0.7349210431805402,
        -0.6905130307698499,
        -0.7398430712469897,
        -0.7263864235015054,
        -0.7586337322751988,
        -0.7477408178789672,
        -0.736026868734827,
        -0.7471355272336513,
        -0.7356677493365607,
        -0.7474650401831964,
        -0.7581432408647654,
        -0.7352575987671965,
        -0.7428672608166419,
        -0.7413060996511296,
        -0.7555784737677245,
        -0.7578293487221803,
        -0.6467260836686581,
        -0.7242560952054251,
        -0.6064305942944742,
        -0.6737492202356963,
        -0.7399275175736741,
        -0.7501666964771729,
        -0.6961472164748312,
        -0.7363936353782052,
        -0.7633287727405667,
        -0.7596653646063383,
        -0.7447543454223045,
        -0.7282007673740889,
        -0.756625098055999,
        -0.6801087258326113,
        -0.742019453113778,
        -0.7520999795615508,
        -0.7568406546979343,
        -0.7429932514433941,
        -0.721654728160766,
        -0.7311326591606647,
        -0.7525256105966992,
        -0.7563885468658684,
        -0.7464988874136542,
        -0.6933107722683653,
        -0.738853850788231,
        -0.7518919062078342,
        -0.7289762784560221,
        -0.7380433206789103
      ],
      [
        -0.7394114695613678,
        -0.7259597846724799,
        -0.7342416072688307,
        -0.7396995587387398,
        -0.7501228535569953,
        -0.7052075348979896,
        -0.7114962438794475,
        -0.6898029914828446,
        -0.7285214234151883,
        -0.7234741017930943,
        -0.743633399492855,
        -0.7285955200342951,
        -0.756419319528918,
        -0.7398088547741716,
        -0.7403135572354933,
        -0.7501306859001995,
        -0.7319964419934754,
        -0.7138976870051046,
        1.0,
        -0.7300910162673828,
        -0.7338129743309452,
        -0.7465600781892661,
        -0.747238310142007,
        -0.7466076456510049,
        -0.7452981456766398,
        -0.7501195017573361,
        -0.7374707318386582,
        -0.7509697898988857,
        -0.7398146588626883,
        -0.7558591519404314,
        -0.729842246859063,
        -0.7117589703316687,
        -0.7136461172468,
        -0.7533432205284418,
        -0.7596758876916188,
        -0.7298069861637186,
        -0.7260449206057277,
        -0.697999994837792,
        -0.7097376111608525,
        -0.741480671651547,
        -0.737244120468493,
        -0.686814406511033,
        -0.7046141305148925,
        -0.7619543989456465,
        -0.723926507673204,
        -0.7355955751283678,
        -0.7392039286968122,
        -0.7520106991283689,
        -0.7159712445973656,
        -0.7469149810538227,
        -0.7275138040705801,
        -0.7520392769554167,
        -0.7284769920699274,
        -0.7490251056783179,
        -0.7269692854784662,
        -0.7387943877277156,
        -0.7542173257234572,
        -0.7330296353467644,
        -0.7409436975762151,
        -0.7073964192291997,
        -0.7453005639881061,
        -0.7411858453706718,
        -0.7219797906572766
      ],
      [
        -0.749457463701072,
        -0.7326801652703998,
        -0.7353605304239712,
        -0.7520783551855019,
        -0.7359770383353229,
        -0.748526465518218,
        -0.7497309028703287,
        -0.544968323335406,
        -0.7515242532373774,
        -0.7403380025475802,
        -0.7534776687137386,
        -0.7502193342341461,
        -0.7611420283942201,
        -0.7542335814918246,
        -0.7461422431957714,
        -0.6919933762145345,
        -0.7183386464218052,
        -0.7349210431805402,
        -0.7300910162673828,
        1.0,
        -0.7398408135359096,
        -0.7367248469495109,
        -0.7410848195952568,
        -0.7426974250952745,
        -0.751722168170817,
        -0.7516840801552487,
        -0.7544909378058907,
        -0.7565452392329038,
        -0.7595697400251724,
        -0.7561372892438898,
        -0.7326942203971779,
        -0.7391508266522154,
        -0.7185394906083336,
        -0.7234462693590815,
        -0.7579560739936501,
        -0.7344371422652753,
        -0.7348489607033791,
        -0.7118551743795473,
        -0.7295343296399506,
        -0.7530021056542029,
        -0.7546051185168994,
        -0.7184522654435073,
        -0.7492484362254273,
        -0.7617029683949751,
        -0.7617886607825675,
        -0.748536728554595,
        -0.7278016940940681,
        -0.7033351223957756,
        -0.7455261956549549,
        -0.6449707357461036,
        -0.754071480134,
        -0.706969490760111,
        -0.7439927563012662,
        -0.7000674125952948,
        -0.7225340726440674,
        -0.7303780455501308,
        -0.6965970296524572,
        -0.7517706133387256,
        -0.7484341626880934,
        -0.7478005862092905,
        -0.7408603907238358,
        -0.7502600572381597,
        -0.741409924313837
      ],
      [
        -0.7376859772833098,
        -0.7570721412210752,
        -0.7504337858106379,
        -0.732197461024367,
        -0.7036601042950887,
        -0.7283337248950377,
        -0.7326774667629241,
        -0.6430502325054632,
        -0.7527128381008807,
        -0.6928451436696361,
        -0.7413608465904379,
        -0.739475087555713,
        -0.7562862914783203,
        -0.7530766998695688,
        -0.7515151136810341,
        -0.7574433547601909,
        -0.742340417561043,
        -0.6905130307698499,
        -0.7338129743309452,
        -0.7398408135359096,
        1.0,
        -0.7584399135832741,
        -0.7585509021873962,
        -0.761528342156474,
        -0.741679218725634,
        -0.7387924194066603,
        -0.7494444079235583,
        -0.7062387892876383,
        -0.7579120537372414,
        -0.7615752790023191,
        -0.7351310072075123,
        -0.7395624710425099,
        -0.7394630383175365,
        -0.7552082453719826,
        -0.7486842355770934,
        -0.7468945455839759,
        -0.7449963597774814,
        -0.5566582300737872,
        -0.7198221978051085,
        -0.741957678103033,
        -0.7353492915855999,
        -0.7214733995280407,
        -0.733169189065249,
        -0.7605729972017414,
        -0.755574886905279,
        -0.7388688769239634,
        -0.7369792483453859,
        -0.7592089347139943,
        -0.7367749587150356,
        -0.7305691984775033,
        -0.7476831189274877,
        -0.7569178991844796,
        -0.737846862595111,
        -0.7462782192353599,
        -0.727722303616325,
        -0.7512307627749187,
        -0.7569259540115428,
        -0.7393647187902921,
        -0.7453788124204961,
        -0.734390023502397,
        -0.7515262891241818,
        -0.7407804488673881,
        -0.7417040782606825
      ],
      [
        -0.7516560018103974,
        -0.7549566258849505,
        -0.7385422923764001,
        -0.7481705058273407,
        -0.7562143867700757,
        -0.731234341140909,
        -0.7397795898635778,
        -0.7443554713862617,
        -0.7592495332481336,
        -0.7458749307973349,
        -0.7568738755068514,
        -0.7400653841533631,
        -0.7608976148318906,
        -0.7569427070838202,
        -0.7287403622763653,
        -0.7394335549210987,
        -0.7400016472928418,
        -0.7398430712469897,
        -0.7465600781892661,
        -0.7367248469495109,
        -0.7584399135832741,
        1.0,
        -0.7532008522222153,
        -0.7307148573613393,
        -0.7533809532045483,
        -0.7587336174945332,
        -0.7569522241050488,
        -0.7516619956934066,
        -0.7610087828889416,
        -0.7544633833787944,
        -0.7455448952667376,
        -0.7454517089044458,
        -0.7377687923074823,
        -0.7620512668887556,
        -0.7591558175464428,
        -0.7290792295069155,
        -0.7440278858407512,
        -0.747979954603019,
        -0.7384562457033359,
        -0.7611392039405456,
        -0.7563656748009824,
        -0.7379917367044253,
        -0.7446844738444083,
        -0.7622110689401367,
        -0.7553013149075931,
        -0.7396708480692453,
        -0.7540393367676546,
        -0.7610571830475947,
        -0.7534489477177505,
        -0.753073226587935,
        -0.7563120059355308,
        -0.7509644816137284,
        -0.6577008743163792,
        -0.6850964403201157,
        -0.7447219804584725,
        -0.7235470971440513,
        -0.7593013037558329,
        -0.7438799538745959,
        -0.7601459234534929,
        -0.7276685166779492,
        -0.7448272562717175,
        -0.7491492612402233,
        -0.7481312846817514
      ],
      [
        -0.7533194107382375,
        -0.755092694582164,
        -0.7239809116425324,
        -0.7518834431412562,
        -0.7580653768499476,
        -0.7532383146792564,
        -0.7381400424442259,
        -0.7109928969491967,
        -0.729447792734438,
        -0.7467536184307475,
        -0.7508722492031974,
        -0.7561207204785851,
        -0.7616757447486693,
        -0.7574944264493032,
        -0.7019135951403843,
        -0.7511690945667027,
        -0.7437512641514727,
        -0.7263864235015054,
        -0.747238310142007,
        -0.7410848195952568,
        -0.7585509021873962,
        -0.7532008522222153,
        1.0,
        -0.7489487108583437,
        -0.7549901087647617,
        -0.7497041325266189,
        -0.7564097768526383,
        -0.7539619771442416,
        -0.761741751295446,
        -0.7175421714381394,
        -0.7239232057636154,
        -0.7451764475937037,
        -0.7523952085341316,
        -0.7616573831036535,
        -0.7520756668385009,
        -0.7335620691802048,
        -0.7338850358364215,
        -0.7532822865817101,
        -0.7374160692412788,
        -0.7577988462345551,
        -0.7609830683713048,
        -0.7340983868958975,
        -0.7493401607120679,
        -0.7635718483842577,
        -0.7635911099793161,
        -0.7550876264166679,
        -0.7413692333336235,
        -0.7604387837249765,
        -0.7492358812374382,
        -0.7578963728670711,
        -0.7521170987089747,
        -0.7590265720042848,
        -0.755569173253366,
        -0.7338931466455467,
        -0.7168264165034604,
        -0.7514194333013784,
        -0.5923680244140741,
        -0.7570568534876969,
        -0.7155001239801007,
        -0.7388555765535973,
        -0.7252067740652067,
        -0.7535781785850593,
        -0.741008287942285
      ],
      [
        -0.7559298701487032,
        -0.7222275642606288,
        -0.7512651615917865,
        -0.7611797299104472,
        -0.7627797694953138,
        -0.754399799989788,
        -0.7496401814674518,
        -0.7571177051246596,
        -0.7611572654648756,
        -0.7565000826899021,
        -0.7586381614628321,
        -0.755239578499816,
        -0.7632766592352842,
        -0.7467488133310605,
        -0.7352426563362924,
        -0.7309178215891523,
        -0.757086141473238,
        -0.7586337322751988,
        -0.7466076456510049,
        -0.7426974250952745,
        -0.761528342156474,
        -0.7307148573613393,
        -0.7489487108583437,
        1.0,
        -0.756312290301871,
        -0.7599966957575455,
        -0.7595345729018506,
        -0.7594179470453379,
        -0.7615957935261266,
        -0.7491555435346249,
        -0.7544945511298625,
        -0.7414038537487697,
        -0.7455477397832337,
        -0.7641544351121432,
        -0.7635835469943346,
        -0.7471584703035783,
        -0.7546723624358114,
        -0.7611743020162043,
        -0.7509859995520405,
        -0.7626395109690838,
        -0.7606496251004802,
        -0.7481367350265297,
        -0.746168677489911,
        -0.7631809746934615,
        -0.7518217371242883,
        -0.7579226995631828,
        -0.7612111461567637,
        -0.762344825867208,
        -0.7579797135720105,
        -0.7607105984662195,
        -0.7548647866910192,
        -0.7608698460652745,
        -0.7462319008340078,
        -0.7173333615571347,
        -0.7554533219204139,
        -0.7451993888702011,
        -0.7610333941816525,
        -0.7416263371141083,
        -0.756142285890175,
        -0.7450664976010325,
        -0.7489740035136637,
        -0.7570928103173032,
        -0.7553971162188514
      ],
      [
        -0.7186936720393173,
        -0.7225385980910091,
        -0.7413195284375661,
        -0.7063957233404999,
        -0.7307600584599425,
        -0.7029918322363211,
        -0.7173708839989812,
        -0.7426274350572875,
        -0.7537171251904609,
        -0.7296390347497935,
        -0.6977486793943279,
        -0.6696388823935233,
        -0.7427710698270571,
        -0.7448771605686522,
        -0.7403335452574349,
        -0.7530808130158692,
        -0.6647707399103868,
        -0.7477408178789672,
        -0.7452981456766398,
        -0.751722168170817,
        -0.741679218725634,
        -0.7533809532045483,
        -0.7549901087647617,
        -0.756312290301871,
        1.0,
        -0.7273858536576561,
        -0.7360288790208931,
        -0.732389376901344,
        -0.7570417115446445,
        -0.723740481965192,
        -0.7081918539844922,
        -0.7302961921282762,
        -0.7231276640986561,
        -0.7536557070993608,
        -0.6493766010808876,
        -0.7358068186044762,
        -0.741996869100926,
        -0.7419593962199504,
        -0.7365488589744706,
        -0.7453675654916753,
        -0.7282611317458665,
        -0.725532515933969,
        -0.726077946465034,
        -0.7469878859437435,
        -0.7507060960935733,
        -0.565497883791513,
        -0.7340640655449646,
        -0.7497484617473984,
        -0.7480809716314225,
        -0.7461955012055335,
        -0.7419011404854414,
        -0.7353956887589063,
        -0.7151050274859677,
        -0.7206644437550427,
        -0.7354007807761689,
        -0.7382465085859757,
        -0.7516475741796513,
        -0.7105098950911929,
        -0.7564649958895568,
        -0.7146290890119296,
        -0.7480771824530343,
        -0.7240681155497579,
        -0.7428444144215259
      ],
      [
        -0.7310912003061609,
        -0.7471393514900166,
        -0.7524067396325614,
        -0.7326857172107734,
        -0.7594092486579151,
        -0.7442112834942588,
        -0.7045608688588003,
        -0.7462605806764082,
        -0.7543934901309486,
        -0.7449586983270574,
        -0.7239692625247067,
        -0.7487858248396606,
        -0.7605526987399293,
        -0.7555274833899421,
        -0.7370422718656451,
        -0.7509957214281835,
        -0.6956734293389146,
        -0.736026868734827,
        -0.7501195017573361,
        -0.7516840801552487,
        -0.7387924194066603,
        -0.7587336174945332,
        -0.7497041325266189,
        -0.7599966957575455,
        -0.7273858536576561,
        1.0,
        -0.7462334691943658,
        -0.5364204519717597,
        -0.7595416988036352,
        -0.7563523877220744,
        -0.5363932073646345,
        -0.7419220852172844,
        -0.7276052146259953,
        -0.7521370579879683,
        -0.6932920682125356,
        -0.7382934862364363,
        -0.7448593134008392,
        -0.7425168099312313,
        -0.7402684468563274,
        -0.7413566019067566,
        -0.7490900044470081,
        -0.7378212873130864,
        -0.7447198178105099,
        -0.7613988189937015,
        -0.7615789132120581,
        -0.7350865349637106,
        -0.7305778269329686,
        -0.7572193421842723,
        -0.7509899624463798,
        -0.7460407016586974,
        -0.7562234287178047,
        -0.7542193667851029,
        -0.7496476743561178,
        -0.7479203371151049,
        -0.7262174088769418,
        -0.749238134813349,
        -0.7477732431041948,
        -0.7501563113845351,
        -0.754105352915589,
        -0.7115728298983066,
        -0.7497887461196637,
        -0.7480437672996008,
        -0.7498751850681677
      ],
      [
        -0.7494851770491188,
        -0.7480060828121924,
        -0.7231697260413643,
        -0.7296077745998191,
        -0.7540719539246604,
        -0.6913658410026494,
        -0.7224433113530195,
        -0.7488560650323879,
        -0.7561062242551642,
        -0.7447943826173493,
        -0.7372353897211303,
        -0.7114484802022284,
        -0.7353739411619378,
        -0.7228259478770624,
        -0.7280101124156796,
        -0.7550564839803052,
        -0.7338701657432407,
        -0.7471355272336513,
        -0.7374707318386582,
        -0.7544909378058907,
        -0.7494444079235583,
        -0.7569522241050488,
        -0.7564097768526383,
        -0.7595345729018506,
        -0.7360288790208931,
        -0.7462334691943658,
        1.0,
        -0.7573218354592277,
        -0.7541835090377456,
        -0.756819194368443,
        -0.7536406844667897,
        -0.7011561262546189,
        -0.7458055327371642,
        -0.7587951181268191,
        -0.7588647360809433,
        -0.7360391217145577,
        -0.7420077363929637,
        -0.7428480687387038,
        -0.731121715332284,
        -0.758644335171794,
        -0.7150731855893846,
        -0.739671548079879,
        -0.7130799327988081,
        -0.7618743725208232,
        -0.720334269222972,
        -0.7292475240437277,
        -0.7584699063806458,
        -0.7590633894529816,
        -0.7524831306053342,
        -0.7502384491575971,
        -0.755929667728799,
        -0.7604460183803106,
        -0.719290017716016,
        -0.7549158918467829,
        -0.7526906169136396,
        -0.7501983243350965,
        -0.7600122371005561,
        -0.7165041820496464,
        -0.7590279225761681,
        -0.732942121993975,
        -0.7594431801442718,
        -0.7367768537728072,
        -0.751287640068626
      ],
      [
        -0.6771002333419165,
        -0.7459091075703173,
        -0.7486634605835404,
        -0.7390990623075904,
        -0.7593152916517829,
        -0.7368406806284452,
        -0.7449861619891651,
        -0.7427621222058756,
        -0.7603685588136551,
        -0.742982612391458,
        -0.7333217632013653,
        -0.7392308319614989,
        -0.758846650665973,
        -0.7503472480863801,
        -0.745690842868634,
        -0.754665695288701,
        -0.73064056576349,
        -0.7356677493365607,
        -0.7509697898988857,
        -0.7565452392329038,
        -0.7062387892876383,
        -0.7516619956934066,
        -0.7539619771442416,
        -0.7594179470453379,
        -0.732389376901344,
        -0.5364204519717597,
        -0.7573218354592277,
        1.0,
        -0.7526199330761957,
        -0.7535756383258072,
        -0.6706384665838017,
        -0.7448192685897759,
        -0.7244366935564426,
        -0.757311760723399,
        -0.7385345913917043,
        -0.7084209131647585,
        -0.7460623767628547,
        -0.7220963591039122,
        -0.7374333249326063,
        -0.7503783316365314,
        -0.7471192652561897,
        -0.732256682135966,
        -0.7377185421361566,
        -0.763055879751684,
        -0.7556711120225197,
        -0.7397518506645999,
        -0.7324501874934661,
        -0.7576388359282893,
        -0.749023647962999,
        -0.752629778108604,
        -0.745220287487746,
        -0.7526965413902116,
        -0.7375479778920425,
        -0.7289399544110231,
        -0.732805148990546,
        -0.7493344527900926,
        -0.7585135677770286,
        -0.7379311441134244,
        -0.7569085809608722,
        -0.7363343447707413,
        -0.7509809969584464,
        -0.7453649667796103,
        -0.7440575681235942
      ],
      [
        -0.7444216683438276,
        -0.7417058926843962,
        -0.7386042472826398,
        -0.754973835391416,
        -0.7640538094275258,
        -0.7415330050442323,
        -0.7535840079216571,
        -0.7548919606137409,
        -0.7584658019367081,
        -0.7538862975159978,
        -0.7550898499881493,
        -0.7454859792213886,
        -0.7627386851635736,
        -0.7569332872548622,
        -0.755933831160423,
        -0.7624050920972634,
        -0.7507630959538694,
        -0.7474650401831964,
        -0.7398146588626883,
        -0.7595697400251724,
        -0.7579120537372414,
        -0.7610087828889416,
        -0.761741751295446,
        -0.7615957935261266,
        -0.7570417115446445,
        -0.7595416988036352,
        -0.7541835090377456,
        -0.7526199330761957,
        1.0,
        -0.7623730823746229,
        -0.7551589020102165,
        -0.7553072217510703,
        -0.7563170382390291,
        -0.7606709242541426,
        -0.7633617634736718,
        -0.7426809507574038,
        -0.7384644682693786,
        -0.7521187114523615,
        -0.7349749734272295,
        -0.7630306875170965,
        -0.75679152038274,
        -0.7330012373595529,
        -0.7491456773922294,
        -0.764302809086421,
        -0.7607514546254122,
        -0.7458743550554439,
        -0.755801567625747,
        -0.7627526121843935,
        -0.7261509377219479,
        -0.7493936376725204,
        -0.7543781728286556,
        -0.7572479502248597,
        -0.7535770109137017,
        -0.7507578764831326,
        -0.7559810282591994,
        -0.7617808178148722,
        -0.7621909178519823,
        -0.7541072390977209,
        -0.7602560479544047,
        -0.7508885643278596,
        -0.7613791638433085,
        -0.7549725877178375,
        -0.7490731464837349
      ],
      [
        -0.7358644709196509,
        -0.7520964661481403,
        -0.7494581511078311,
        -0.7551187634529133,
        -0.7624072351513965,
        -0.7523786318362866,
        -0.7497406786705298,
        -0.7585813284391014,
        -0.7616668278461107,
        -0.7570453856362951,
        -0.7574576083938037,
        -0.7537010007933316,
        -0.7551758362129177,
        -0.7342979341152944,
        -0.7438291142627068,
        -0.7540775867261567,
        -0.6978634868142879,
        -0.7581432408647654,
        -0.7558591519404314,
        -0.7561372892438898,
        -0.7615752790023191,
        -0.7544633833787944,
        -0.7175421714381394,
        -0.7491555435346249,
        -0.723740481965192,
        -0.7563523877220744,
        -0.756819194368443,
        -0.7535756383258072,
        -0.7623730823746229,
        1.0,
        -0.7474765495999547,
        -0.7468826246751614,
        -0.7439304975385631,
        -0.7638642028861058,
        -0.7354670143664562,
        -0.7470419793591467,
        -0.7504207021322715,
        -0.7583201677019935,
        -0.7521822912063467,
        -0.7587845035740499,
        -0.7569086482197093,
        -0.7487488615805162,
        -0.7451499160390636,
        -0.7628948083664708,
        -0.7575724985092084,
        -0.7514237737275551,
        -0.7536093077790634,
        -0.761554539404621,
        -0.7565685571122112,
        -0.7633206425065866,
        -0.7582234694015522,
        -0.7610904114668631,
        -0.7479323006494794,
        -0.7398414533822618,
        -0.7470502399106398,
        -0.7437820221974762,
        -0.7538358121793248,
        -0.7465111859907957,
        -0.757747791775523,
        -0.7483073574290988,
        -0.47892431684302844,
        -0.7535630096417774,
        -0.7545279183443927
      ],
      [
        -0.744387800408285,
        -0.6257378649278402,
        -0.7378100929745579,
        -0.7468163241337252,
        -0.7613343515913015,
        -0.7304986394238857,
        -0.7265693593762917,
        -0.7123638130581953,
        -0.7368264555440976,
        -0.7397869528202929,
        -0.6971831670990614,
        -0.7230139076187625,
        -0.7553088020301211,
        -0.7526745722249946,
        -0.7319973433829937,
        -0.7559171252596051,
        -0.7067862383976323,
        -0.7352575987671965,
        -0.729842246859063,
        -0.7326942203971779,
        -0.7351310072075123,
        -0.7455448952667376,
        -0.7239232057636154,
        -0.7544945511298625,
        -0.7081918539844922,
        -0.5363932073646345,
        -0.7536406844667897,
        -0.6706384665838017,
        -0.7551589020102165,
        -0.7474765495999547,
        1.0,
        -0.7376247729176122,
        -0.7418712249363923,
        -0.745432250969081,
        -0.7466491694009587,
        -0.5768231018123784,
        -0.7348455993226972,
        -0.7134386968660602,
        -0.7201448064505469,
        -0.73388182328705,
        -0.7512899424487118,
        -0.7170919285413361,
        -0.7396765937262711,
        -0.7525377339812169,
        -0.759245697001772,
        -0.7261182885284152,
        -0.7142969958037106,
        -0.741872367221686,
        -0.7445770505440119,
        -0.7417317507645349,
        -0.7473578572297506,
        -0.7281000919522322,
        -0.7362721496621007,
        -0.720801926049411,
        -0.17071196666579602,
        -0.7394162653734615,
        -0.7483271317631301,
        -0.7451060398652736,
        -0.7437438313764644,
        -0.5719124623235476,
        -0.5164212156866745,
        -0.7387208404610132,
        -0.5731494539351198
      ],
      [
        -0.721838889760614,
        -0.7232818631245541,
        -0.7461883814452517,
        -0.7351845050202337,
        -0.7501136573714109,
        -0.7040404781855296,
        -0.6924445410749467,
        -0.7328963130711545,
        -0.7517260760949953,
        -0.7335944167353086,
        -0.7344122117690238,
        -0.7315414942947702,
        -0.7492430508545374,
        -0.546889316044812,
        -0.7321774421939664,
        -0.749044512833229,
        -0.7166793089126828,
        -0.7428672608166419,
        -0.7117589703316687,
        -0.7391508266522154,
        -0.7395624710425099,
        -0.7454517089044458,
        -0.7451764475937037,
        -0.7414038537487697,
        -0.7302961921282762,
        -0.7419220852172844,
        -0.7011561262546189,
        -0.7448192685897759,
        -0.7553072217510703,
        -0.7468826246751614,
        -0.7376247729176122,
        1.0,
        -0.7395240361567976,
        -0.757883224127596,
        -0.7514205023140239,
        -0.7429695714996275,
        -0.7297160081508127,
        -0.7358832572415419,
        -0.7326019876578458,
        -0.75343307336348,
        -0.7128742308917004,
        -0.7252190616194386,
        -0.6964687146195361,
        -0.7613079369807279,
        -0.7193503963169022,
        -0.7360884988576522,
        -0.7455520378948932,
        -0.7555332304460387,
        -0.7442028252086681,
        -0.746308193937913,
        -0.7460596266620221,
        -0.7542445999923597,
        -0.7087526631792602,
        -0.7502180750509255,
        -0.7328609894257729,
        -0.7254841926755271,
        -0.7522492652855003,
        -0.7004612409095531,
        -0.7569350644647446,
        -0.7116977648152772,
        -0.7489740380754313,
        -0.7349543309003176,
        -0.7419844394274819
      ],
      [
        -0.7197004118169547,
        -0.7498034784291612,
        -0.735075300217667,
        -0.7113937632037182,
        -0.733816775480871,
        -0.7128308010335079,
        -0.7388575949918813,
        -0.728301020177391,
        -0.7547415169501577,
        -0.7378759543590333,
        -0.7427430949797196,
        -0.7206975773114077,
        -0.75415734809935,
        -0.7309088964101604,
        -0.7371214316621718,
        -0.740290623760697,
        -0.7151411131382783,
        -0.7413060996511296,
        -0.7136461172468,
        -0.7185394906083336,
        -0.7394630383175365,
        -0.7377687923074823,
        -0.7523952085341316,
        -0.7455477397832337,
        -0.7231276640986561,
        -0.7276052146259953,
        -0.7458055327371642,
        -0.7244366935564426,
        -0.7563170382390291,
        -0.7439304975385631,
        -0.7418712249363923,
        -0.7395240361567976,
        1.0,
        -0.7533137522183307,
        -0.7188511541460463,
        -0.7252123901175951,
        -0.7404738168258822,
        -0.7324383315908063,
        -0.7314285857181697,
        -0.7476898865953745,
        -0.681072215317015,
        -0.717795536094076,
        -0.7252658888042907,
        -0.7553462491804035,
        -0.7362801528735993,
        -0.7200459173411876,
        -0.7452238095120425,
        -0.7557112002650019,
        -0.7442819503519317,
        -0.7451642391590765,
        -0.7243882052432806,
        -0.7492125801072826,
        -0.6728572805244194,
        -0.7378858277436104,
        -0.7411592085206256,
        -0.7313262601831558,
        -0.7519109001569464,
        -0.7296372055365451,
        -0.7564534838569272,
        -0.7350136830462233,
        -0.749769499851628,
        -0.7441705038567626,
        -0.7052775628719301
      ],
      [
        -0.7605370982727153,
        -0.7618786308835209,
        -0.7604403044787957,
        -0.7566629353064416,
        -0.7624365379083391,
        -0.7543909019654892,
        -0.7527804223224228,
        -0.7406664627962788,
        -0.7481902695773085,
        -0.7572736878830535,
        -0.7479354908603716,
        -0.7544440930783083,
        -0.7626335665893565,
        -0.7623510850457959,
        -0.7577101964438854,
        -0.7580821834334825,
        -0.7437010265814198,
        -0.7555784737677245,
        -0.7533432205284418,
        -0.7234462693590815,
        -0.7552082453719826,
        -0.7620512668887556,
        -0.7616573831036535,
        -0.7641544351121432,
        -0.7536557070993608,
        -0.7521370579879683,
        -0.7587951181268191,
        -0.757311760723399,
        -0.7606709242541426,
        -0.7638642028861058,
        -0.745432250969081,
        -0.757883224127596,
        -0.7533137522183307,
        1.0,
        -0.7632035588766453,
        -0.7600175017497146,
        -0.7597987612909023,
        -0.7491919607598114,
        -0.7557683296131281,
        -0.7596532910914502,
        -0.7596936145253312,
        -0.7445982867778589,
        -0.7588129051343181,
        -0.7634689454954258,
        -0.7633744208269017,
        -0.7457046672000962,
        -0.7323685671215097,
        -0.5958227291813818,
        -0.7576049102553322,
        -0.49504697579512513,
        -0.7604426107391509,
        -0.7494852802017052,
        -0.7588254858610562,
        -0.7577049214419972,
        -0.7419446376435441,
        -0.7598628957556288,
        -0.76090299058398,
        -0.7605516675515895,
        -0.7077541548304713,
        -0.7533047649317972,
        -0.7524310701511709,
        -0.7567725820372224,
        -0.7539743271380612
      ],
      [
        -0.7577878673899336,
        -0.7619468451201703,
        -0.7593830683591272,
        -0.7326493695691781,
        -0.7606016210819443,
        -0.7512795414800344,
        -0.7510883606974044,
        -0.7499307110896962,
        -0.7634399195722339,
        -0.7463076920021081,
        -0.7568794408570413,
        -0.7533059516245493,
        -0.7603228948563587,
        -0.7618769948068771,
        -0.7590003520638431,
        -0.7492013021158741,
        -0.638478985262902,
        -0.7578293487221803,
        -0.7596758876916188,
        -0.7579560739936501,
        -0.7486842355770934,
        -0.7591558175464428,
        -0.7520756668385009,
        -0.7635835469943346,
        -0.6493766010808876,
        -0.6932920682125356,
        -0.7588647360809433,
        -0.7385345913917043,
        -0.7633617634736718,
        -0.7354670143664562,
        -0.7466491694009587,
        -0.7514205023140239,
        -0.7188511541460463,
        -0.7632035588766453,
        1.0,
        -0.7575363446339436,
        -0.7562798044694401,
        -0.7495574608274502,
        -0.7514049163908578,
        -0.6707827401119191,
        -0.7503469228207521,
        -0.7420551333481301,
        -0.7548051739572852,
        -0.7496696730251894,
        -0.7637933734168193,
        -0.7449469523490417,
        -0.7442883407137532,
        -0.762756109237752,
        -0.7569309411802261,
        -0.7603935486154069,
        -0.7560405699421431,
        -0.7602272518377757,
        -0.7513730368424723,
        -0.7367261158415335,
        -0.749495767990395,
        -0.7470509716565696,
        -0.7478745929079639,
        -0.7540818867807957,
        -0.7629808745210391,
        -0.752295908361919,
        -0.7549195306902549,
        -0.7547698750016774,
        -0.7551209829350223
      ],
      [
        -0.6932376424475961,
        -0.5962233793962364,
        0.05923531204684884,
        -0.7459056639890569,
        -0.7472315650307106,
        -0.7205464028065507,
        -0.7347657582204862,
        -0.7368170027234893,
        -0.7416497201674996,
        -0.720341344101334,
        -0.741089697514732,
        -0.6981392909715658,
        -0.7585448064038018,
        -0.7435814508169715,
        -0.71888279411916,
        -0.7464883158785731,
        -0.7273700916326962,
        -0.6467260836686581,
        -0.7298069861637186,
        -0.7344371422652753,
        -0.7468945455839759,
        -0.7290792295069155,
        -0.7335620691802048,
        -0.7471584703035783,
        -0.7358068186044762,
        -0.7382934862364363,
        -0.7360391217145577,
        -0.7084209131647585,
        -0.7426809507574038,
        -0.7470419793591467,
        -0.5768231018123784,
        -0.7429695714996275,
        -0.7252123901175951,
        -0.7600175017497146,
        -0.7575363446339436,
        1.0,
        -0.7072747047425774,
        -0.7342177273293096,
        -0.4473754780552788,
        -0.7598560965255108,
        -0.7479971217812111,
        -0.6909254995709896,
        -0.7333175897759346,
        -0.7641515308580166,
        -0.7485135204007602,
        -0.7333446145753395,
        -0.7482080215782041,
        -0.7588106667656602,
        -0.6987242974105259,
        -0.75457219443597,
        -0.7498771147886707,
        -0.7513844675669278,
        -0.698152059257626,
        -0.678313441857258,
        -0.7188140803434959,
        -0.743807381265802,
        -0.7524844492986053,
        -0.7237647805244534,
        -0.7518697989888512,
        -0.41728608664121547,
        -0.7380146974775512,
        -0.7249513059462938,
        -0.7300155801715826
      ],
      [
        -0.7265610036872362,
        -0.7296933336347764,
        -0.6994914496307378,
        -0.7405765573211309,
        -0.7392274718168979,
        -0.7232872889398663,
        -0.6492266926883778,
        -0.739026864735645,
        -0.7470854264117288,
        -0.7383700621879828,
        -0.738622169145543,
        -0.7371065943835815,
        -0.7578632717218361,
        -0.7346973930601532,
        -0.7196660342639574,
        -0.7490609693366124,
        -0.7309243757608324,
        -0.7242560952054251,
        -0.7260449206057277,
        -0.7348489607033791,
        -0.7449963597774814,
        -0.7440278858407512,
        -0.7338850358364215,
        -0.7546723624358114,
        -0.741996869100926,
        -0.7448593134008392,
        -0.7420077363929637,
        -0.7460623767628547,
        -0.7384644682693786,
        -0.7504207021322715,
        -0.7348455993226972,
        -0.7297160081508127,
        -0.7404738168258822,
        -0.7597987612909023,
        -0.7562798044694401,
        -0.7072747047425774,
        1.0,
        -0.7369963083398782,
        -0.712843563049149,
        -0.7567613843306729,
        -0.7470171402709345,
        -0.6993777559729644,
        -0.7203685702284537,
        -0.7613573182117341,
        -0.742846397266613,
        -0.736987563169758,
        -0.7422765872766987,
        -0.7580487710332191,
        -0.7226517894626623,
        -0.7518568882335954,
        -0.7520729458282723,
        -0.7557243286488131,
        -0.7314663303231601,
        -0.7387188188470475,
        -0.7417905931219125,
        -0.7442602597497019,
        -0.7539370583353611,
        -0.7346874615477712,
        -0.7488617088441119,
        -0.678764640803747,
        -0.7441406537149731,
        -0.7322833365572246,
        -0.7467315475413869
      ],
      [
        -0.7480175328277825,
        -0.7468838486896908,
        -0.7331749773248637,
        -0.7464292360940954,
        -0.7575305107521897,
        -0.7256394897025098,
        -0.7426091491281124,
        -0.2571384146717612,
        -0.7430536648630965,
        -0.6399765226892397,
        -0.7345351047067823,
        -0.722285192568128,
        -0.7605832344637438,
        -0.7559852202918793,
        -0.7468959804926578,
        -0.7501293640392921,
        -0.7330100280424477,
        -0.6064305942944742,
        -0.697999994837792,
        -0.7118551743795473,
        -0.5566582300737872,
        -0.747979954603019,
        -0.7532822865817101,
        -0.7611743020162043,
        -0.7419593962199504,
        -0.7425168099312313,
        -0.7428480687387038,
        -0.7220963591039122,
        -0.7521187114523615,
        -0.7583201677019935,
        -0.7134386968660602,
        -0.7358832572415419,
        -0.7324383315908063,
        -0.7491919607598114,
        -0.7495574608274502,
        -0.7342177273293096,
        -0.7369963083398782,
        1.0,
        -0.7021922759966999,
        -0.7037202290331075,
        -0.7480819989911944,
        -0.7016539625664681,
        -0.7341062503348181,
        -0.7618647458727654,
        -0.7593639743046354,
        -0.7364478348986584,
        -0.735886155502445,
        -0.752896805772757,
        -0.7261193833871198,
        -0.7173973070923431,
        -0.7469419811181598,
        -0.7456192724647517,
        -0.7421027215717884,
        -0.7418156136587413,
        -0.7007692085604449,
        -0.7501066764533992,
        -0.7480025587389381,
        -0.7453917095247262,
        -0.7410414540088057,
        -0.7372961309385031,
        -0.7374056047214455,
        -0.7265644233568535,
        -0.727241860538141
      ],
      [
        -0.70754583639552,
        -0.7303538557450033,
        -0.4388384160640052,
        -0.736012206576337,
        -0.7367105805981873,
        -0.7228464916130858,
        -0.7123608667235721,
        -0.7092989063532474,
        -0.7336344045041907,
        -0.6957825944448863,
        -0.7413931776604877,
        -0.7064693063763778,
        -0.7568116251120898,
        -0.7509342695481624,
        -0.7241827876494087,
        -0.7470678698355933,
        -0.7186705775590054,
        -0.6737492202356963,
        -0.7097376111608525,
        -0.7295343296399506,
        -0.7198221978051085,
        -0.7384562457033359,
        -0.7374160692412788,
        -0.7509859995520405,
        -0.7365488589744706,
        -0.7402684468563274,
        -0.731121715332284,
        -0.7374333249326063,
        -0.7349749734272295,
        -0.7521822912063467,
        -0.7201448064505469,
        -0.7326019876578458,
        -0.7314285857181697,
        -0.7557683296131281,
        -0.7514049163908578,
        -0.4473754780552788,
        -0.712843563049149,
        -0.7021922759966999,
        1.0,
        -0.7500787929962695,
        -0.7422263489170096,
        -0.6883725474360018,
        -0.7255821935314867,
        -0.7597646989212843,
        -0.7568463972138426,
        -0.7337242690027533,
        -0.743124009016406,
        -0.7570567023477979,
        -0.7055038730147274,
        -0.7441933975729379,
        -0.7478296266912494,
        -0.7475056694801847,
        -0.7224340458280875,
        -0.7068026363281442,
        -0.7271942904689697,
        -0.7336315621490868,
        -0.7508570089564712,
        -0.7214542963616578,
        -0.745414569300628,
        -0.701974153616556,
        -0.7463068385817381,
        -0.7154412013442313,
        -0.7331051842683516
      ],
      [
        -0.7546113840012988,
        -0.7550914003082028,
        -0.7603488896263197,
        -0.7573307443063558,
        -0.7619916694170406,
        -0.752145465303522,
        -0.7594287431189342,
        -0.6812671753767594,
        -0.7619219366289329,
        -0.7402112479798412,
        -0.7391071767750405,
        -0.7533448761206354,
        -0.760363931688528,
        -0.7613105127531794,
        -0.7579255731766033,
        -0.7575868191665119,
        -0.7369583701107929,
        -0.7399275175736741,
        -0.741480671651547,
        -0.7530021056542029,
        -0.741957678103033,
        -0.7611392039405456,
        -0.7577988462345551,
        -0.7626395109690838,
        -0.7453675654916753,
        -0.7413566019067566,
        -0.758644335171794,
        -0.7503783316365314,
        -0.7630306875170965,
        -0.7587845035740499,
        -0.73388182328705,
        -0.75343307336348,
        -0.7476898865953745,
        -0.7596532910914502,
        -0.6707827401119191,
        -0.7598560965255108,
        -0.7567613843306729,
        -0.7037202290331075,
        -0.7500787929962695,
        1.0,
        -0.75482191489158,
        -0.7381022734832261,
        -0.7544855365577008,
        -0.7524987040762043,
        -0.7644435414508792,
        -0.7521416509312224,
        -0.7285442167695395,
        -0.7571395881410657,
        -0.7539800052187351,
        -0.7556852735700104,
        -0.7524483507859365,
        -0.7558380037389811,
        -0.7548798421449905,
        -0.7452424802388067,
        -0.7358828062032143,
        -0.7462990060511383,
        -0.7540259889031335,
        -0.757301000355142,
        -0.7550424907374591,
        -0.7584508678533989,
        -0.7520266213067464,
        -0.7542853896797763,
        -0.7506035448507449
      ],
      [
        -0.7420263944390819,
        -0.7402691624815034,
        -0.7514744446557933,
        -0.7070689701130424,
        -0.750504704719696,
        -0.6874310093844989,
        -0.7421984192916637,
        -0.7448755855890108,
        -0.7547407328199647,
        -0.7055124805605568,
        -0.7426918008124932,
        -0.7223377578977257,
        -0.7443107690229754,
        -0.7181939805769119,
        -0.7524166699760603,
        -0.7430295667863585,
        -0.727546214696426,
        -0.7501666964771729,
        -0.737244120468493,
        -0.7546051185168994,
        -0.7353492915855999,
        -0.7563656748009824,
        -0.7609830683713048,
        -0.7606496251004802,
        -0.7282611317458665,
        -0.7490900044470081,
        -0.7150731855893846,
        -0.7471192652561897,
        -0.75679152038274,
        -0.7569086482197093,
        -0.7512899424487118,
        -0.7128742308917004,
        -0.681072215317015,
        -0.7596936145253312,
        -0.7503469228207521,
        -0.7479971217812111,
        -0.7470171402709345,
        -0.7480819989911944,
        -0.7422263489170096,
        -0.75482191489158,
        1.0,
        -0.7295131120911953,
        -0.7177490191111151,
        -0.7575567200232117,
        -0.7042590292639936,
        -0.7245219015531046,
        -0.7548130053729315,
        -0.7588357853590044,
        -0.7484920043029889,
        -0.7504433044217471,
        -0.7410891273278456,
        -0.7597719817388291,
        -0.6083697258907728,
        -0.756976106073583,
        -0.7512190531022855,
        -0.7479585649742181,
        -0.7585933398680675,
        -0.7135817377570703,
        -0.7596642289706377,
        -0.7407829923210079,
        -0.7592308441659903,
        -0.741701130790405,
        -0.7285356235940659
      ],
      [
        -0.6962523752783026,
        -0.7280268052610681,
        -0.6822440103056653,
        -0.7082690609474637,
        -0.7346106635156204,
        -0.7072311544300964,
        -0.718857278594067,
        -0.6888714670032143,
        -0.7101234022711715,
        -0.699789832690221,
        -0.7301759459219015,
        -0.7189063526334808,
        -0.7541874376519492,
        -0.7386619993936994,
        -0.7228065052341005,
        -0.7331473930424376,
        -0.7092400940920283,
        -0.6961472164748312,
        -0.686814406511033,
        -0.7184522654435073,
        -0.7214733995280407,
        -0.7379917367044253,
        -0.7340983868958975,
        -0.7481367350265297,
        -0.725532515933969,
        -0.7378212873130864,
        -0.739671548079879,
        -0.732256682135966,
        -0.7330012373595529,
        -0.7487488615805162,
        -0.7170919285413361,
        -0.7252190616194386,
        -0.717795536094076,
        -0.7445982867778589,
        -0.7420551333481301,
        -0.6909254995709896,
        -0.6993777559729644,
        -0.7016539625664681,
        -0.6883725474360018,
        -0.7381022734832261,
        -0.7295131120911953,
        1.0,
        -0.703597061723197,
        -0.7599213656690397,
        -0.7373617280097651,
        -0.7224567692412326,
        -0.7320453523334303,
        -0.7497959807823671,
        -0.6902367571709845,
        -0.7401688973668343,
        -0.7415607357158642,
        -0.7437687588159815,
        -0.7104394318739233,
        -0.7237438039496473,
        -0.7236071503957063,
        -0.7402322397528762,
        -0.7504444338869256,
        -0.7197739919165129,
        -0.741179854884715,
        -0.7095599322946341,
        -0.7393569462989048,
        -0.7190824329804537,
        -0.708102355696386
      ],
      [
        -0.6534945278977999,
        -0.7345756162691097,
        -0.7320345433778261,
        -0.698241663563836,
        -0.7508196239567113,
        -0.6711234524116471,
        -0.7032480366578777,
        -0.7361943700538921,
        -0.7545264638849047,
        -0.7276557365457879,
        -0.7268442513549322,
        -0.7179219256071847,
        -0.7349916249938437,
        -0.6704761657245772,
        -0.7308812349156787,
        -0.7462542048465188,
        -0.7202606982816752,
        -0.7363936353782052,
        -0.7046141305148925,
        -0.7492484362254273,
        -0.733169189065249,
        -0.7446844738444083,
        -0.7493401607120679,
        -0.746168677489911,
        -0.726077946465034,
        -0.7447198178105099,
        -0.7130799327988081,
        -0.7377185421361566,
        -0.7491456773922294,
        -0.7451499160390636,
        -0.7396765937262711,
        -0.6964687146195361,
        -0.7252658888042907,
        -0.7588129051343181,
        -0.7548051739572852,
        -0.7333175897759346,
        -0.7203685702284537,
        -0.7341062503348181,
        -0.7255821935314867,
        -0.7544855365577008,
        -0.7177490191111151,
        -0.703597061723197,
        1.0,
        -0.7607700832268689,
        -0.6963529064730583,
        -0.7190349903783093,
        -0.7495486190941237,
        -0.7543561402041561,
        -0.7324179672016079,
        -0.7517721031251927,
        -0.744093268760876,
        -0.7556729645541713,
        -0.6282452104260623,
        -0.7421579230036089,
        -0.7402285147002424,
        -0.7277057042774988,
        -0.7574310509086307,
        -0.5605952413087381,
        -0.7564179398097024,
        -0.712425337036735,
        -0.7501546168216024,
        -0.7042594310471656,
        -0.7306392839952518
      ],
      [
        -0.7623266592651814,
        -0.7631254174222245,
        -0.7643223207014724,
        -0.7441034803936635,
        -0.7479682021777894,
        -0.7531127668651079,
        -0.7622138593473107,
        -0.756689060100786,
        -0.7633751666906623,
        -0.7621698352812682,
        -0.7599256457907437,
        -0.7527043677793784,
        -0.7450127161945175,
        -0.7640108197993731,
        -0.7621838107207388,
        -0.7596006783428957,
        -0.7566587414836381,
        -0.7633287727405667,
        -0.7619543989456465,
        -0.7617029683949751,
        -0.7605729972017414,
        -0.7622110689401367,
        -0.7635718483842577,
        -0.7631809746934615,
        -0.7469878859437435,
        -0.7613988189937015,
        -0.7618743725208232,
        -0.763055879751684,
        -0.764302809086421,
        -0.7628948083664708,
        -0.7525377339812169,
        -0.7613079369807279,
        -0.7553462491804035,
        -0.7634689454954258,
        -0.7496696730251894,
        -0.7641515308580166,
        -0.7613573182117341,
        -0.7618647458727654,
        -0.7597646989212843,
        -0.7524987040762043,
        -0.7575567200232117,
        -0.7599213656690397,
        -0.7607700832268689,
        1.0,
        -0.7634107657723699,
        -0.7592634370638175,
        -0.7567591147053545,
        -0.7615759289706787,
        -0.7634121166603753,
        -0.7621237426539851,
        -0.756891860737064,
        -0.7585275942015737,
        -0.7588760435246085,
        -0.7620478943869786,
        -0.7545071508574166,
        -0.7599146520788532,
        -0.761594393309654,
        -0.7612637735277183,
        -0.7625140430867551,
        -0.7615515296106211,
        -0.7593610769093542,
        -0.7603294057871257,
        -0.7584960279721032
      ],
      [
        -0.705201598859701,
        -0.7314439346155809,
        -0.7569232553371624,
        -0.7377191567092831,
        -0.763750249796141,
        -0.6828595033969156,
        -0.747295631019901,
        -0.7603428164294443,
        -0.7635213041424266,
        -0.7503270067342831,
        -0.748430859348934,
        -0.7382766815840816,
        -0.7441097522938089,
        -0.5924042142604334,
        -0.7536241756729235,
        -0.750718246458852,
        -0.7464922748691439,
        -0.7596653646063383,
        -0.723926507673204,
        -0.7617886607825675,
        -0.755574886905279,
        -0.7553013149075931,
        -0.7635911099793161,
        -0.7518217371242883,
        -0.7507060960935733,
        -0.7615789132120581,
        -0.720334269222972,
        -0.7556711120225197,
        -0.7607514546254122,
        -0.7575724985092084,
        -0.759245697001772,
        -0.7193503963169022,
        -0.7362801528735993,
        -0.7633744208269017,
        -0.7637933734168193,
        -0.7485135204007602,
        -0.742846397266613,
        -0.7593639743046354,
        -0.7568463972138426,
        -0.7644435414508792,
        -0.7042590292639936,
        -0.7373617280097651,
        -0.6963529064730583,
        -0.7634107657723699,
        1.0,
        -0.7357871368092163,
        -0.7628588251169031,
        -0.7615799705421955,
        -0.7510982946689284,
        -0.759737530341632,
        -0.7517260887919432,
        -0.7641316591951106,
        -0.7042337867067099,
        -0.7612869757156171,
        -0.7601414617820932,
        -0.7434818749838826,
        -0.761285899452569,
        -0.7134019067800967,
        -0.7646083967189584,
        -0.743091043811272,
        -0.761718244432458,
        -0.7520870711365495,
        -0.7398513469129231
      ],
      [
        -0.7443879117287978,
        -0.7399547350996566,
        -0.7509423087789531,
        -0.46402848921734785,
        -0.7458540011354508,
        -0.37434685142660407,
        -0.6539168061463327,
        -0.7332314721802669,
        -0.7475982308734833,
        -0.7412429537771736,
        -0.7049935581502826,
        -0.4121717377791376,
        -0.7329157230642012,
        -0.7494135345370769,
        -0.7506590537177897,
        -0.7531847467904674,
        -0.7169964657470362,
        -0.7447543454223045,
        -0.7355955751283678,
        -0.748536728554595,
        -0.7388688769239634,
        -0.7396708480692453,
        -0.7550876264166679,
        -0.7579226995631828,
        -0.565497883791513,
        -0.7350865349637106,
        -0.7292475240437277,
        -0.7397518506645999,
        -0.7458743550554439,
        -0.7514237737275551,
        -0.7261182885284152,
        -0.7360884988576522,
        -0.7200459173411876,
        -0.7457046672000962,
        -0.7449469523490417,
        -0.7333446145753395,
        -0.736987563169758,
        -0.7364478348986584,
        -0.7337242690027533,
        -0.7521416509312224,
        -0.7245219015531046,
        -0.7224567692412326,
        -0.7190349903783093,
        -0.7592634370638175,
        -0.7357871368092163,
        1.0,
        -0.7326662762576824,
        -0.7513948449373704,
        -0.7485269251787108,
        -0.7193706393374486,
        -0.7502338763194485,
        -0.7444700716090744,
        -0.7078134237521686,
        -0.7332715732699567,
        -0.7421147302616007,
        -0.740527331046519,
        -0.7514938939856738,
        -0.7190052924344978,
        -0.7535051063811569,
        -0.5959168756485389,
        -0.7511261132278758,
        -0.7108087808445837,
        -0.739227284705851
      ],
      [
        -0.7470431536148648,
        -0.7485798797233647,
        -0.7490258745254357,
        -0.7505409596074906,
        -0.7440617409235326,
        -0.745035382209874,
        -0.7479109141735134,
        -0.7370225351875269,
        -0.7312615968125309,
        -0.7492719872525382,
        -0.7401114058901321,
        -0.745900016415667,
        -0.7617910738977296,
        -0.7593946878997812,
        -0.7322220474946,
        -0.7548850776453031,
        -0.7186981837574637,
        -0.7282007673740889,
        -0.7392039286968122,
        -0.7278016940940681,
        -0.7369792483453859,
        -0.7540393367676546,
        -0.7413692333336235,
        -0.7612111461567637,
        -0.7340640655449646,
        -0.7305778269329686,
        -0.7584699063806458,
        -0.7324501874934661,
        -0.755801567625747,
        -0.7536093077790634,
        -0.7142969958037106,
        -0.7455520378948932,
        -0.7452238095120425,
        -0.7323685671215097,
        -0.7442883407137532,
        -0.7482080215782041,
        -0.7422765872766987,
        -0.735886155502445,
        -0.743124009016406,
        -0.7285442167695395,
        -0.7548130053729315,
        -0.7320453523334303,
        -0.7495486190941237,
        -0.7567591147053545,
        -0.7628588251169031,
        -0.7326662762576824,
        1.0,
        -0.7298075792809866,
        -0.7470531792516055,
        -0.7221231140231764,
        -0.744577502745675,
        -0.7455439125558241,
        -0.7516393241070174,
        -0.7414638534344735,
        -0.7223389323230014,
        -0.7515437948318738,
        -0.7513332467307452,
        -0.7528162554599052,
        -0.6135520573416533,
        -0.7471181361149961,
        -0.7393584351540196,
        -0.7479984972334175,
        -0.7482459216299995
      ],
      [
        -0.758584219246286,
        -0.7593665121755859,
        -0.7594788598051296,
        -0.7564231652472064,
        -0.7615505012517942,
        -0.7533475223094308,
        -0.7530376559728613,
        -0.747936212016439,
        -0.7546158846801312,
        -0.7566955641732971,
        -0.7477433676667254,
        -0.7551741082594159,
        -0.7625950335665299,
        -0.7587804819520598,
        -0.7560650666277168,
        -0.7594952460699761,
        -0.738245394933909,
        -0.756625098055999,
        -0.7520106991283689,
        -0.7033351223957756,
        -0.7592089347139943,
        -0.7610571830475947,
        -0.7604387837249765,
        -0.762344825867208,
        -0.7497484617473984,
        -0.7572193421842723,
        -0.7590633894529816,
        -0.7576388359282893,
        -0.7627526121843935,
        -0.761554539404621,
        -0.741872367221686,
        -0.7555332304460387,
        -0.7557112002650019,
        -0.5958227291813818,
        -0.762756109237752,
        -0.7588106667656602,
        -0.7580487710332191,
        -0.752896805772757,
        -0.7570567023477979,
        -0.7571395881410657,
        -0.7588357853590044,
        -0.7497959807823671,
        -0.7543561402041561,
        -0.7615759289706787,
        -0.7615799705421955,
        -0.7513948449373704,
        -0.7298075792809866,
        1.0,
        -0.7583476376131901,
        -0.738325320105488,
        -0.7616767298635931,
        -0.745091452610945,
        -0.7572410821615498,
        -0.7555076675800108,
        -0.7368576881946867,
        -0.7549470649555052,
        -0.7543623080014942,
        -0.7579666759631041,
        -0.7458967360057485,
        -0.7531665643387593,
        -0.7503566075345885,
        -0.754087467964539,
        -0.7521077383185402
      ],
      [
        -0.7139616165987281,
        -0.7457901400947926,
        -0.6772921816344762,
        -0.7456809151036272,
        -0.7472648444318779,
        -0.7384008328735436,
        -0.737015726431868,
        -0.7290194168873794,
        -0.7383126602062576,
        -0.7268687946653185,
        -0.7485776444396779,
        -0.7382376060758402,
        -0.7592828585150297,
        -0.7460903524682707,
        -0.7419945794083296,
        -0.7522316578503359,
        -0.7339819063475898,
        -0.6801087258326113,
        -0.7159712445973656,
        -0.7455261956549549,
        -0.7367749587150356,
        -0.7534489477177505,
        -0.7492358812374382,
        -0.7579797135720105,
        -0.7480809716314225,
        -0.7509899624463798,
        -0.7524831306053342,
        -0.749023647962999,
        -0.7261509377219479,
        -0.7565685571122112,
        -0.7445770505440119,
        -0.7442028252086681,
        -0.7442819503519317,
        -0.7576049102553322,
        -0.7569309411802261,
        -0.6987242974105259,
        -0.7226517894626623,
        -0.7261193833871198,
        -0.7055038730147274,
        -0.7539800052187351,
        -0.7484920043029889,
        -0.6902367571709845,
        -0.7324179672016079,
        -0.7634121166603753,
        -0.7510982946689284,
        -0.7485269251787108,
        -0.7470531792516055,
        -0.7583476376131901,
        1.0,
        -0.7511834478386028,
        -0.751037968312114,
        -0.7562541702294654,
        -0.7331487853375953,
        -0.7454709174894494,
        -0.7436483588899997,
        -0.7524635999416367,
        -0.757171767176562,
        -0.7286840263367934,
        -0.7525496940580794,
        -0.7382929379079407,
        -0.7552923318219888,
        -0.7327501413241907,
        -0.7392254813673421
      ],
      [
        -0.7559999645284105,
        -0.7568118084572885,
        -0.7544526872467433,
        -0.7423374582448168,
        -0.744527326424408,
        -0.7361924071008688,
        -0.734931796060853,
        -0.6466716471632744,
        -0.7501028467700257,
        -0.7418831821132057,
        -0.7508507014290544,
        -0.7428566223695332,
        -0.7588493980326413,
        -0.7603312061258919,
        -0.7594496351101607,
        -0.7361905184833301,
        -0.7382371577467226,
        -0.742019453113778,
        -0.7469149810538227,
        -0.6449707357461036,
        -0.7305691984775033,
        -0.753073226587935,
        -0.7578963728670711,
        -0.7607105984662195,
        -0.7461955012055335,
        -0.7460407016586974,
        -0.7502384491575971,
        -0.752629778108604,
        -0.7493936376725204,
        -0.7633206425065866,
        -0.7417317507645349,
        -0.746308193937913,
        -0.7451642391590765,
        -0.49504697579512513,
        -0.7603935486154069,
        -0.75457219443597,
        -0.7518568882335954,
        -0.7173973070923431,
        -0.7441933975729379,
        -0.7556852735700104,
        -0.7504433044217471,
        -0.7401688973668343,
        -0.7517721031251927,
        -0.7621237426539851,
        -0.759737530341632,
        -0.7193706393374486,
        -0.7221231140231764,
        -0.738325320105488,
        -0.7511834478386028,
        1.0,
        -0.7546343204466973,
        -0.7349988411188086,
        -0.7456450770355232,
        -0.7412680268047438,
        -0.722691479746931,
        -0.7509890126493092,
        -0.7359980180152218,
        -0.7529243207432927,
        -0.7466149438374445,
        -0.7388672940969494,
        -0.7514289763418753,
        -0.7482846090489139,
        -0.7487473949968335
      ],
      [
        -0.7385284680212711,
        -0.7519984512295064,
        -0.7512734367693895,
        -0.7502427676619333,
        -0.7456318233482155,
        -0.7491685612875872,
        -0.7522068487074507,
        -0.749199333137887,
        -0.7542747727692654,
        -0.7553710989935546,
        -0.7522637186342636,
        -0.7504191713021466,
        -0.7587580650197907,
        -0.7473386825496127,
        -0.7434139354002407,
        -0.7544386283467053,
        -0.7486589779386134,
        -0.7520999795615508,
        -0.7275138040705801,
        -0.754071480134,
        -0.7476831189274877,
        -0.7563120059355308,
        -0.7521170987089747,
        -0.7548647866910192,
        -0.7419011404854414,
        -0.7562234287178047,
        -0.755929667728799,
        -0.745220287487746,
        -0.7543781728286556,
        -0.7582234694015522,
        -0.7473578572297506,
        -0.7460596266620221,
        -0.7243882052432806,
        -0.7604426107391509,
        -0.7560405699421431,
        -0.7498771147886707,
        -0.7520729458282723,
        -0.7469419811181598,
        -0.7478296266912494,
        -0.7524483507859365,
        -0.7410891273278456,
        -0.7415607357158642,
        -0.744093268760876,
        -0.756891860737064,
        -0.7517260887919432,
        -0.7502338763194485,
        -0.744577502745675,
        -0.7616767298635931,
        -0.751037968312114,
        -0.7546343204466973,
        1.0,
        -0.7578230861269498,
        -0.7234483842954422,
        -0.7491464572054154,
        -0.7455223553907847,
        -0.7532715973728841,
        -0.7569436233474838,
        -0.7430410942568932,
        -0.7570371663093001,
        -0.7515835538305617,
        -0.7558389806074319,
        -0.7496910517549742,
        -0.7530337308895925
      ],
      [
        -0.75688275359419,
        -0.7540135024424037,
        -0.7563372065557226,
        -0.7558314192913518,
        -0.762712760394699,
        -0.754047567037005,
        -0.754832675129222,
        -0.7209839704626806,
        -0.7608708054276798,
        -0.7585886607180476,
        -0.7540853011122897,
        -0.7497334625069922,
        -0.7569955785761788,
        -0.7615655967059396,
        -0.7535294848707077,
        -0.7516803146514894,
        -0.737249860087885,
        -0.7568406546979343,
        -0.7520392769554167,
        -0.706969490760111,
        -0.7569178991844796,
        -0.7509644816137284,
        -0.7590265720042848,
        -0.7608698460652745,
        -0.7353956887589063,
        -0.7542193667851029,
        -0.7604460183803106,
        -0.7526965413902116,
        -0.7572479502248597,
        -0.7610904114668631,
        -0.7281000919522322,
        -0.7542445999923597,
        -0.7492125801072826,
        -0.7494852802017052,
        -0.7602272518377757,
        -0.7513844675669278,
        -0.7557243286488131,
        -0.7456192724647517,
        -0.7475056694801847,
        -0.7558380037389811,
        -0.7597719817388291,
        -0.7437687588159815,
        -0.7556729645541713,
        -0.7585275942015737,
        -0.7641316591951106,
        -0.7444700716090744,
        -0.7455439125558241,
        -0.745091452610945,
        -0.7562541702294654,
        -0.7349988411188086,
        -0.7578230861269498,
        1.0,
        -0.7528364106751164,
        -0.742390640424748,
        -0.7025982052910755,
        -0.7482767215835294,
        -0.7531925758819984,
        -0.7570266884754003,
        -0.760564746152604,
        -0.7521837827349922,
        -0.7462058711536552,
        -0.754394384633501,
        -0.7507170408632599
      ],
      [
        -0.6978475099011835,
        -0.7219716376585772,
        -0.7290831136781877,
        -0.6993780646195139,
        -0.738727556976376,
        -0.637215819524907,
        -0.7316896136810752,
        -0.7384876487644401,
        -0.7554781182524899,
        -0.723154674070181,
        -0.7308474968217286,
        -0.7071927251419547,
        -0.7438131428430255,
        -0.6841255740937822,
        -0.7315412482224481,
        -0.7391922843255391,
        -0.7245720871163215,
        -0.7429932514433941,
        -0.7284769920699274,
        -0.7439927563012662,
        -0.737846862595111,
        -0.6577008743163792,
        -0.755569173253366,
        -0.7462319008340078,
        -0.7151050274859677,
        -0.7496476743561178,
        -0.719290017716016,
        -0.7375479778920425,
        -0.7535770109137017,
        -0.7479323006494794,
        -0.7362721496621007,
        -0.7087526631792602,
        -0.6728572805244194,
        -0.7588254858610562,
        -0.7513730368424723,
        -0.698152059257626,
        -0.7314663303231601,
        -0.7421027215717884,
        -0.7224340458280875,
        -0.7548798421449905,
        -0.6083697258907728,
        -0.7104394318739233,
        -0.6282452104260623,
        -0.7588760435246085,
        -0.7042337867067099,
        -0.7078134237521686,
        -0.7516393241070174,
        -0.7572410821615498,
        -0.7331487853375953,
        -0.7456450770355232,
        -0.7234483842954422,
        -0.7528364106751164,
        1.0,
        -0.7375885312526693,
        -0.7468431440207823,
        -0.7258620225634779,
        -0.7549897915808201,
        -0.27248256690335326,
        -0.757778550000695,
        -0.7101504140439288,
        -0.7512862296117255,
        -0.6958312876449099,
        -0.7345749148590199
      ],
      [
        -0.7310736892244291,
        -0.734433567238703,
        -0.684379441578413,
        -0.7479810941311116,
        -0.73993582857462,
        -0.743817155046157,
        -0.7514408183037798,
        -0.7237627294447759,
        -0.7481197477030312,
        -0.7279715617988025,
        -0.7385383336791209,
        -0.7325812891770822,
        -0.7568201871080452,
        -0.7578488731190107,
        -0.7284696947552609,
        -0.746145974187535,
        -0.7378126589417559,
        -0.721654728160766,
        -0.7490251056783179,
        -0.7000674125952948,
        -0.7462782192353599,
        -0.6850964403201157,
        -0.7338931466455467,
        -0.7173333615571347,
        -0.7206644437550427,
        -0.7479203371151049,
        -0.7549158918467829,
        -0.7289399544110231,
        -0.7507578764831326,
        -0.7398414533822618,
        -0.720801926049411,
        -0.7502180750509255,
        -0.7378858277436104,
        -0.7577049214419972,
        -0.7367261158415335,
        -0.678313441857258,
        -0.7387188188470475,
        -0.7418156136587413,
        -0.7068026363281442,
        -0.7452424802388067,
        -0.756976106073583,
        -0.7237438039496473,
        -0.7421579230036089,
        -0.7620478943869786,
        -0.7612869757156171,
        -0.7332715732699567,
        -0.7414638534344735,
        -0.7555076675800108,
        -0.7454709174894494,
        -0.7412680268047438,
        -0.7491464572054154,
        -0.742390640424748,
        -0.7375885312526693,
        1.0,
        -0.7349085012515086,
        -0.7430427052113946,
        -0.7451980096928112,
        -0.7417473525069276,
        -0.7511663737238703,
        -0.7409643015034724,
        -0.7362530958786283,
        -0.7376014616643898,
        -0.7482563376076006
      ],
      [
        -0.7590398706449911,
        -0.7418961450627461,
        -0.742887800500452,
        -0.754331108560374,
        -0.7593535198404171,
        -0.7462767066213414,
        -0.7298539183618891,
        -0.6977189039865336,
        -0.7365445199722569,
        -0.7411056840806637,
        -0.7398008737722033,
        -0.745809415578731,
        -0.7560319081352902,
        -0.7559677472066287,
        -0.7447208283136928,
        -0.7525467997317821,
        -0.6846974096997025,
        -0.7311326591606647,
        -0.7269692854784662,
        -0.7225340726440674,
        -0.727722303616325,
        -0.7447219804584725,
        -0.7168264165034604,
        -0.7554533219204139,
        -0.7354007807761689,
        -0.7262174088769418,
        -0.7526906169136396,
        -0.732805148990546,
        -0.7559810282591994,
        -0.7470502399106398,
        -0.17071196666579602,
        -0.7328609894257729,
        -0.7411592085206256,
        -0.7419446376435441,
        -0.749495767990395,
        -0.7188140803434959,
        -0.7417905931219125,
        -0.7007692085604449,
        -0.7271942904689697,
        -0.7358828062032143,
        -0.7512190531022855,
        -0.7236071503957063,
        -0.7402285147002424,
        -0.7545071508574166,
        -0.7601414617820932,
        -0.7421147302616007,
        -0.7223389323230014,
        -0.7368576881946867,
        -0.7436483588899997,
        -0.722691479746931,
        -0.7455223553907847,
        -0.7025982052910755,
        -0.7468431440207823,
        -0.7349085012515086,
        1.0,
        -0.7438322831514905,
        -0.7449182765906565,
        -0.7513222200134881,
        -0.7430882140905486,
        -0.7154140014224458,
        -0.507110515108633,
        -0.7451521140681994,
        -0.562911078415647
      ],
      [
        -0.741895126989955,
        -0.7518137364897136,
        -0.7477717730903577,
        -0.748642583857619,
        -0.754456064808947,
        -0.7329856866713496,
        -0.7326689240341127,
        -0.7474964608857395,
        -0.7587395241407595,
        -0.7531321422508223,
        -0.7529578029155122,
        -0.7369114543888969,
        -0.7569323384213051,
        -0.7379374105738585,
        -0.7453282390303686,
        -0.7457589151188027,
        -0.7375295413876306,
        -0.7525256105966992,
        -0.7387943877277156,
        -0.7303780455501308,
        -0.7512307627749187,
        -0.7235470971440513,
        -0.7514194333013784,
        -0.7451993888702011,
        -0.7382465085859757,
        -0.749238134813349,
        -0.7501983243350965,
        -0.7493344527900926,
        -0.7617808178148722,
        -0.7437820221974762,
        -0.7394162653734615,
        -0.7254841926755271,
        -0.7313262601831558,
        -0.7598628957556288,
        -0.7470509716565696,
        -0.743807381265802,
        -0.7442602597497019,
        -0.7501066764533992,
        -0.7336315621490868,
        -0.7462990060511383,
        -0.7479585649742181,
        -0.7402322397528762,
        -0.7277057042774988,
        -0.7599146520788532,
        -0.7434818749838826,
        -0.740527331046519,
        -0.7515437948318738,
        -0.7549470649555052,
        -0.7524635999416367,
        -0.7509890126493092,
        -0.7532715973728841,
        -0.7482767215835294,
        -0.7258620225634779,
        -0.7430427052113946,
        -0.7438322831514905,
        1.0,
        -0.7530434709401186,
        -0.7346860573776713,
        -0.758506392808836,
        -0.7380608776879332,
        -0.7389779560084955,
        -0.7295836689803497,
        -0.7480555218237503
      ],
      [
        -0.75768859605403,
        -0.7545906897972674,
        -0.7532979609217431,
        -0.7544408078071352,
        -0.7516485568738531,
        -0.7550787614745493,
        -0.7538050371640945,
        -0.6003478584190824,
        -0.7524462258117164,
        -0.757576411134079,
        -0.758292762789526,
        -0.752830641929884,
        -0.7608760273111981,
        -0.7596518005642771,
        -0.7545292951968818,
        -0.7539573428948814,
        -0.7403440485420905,
        -0.7563885468658684,
        -0.7542173257234572,
        -0.6965970296524572,
        -0.7569259540115428,
        -0.7593013037558329,
        -0.5923680244140741,
        -0.7610333941816525,
        -0.7516475741796513,
        -0.7477732431041948,
        -0.7600122371005561,
        -0.7585135677770286,
        -0.7621909178519823,
        -0.7538358121793248,
        -0.7483271317631301,
        -0.7522492652855003,
        -0.7519109001569464,
        -0.76090299058398,
        -0.7478745929079639,
        -0.7524844492986053,
        -0.7539370583353611,
        -0.7480025587389381,
        -0.7508570089564712,
        -0.7540259889031335,
        -0.7585933398680675,
        -0.7504444338869256,
        -0.7574310509086307,
        -0.761594393309654,
        -0.761285899452569,
        -0.7514938939856738,
        -0.7513332467307452,
        -0.7543623080014942,
        -0.757171767176562,
        -0.7359980180152218,
        -0.7569436233474838,
        -0.7531925758819984,
        -0.7549897915808201,
        -0.7451980096928112,
        -0.7449182765906565,
        -0.7530434709401186,
        1.0,
        -0.7572922486690159,
        -0.7531766514183793,
        -0.7532311540197172,
        -0.7528802355345114,
        -0.7540301904270628,
        -0.7543990846144413
      ],
      [
        -0.6766428319272964,
        -0.7175014669830482,
        -0.7279705125998435,
        -0.7158306956739571,
        -0.7409641191672427,
        -0.6369284083861503,
        -0.7315478381300516,
        -0.7479292047562822,
        -0.7550405417779447,
        -0.7297831292974863,
        -0.7244272894095889,
        -0.7101556064922716,
        -0.7390864630914147,
        -0.6847981576105547,
        -0.7283461635345359,
        -0.7518197456148683,
        -0.7288105770207763,
        -0.7464988874136542,
        -0.7330296353467644,
        -0.7517706133387256,
        -0.7393647187902921,
        -0.7438799538745959,
        -0.7570568534876969,
        -0.7416263371141083,
        -0.7105098950911929,
        -0.7501563113845351,
        -0.7165041820496464,
        -0.7379311441134244,
        -0.7541072390977209,
        -0.7465111859907957,
        -0.7451060398652736,
        -0.7004612409095531,
        -0.7296372055365451,
        -0.7605516675515895,
        -0.7540818867807957,
        -0.7237647805244534,
        -0.7346874615477712,
        -0.7453917095247262,
        -0.7214542963616578,
        -0.757301000355142,
        -0.7135817377570703,
        -0.7197739919165129,
        -0.5605952413087381,
        -0.7612637735277183,
        -0.7134019067800967,
        -0.7190052924344978,
        -0.7528162554599052,
        -0.7579666759631041,
        -0.7286840263367934,
        -0.7529243207432927,
        -0.7430410942568932,
        -0.7570266884754003,
        -0.27248256690335326,
        -0.7417473525069276,
        -0.7513222200134881,
        -0.7346860573776713,
        -0.7572922486690159,
        1.0,
        -0.7598350608217965,
        -0.7280158163630666,
        -0.7546716566952167,
        -0.6798227051516628,
        -0.7422118977893981
      ],
      [
        -0.7617430783459412,
        -0.7570075548923885,
        -0.7485749347832082,
        -0.7605396304516765,
        -0.761990151517549,
        -0.7556414269410833,
        -0.7581464678626787,
        -0.7362381508344372,
        -0.6495174686090163,
        -0.7449975224992033,
        -0.7527135877533142,
        -0.7565114206828125,
        -0.7626819210762914,
        -0.7631692773633236,
        -0.7503174915013231,
        -0.7599207429305073,
        -0.7524474021254332,
        -0.6933107722683653,
        -0.7409436975762151,
        -0.7484341626880934,
        -0.7453788124204961,
        -0.7601459234534929,
        -0.7155001239801007,
        -0.756142285890175,
        -0.7564649958895568,
        -0.754105352915589,
        -0.7590279225761681,
        -0.7569085809608722,
        -0.7602560479544047,
        -0.757747791775523,
        -0.7437438313764644,
        -0.7569350644647446,
        -0.7564534838569272,
        -0.7077541548304713,
        -0.7629808745210391,
        -0.7518697989888512,
        -0.7488617088441119,
        -0.7410414540088057,
        -0.745414569300628,
        -0.7550424907374591,
        -0.7596642289706377,
        -0.741179854884715,
        -0.7564179398097024,
        -0.7625140430867551,
        -0.7646083967189584,
        -0.7535051063811569,
        -0.6135520573416533,
        -0.7458967360057485,
        -0.7525496940580794,
        -0.7466149438374445,
        -0.7570371663093001,
        -0.760564746152604,
        -0.757778550000695,
        -0.7511663737238703,
        -0.7430882140905486,
        -0.758506392808836,
        -0.7531766514183793,
        -0.7598350608217965,
        1.0,
        -0.7539699944300595,
        -0.7541044901404346,
        -0.7552792145943615,
        -0.7547798635385504
      ],
      [
        -0.7398049812013268,
        -0.5598712531448224,
        -0.732194068796414,
        -0.7163889385872093,
        -0.7051434039464001,
        -0.6836992768993673,
        0.2395609866099565,
        -0.7367936675500708,
        -0.7517258455548466,
        -0.7300740585097032,
        -0.7213343029014752,
        -0.699109467411465,
        -0.7473600531424125,
        -0.7402706663783389,
        -0.7188369207022883,
        -0.7414650923115563,
        -0.7254678879598772,
        -0.738853850788231,
        -0.7073964192291997,
        -0.7478005862092905,
        -0.734390023502397,
        -0.7276685166779492,
        -0.7388555765535973,
        -0.7450664976010325,
        -0.7146290890119296,
        -0.7115728298983066,
        -0.732942121993975,
        -0.7363343447707413,
        -0.7508885643278596,
        -0.7483073574290988,
        -0.5719124623235476,
        -0.7116977648152772,
        -0.7350136830462233,
        -0.7533047649317972,
        -0.752295908361919,
        -0.41728608664121547,
        -0.678764640803747,
        -0.7372961309385031,
        -0.701974153616556,
        -0.7584508678533989,
        -0.7407829923210079,
        -0.7095599322946341,
        -0.712425337036735,
        -0.7615515296106211,
        -0.743091043811272,
        -0.5959168756485389,
        -0.7471181361149961,
        -0.7531665643387593,
        -0.7382929379079407,
        -0.7388672940969494,
        -0.7515835538305617,
        -0.7521837827349922,
        -0.7101504140439288,
        -0.7409643015034724,
        -0.7154140014224458,
        -0.7380608776879332,
        -0.7532311540197172,
        -0.7280158163630666,
        -0.7539699944300595,
        1.0,
        -0.7269582377391948,
        -0.7200293755317084,
        -0.730272883350701
      ],
      [
        -0.7535672709924157,
        -0.7460829410932296,
        -0.7491775238381835,
        -0.7568768926044047,
        -0.761383480744154,
        -0.7532619406147736,
        -0.7332624024643166,
        -0.7344910716754749,
        -0.7504502046903175,
        -0.7550650265710604,
        -0.7520047607961442,
        -0.7524705660542115,
        -0.7555291072145798,
        -0.7580534383488989,
        -0.7434604491038188,
        -0.7541495693847495,
        -0.7264482159937582,
        -0.7518919062078342,
        -0.7453005639881061,
        -0.7408603907238358,
        -0.7515262891241818,
        -0.7448272562717175,
        -0.7252067740652067,
        -0.7489740035136637,
        -0.7480771824530343,
        -0.7497887461196637,
        -0.7594431801442718,
        -0.7509809969584464,
        -0.7613791638433085,
        -0.47892431684302844,
        -0.5164212156866745,
        -0.7489740380754313,
        -0.749769499851628,
        -0.7524310701511709,
        -0.7549195306902549,
        -0.7380146974775512,
        -0.7441406537149731,
        -0.7374056047214455,
        -0.7463068385817381,
        -0.7520266213067464,
        -0.7592308441659903,
        -0.7393569462989048,
        -0.7501546168216024,
        -0.7593610769093542,
        -0.761718244432458,
        -0.7511261132278758,
        -0.7393584351540196,
        -0.7503566075345885,
        -0.7552923318219888,
        -0.7514289763418753,
        -0.7558389806074319,
        -0.7462058711536552,
        -0.7512862296117255,
        -0.7362530958786283,
        -0.507110515108633,
        -0.7389779560084955,
        -0.7528802355345114,
        -0.7546716566952167,
        -0.7541044901404346,
        -0.7269582377391948,
        1.0,
        -0.7535476980545205,
        -0.6066930793900991
      ],
      [
        -0.7190941121975267,
        -0.7488610211747795,
        -0.7205530193734564,
        -0.7251448713805777,
        -0.7411263283234834,
        -0.7051450692776349,
        -0.7212009031480084,
        -0.7341377629136782,
        -0.7459592916830733,
        -0.7366592862849233,
        -0.7328297855674366,
        -0.691961281584007,
        -0.7493614246435947,
        -0.7427160863447257,
        -0.720089134886224,
        -0.750949493756909,
        -0.7306196236692062,
        -0.7289762784560221,
        -0.7411858453706718,
        -0.7502600572381597,
        -0.7407804488673881,
        -0.7491492612402233,
        -0.7535781785850593,
        -0.7570928103173032,
        -0.7240681155497579,
        -0.7480437672996008,
        -0.7367768537728072,
        -0.7453649667796103,
        -0.7549725877178375,
        -0.7535630096417774,
        -0.7387208404610132,
        -0.7349543309003176,
        -0.7441705038567626,
        -0.7567725820372224,
        -0.7547698750016774,
        -0.7249513059462938,
        -0.7322833365572246,
        -0.7265644233568535,
        -0.7154412013442313,
        -0.7542853896797763,
        -0.741701130790405,
        -0.7190824329804537,
        -0.7042594310471656,
        -0.7603294057871257,
        -0.7520870711365495,
        -0.7108087808445837,
        -0.7479984972334175,
        -0.754087467964539,
        -0.7327501413241907,
        -0.7482846090489139,
        -0.7496910517549742,
        -0.754394384633501,
        -0.6958312876449099,
        -0.7376014616643898,
        -0.7451521140681994,
        -0.7295836689803497,
        -0.7540301904270628,
        -0.6798227051516628,
        -0.7552792145943615,
        -0.7200293755317084,
        -0.7535476980545205,
        1.0,
        -0.7469824310639059
      ],
      [
        -0.7344395987948842,
        -0.7459480093210761,
        -0.7408101846529374,
        -0.7436556494377821,
        -0.7528380963021246,
        -0.7298551470214636,
        -0.7397981457983164,
        -0.7206531650198051,
        -0.7494810739837472,
        -0.7264399933074843,
        -0.7492156784468885,
        -0.7380949496432134,
        -0.7585907285769294,
        -0.7479428014106126,
        -0.7504821770957081,
        -0.7448121453625574,
        -0.7395859874920708,
        -0.7380433206789103,
        -0.7219797906572766,
        -0.741409924313837,
        -0.7417040782606825,
        -0.7481312846817514,
        -0.741008287942285,
        -0.7553971162188514,
        -0.7428444144215259,
        -0.7498751850681677,
        -0.751287640068626,
        -0.7440575681235942,
        -0.7490731464837349,
        -0.7545279183443927,
        -0.5731494539351198,
        -0.7419844394274819,
        -0.7052775628719301,
        -0.7539743271380612,
        -0.7551209829350223,
        -0.7300155801715826,
        -0.7467315475413869,
        -0.727241860538141,
        -0.7331051842683516,
        -0.7506035448507449,
        -0.7285356235940659,
        -0.708102355696386,
        -0.7306392839952518,
        -0.7584960279721032,
        -0.7398513469129231,
        -0.739227284705851,
        -0.7482459216299995,
        -0.7521077383185402,
        -0.7392254813673421,
        -0.7487473949968335,
        -0.7530337308895925,
        -0.7507170408632599,
        -0.7345749148590199,
        -0.7482563376076006,
        -0.562911078415647,
        -0.7480555218237503,
        -0.7543990846144413,
        -0.7422118977893981,
        -0.7547798635385504,
        -0.730272883350701,
        -0.6066930793900991,
        -0.7469824310639059,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        111,
        24,
        9,
        15,
        10,
        76,
        47,
        39,
        49,
        18,
        17,
        16,
        21,
        31,
        36,
        13,
        55,
        12,
        18,
        17,
        24,
        17,
        22,
        12,
        23,
        16,
        30,
        24,
        9,
        9,
        12,
        36,
        19,
        4,
        6,
        17,
        28,
        23,
        38,
        11,
        31,
        50,
        15,
        0,
        11,
        19,
        9,
        4,
        12,
        9,
        19,
        2,
        32,
        28,
        30,
        35,
        1,
        23,
        12,
        7,
        1,
        37,
        7
      ],
      "2020-02": [
        207,
        39,
        14,
        13,
        19,
        148,
        82,
        25,
        58,
        34,
        22,
        18,
        25,
        92,
        40,
        27,
        74,
        21,
        51,
        20,
        40,
        13,
        21,
        27,
        42,
        22,
        60,
        22,
        6,
        18,
        7,
        61,
        45,
        12,
        5,
        56,
        45,
        33,
        49,
        4,
        62,
        81,
        29,
        0,
        40,
        24,
        7,
        7,
        40,
        18,
        25,
        14,
        57,
        26,
        28,
        30,
        9,
        28,
        17,
        15,
        8,
        46,
        18
      ],
      "2020-03": [
        122,
        23,
        10,
        15,
        11,
        97,
        71,
        28,
        32,
        33,
        21,
        22,
        14,
        52,
        34,
        20,
        71,
        21,
        40,
        24,
        53,
        16,
        23,
        13,
        35,
        38,
        21,
        36,
        16,
        22,
        7,
        46,
        32,
        9,
        9,
        29,
        36,
        42,
        38,
        6,
        38,
        73,
        19,
        0,
        18,
        14,
        12,
        4,
        46,
        15,
        23,
        9,
        43,
        22,
        42,
        23,
        28,
        22,
        12,
        16,
        4,
        50,
        7
      ],
      "2020-04": [
        158,
        23,
        24,
        14,
        15,
        85,
        47,
        45,
        53,
        29,
        23,
        19,
        19,
        36,
        48,
        21,
        70,
        27,
        34,
        34,
        44,
        22,
        25,
        13,
        46,
        21,
        34,
        34,
        23,
        13,
        15,
        49,
        32,
        14,
        8,
        31,
        45,
        41,
        55,
        16,
        32,
        80,
        22,
        0,
        22,
        12,
        19,
        8,
        52,
        23,
        21,
        7,
        24,
        28,
        49,
        25,
        63,
        16,
        8,
        8,
        6,
        35,
        11
      ],
      "2020-05": [
        149,
        23,
        17,
        21,
        17,
        84,
        47,
        31,
        73,
        26,
        22,
        20,
        19,
        25,
        57,
        19,
        65,
        24,
        36,
        27,
        40,
        27,
        27,
        27,
        38,
        31,
        28,
        28,
        8,
        23,
        14,
        49,
        18,
        13,
        11,
        33,
        49,
        45,
        47,
        4,
        41,
        65,
        24,
        1,
        13,
        20,
        13,
        6,
        33,
        21,
        24,
        2,
        25,
        25,
        46,
        26,
        50,
        29,
        17,
        17,
        5,
        46,
        4
      ],
      "2020-06": [
        271,
        44,
        12,
        36,
        19,
        219,
        129,
        43,
        63,
        44,
        17,
        30,
        28,
        111,
        69,
        40,
        91,
        20,
        77,
        34,
        76,
        28,
        19,
        48,
        55,
        31,
        83,
        36,
        33,
        21,
        12,
        97,
        70,
        13,
        13,
        55,
        38,
        52,
        67,
        13,
        99,
        117,
        48,
        0,
        66,
        28,
        12,
        7,
        63,
        39,
        36,
        10,
        66,
        38,
        46,
        45,
        54,
        57,
        15,
        42,
        4,
        93,
        30
      ],
      "2020-07": [
        173,
        59,
        11,
        15,
        14,
        123,
        85,
        34,
        82,
        45,
        19,
        14,
        24,
        66,
        65,
        32,
        66,
        24,
        64,
        33,
        58,
        16,
        33,
        28,
        43,
        35,
        35,
        42,
        22,
        22,
        8,
        53,
        49,
        7,
        11,
        40,
        30,
        52,
        58,
        10,
        39,
        84,
        32,
        0,
        25,
        21,
        12,
        7,
        57,
        28,
        32,
        6,
        48,
        23,
        48,
        32,
        50,
        30,
        15,
        18,
        9,
        62,
        17
      ],
      "2020-08": [
        154,
        42,
        10,
        10,
        14,
        62,
        45,
        29,
        85,
        21,
        12,
        13,
        17,
        27,
        59,
        18,
        54,
        14,
        42,
        17,
        43,
        15,
        24,
        13,
        33,
        24,
        42,
        34,
        8,
        19,
        14,
        43,
        28,
        10,
        12,
        26,
        25,
        46,
        48,
        3,
        34,
        63,
        11,
        0,
        14,
        13,
        12,
        13,
        34,
        25,
        22,
        10,
        26,
        33,
        37,
        16,
        52,
        15,
        17,
        12,
        6,
        40,
        10
      ],
      "2020-09": [
        133,
        34,
        11,
        18,
        11,
        79,
        65,
        26,
        43,
        14,
        15,
        23,
        14,
        45,
        48,
        23,
        72,
        17,
        29,
        28,
        33,
        27,
        14,
        24,
        30,
        24,
        36,
        19,
        14,
        24,
        8,
        63,
        19,
        10,
        8,
        37,
        35,
        41,
        43,
        12,
        39,
        53,
        24,
        0,
        14,
        19,
        11,
        9,
        40,
        24,
        20,
        9,
        35,
        21,
        43,
        25,
        56,
        19,
        13,
        24,
        8,
        44,
        5
      ],
      "2020-10": [
        256,
        44,
        31,
        20,
        22,
        137,
        120,
        39,
        114,
        46,
        23,
        17,
        29,
        85,
        52,
        47,
        99,
        27,
        68,
        40,
        55,
        21,
        20,
        44,
        57,
        27,
        59,
        37,
        21,
        29,
        13,
        56,
        63,
        17,
        12,
        51,
        60,
        40,
        64,
        11,
        79,
        105,
        34,
        1,
        28,
        24,
        22,
        9,
        69,
        40,
        27,
        3,
        49,
        38,
        45,
        44,
        58,
        36,
        15,
        28,
        5,
        45,
        20
      ],
      "2020-11": [
        195,
        43,
        12,
        24,
        17,
        111,
        75,
        37,
        57,
        21,
        18,
        17,
        29,
        65,
        41,
        28,
        72,
        22,
        44,
        37,
        61,
        31,
        25,
        29,
        38,
        23,
        43,
        31,
        19,
        19,
        12,
        57,
        36,
        9,
        10,
        35,
        38,
        36,
        50,
        8,
        46,
        72,
        30,
        1,
        12,
        19,
        16,
        7,
        42,
        35,
        33,
        7,
        27,
        33,
        40,
        29,
        47,
        24,
        19,
        22,
        4,
        37,
        10
      ],
      "2020-12": [
        149,
        50,
        10,
        21,
        22,
        104,
        73,
        34,
        40,
        16,
        18,
        16,
        30,
        57,
        47,
        26,
        62,
        17,
        53,
        30,
        45,
        22,
        17,
        30,
        49,
        24,
        42,
        34,
        16,
        18,
        12,
        64,
        47,
        9,
        14,
        36,
        28,
        42,
        40,
        14,
        48,
        93,
        26,
        0,
        28,
        14,
        16,
        13,
        38,
        24,
        35,
        11,
        39,
        37,
        51,
        40,
        55,
        14,
        10,
        23,
        5,
        47,
        15
      ],
      "2021-01": [
        113,
        30,
        9,
        16,
        14,
        77,
        47,
        21,
        33,
        17,
        20,
        10,
        25,
        34,
        40,
        17,
        85,
        19,
        21,
        24,
        31,
        17,
        24,
        18,
        32,
        25,
        37,
        26,
        15,
        17,
        18,
        36,
        28,
        9,
        17,
        31,
        29,
        26,
        40,
        7,
        33,
        58,
        22,
        0,
        15,
        5,
        20,
        5,
        30,
        22,
        25,
        6,
        25,
        31,
        43,
        25,
        40,
        20,
        8,
        20,
        5,
        36,
        7
      ],
      "2021-02": [
        204,
        65,
        13,
        22,
        15,
        126,
        80,
        31,
        63,
        32,
        29,
        14,
        34,
        95,
        39,
        32,
        82,
        13,
        54,
        34,
        29,
        18,
        24,
        32,
        42,
        14,
        30,
        25,
        18,
        17,
        14,
        61,
        67,
        17,
        11,
        35,
        32,
        41,
        52,
        5,
        65,
        67,
        32,
        1,
        34,
        19,
        13,
        10,
        46,
        17,
        30,
        7,
        40,
        25,
        38,
        25,
        32,
        30,
        12,
        22,
        9,
        40,
        23
      ],
      "2021-03": [
        168,
        51,
        10,
        15,
        14,
        103,
        79,
        44,
        52,
        21,
        16,
        13,
        26,
        54,
        40,
        35,
        74,
        22,
        56,
        30,
        64,
        22,
        28,
        25,
        48,
        23,
        33,
        38,
        23,
        19,
        12,
        55,
        41,
        11,
        9,
        46,
        36,
        45,
        70,
        8,
        37,
        73,
        17,
        0,
        24,
        21,
        15,
        8,
        43,
        28,
        19,
        8,
        33,
        52,
        45,
        26,
        29,
        20,
        18,
        22,
        2,
        49,
        11
      ],
      "2021-04": [
        157,
        39,
        8,
        20,
        9,
        80,
        67,
        31,
        82,
        26,
        13,
        10,
        21,
        27,
        35,
        15,
        74,
        17,
        44,
        36,
        54,
        25,
        24,
        22,
        42,
        30,
        24,
        26,
        19,
        14,
        19,
        50,
        38,
        12,
        7,
        37,
        52,
        27,
        52,
        10,
        39,
        82,
        23,
        1,
        16,
        19,
        16,
        4,
        41,
        21,
        37,
        10,
        31,
        38,
        48,
        26,
        51,
        24,
        17,
        23,
        8,
        52,
        11
      ],
      "2021-05": [
        145,
        36,
        10,
        30,
        17,
        100,
        83,
        27,
        62,
        33,
        20,
        20,
        20,
        51,
        42,
        33,
        68,
        16,
        38,
        21,
        45,
        30,
        22,
        27,
        32,
        22,
        51,
        32,
        22,
        22,
        14,
        44,
        44,
        13,
        8,
        30,
        30,
        39,
        50,
        12,
        47,
        66,
        28,
        1,
        16,
        13,
        15,
        10,
        38,
        24,
        21,
        6,
        43,
        43,
        32,
        19,
        43,
        29,
        16,
        18,
        7,
        40,
        17
      ],
      "2021-06": [
        230,
        76,
        16,
        36,
        19,
        144,
        141,
        32,
        73,
        40,
        18,
        21,
        31,
        94,
        50,
        53,
        92,
        34,
        77,
        23,
        49,
        51,
        26,
        49,
        61,
        28,
        56,
        28,
        32,
        29,
        19,
        65,
        60,
        9,
        15,
        45,
        55,
        47,
        70,
        11,
        70,
        127,
        41,
        0,
        40,
        22,
        11,
        7,
        68,
        35,
        42,
        13,
        49,
        42,
        61,
        36,
        35,
        49,
        13,
        28,
        6,
        64,
        29
      ],
      "2021-07": [
        158,
        62,
        8,
        23,
        14,
        91,
        56,
        34,
        62,
        24,
        15,
        18,
        23,
        43,
        48,
        33,
        75,
        21,
        38,
        30,
        42,
        29,
        17,
        27,
        44,
        21,
        32,
        26,
        16,
        27,
        6,
        49,
        43,
        12,
        8,
        27,
        19,
        43,
        48,
        15,
        53,
        95,
        22,
        0,
        21,
        21,
        11,
        10,
        40,
        28,
        32,
        2,
        32,
        38,
        36,
        27,
        26,
        27,
        14,
        26,
        9,
        48,
        23
      ],
      "2021-08": [
        108,
        49,
        12,
        17,
        15,
        67,
        61,
        24,
        38,
        24,
        7,
        13,
        24,
        28,
        45,
        19,
        44,
        17,
        39,
        26,
        31,
        18,
        20,
        24,
        52,
        35,
        29,
        29,
        20,
        14,
        13,
        49,
        26,
        5,
        8,
        28,
        22,
        33,
        42,
        8,
        24,
        52,
        13,
        0,
        8,
        8,
        12,
        8,
        23,
        17,
        25,
        6,
        23,
        37,
        38,
        26,
        29,
        18,
        13,
        15,
        6,
        51,
        12
      ],
      "2021-09": [
        166,
        48,
        16,
        30,
        17,
        76,
        66,
        39,
        46,
        17,
        15,
        18,
        24,
        47,
        53,
        29,
        70,
        17,
        50,
        26,
        45,
        30,
        30,
        37,
        37,
        36,
        40,
        31,
        13,
        18,
        14,
        53,
        40,
        12,
        13,
        28,
        47,
        38,
        48,
        11,
        37,
        73,
        28,
        0,
        12,
        13,
        12,
        10,
        48,
        29,
        31,
        8,
        34,
        41,
        38,
        33,
        41,
        20,
        16,
        32,
        3,
        54,
        12
      ],
      "2021-10": [
        252,
        76,
        16,
        43,
        19,
        135,
        147,
        38,
        100,
        33,
        26,
        24,
        37,
        77,
        66,
        45,
        98,
        28,
        64,
        31,
        57,
        24,
        29,
        43,
        48,
        31,
        71,
        42,
        38,
        23,
        11,
        54,
        61,
        6,
        15,
        47,
        47,
        51,
        65,
        11,
        61,
        99,
        27,
        0,
        31,
        13,
        15,
        15,
        70,
        36,
        41,
        10,
        45,
        41,
        41,
        27,
        47,
        37,
        13,
        34,
        7,
        49,
        30
      ],
      "2021-11": [
        173,
        56,
        10,
        21,
        25,
        90,
        77,
        33,
        67,
        34,
        22,
        24,
        20,
        59,
        40,
        51,
        84,
        18,
        35,
        33,
        64,
        21,
        24,
        31,
        51,
        30,
        36,
        30,
        19,
        19,
        14,
        54,
        42,
        16,
        17,
        30,
        31,
        38,
        61,
        14,
        51,
        68,
        19,
        2,
        21,
        16,
        19,
        12,
        41,
        24,
        22,
        6,
        40,
        58,
        46,
        22,
        31,
        23,
        10,
        32,
        4,
        53,
        18
      ],
      "2021-12": [
        177,
        52,
        15,
        23,
        14,
        91,
        93,
        30,
        43,
        33,
        18,
        19,
        33,
        56,
        41,
        34,
        67,
        20,
        51,
        23,
        57,
        21,
        22,
        25,
        39,
        35,
        25,
        31,
        34,
        31,
        15,
        54,
        42,
        12,
        12,
        32,
        51,
        46,
        48,
        10,
        36,
        79,
        26,
        0,
        19,
        15,
        8,
        14,
        53,
        22,
        25,
        16,
        30,
        47,
        39,
        33,
        24,
        23,
        16,
        18,
        3,
        40,
        21
      ],
      "2022-01": [
        160,
        55,
        10,
        18,
        26,
        88,
        95,
        32,
        38,
        22,
        16,
        13,
        15,
        43,
        42,
        26,
        74,
        16,
        32,
        18,
        30,
        17,
        14,
        21,
        41,
        20,
        40,
        23,
        22,
        20,
        12,
        44,
        41,
        7,
        5,
        28,
        32,
        22,
        61,
        7,
        33,
        71,
        29,
        1,
        21,
        15,
        16,
        12,
        35,
        14,
        24,
        4,
        31,
        44,
        43,
        22,
        23,
        29,
        13,
        17,
        6,
        25,
        12
      ],
      "2022-02": [
        195,
        68,
        17,
        36,
        26,
        124,
        104,
        26,
        83,
        27,
        12,
        23,
        38,
        90,
        56,
        44,
        85,
        20,
        51,
        25,
        41,
        31,
        10,
        48,
        43,
        31,
        48,
        22,
        24,
        20,
        10,
        61,
        60,
        15,
        10,
        38,
        39,
        25,
        56,
        8,
        54,
        77,
        34,
        1,
        34,
        13,
        23,
        6,
        43,
        30,
        32,
        9,
        37,
        55,
        53,
        27,
        26,
        37,
        13,
        27,
        3,
        52,
        25
      ],
      "2022-03": [
        198,
        52,
        19,
        34,
        24,
        98,
        88,
        44,
        104,
        28,
        13,
        28,
        47,
        49,
        54,
        43,
        86,
        19,
        41,
        33,
        50,
        24,
        19,
        33,
        55,
        34,
        46,
        42,
        30,
        20,
        8,
        54,
        45,
        7,
        14,
        40,
        42,
        50,
        69,
        9,
        45,
        75,
        29,
        0,
        22,
        16,
        15,
        5,
        50,
        38,
        33,
        13,
        39,
        57,
        60,
        24,
        28,
        28,
        8,
        25,
        4,
        53,
        22
      ],
      "2022-04": [
        138,
        53,
        20,
        31,
        21,
        56,
        66,
        34,
        68,
        22,
        12,
        11,
        24,
        31,
        50,
        30,
        75,
        21,
        40,
        34,
        45,
        23,
        15,
        27,
        49,
        14,
        38,
        31,
        26,
        18,
        16,
        47,
        35,
        17,
        11,
        36,
        31,
        34,
        60,
        11,
        37,
        75,
        23,
        0,
        9,
        13,
        9,
        12,
        47,
        19,
        20,
        6,
        34,
        50,
        46,
        27,
        20,
        24,
        12,
        20,
        5,
        36,
        11
      ],
      "2022-05": [
        232,
        90,
        24,
        34,
        33,
        113,
        101,
        40,
        60,
        30,
        14,
        21,
        46,
        83,
        47,
        44,
        87,
        32,
        60,
        33,
        45,
        35,
        30,
        48,
        53,
        29,
        50,
        42,
        30,
        29,
        15,
        45,
        54,
        8,
        11,
        42,
        57,
        48,
        65,
        10,
        59,
        82,
        34,
        1,
        34,
        24,
        11,
        11,
        69,
        44,
        26,
        13,
        34,
        47,
        49,
        38,
        16,
        38,
        12,
        37,
        5,
        45,
        22
      ],
      "2022-06": [
        244,
        91,
        9,
        39,
        16,
        121,
        122,
        40,
        67,
        51,
        16,
        25,
        36,
        94,
        51,
        64,
        77,
        35,
        72,
        27,
        58,
        48,
        9,
        52,
        58,
        32,
        58,
        38,
        30,
        26,
        11,
        59,
        67,
        12,
        12,
        49,
        47,
        37,
        66,
        5,
        67,
        102,
        34,
        0,
        37,
        21,
        15,
        9,
        67,
        40,
        33,
        10,
        49,
        59,
        52,
        33,
        27,
        39,
        11,
        34,
        11,
        43,
        39
      ],
      "2022-07": [
        148,
        62,
        14,
        28,
        24,
        76,
        75,
        40,
        57,
        34,
        16,
        19,
        30,
        46,
        43,
        22,
        72,
        17,
        45,
        36,
        60,
        25,
        12,
        42,
        42,
        23,
        39,
        25,
        36,
        18,
        17,
        41,
        51,
        12,
        11,
        33,
        30,
        47,
        63,
        11,
        23,
        77,
        22,
        1,
        21,
        16,
        15,
        10,
        45,
        19,
        31,
        9,
        34,
        38,
        44,
        32,
        33,
        23,
        17,
        21,
        3,
        38,
        27
      ],
      "2022-08": [
        119,
        62,
        9,
        26,
        26,
        75,
        82,
        34,
        39,
        30,
        10,
        13,
        33,
        42,
        53,
        37,
        67,
        13,
        33,
        23,
        34,
        22,
        15,
        29,
        46,
        20,
        27,
        18,
        21,
        20,
        6,
        46,
        40,
        16,
        16,
        30,
        23,
        25,
        55,
        10,
        18,
        64,
        16,
        0,
        13,
        19,
        11,
        10,
        38,
        24,
        29,
        7,
        32,
        45,
        43,
        26,
        21,
        23,
        10,
        27,
        11,
        38,
        20
      ],
      "2022-09": [
        166,
        60,
        13,
        34,
        28,
        73,
        82,
        43,
        43,
        48,
        13,
        15,
        25,
        50,
        43,
        34,
        88,
        16,
        46,
        34,
        51,
        35,
        20,
        40,
        53,
        27,
        36,
        32,
        24,
        18,
        4,
        63,
        56,
        13,
        13,
        33,
        42,
        30,
        53,
        8,
        41,
        73,
        28,
        0,
        19,
        19,
        19,
        2,
        46,
        29,
        30,
        8,
        31,
        44,
        48,
        29,
        28,
        33,
        8,
        22,
        1,
        41,
        22
      ],
      "2022-10": [
        278,
        101,
        41,
        44,
        28,
        127,
        142,
        56,
        77,
        62,
        18,
        17,
        43,
        71,
        77,
        57,
        95,
        30,
        74,
        24,
        66,
        37,
        29,
        47,
        57,
        33,
        46,
        32,
        36,
        29,
        17,
        65,
        83,
        18,
        19,
        57,
        39,
        63,
        86,
        17,
        58,
        129,
        31,
        0,
        43,
        20,
        17,
        12,
        86,
        40,
        44,
        10,
        51,
        50,
        55,
        39,
        32,
        48,
        15,
        45,
        8,
        55,
        41
      ],
      "2022-11": [
        218,
        76,
        28,
        36,
        40,
        91,
        123,
        30,
        96,
        60,
        16,
        19,
        45,
        59,
        49,
        53,
        84,
        24,
        56,
        35,
        57,
        32,
        14,
        42,
        41,
        40,
        38,
        25,
        39,
        20,
        9,
        73,
        49,
        17,
        28,
        36,
        53,
        48,
        63,
        8,
        49,
        94,
        24,
        1,
        26,
        13,
        15,
        9,
        63,
        48,
        31,
        8,
        37,
        54,
        42,
        31,
        25,
        33,
        10,
        41,
        8,
        40,
        28
      ],
      "2022-12": [
        158,
        57,
        25,
        36,
        26,
        56,
        64,
        22,
        54,
        33,
        19,
        16,
        43,
        33,
        45,
        30,
        71,
        23,
        37,
        31,
        29,
        27,
        27,
        29,
        51,
        23,
        24,
        28,
        15,
        22,
        6,
        50,
        44,
        9,
        15,
        37,
        31,
        35,
        47,
        12,
        25,
        84,
        23,
        1,
        14,
        14,
        11,
        8,
        51,
        22,
        23,
        7,
        28,
        69,
        37,
        22,
        20,
        30,
        9,
        22,
        5,
        41,
        18
      ],
      "2023-01": [
        173,
        46,
        19,
        40,
        24,
        78,
        71,
        16,
        41,
        48,
        8,
        11,
        30,
        61,
        38,
        46,
        75,
        14,
        33,
        11,
        39,
        27,
        15,
        30,
        43,
        34,
        22,
        21,
        26,
        18,
        17,
        45,
        26,
        10,
        9,
        25,
        40,
        25,
        66,
        5,
        30,
        58,
        18,
        1,
        14,
        7,
        15,
        10,
        49,
        26,
        22,
        7,
        31,
        50,
        36,
        22,
        18,
        25,
        11,
        33,
        3,
        35,
        16
      ],
      "2023-02": [
        195,
        79,
        25,
        33,
        45,
        114,
        102,
        29,
        55,
        80,
        11,
        21,
        25,
        105,
        65,
        48,
        77,
        19,
        56,
        34,
        51,
        29,
        23,
        62,
        57,
        33,
        41,
        33,
        24,
        19,
        11,
        57,
        64,
        6,
        20,
        34,
        40,
        37,
        70,
        8,
        46,
        74,
        30,
        0,
        34,
        19,
        14,
        7,
        50,
        31,
        20,
        14,
        49,
        45,
        37,
        34,
        27,
        32,
        11,
        29,
        6,
        49,
        27
      ],
      "2023-03": [
        183,
        77,
        33,
        34,
        32,
        95,
        100,
        35,
        64,
        72,
        24,
        19,
        42,
        70,
        38,
        44,
        102,
        33,
        48,
        48,
        59,
        37,
        11,
        42,
        43,
        40,
        38,
        37,
        45,
        32,
        13,
        48,
        56,
        18,
        10,
        58,
        37,
        36,
        93,
        7,
        48,
        90,
        22,
        0,
        32,
        23,
        13,
        15,
        55,
        34,
        38,
        10,
        46,
        80,
        59,
        28,
        24,
        32,
        9,
        33,
        7,
        43,
        31
      ],
      "2023-04": [
        156,
        62,
        40,
        35,
        25,
        83,
        76,
        41,
        42,
        69,
        19,
        9,
        37,
        33,
        41,
        33,
        62,
        16,
        43,
        31,
        50,
        25,
        27,
        39,
        58,
        27,
        29,
        31,
        28,
        23,
        15,
        45,
        48,
        10,
        27,
        39,
        22,
        27,
        79,
        10,
        32,
        70,
        11,
        0,
        14,
        26,
        17,
        16,
        45,
        28,
        28,
        17,
        34,
        58,
        43,
        24,
        13,
        28,
        9,
        23,
        3,
        33,
        11
      ],
      "2023-05": [
        219,
        97,
        85,
        41,
        39,
        147,
        124,
        51,
        95,
        95,
        16,
        25,
        35,
        89,
        62,
        58,
        114,
        31,
        76,
        42,
        73,
        39,
        24,
        60,
        56,
        43,
        71,
        45,
        48,
        31,
        10,
        76,
        74,
        12,
        15,
        87,
        60,
        47,
        127,
        14,
        69,
        121,
        36,
        1,
        28,
        21,
        21,
        9,
        71,
        40,
        23,
        8,
        50,
        89,
        51,
        42,
        16,
        43,
        7,
        39,
        11,
        41,
        34
      ],
      "2023-06": [
        270,
        97,
        42,
        41,
        47,
        118,
        132,
        33,
        97,
        87,
        16,
        17,
        42,
        71,
        64,
        72,
        108,
        25,
        55,
        28,
        62,
        35,
        16,
        57,
        57,
        48,
        64,
        50,
        47,
        30,
        7,
        59,
        67,
        10,
        26,
        93,
        36,
        47,
        102,
        19,
        61,
        74,
        16,
        2,
        31,
        21,
        13,
        10,
        63,
        38,
        37,
        12,
        42,
        90,
        53,
        33,
        17,
        34,
        15,
        43,
        7,
        55,
        33
      ],
      "2023-07": [
        197,
        69,
        37,
        33,
        29,
        98,
        87,
        53,
        42,
        63,
        11,
        14,
        42,
        52,
        49,
        40,
        88,
        26,
        36,
        25,
        47,
        34,
        16,
        45,
        48,
        28,
        28,
        23,
        25,
        27,
        6,
        49,
        44,
        16,
        15,
        73,
        40,
        37,
        69,
        8,
        35,
        85,
        25,
        0,
        26,
        14,
        16,
        7,
        56,
        27,
        32,
        16,
        30,
        76,
        51,
        25,
        18,
        32,
        12,
        33,
        5,
        36,
        29
      ],
      "2023-08": [
        133,
        75,
        47,
        35,
        30,
        83,
        85,
        36,
        59,
        48,
        14,
        16,
        37,
        37,
        60,
        39,
        76,
        34,
        40,
        39,
        56,
        22,
        18,
        25,
        43,
        33,
        29,
        41,
        29,
        29,
        13,
        50,
        51,
        17,
        14,
        81,
        26,
        46,
        52,
        4,
        32,
        74,
        18,
        0,
        18,
        11,
        8,
        9,
        34,
        27,
        30,
        11,
        24,
        64,
        57,
        27,
        16,
        27,
        11,
        40,
        11,
        41,
        30
      ],
      "2023-09": [
        158,
        66,
        40,
        31,
        30,
        90,
        100,
        35,
        84,
        71,
        14,
        17,
        54,
        38,
        50,
        37,
        98,
        28,
        39,
        41,
        55,
        34,
        20,
        34,
        52,
        40,
        42,
        50,
        39,
        27,
        12,
        51,
        68,
        21,
        23,
        88,
        33,
        41,
        77,
        10,
        38,
        87,
        16,
        0,
        28,
        15,
        15,
        10,
        54,
        31,
        38,
        12,
        30,
        74,
        59,
        25,
        24,
        23,
        14,
        31,
        6,
        32,
        22
      ],
      "2023-10": [
        242,
        108,
        116,
        51,
        67,
        129,
        144,
        40,
        62,
        110,
        22,
        25,
        48,
        93,
        52,
        78,
        135,
        44,
        60,
        46,
        75,
        41,
        29,
        56,
        78,
        48,
        65,
        28,
        48,
        31,
        10,
        63,
        70,
        14,
        37,
        169,
        55,
        59,
        142,
        18,
        63,
        105,
        38,
        0,
        34,
        20,
        16,
        12,
        89,
        45,
        32,
        11,
        60,
        99,
        63,
        37,
        13,
        42,
        16,
        40,
        4,
        65,
        47
      ],
      "2023-11": [
        165,
        79,
        45,
        39,
        42,
        86,
        97,
        46,
        47,
        102,
        16,
        13,
        61,
        48,
        35,
        53,
        95,
        30,
        38,
        61,
        49,
        26,
        18,
        40,
        57,
        31,
        40,
        36,
        47,
        31,
        16,
        48,
        57,
        14,
        29,
        114,
        29,
        58,
        92,
        17,
        45,
        88,
        30,
        0,
        20,
        17,
        14,
        13,
        62,
        41,
        29,
        15,
        42,
        81,
        52,
        19,
        20,
        33,
        8,
        28,
        5,
        40,
        34
      ],
      "2023-12": [
        191,
        89,
        78,
        36,
        35,
        70,
        103,
        34,
        57,
        93,
        17,
        15,
        35,
        54,
        42,
        55,
        128,
        27,
        52,
        36,
        60,
        33,
        14,
        43,
        52,
        44,
        29,
        30,
        44,
        27,
        8,
        58,
        57,
        8,
        35,
        98,
        31,
        30,
        73,
        12,
        41,
        96,
        26,
        1,
        21,
        27,
        14,
        15,
        51,
        32,
        34,
        11,
        48,
        94,
        60,
        20,
        14,
        30,
        15,
        36,
        11,
        34,
        30
      ],
      "2024-01": [
        152,
        63,
        55,
        46,
        37,
        79,
        83,
        30,
        59,
        48,
        23,
        12,
        27,
        41,
        46,
        43,
        107,
        22,
        42,
        32,
        39,
        30,
        21,
        41,
        57,
        49,
        38,
        28,
        32,
        25,
        16,
        50,
        52,
        17,
        33,
        107,
        24,
        35,
        66,
        16,
        22,
        71,
        15,
        0,
        17,
        19,
        24,
        10,
        33,
        27,
        29,
        17,
        30,
        97,
        46,
        25,
        16,
        30,
        13,
        31,
        8,
        27,
        19
      ],
      "2024-02": [
        270,
        86,
        147,
        59,
        52,
        127,
        165,
        31,
        62,
        122,
        10,
        26,
        40,
        104,
        53,
        72,
        138,
        38,
        62,
        53,
        60,
        44,
        26,
        58,
        67,
        47,
        63,
        38,
        40,
        27,
        13,
        86,
        103,
        13,
        32,
        251,
        45,
        50,
        142,
        9,
        69,
        109,
        45,
        0,
        45,
        23,
        19,
        13,
        78,
        37,
        43,
        11,
        69,
        152,
        61,
        52,
        17,
        47,
        13,
        43,
        9,
        47,
        36
      ],
      "2024-03": [
        212,
        81,
        74,
        30,
        39,
        93,
        112,
        38,
        57,
        97,
        15,
        20,
        41,
        57,
        60,
        54,
        125,
        37,
        53,
        46,
        61,
        33,
        26,
        41,
        63,
        36,
        41,
        32,
        61,
        29,
        20,
        66,
        62,
        16,
        29,
        159,
        37,
        47,
        118,
        17,
        43,
        124,
        31,
        0,
        29,
        18,
        17,
        11,
        74,
        34,
        36,
        20,
        37,
        91,
        58,
        29,
        12,
        25,
        16,
        36,
        6,
        50,
        38
      ],
      "2024-04": [
        156,
        67,
        81,
        34,
        28,
        65,
        83,
        31,
        43,
        77,
        14,
        12,
        44,
        33,
        38,
        46,
        74,
        34,
        33,
        41,
        55,
        34,
        16,
        42,
        46,
        29,
        26,
        44,
        41,
        15,
        9,
        46,
        69,
        17,
        22,
        147,
        32,
        37,
        93,
        19,
        35,
        81,
        23,
        0,
        13,
        18,
        15,
        11,
        51,
        27,
        28,
        10,
        30,
        116,
        47,
        22,
        10,
        26,
        14,
        25,
        5,
        38,
        24
      ],
      "2024-05": [
        280,
        123,
        91,
        50,
        67,
        143,
        112,
        30,
        54,
        127,
        22,
        20,
        56,
        96,
        52,
        75,
        161,
        43,
        74,
        42,
        49,
        57,
        20,
        57,
        61,
        43,
        86,
        43,
        52,
        37,
        11,
        61,
        95,
        16,
        35,
        183,
        32,
        39,
        129,
        15,
        51,
        115,
        30,
        0,
        45,
        17,
        15,
        13,
        72,
        41,
        44,
        16,
        60,
        117,
        67,
        33,
        13,
        43,
        16,
        37,
        8,
        58,
        39
      ],
      "2024-06": [
        244,
        80,
        142,
        55,
        55,
        107,
        151,
        24,
        99,
        127,
        13,
        28,
        39,
        74,
        53,
        72,
        131,
        51,
        55,
        35,
        80,
        43,
        8,
        61,
        47,
        33,
        58,
        40,
        49,
        28,
        11,
        55,
        94,
        11,
        28,
        244,
        34,
        35,
        159,
        11,
        53,
        106,
        30,
        2,
        37,
        26,
        21,
        11,
        83,
        27,
        29,
        14,
        52,
        152,
        56,
        34,
        8,
        43,
        11,
        48,
        6,
        51,
        29
      ],
      "2024-07": [
        179,
        79,
        101,
        47,
        38,
        86,
        111,
        23,
        71,
        67,
        23,
        21,
        47,
        37,
        45,
        63,
        108,
        45,
        35,
        47,
        47,
        28,
        21,
        38,
        48,
        34,
        42,
        39,
        47,
        26,
        6,
        54,
        67,
        12,
        21,
        174,
        35,
        37,
        129,
        14,
        42,
        82,
        23,
        1,
        19,
        26,
        19,
        16,
        42,
        18,
        32,
        14,
        30,
        95,
        46,
        18,
        9,
        23,
        14,
        36,
        5,
        42,
        24
      ],
      "2024-08": [
        143,
        64,
        64,
        47,
        45,
        64,
        89,
        15,
        58,
        59,
        19,
        17,
        47,
        39,
        53,
        47,
        115,
        28,
        38,
        37,
        42,
        31,
        10,
        37,
        64,
        33,
        48,
        24,
        24,
        27,
        8,
        50,
        58,
        25,
        33,
        125,
        39,
        26,
        77,
        10,
        38,
        76,
        18,
        0,
        12,
        22,
        10,
        14,
        53,
        29,
        27,
        15,
        33,
        95,
        51,
        24,
        10,
        34,
        11,
        29,
        10,
        34,
        26
      ],
      "2024-09": [
        177,
        82,
        78,
        52,
        38,
        106,
        92,
        28,
        90,
        79,
        20,
        16,
        47,
        46,
        60,
        49,
        123,
        30,
        36,
        48,
        49,
        21,
        19,
        40,
        62,
        49,
        46,
        40,
        34,
        32,
        13,
        46,
        59,
        19,
        33,
        142,
        28,
        36,
        109,
        14,
        42,
        88,
        22,
        1,
        25,
        27,
        25,
        9,
        53,
        50,
        32,
        13,
        30,
        107,
        58,
        26,
        8,
        35,
        7,
        38,
        11,
        38,
        30
      ],
      "2024-10": [
        297,
        116,
        236,
        72,
        94,
        164,
        134,
        34,
        77,
        199,
        26,
        29,
        59,
        83,
        62,
        97,
        207,
        64,
        54,
        65,
        69,
        36,
        29,
        55,
        70,
        51,
        97,
        57,
        62,
        52,
        12,
        90,
        121,
        23,
        39,
        323,
        49,
        41,
        172,
        13,
        89,
        149,
        35,
        0,
        49,
        27,
        22,
        26,
        91,
        48,
        40,
        8,
        74,
        160,
        69,
        60,
        21,
        41,
        15,
        49,
        15,
        58,
        54
      ],
      "2024-11": [
        166,
        72,
        117,
        38,
        56,
        78,
        98,
        30,
        48,
        104,
        18,
        22,
        48,
        48,
        56,
        64,
        125,
        56,
        51,
        50,
        70,
        29,
        19,
        49,
        61,
        46,
        39,
        50,
        47,
        42,
        12,
        66,
        65,
        25,
        35,
        162,
        25,
        35,
        101,
        28,
        42,
        96,
        24,
        0,
        22,
        25,
        19,
        14,
        50,
        37,
        42,
        18,
        57,
        114,
        73,
        25,
        13,
        24,
        12,
        35,
        12,
        46,
        28
      ],
      "2024-12": [
        196,
        104,
        94,
        52,
        43,
        88,
        123,
        33,
        56,
        130,
        13,
        17,
        46,
        42,
        58,
        63,
        118,
        79,
        36,
        48,
        82,
        22,
        14,
        47,
        51,
        67,
        61,
        42,
        50,
        36,
        16,
        64,
        70,
        25,
        40,
        142,
        37,
        52,
        127,
        12,
        39,
        85,
        26,
        0,
        22,
        14,
        13,
        15,
        56,
        33,
        34,
        16,
        56,
        153,
        65,
        21,
        12,
        35,
        13,
        42,
        11,
        51,
        27
      ],
      "2025-01": [
        199,
        71,
        110,
        47,
        49,
        76,
        82,
        26,
        56,
        96,
        17,
        25,
        42,
        76,
        28,
        60,
        99,
        37,
        35,
        54,
        56,
        27,
        23,
        44,
        66,
        42,
        55,
        36,
        30,
        32,
        11,
        70,
        69,
        20,
        26,
        169,
        18,
        36,
        77,
        14,
        39,
        86,
        22,
        0,
        26,
        25,
        8,
        5,
        58,
        36,
        21,
        12,
        46,
        132,
        76,
        27,
        18,
        27,
        16,
        44,
        7,
        38,
        24
      ],
      "2025-02": [
        282,
        86,
        238,
        58,
        76,
        109,
        124,
        15,
        62,
        175,
        19,
        25,
        57,
        103,
        60,
        82,
        158,
        75,
        49,
        48,
        59,
        41,
        17,
        42,
        80,
        30,
        68,
        56,
        55,
        46,
        10,
        69,
        107,
        32,
        33,
        299,
        34,
        29,
        178,
        16,
        71,
        128,
        25,
        1,
        58,
        27,
        31,
        23,
        87,
        48,
        44,
        16,
        43,
        191,
        68,
        37,
        15,
        48,
        16,
        46,
        12,
        51,
        47
      ],
      "2025-03": [
        257,
        104,
        136,
        52,
        49,
        89,
        97,
        15,
        44,
        126,
        20,
        23,
        66,
        70,
        41,
        72,
        136,
        54,
        32,
        48,
        80,
        27,
        18,
        63,
        77,
        49,
        67,
        53,
        56,
        23,
        9,
        66,
        96,
        12,
        32,
        198,
        26,
        44,
        134,
        18,
        56,
        101,
        28,
        1,
        27,
        18,
        29,
        15,
        61,
        40,
        43,
        19,
        45,
        145,
        51,
        30,
        10,
        33,
        17,
        29,
        7,
        42,
        31
      ],
      "2025-04": [
        177,
        72,
        126,
        43,
        42,
        72,
        82,
        22,
        32,
        69,
        13,
        19,
        66,
        41,
        40,
        50,
        102,
        44,
        36,
        48,
        61,
        28,
        19,
        38,
        71,
        38,
        51,
        45,
        40,
        32,
        15,
        53,
        74,
        19,
        30,
        165,
        25,
        30,
        119,
        12,
        46,
        82,
        10,
        0,
        19,
        23,
        15,
        13,
        59,
        30,
        33,
        23,
        49,
        163,
        58,
        17,
        14,
        39,
        7,
        27,
        13,
        34,
        23
      ],
      "2025-05": [
        346,
        106,
        304,
        79,
        66,
        149,
        140,
        19,
        89,
        166,
        24,
        36,
        66,
        87,
        62,
        93,
        189,
        96,
        66,
        62,
        88,
        52,
        12,
        68,
        98,
        50,
        92,
        51,
        81,
        48,
        14,
        79,
        145,
        18,
        44,
        294,
        52,
        39,
        203,
        16,
        92,
        147,
        40,
        2,
        40,
        30,
        28,
        19,
        104,
        42,
        59,
        22,
        63,
        198,
        89,
        39,
        13,
        58,
        19,
        41,
        8,
        53,
        55
      ],
      "2025-06": [
        280,
        89,
        176,
        46,
        64,
        83,
        112,
        15,
        80,
        145,
        14,
        25,
        66,
        73,
        61,
        91,
        137,
        57,
        31,
        52,
        57,
        39,
        12,
        55,
        72,
        63,
        64,
        40,
        47,
        36,
        9,
        60,
        96,
        22,
        30,
        234,
        45,
        37,
        178,
        11,
        58,
        97,
        18,
        1,
        31,
        27,
        19,
        18,
        77,
        40,
        51,
        12,
        44,
        163,
        63,
        29,
        16,
        61,
        19,
        35,
        10,
        50,
        48
      ],
      "2025-07": [
        239,
        67,
        138,
        59,
        66,
        70,
        97,
        31,
        59,
        104,
        21,
        22,
        79,
        36,
        69,
        64,
        99,
        43,
        40,
        60,
        47,
        27,
        18,
        37,
        53,
        45,
        59,
        54,
        53,
        38,
        12,
        58,
        111,
        25,
        26,
        170,
        38,
        31,
        109,
        22,
        50,
        88,
        27,
        0,
        24,
        14,
        18,
        24,
        56,
        35,
        35,
        10,
        44,
        208,
        65,
        31,
        15,
        25,
        15,
        36,
        9,
        38,
        27
      ],
      "2025-08": [
        199,
        79,
        142,
        45,
        48,
        77,
        113,
        23,
        75,
        80,
        12,
        21,
        64,
        48,
        82,
        60,
        146,
        46,
        40,
        67,
        38,
        31,
        14,
        54,
        53,
        42,
        44,
        30,
        57,
        38,
        12,
        58,
        89,
        26,
        31,
        187,
        31,
        26,
        128,
        18,
        46,
        88,
        22,
        1,
        20,
        24,
        24,
        23,
        56,
        34,
        44,
        12,
        40,
        133,
        85,
        28,
        16,
        36,
        17,
        30,
        3,
        46,
        27
      ],
      "2025-09": [
        107,
        39,
        48,
        29,
        21,
        32,
        55,
        15,
        30,
        32,
        6,
        7,
        41,
        20,
        27,
        34,
        61,
        16,
        22,
        42,
        37,
        11,
        10,
        29,
        24,
        21,
        30,
        11,
        12,
        18,
        9,
        30,
        53,
        13,
        21,
        71,
        15,
        10,
        61,
        8,
        19,
        35,
        7,
        0,
        10,
        10,
        12,
        11,
        29,
        18,
        15,
        12,
        19,
        79,
        38,
        19,
        8,
        20,
        5,
        12,
        7,
        18,
        15
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Reward-Free Policy Space Compression for Reinforcement Learning",
          "year": "2022-02",
          "abstract": "In reinforcement learning, we encode the potential behaviors of an agent\ninteracting with an environment into an infinite set of policies, the policy\nspace, typically represented by a family of parametric functions. Dealing with\nsuch a policy space is a hefty challenge, which often causes sample and\ncomputation inefficiencies. However, we argue that a limited number of policies\nare actually relevant when we also account for the structure of the environment\nand of the policy parameterization, as many of them would induce very similar\ninteractions, i.e., state-action distributions. In this paper, we seek for a\nreward-free compression of the policy space into a finite set of representative\npolicies, such that, given any policy $\\pi$, the minimum R\\'enyi divergence\nbetween the state-action distributions of the representative policies and the\nstate-action distribution of $\\pi$ is bounded. We show that this compression of\nthe policy space can be formulated as a set cover problem, and it is inherently\nNP-hard. Nonetheless, we propose a game-theoretic reformulation for which a\nlocally optimal solution can be efficiently found by iteratively stretching the\ncompressed space to cover an adversarial policy. Finally, we provide an\nempirical evaluation to illustrate the compression procedure in simple domains,\nand its ripple effects in reinforcement learning.",
          "arxiv_id": "2202.11079v1"
        },
        {
          "title": "Robust Deep Reinforcement Learning for Quadcopter Control",
          "year": "2021-11",
          "abstract": "Deep reinforcement learning (RL) has made it possible to solve complex\nrobotics problems using neural networks as function approximators. However, the\npolicies trained on stationary environments suffer in terms of generalization\nwhen transferred from one environment to another. In this work, we use Robust\nMarkov Decision Processes (RMDP) to train the drone control policy, which\ncombines ideas from Robust Control and RL. It opts for pessimistic optimization\nto handle potential gaps between policy transfer from one environment to\nanother. The trained control policy is tested on the task of quadcopter\npositional control. RL agents were trained in a MuJoCo simulator. During\ntesting, different environment parameters (unseen during the training) were\nused to validate the robustness of the trained policy for transfer from one\nenvironment to another. The robust policy outperformed the standard agents in\nthese environments, suggesting that the added robustness increases generality\nand can adapt to non-stationary environments.\n  Codes: https://github.com/adipandas/gym_multirotor",
          "arxiv_id": "2111.03915v1"
        },
        {
          "title": "Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples",
          "year": "2020-06",
          "abstract": "Despite the fact that deep reinforcement learning (RL) has surpassed\nhuman-level performances in various tasks, it still has several fundamental\nchallenges. First, most RL methods require intensive data from the exploration\nof the environment to achieve satisfactory performance. Second, the use of\nneural networks in RL renders it hard to interpret the internals of the system\nin a way that humans can understand. To address these two challenges, we\npropose a framework that enables an RL agent to reason over its exploration\nprocess and distill high-level knowledge for effectively guiding its future\nexplorations. Specifically, we propose a novel RL algorithm that learns\nhigh-level knowledge in the form of a finite reward automaton by using the L*\nlearning algorithm. We prove that in episodic RL, a finite reward automaton can\nexpress any non-Markovian bounded reward functions with finitely many reward\nvalues and approximate any non-Markovian bounded reward function (with\ninfinitely many reward values) with arbitrary precision. We also provide a\nlower bound for the episode length such that the proposed RL approach almost\nsurely converges to an optimal policy in the limit. We test this approach on\ntwo RL environments with non-Markovian reward functions, choosing a variety of\ntasks with increasing complexity for each environment. We compare our algorithm\nwith the state-of-the-art RL algorithms for non-Markovian reward functions,\nsuch as Joint Inference of Reward machines and Policies for RL (JIRP), Learning\nReward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show\nthat our algorithm converges to an optimal policy faster than other baseline\nmethods.",
          "arxiv_id": "2006.15714v4"
        }
      ],
      "1": [
        {
          "title": "SaFL: Sybil-aware Federated Learning with Application to Face Recognition",
          "year": "2023-11",
          "abstract": "Federated Learning (FL) is a machine learning paradigm to conduct\ncollaborative learning among clients on a joint model. The primary goal is to\nshare clients' local training parameters with an integrating server while\npreserving their privacy. This method permits to exploit the potential of\nmassive mobile users' data for the benefit of machine learning models'\nperformance while keeping sensitive data on local devices. On the downside, FL\nraises security and privacy concerns that have just started to be studied. To\naddress some of the key threats in FL, researchers have proposed to use secure\naggregation methods (e.g. homomorphic encryption, secure multiparty\ncomputation, etc.). These solutions improve some security and privacy metrics,\nbut at the same time bring about other serious threats such as poisoning\nattacks, backdoor attacks, and free running attacks. This paper proposes a new\ndefense method against poisoning attacks in FL called SaFL (Sybil-aware\nFederated Learning) that minimizes the effect of sybils with a novel\ntime-variant aggregation scheme.",
          "arxiv_id": "2311.04346v2"
        },
        {
          "title": "PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning",
          "year": "2023-10",
          "abstract": "Federated learning (FL) is a paradigm that allows several client devices and\na server to collaboratively train a global model, by exchanging only model\nupdates, without the devices sharing their local training data. These devices\nare often constrained in terms of communication and computation resources, and\ncan further benefit from model pruning -- a paradigm that is widely used to\nreduce the size and complexity of models. Intuitively, by making local models\ncoarser, pruning is expected to also provide some protection against privacy\nattacks in the context of FL. However this protection has not been previously\ncharacterized, formally or experimentally, and it is unclear if it is\nsufficient against state-of-the-art attacks.\n  In this paper, we perform the first investigation of privacy guarantees for\nmodel pruning in FL. We derive information-theoretic upper bounds on the amount\nof information leaked by pruned FL models. We complement and validate these\ntheoretical findings, with comprehensive experiments that involve\nstate-of-the-art privacy attacks, on several state-of-the-art FL pruning\nschemes, using benchmark datasets. This evaluation provides valuable insights\ninto the choices and parameters that can affect the privacy protection provided\nby pruning. Based on these insights, we introduce PriPrune -- a privacy-aware\nalgorithm for local model pruning, which uses a personalized per-client defense\nmask and adapts the defense pruning rate so as to jointly optimize privacy and\nmodel performance. PriPrune is universal in that can be applied after any\npruned FL scheme on the client, without modification, and protects against any\ninversion attack by the server. Our empirical evaluation demonstrates that\nPriPrune significantly improves the privacy-accuracy tradeoff compared to\nstate-of-the-art pruned FL schemes that do not take privacy into account.",
          "arxiv_id": "2310.19958v2"
        },
        {
          "title": "HyFed: A Hybrid Federated Framework for Privacy-preserving Machine Learning",
          "year": "2021-05",
          "abstract": "Federated learning (FL) enables multiple clients to jointly train a global\nmodel under the coordination of a central server. Although FL is a\nprivacy-aware paradigm, where raw data sharing is not required, recent studies\nhave shown that FL might leak the private data of a client through the model\nparameters shared with the server or the other clients. In this paper, we\npresent the HyFed framework, which enhances the privacy of FL while preserving\nthe utility of the global model. HyFed provides developers with a generic API\nto develop federated, privacy-preserving algorithms. HyFed supports both\nsimulation and federated operation modes and its source code is publicly\navailable at https://github.com/tum-aimed/hyfed.",
          "arxiv_id": "2105.10545v2"
        }
      ],
      "2": [
        {
          "title": "Towards Reasoning Ability of Small Language Models",
          "year": "2025-02",
          "abstract": "Reasoning has long been viewed as an emergent property of large language\nmodels (LLMs), appearing at or above a certain scale ($\\sim$100B parameters).\nHowever, recent studies challenge this assumption, showing that small language\nmodels (SLMs) can also achieve competitive reasoning performance. SLMs are\nincreasingly favored for their efficiency and deployability. However, there is\na lack of systematic study on the reasoning abilities of diverse SLMs,\nincluding those trained from scratch or derived from LLMs through quantization,\npruning, and distillation. This raises a critical question: Can SLMs achieve\nreasoning abilities comparable to LLMs? In this work, we systematically survey,\nbenchmark, and analyze 72 SLMs from six model families across 14 reasoning\nbenchmarks. For reliable evaluation, we examine four evaluation methods and\ncompare four LLM judges against human evaluations on 800 data points. We repeat\nall experiments three times to ensure a robust performance assessment.\nAdditionally, we analyze the impact of different prompting strategies in small\nmodels. Beyond accuracy, we also evaluate model robustness under adversarial\nconditions and intermediate reasoning steps. Our findings challenge the\nassumption that scaling is the only way to achieve strong reasoning. Instead,\nwe foresee a future where SLMs with strong reasoning capabilities can be\ndeveloped through structured training or post-training compression. They can\nserve as efficient alternatives to LLMs for reasoning-intensive tasks.",
          "arxiv_id": "2502.11569v2"
        },
        {
          "title": "Several categories of Large Language Models (LLMs): A Short Survey",
          "year": "2023-07",
          "abstract": "Large Language Models(LLMs)have become effective tools for natural language\nprocessing and have been used in many different fields. This essay offers a\nsuccinct summary of various LLM subcategories. The survey emphasizes recent\ndevelopments and efforts made for various LLM kinds, including task-based\nfinancial LLMs, multilingual language LLMs, biomedical and clinical LLMs,\nvision language LLMs, and code language models. The survey gives a general\nsummary of the methods, attributes, datasets, transformer models, and\ncomparison metrics applied in each category of LLMs. Furthermore, it highlights\nunresolved problems in the field of developing chatbots and virtual assistants,\nsuch as boosting natural language processing, enhancing chatbot intelligence,\nand resolving moral and legal dilemmas. The purpose of this study is to provide\nreaders, developers, academics, and users interested in LLM-based chatbots and\nvirtual intelligent assistant technologies with useful information and future\ndirections.",
          "arxiv_id": "2307.10188v1"
        },
        {
          "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
          "year": "2022-11",
          "abstract": "Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.",
          "arxiv_id": "2211.10438v7"
        }
      ],
      "3": [
        {
          "title": "Error Analysis and Numerical Algorithm for PDE Approximation with Hidden-Layer Concatenated Physics Informed Neural Networks",
          "year": "2024-06",
          "abstract": "We present the hidden-layer concatenated physics informed neural network\n(HLConcPINN) method, which combines hidden-layer concatenated feed-forward\nneural networks, a modified block time marching strategy, and a physics\ninformed approach for approximating partial differential equations (PDEs). We\nanalyze the convergence properties and establish the error bounds of this\nmethod for two types of PDEs: parabolic (exemplified by the heat and Burgers'\nequations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon\nequations). We show that its approximation error of the solution can be\neffectively controlled by the training loss for dynamic simulations with long\ntime horizons. The HLConcPINN method in principle allows an arbitrary number of\nhidden layers not smaller than two and any of the commonly-used smooth\nactivation functions for the hidden layers beyond the first two, with\ntheoretical guarantees. This generalizes several recent neural-network\ntechniques, which have theoretical guarantees but are confined to two hidden\nlayers in the network architecture and the $\\tanh$ activation function. Our\ntheoretical analyses subsequently inform the formulation of appropriate\ntraining loss functions for these PDEs, leading to physics informed neural\nnetwork (PINN) type computational algorithms that differ from the standard PINN\nformulation. Ample numerical experiments are presented based on the proposed\nalgorithm to validate the effectiveness of this method and confirm aspects of\nthe theoretical analyses.",
          "arxiv_id": "2406.06350v1"
        },
        {
          "title": "Solving inverse-PDE problems with physics-aware neural networks",
          "year": "2020-01",
          "abstract": "We propose a novel composite framework to find unknown fields in the context\nof inverse problems for partial differential equations (PDEs). We blend the\nhigh expressibility of deep neural networks as universal function estimators\nwith the accuracy and reliability of existing numerical algorithms for partial\ndifferential equations as custom layers in semantic autoencoders. Our design\nbrings together techniques of computational mathematics, machine learning and\npattern recognition under one umbrella to incorporate domain-specific knowledge\nand physical constraints to discover the underlying hidden fields. The network\nis explicitly aware of the governing physics through a hard-coded PDE solver\nlayer in contrast to most existing methods that incorporate the governing\nequations in the loss function or rely on trainable convolutional layers to\ndiscover proper discretizations from data. This subsequently focuses the\ncomputational load to only the discovery of the hidden fields and therefore is\nmore data efficient. We call this architecture Blended inverse-PDE networks\n(hereby dubbed BiPDE networks) and demonstrate its applicability for recovering\nthe variable diffusion coefficient in Poisson problems in one and two spatial\ndimensions, as well as the diffusion coefficient in the time-dependent and\nnonlinear Burgers' equation in one dimension. We also show that this approach\nis robust to noise.",
          "arxiv_id": "2001.03608v3"
        },
        {
          "title": "On Neural Differential Equations",
          "year": "2022-02",
          "abstract": "The conjoining of dynamical systems and deep learning has become a topic of\ngreat interest. In particular, neural differential equations (NDEs) demonstrate\nthat neural networks and differential equation are two sides of the same coin.\nTraditional parameterised differential equations are a special case. Many\npopular neural network architectures, such as residual networks and recurrent\nnetworks, are discretisations.\n  NDEs are suitable for tackling generative problems, dynamical systems, and\ntime series (particularly in physics, finance, ...) and are thus of interest to\nboth modern machine learning and traditional mathematical modelling. NDEs offer\nhigh-capacity function approximation, strong priors on model space, the ability\nto handle irregular data, memory efficiency, and a wealth of available theory\non both sides.\n  This doctoral thesis provides an in-depth survey of the field.\n  Topics include: neural ordinary differential equations (e.g. for hybrid\nneural/mechanistic modelling of physical systems); neural controlled\ndifferential equations (e.g. for learning functions of irregular time series);\nand neural stochastic differential equations (e.g. to produce generative models\ncapable of representing complex stochastic dynamics, or sampling from complex\nhigh-dimensional distributions).\n  Further topics include: numerical methods for NDEs (e.g. reversible\ndifferential equations solvers, backpropagation through differential equations,\nBrownian reconstruction); symbolic regression for dynamical systems (e.g. via\nregularised evolution); and deep implicit models (e.g. deep equilibrium models,\ndifferentiable optimisation).\n  We anticipate this thesis will be of interest to anyone interested in the\nmarriage of deep learning with dynamical systems, and hope it will provide a\nuseful reference for the current state of the art.",
          "arxiv_id": "2202.02435v1"
        }
      ],
      "4": [
        {
          "title": "Building explainable graph neural network by sparse learning for the drug-protein binding prediction",
          "year": "2023-08",
          "abstract": "Explainable Graph Neural Networks (GNNs) have been developed and applied to\ndrug-protein binding prediction to identify the key chemical structures in a\ndrug that have active interactions with the target proteins. However, the key\nstructures identified by the current explainable GNN models are typically\nchemically invalid. Furthermore, a threshold needs to be manually selected to\npinpoint the key structures from the rest. To overcome the limitations of the\ncurrent explainable GNN models, we propose our SLGNN, which stands for using\nSparse Learning to Graph Neural Networks. Our SLGNN relies on using a\nchemical-substructure-based graph (where nodes are chemical substructures) to\nrepresent a drug molecule. Furthermore, SLGNN incorporates generalized fussed\nlasso with message-passing algorithms to identify connected subgraphs that are\ncritical for the drug-protein binding prediction. Due to the use of the\nchemical-substructure-based graph, it is guaranteed that any subgraphs in a\ndrug identified by our SLGNN are chemically valid structures. These structures\ncan be further interpreted as the key chemical structures for the drug to bind\nto the target protein. We demonstrate the explanatory power of our SLGNN by\nfirst showing all the key structures identified by our SLGNN are chemically\nvalid. In addition, we illustrate that the key structures identified by our\nSLGNN have more predictive power than the key structures identified by the\ncompeting methods. At last, we use known drug-protein binding data to show the\nkey structures identified by our SLGNN contain most of the binding sites.",
          "arxiv_id": "2309.12906v1"
        },
        {
          "title": "A biologically-inspired multi-modal evaluation of molecular generative machine learning",
          "year": "2022-08",
          "abstract": "While generative models have recently become ubiquitous in many scientific\nareas, less attention has been paid to their evaluation. For molecular\ngenerative models, the state-of-the-art examines their output in isolation or\nin relation to its input. However, their biological and functional properties,\nsuch as ligand-target interaction is not being addressed. In this study, a\nnovel biologically-inspired benchmark for the evaluation of molecular\ngenerative models is proposed. Specifically, three diverse reference datasets\nare designed and a set of metrics are introduced which are directly relevant to\nthe drug discovery process. In particular we propose a recreation metric, apply\ndrug-target affinity prediction and molecular docking as complementary\ntechniques for the evaluation of generative outputs. While all three metrics\nshow consistent results across the tested generative models, a more detailed\ncomparison of drug-target affinity binding and molecular docking scores\nrevealed that unimodal predictiors can lead to erroneous conclusions about\ntarget binding on a molecular level and a multi-modal approach is thus\npreferrable. The key advantage of this framework is that it incorporates prior\nphysico-chemical domain knowledge into the benchmarking process by focusing\nexplicitly on ligand-target interactions and thus creating a highly efficient\ntool not only for evaluating molecular generative outputs in particular, but\nalso for enriching the drug discovery process in general.",
          "arxiv_id": "2208.09658v2"
        },
        {
          "title": "Structure-aware generation of drug-like molecules",
          "year": "2021-11",
          "abstract": "Structure-based drug design involves finding ligand molecules that exhibit\nstructural and chemical complementarity to protein pockets. Deep generative\nmethods have shown promise in proposing novel molecules from scratch (de-novo\ndesign), avoiding exhaustive virtual screening of chemical space. Most\ngenerative de-novo models fail to incorporate detailed ligand-protein\ninteractions and 3D pocket structures. We propose a novel supervised model that\ngenerates molecular graphs jointly with 3D pose in a discretised molecular\nspace. Molecules are built atom-by-atom inside pockets, guided by structural\ninformation from crystallographic data. We evaluate our model using a docking\nbenchmark and find that guided generation improves predicted binding affinities\nby 8% and drug-likeness scores by 10% over the baseline. Furthermore, our model\nproposes molecules with binding scores exceeding some known ligands, which\ncould be useful in future wet-lab studies.",
          "arxiv_id": "2111.04107v1"
        }
      ],
      "5": [
        {
          "title": "Non-convergence of stochastic gradient descent in the training of deep neural networks",
          "year": "2020-06",
          "abstract": "Deep neural networks have successfully been trained in various application\nareas with stochastic gradient descent. However, there exists no rigorous\nmathematical explanation why this works so well. The training of neural\nnetworks with stochastic gradient descent has four different discretization\nparameters: (i) the network architecture; (ii) the amount of training data;\n(iii) the number of gradient steps; and (iv) the number of randomly initialized\ngradient trajectories. While it can be shown that the approximation error\nconverges to zero if all four parameters are sent to infinity in the right\norder, we demonstrate in this paper that stochastic gradient descent fails to\nconverge for ReLU networks if their depth is much larger than their width and\nthe number of random initializations does not increase to infinity fast enough.",
          "arxiv_id": "2006.07075v2"
        },
        {
          "title": "A Mean-Field Analysis of Neural Stochastic Gradient Descent-Ascent for Functional Minimax Optimization",
          "year": "2024-04",
          "abstract": "This paper studies minimax optimization problems defined over\ninfinite-dimensional function classes of overparameterized two-layer neural\nnetworks. In particular, we consider the minimax optimization problem stemming\nfrom estimating linear functional equations defined by conditional\nexpectations, where the objective functions are quadratic in the functional\nspaces. We address (i) the convergence of the stochastic gradient\ndescent-ascent algorithm and (ii) the representation learning of the neural\nnetworks. We establish convergence under the mean-field regime by considering\nthe continuous-time and infinite-width limit of the optimization dynamics.\nUnder this regime, the stochastic gradient descent-ascent corresponds to a\nWasserstein gradient flow over the space of probability measures defined over\nthe space of neural network parameters. We prove that the Wasserstein gradient\nflow converges globally to a stationary point of the minimax objective at a\n$O(T^{-1} + \\alpha^{-1})$ sublinear rate, and additionally finds the solution\nto the functional equation when the regularizer of the minimax objective is\nstrongly convex. Here $T$ denotes the time and $\\alpha$ is a scaling parameter\nof the neural networks. In terms of representation learning, our results show\nthat the feature representation induced by the neural networks is allowed to\ndeviate from the initial one by the magnitude of $O(\\alpha^{-1})$, measured in\nterms of the Wasserstein distance. Finally, we apply our general results to\nconcrete examples including policy evaluation, nonparametric instrumental\nvariable regression, asset pricing, and adversarial Riesz representer\nestimation.",
          "arxiv_id": "2404.12312v3"
        },
        {
          "title": "Tackling benign nonconvexity with smoothing and stochastic gradients",
          "year": "2022-02",
          "abstract": "Non-convex optimization problems are ubiquitous in machine learning,\nespecially in Deep Learning. While such complex problems can often be\nsuccessfully optimized in practice by using stochastic gradient descent (SGD),\ntheoretical analysis cannot adequately explain this success. In particular, the\nstandard analyses do not show global convergence of SGD on non-convex\nfunctions, and instead show convergence to stationary points (which can also be\nlocal minima or saddle points). We identify a broad class of nonconvex\nfunctions for which we can show that perturbed SGD (gradient descent perturbed\nby stochastic noise -- covering SGD as a special case) converges to a global\nminimum (or a neighborhood thereof), in contrast to gradient descent without\nnoise that can get stuck in local minima far from a global solution. For\nexample, on non-convex functions that are relatively close to a convex-like\n(strongly convex or PL) function we show that SGD can converge linearly to a\nglobal optimum.",
          "arxiv_id": "2202.09052v1"
        }
      ],
      "6": [
        {
          "title": "Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks",
          "year": "2024-06",
          "abstract": "In recent years, graph neural networks (GNNs) have emerged as a potent tool\nfor learning on graph-structured data and won fruitful successes in varied\nfields. The majority of GNNs follow the message-passing paradigm, where\nrepresentations of each node are learned by recursively aggregating features of\nits neighbors. However, this mechanism brings severe over-smoothing and\nefficiency issues over high-degree graphs (HDGs), wherein most nodes have\ndozens (or even hundreds) of neighbors, such as social networks, transaction\ngraphs, power grids, etc. Additionally, such graphs usually encompass rich and\ncomplex structure semantics, which are hard to capture merely by feature\naggregations in GNNs. Motivated by the above limitations, we propose TADA, an\nefficient and effective front-mounted data augmentation framework for GNNs on\nHDGs. Under the hood, TADA includes two key modules: (i) feature expansion with\nstructure embeddings, and (ii) topology- and attribute-aware graph\nsparsification. The former obtains augmented node features and enhanced model\ncapacity by encoding the graph structure into high-quality structure embeddings\nwith our highly-efficient sketching method. Further, by exploiting\ntask-relevant features extracted from graph structures and attributes, the\nsecond module enables the accurate identification and reduction of numerous\nredundant/noisy edges from the input graph, thereby alleviating over-smoothing\nand facilitating faster feature aggregations over HDGs. Empirically, TADA\nconsiderably improves the predictive performance of mainstream GNN models on 8\nreal homophilic/heterophilic HDGs in terms of node classification, while\nachieving efficient training and inference processes.",
          "arxiv_id": "2406.05482v4"
        },
        {
          "title": "Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network",
          "year": "2024-03",
          "abstract": "Under circumstances of heterophily, where nodes with different labels tend to\nbe connected based on semantic meanings, Graph Neural Networks (GNNs) often\nexhibit suboptimal performance. Current studies on graph heterophily mainly\nfocus on aggregation calibration or neighbor extension and address the\nheterophily issue by utilizing node features or structural information to\nimprove GNN representations. In this paper, we propose and demonstrate that the\nvaluable semantic information inherent in heterophily can be utilized\neffectively in graph learning by investigating the distribution of neighbors\nfor each individual node within the graph. The theoretical analysis is carried\nout to demonstrate the efficacy of the idea in enhancing graph learning. Based\non this analysis, we propose HiGNN, an innovative approach that constructs an\nadditional new graph structure, that integrates heterophilous information by\nleveraging node distribution to enhance connectivity between nodes that share\nsimilar semantic characteristics. We conduct empirical assessments on node\nclassification tasks using both homophilous and heterophilous benchmark\ndatasets and compare HiGNN to popular GNN baselines and SoTA methods,\nconfirming the effectiveness in improving graph representations. In addition,\nby incorporating heterophilous information, we demonstrate a notable\nenhancement in existing GNN-based approaches, and the homophily degree across\nreal-world datasets, thus affirming the efficacy of our approach.",
          "arxiv_id": "2403.17351v2"
        },
        {
          "title": "Meta-Inductive Node Classification across Graphs",
          "year": "2021-05",
          "abstract": "Semi-supervised node classification on graphs is an important research\nproblem, with many real-world applications in information retrieval such as\ncontent classification on a social network and query intent classification on\nan e-commerce query graph. While traditional approaches are largely\ntransductive, recent graph neural networks (GNNs) integrate node features with\nnetwork structures, thus enabling inductive node classification models that can\nbe applied to new nodes or even new graphs in the same feature space. However,\ninter-graph differences still exist across graphs within the same domain. Thus,\ntraining just one global model (e.g., a state-of-the-art GNN) to handle all new\ngraphs, whilst ignoring the inter-graph differences, can lead to suboptimal\nperformance.\n  In this paper, we study the problem of inductive node classification across\ngraphs. Unlike existing one-model-fits-all approaches, we propose a novel\nmeta-inductive framework called MI-GNN to customize the inductive model to each\ngraph under a meta-learning paradigm. That is, MI-GNN does not directly learn\nan inductive model; it learns the general knowledge of how to train a model for\nsemi-supervised node classification on new graphs. To cope with the differences\nacross graphs, MI-GNN employs a dual adaptation mechanism at both the graph and\ntask levels. More specifically, we learn a graph prior to adapt for the\ngraph-level differences, and a task prior to adapt for the task-level\ndifferences conditioned on a graph. Extensive experiments on five real-world\ngraph collections demonstrate the effectiveness of our proposed model.",
          "arxiv_id": "2105.06725v2"
        }
      ],
      "7": [
        {
          "title": "Deep Cerebellar Nuclei Segmentation via Semi-Supervised Deep Context-Aware Learning from 7T Diffusion MRI",
          "year": "2020-04",
          "abstract": "Deep cerebellar nuclei are a key structure of the cerebellum that are\ninvolved in processing motor and sensory information. It is thus a crucial step\nto accurately segment deep cerebellar nuclei for the understanding of the\ncerebellum system and its utility in deep brain stimulation treatment. However,\nit is challenging to clearly visualize such small nuclei under standard\nclinical magnetic resonance imaging (MRI) protocols and therefore precise\nsegmentation is not feasible. Recent advances in 7 Tesla (T) MRI technology and\ngreat potential of deep neural networks facilitate automatic patient-specific\nsegmentation. In this paper, we propose a novel deep learning framework\n(referred to as DCN-Net) for fast, accurate, and robust patient-specific\nsegmentation of deep cerebellar dentate and interposed nuclei on 7T diffusion\nMRI. DCN-Net effectively encodes contextual information on the patch images\nwithout consecutive pooling operations and adding complexity via proposed\ndilated dense blocks. During the end-to-end training, label probabilities of\ndentate and interposed nuclei are independently learned with a hybrid loss,\nhandling highly imbalanced data. Finally, we utilize self-training strategies\nto cope with the problem of limited labeled data. To this end, auxiliary\ndentate and interposed nuclei labels are created on unlabeled data by using\nDCN-Net trained on manual labels. We validate the proposed framework using 7T\nB0 MRIs from 60 subjects. Experimental results demonstrate that DCN-Net\nprovides better segmentation than atlas-based deep cerebellar nuclei\nsegmentation tools and other state-of-the-art deep neural networks in terms of\naccuracy and consistency. We further prove the effectiveness of the proposed\ncomponents within DCN-Net in dentate and interposed nuclei segmentation.",
          "arxiv_id": "2004.09788v3"
        },
        {
          "title": "Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images",
          "year": "2020-09",
          "abstract": "COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging\nexams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,\nand uses less radiation. Here, we demonstrate the impact of lung segmentation\nin COVID-19 identification using CXR images and evaluate which contents of the\nimage influenced the most. Semantic segmentation was performed using a U-Net\nCNN architecture, and the classification using three CNN architectures (VGG,\nResNet, and Inception). Explainable Artificial Intelligence techniques were\nemployed to estimate the impact of segmentation. A three-classes database was\ncomposed: lung opacity (pneumonia), COVID-19, and normal. We assessed the\nimpact of creating a CXR image database from different sources, and the\nCOVID-19 generalization from one source to another. The segmentation achieved a\nJaccard distance of 0.034 and a Dice coefficient of 0.982. The classification\nusing segmented images achieved an F1-Score of 0.88 for the multi-class setup,\nand 0.83 for COVID-19 identification. In the cross-dataset scenario, we\nobtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for\nCOVID-19 identification using segmented images. Experiments support the\nconclusion that even after segmentation, there is a strong bias introduced by\nunderlying factors from different sources.",
          "arxiv_id": "2009.09780v4"
        },
        {
          "title": "Synergistic Learning of Lung Lobe Segmentation and Hierarchical Multi-Instance Classification for Automated Severity Assessment of COVID-19 in CT Images",
          "year": "2020-05",
          "abstract": "Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19)\nwill help detect infections early and assess the disease progression.\nEspecially, automated severity assessment of COVID-19 in CT images plays an\nessential role in identifying cases that are in great need of intensive\nclinical care. However, it is often challenging to accurately assess the\nseverity of this disease in CT images, due to variable infection regions in the\nlungs, similar imaging biomarkers, and large inter-case variations. To this\nend, we propose a synergistic learning framework for automated severity\nassessment of COVID-19 in 3D CT images, by jointly performing lung lobe\nsegmentation and multi-instance classification. Considering that only a few\ninfection regions in a CT image are related to the severity assessment, we\nfirst represent each input image by a bag that contains a set of 2D image\npatches (with each cropped from a specific slice). A multi-task multi-instance\ndeep network (called M$^2$UNet) is then developed to assess the severity of\nCOVID-19 patients and also segment the lung lobe simultaneously. Our M$^2$UNet\nconsists of a patch-level encoder, a segmentation sub-network for lung lobe\nsegmentation, and a classification sub-network for severity assessment (with a\nunique hierarchical multi-instance learning strategy). Here, the context\ninformation provided by segmentation can be implicitly employed to improve the\nperformance of severity assessment. Extensive experiments were performed on a\nreal COVID-19 CT image dataset consisting of 666 chest CT images, with results\nsuggesting the effectiveness of our proposed method compared to several\nstate-of-the-art methods.",
          "arxiv_id": "2005.03832v2"
        }
      ],
      "8": [
        {
          "title": "Large-scale unsupervised audio pre-training for video-to-speech synthesis",
          "year": "2023-06",
          "abstract": "Video-to-speech synthesis is the task of reconstructing the speech signal\nfrom a silent video of a speaker. Most established approaches to date involve a\ntwo-step process, whereby an intermediate representation from the video, such\nas a spectrogram, is extracted first and then passed to a vocoder to produce\nthe raw audio. Some recent work has focused on end-to-end synthesis, whereby\nthe generation of raw audio and any intermediate representations is performed\njointly. All such approaches involve training on data from almost exclusively\naudio-visual datasets, i.e. every audio sample has a corresponding video\nsample. This precludes the use of abundant audio-only datasets which may not\nhave a corresponding visual modality (e.g. audiobooks, radio podcasts, speech\nrecognition datasets etc.), as well as audio-only architectures that have been\ndeveloped by the audio machine learning community over the years. In this paper\nwe propose to train encoder-decoder models on more than 3,500 hours of audio\ndata at 24kHz, and then use the pre-trained decoders to initialize the audio\ndecoders for the video-to-speech synthesis task. The pre-training step uses\naudio samples only and does not require labels or corresponding samples from\nother modalities (visual, text). We demonstrate that this pre-training step\nimproves the reconstructed speech and that it is an unexplored way to improve\nthe quality of the generator in a cross-modal task while only requiring samples\nfrom one of the modalities. We conduct experiments using both raw audio and mel\nspectrograms as target outputs and benchmark our models with existing work.",
          "arxiv_id": "2306.15464v2"
        },
        {
          "title": "Accurate synthesis of Dysarthric Speech for ASR data augmentation",
          "year": "2023-08",
          "abstract": "Dysarthria is a motor speech disorder often characterized by reduced speech\nintelligibility through slow, uncoordinated control of speech production\nmuscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers\ncommunicate more effectively. However, robust dysarthria-specific ASR requires\na significant amount of training speech, which is not readily available for\ndysarthric talkers. This paper presents a new dysarthric speech synthesis\nmethod for the purpose of ASR training data augmentation. Differences in\nprosodic and acoustic characteristics of dysarthric spontaneous speech at\nvarying severity levels are important components for dysarthric speech\nmodeling, synthesis, and augmentation. For dysarthric speech synthesis, a\nmodified neural multi-talker TTS is implemented by adding a dysarthria severity\nlevel coefficient and a pause insertion model to synthesize dysarthric speech\nfor varying severity levels. To evaluate the effectiveness for synthesis of\ntraining data for ASR, dysarthria-specific speech recognition was used. Results\nshow that a DNN-HMM model trained on additional synthetic dysarthric speech\nachieves WER improvement of 12.2% compared to the baseline, and that the\naddition of the severity level and pause insertion controls decrease WER by\n6.5%, showing the effectiveness of adding these parameters. Overall results on\nthe TORGO database demonstrate that using dysarthric synthetic speech to\nincrease the amount of dysarthric-patterned speech for training has significant\nimpact on the dysarthric ASR systems. In addition, we have conducted a\nsubjective evaluation to evaluate the dysarthric-ness and similarity of\nsynthesized speech. Our subjective evaluation shows that the perceived\ndysartrhic-ness of synthesized speech is similar to that of true dysarthric\nspeech, especially for higher levels of dysarthria",
          "arxiv_id": "2308.08438v1"
        },
        {
          "title": "Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis",
          "year": "2022-03",
          "abstract": "Since facial actions such as lip movements contain significant information\nabout speech content, it is not surprising that audio-visual speech enhancement\nmethods are more accurate than their audio-only counterparts. Yet,\nstate-of-the-art approaches still struggle to generate clean, realistic speech\nwithout noise artifacts and unnatural distortions in challenging acoustic\nenvironments. In this paper, we propose a novel audio-visual speech enhancement\nframework for high-fidelity telecommunications in AR/VR. Our approach leverages\naudio-visual speech cues to generate the codes of a neural speech codec,\nenabling efficient synthesis of clean, realistic speech from noisy signals.\nGiven the importance of speaker-specific cues in speech, we focus on developing\npersonalized models that work well for individual speakers. We demonstrate the\nefficacy of our approach on a new audio-visual speech dataset collected in an\nunconstrained, large vocabulary setting, as well as existing audio-visual\ndatasets, outperforming speech enhancement baselines on both quantitative\nmetrics and human evaluation studies. Please see the supplemental video for\nqualitative results at\nhttps://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.",
          "arxiv_id": "2203.17263v1"
        }
      ],
      "9": [
        {
          "title": "RL for Consistency Models: Faster Reward Guided Text-to-Image Generation",
          "year": "2024-03",
          "abstract": "Reinforcement learning (RL) has improved guided image generation with\ndiffusion models by directly optimizing rewards that capture image quality,\naesthetics, and instruction following capabilities. However, the resulting\ngenerative policies inherit the same iterative sampling process of diffusion\nmodels that causes slow generation. To overcome this limitation, consistency\nmodels proposed learning a new class of generative models that directly map\nnoise to data, resulting in a model that can generate an image in as few as one\nsampling iteration. In this work, to optimize text-to-image generative models\nfor task specific rewards and enable fast training and inference, we propose a\nframework for fine-tuning consistency models via RL. Our framework, called\nReinforcement Learning for Consistency Model (RLCM), frames the iterative\ninference process of a consistency model as an RL procedure. Comparing to RL\nfinetuned diffusion models, RLCM trains significantly faster, improves the\nquality of the generation measured under the reward objectives, and speeds up\nthe inference procedure by generating high quality images with as few as two\ninference steps. Experimentally, we show that RLCM can adapt text-to-image\nconsistency models to objectives that are challenging to express with\nprompting, such as image compressibility, and those derived from human\nfeedback, such as aesthetic quality. Our code is available at\nhttps://rlcm.owenoertell.com.",
          "arxiv_id": "2404.03673v2"
        },
        {
          "title": "Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis",
          "year": "2023-09",
          "abstract": "Conditional generative models typically demand large annotated training sets\nto achieve high-quality synthesis. As a result, there has been significant\ninterest in designing models that perform plug-and-play generation, i.e., to\nuse a predefined or pretrained model, which is not explicitly trained on the\ngenerative task, to guide the generative process (e.g., using language).\nHowever, such guidance is typically useful only towards synthesizing high-level\nsemantics rather than editing fine-grained details as in image-to-image\ntranslation tasks. To this end, and capitalizing on the powerful fine-grained\ngenerative control offered by the recent diffusion-based generative models, we\nintroduce Steered Diffusion, a generalized framework for photorealistic\nzero-shot conditional image generation using a diffusion model trained for\nunconditional generation. The key idea is to steer the image generation of the\ndiffusion model at inference time via designing a loss using a pre-trained\ninverse model that characterizes the conditional task. This loss modulates the\nsampling trajectory of the diffusion process. Our framework allows for easy\nincorporation of multiple conditions during inference. We present experiments\nusing steered diffusion on several tasks including inpainting, colorization,\ntext-guided semantic editing, and image super-resolution. Our results\ndemonstrate clear qualitative and quantitative improvements over\nstate-of-the-art diffusion-based plug-and-play models while adding negligible\nadditional computational cost.",
          "arxiv_id": "2310.00224v1"
        },
        {
          "title": "Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance",
          "year": "2022-10",
          "abstract": "Diffusion models have achieved unprecedented performance in generative\nmodeling. The commonly-adopted formulation of the latent code of diffusion\nmodels is a sequence of gradually denoised samples, as opposed to the simpler\n(e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper\nprovides an alternative, Gaussian formulation of the latent space of various\ndiffusion models, as well as an invertible DPM-Encoder that maps images into\nthe latent space. While our formulation is purely based on the definition of\ndiffusion models, we demonstrate several intriguing consequences. (1)\nEmpirically, we observe that a common latent space emerges from two diffusion\nmodels trained independently on related domains. In light of this finding, we\npropose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image\ntranslation. Furthermore, applying CycleDiffusion to text-to-image diffusion\nmodels, we show that large-scale text-to-image diffusion models can be used as\nzero-shot image-to-image editors. (2) One can guide pre-trained diffusion\nmodels and GANs by controlling the latent codes in a unified, plug-and-play\nformulation based on energy-based models. Using the CLIP model and a face\nrecognition model as guidance, we demonstrate that diffusion models have better\ncoverage of low-density sub-populations and individuals than GANs. The code is\npublicly available at https://github.com/ChenWu98/cycle-diffusion.",
          "arxiv_id": "2210.05559v2"
        }
      ],
      "10": [
        {
          "title": "Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL",
          "year": "2025-04",
          "abstract": "Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted\nnext-generation wireless networks is critical for mobility management and\nensuring UAV safety and ubiquitous connectivity, especially in dense urban\nenvironments with street canyons and tall buildings. Traditional statistical\nand model-based techniques have been successfully used for path optimization in\ncommunication networks. However, when dynamic channel propagation\ncharacteristics such as line-of-sight (LOS), interference, handover, and\nsignal-to-interference and noise ratio (SINR) are included in path\noptimization, statistical and model-based path planning solutions become\nobsolete since they cannot adapt to the dynamic and time-varying wireless\nchannels, especially in the mmWave bands. In this paper, we propose a novel\nmodel-free actor-critic deep reinforcement learning (AC-DRL) framework for path\noptimization in UAV-assisted 5G mmWave wireless networks, which combines four\nimportant aspects of UAV communication: \\textit{flight time, handover,\nconnectivity and SINR}. We train an AC-RL agent that enables a UAV connected to\na gNB to determine the optimal path to a desired destination in the shortest\npossible time with minimal gNB handover, while maintaining connectivity and the\nhighest possible SINR. We train our model with data from a powerful ray tracing\ntool called Wireless InSite, which uses 3D images of the propagation\nenvironment and provides data that closely resembles the real propagation\nenvironment. The simulation results show that our system has superior\nperformance in tracking high SINR compared to other selected RL algorithms.",
          "arxiv_id": "2504.02688v1"
        },
        {
          "title": "Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks",
          "year": "2025-06",
          "abstract": "As the number of mobile devices continues to grow, interference has become a\nmajor bottleneck in improving data rates in wireless networks. Efficient joint\nchannel and power allocation (JCPA) is crucial for managing interference. In\nthis paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the\nJCPA problem in multi-channel wireless networks. To reduce the computational\ncomplexity of iterative optimization, we further introduce JCPGNN-M, a graph\nneural network-based solution that enables simultaneous multi-channel\nallocation for each user. We reformulate the problem as a Lagrangian function,\nwhich allows us to enforce the total power constraints systematically. Our\nsolution involves combining this Lagrangian framework with GNNs and iteratively\nupdating the Lagrange multipliers and resource allocation scheme. Unlike\nexisting GNN-based methods that limit each user to a single channel, JCPGNN-M\nsupports efficient spectrum reuse and scales well in dense network scenarios.\nSimulation results show that JCPGNN-M achieves better data rate compared to\neWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and\nit can generalize well to larger networks.",
          "arxiv_id": "2506.03813v1"
        },
        {
          "title": "Adaptive Resource Allocation for Semantic Communication Networks",
          "year": "2023-12",
          "abstract": "Semantic communication, recognized as a promising technology for future\nintelligent applications, has received widespread research attention. Despite\nthe potential of semantic communication to enhance transmission reliability,\nespecially in low signal-to-noise (SNR) environments, the critical issue of\nresource allocation and compatibility in the dynamic wireless environment\nremains largely unexplored. In this paper, we propose an adaptive semantic\nresource allocation paradigm with semantic-bit quantization (SBQ) compatibly\nfor existing wireless communications, where the inaccurate environment\nperception introduced by the additional mapping relationship between semantic\nmetrics and transmission metrics is solved. In order to investigate the\nperformance of semantic communication networks, the quality of service for\nsemantic communication (SC-QoS), including the semantic quantization efficiency\n(SQE) and transmission latency, is proposed for the first time. A problem of\nmaximizing the overall effective SC-QoS is formulated by jointly optimizing the\ntransmit beamforming of the base station, the bits for semantic representation,\nthe subchannel assignment, and the bandwidth resource allocation. To address\nthe non-convex formulated problem, an intelligent resource allocation scheme is\nproposed based on a hybrid deep reinforcement learning (DRL) algorithm, where\nthe intelligent agent can perceive both semantic tasks and dynamic wireless\nenvironments. Simulation results demonstrate that our design can effectively\ncombat semantic noise and achieve superior performance in wireless\ncommunications compared to several benchmark schemes. Furthermore, compared to\nmapping-guided paradigm based resource allocation schemes, our proposed\nadaptive scheme can achieve up to 13% performance improvement in terms of\nSC-QoS.",
          "arxiv_id": "2312.01081v1"
        }
      ],
      "11": [
        {
          "title": "Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks",
          "year": "2022-06",
          "abstract": "The large computing and memory cost of deep neural networks (DNNs) often\nprecludes their use in resource-constrained devices. Quantizing the parameters\nand operations to lower bit-precision offers substantial memory and energy\nsavings for neural network inference, facilitating the use of DNNs on edge\ncomputing platforms. Recent efforts at quantizing DNNs have employed a range of\ntechniques encompassing progressive quantization, step-size adaptation, and\ngradient scaling. This paper proposes a new quantization approach for mixed\nprecision convolutional neural networks (CNNs) targeting edge-computing. Our\nmethod establishes a new pareto frontier in model accuracy and memory footprint\ndemonstrating a range of quantized models, delivering best-in-class accuracy\nbelow 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions\nare: (i) hardware-aware heterogeneous differentiable quantization with\ntensor-sliced learned precision, (ii) targeted gradient modification for wgts.\nand acts. to mitigate quantization errors, and (iii) a multi-phase learning\nschedule to address instability in learning arising from updates to the learned\nquantizer and model parameters. We demonstrate the effectiveness of our\ntechniques on the ImageNet dataset across a range of models including\nEfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and\nMobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy).",
          "arxiv_id": "2206.07741v2"
        },
        {
          "title": "Joint Pruning and Channel-wise Mixed-Precision Quantization for Efficient Deep Neural Networks",
          "year": "2024-07",
          "abstract": "The resource requirements of deep neural networks (DNNs) pose significant\nchallenges to their deployment on edge devices. Common approaches to address\nthis issue are pruning and mixed-precision quantization, which lead to latency\nand memory occupation improvements. These optimization techniques are usually\napplied independently. We propose a novel methodology to apply them jointly via\na lightweight gradient-based search, and in a hardware-aware manner, greatly\nreducing the time required to generate Pareto-optimal DNNs in terms of accuracy\nversus cost (i.e., latency or memory). We test our approach on three\nedge-relevant benchmarks, namely CIFAR-10, Google Speech Commands, and Tiny\nImageNet. When targeting the optimization of the memory footprint, we are able\nto achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the\nbaseline networks with all weights quantized at 8 and 2-bit, respectively. Our\nmethod surpasses a previous state-of-the-art approach with up to 56.17% size\nreduction at iso-accuracy. With respect to the sequential application of\nstate-of-the-art pruning and mixed-precision optimizations, we obtain\ncomparable or superior results, but with a significantly lowered training time.\nIn addition, we show how well-tailored cost models can improve the cost versus\naccuracy trade-offs when targeting specific hardware for deployment.",
          "arxiv_id": "2407.01054v2"
        },
        {
          "title": "MSP: An FPGA-Specific Mixed-Scheme, Multi-Precision Deep Neural Network Quantization Framework",
          "year": "2020-09",
          "abstract": "With the tremendous success of deep learning, there exists imminent need to\ndeploy deep learning models onto edge devices. To tackle the limited computing\nand storage resources in edge devices, model compression techniques have been\nwidely used to trim deep neural network (DNN) models for on-device inference\nexecution. This paper targets the commonly used FPGA (field programmable gate\narray) devices as the hardware platforms for DNN edge computing. We focus on\nthe DNN quantization as the main model compression technique, since DNN\nquantization has been of great importance for the implementations of DNN models\non the hardware platforms. The novelty of this work comes in twofold: (i) We\npropose a mixed-scheme DNN quantization method that incorporates both the\nlinear and non-linear number systems for quantization, with the aim to boost\nthe utilization of the heterogeneous computing resources, i.e., LUTs (look up\ntables) and DSPs (digital signal processors) on an FPGA. Note that all the\nexisting (single-scheme) quantization methods can only utilize one type of\nresources (either LUTs or DSPs for the MAC (multiply-accumulate) operations in\ndeep learning computations. (ii) We use a quantization method that supports\nmultiple precisions along the intra-layer dimension, while the existing\nquantization methods apply multi-precision quantization along the inter-layer\ndimension. The intra-layer multi-precision method can uniform the hardware\nconfigurations for different layers to reduce computation overhead and at the\nsame time preserve the model accuracy as the inter-layer approach.",
          "arxiv_id": "2009.07460v2"
        }
      ],
      "12": [
        {
          "title": "Parameterized Quantum Circuits with Quantum Kernels for Machine Learning: A Hybrid Quantum-Classical Approach",
          "year": "2022-09",
          "abstract": "Quantum machine learning (QML) is the use of quantum computing for the\ncomputation of machine learning algorithms. With the prevalence and importance\nof classical data, a hybrid quantum-classical approach to QML is called for.\nParameterized Quantum Circuits (PQCs), and particularly Quantum Kernel PQCs,\nare generally used in the hybrid approach to QML. In this paper we discuss some\nimportant aspects of PQCs with quantum kernels including PQCs, quantum kernels,\nquantum kernels with quantum advantage, and the trainability of quantum\nkernels. We conclude that quantum kernels with hybrid kernel methods, a.k.a.\nquantum kernel methods, offer distinct advantages as a hybrid approach to QML.\nNot only do they apply to Noisy Intermediate-Scale Quantum (NISQ) devices, but\nthey also can be used to solve all types of machine learning problems including\nregression, classification, clustering, and dimension reduction. Furthermore,\nbeyond quantum utility, quantum advantage can be attained if the quantum\nkernels, i.e., the quantum feature encodings, are classically intractable.",
          "arxiv_id": "2209.14449v2"
        },
        {
          "title": "Quantum machine learning and quantum biomimetics: A perspective",
          "year": "2020-04",
          "abstract": "Quantum machine learning has emerged as an exciting and promising paradigm\ninside quantum technologies. It may permit, on the one hand, to carry out more\nefficient machine learning calculations by means of quantum devices, while, on\nthe other hand, to employ machine learning techniques to better control quantum\nsystems. Inside quantum machine learning, quantum reinforcement learning aims\nat developing \"intelligent\" quantum agents that may interact with the outer\nworld and adapt to it, with the strategy of achieving some final goal. Another\nparadigm inside quantum machine learning is that of quantum autoencoders, which\nmay allow one for employing fewer resources in a quantum device via a training\nprocess. Moreover, the field of quantum biomimetics aims at establishing\nanalogies between biological and quantum systems, to look for previously\ninadvertent connections that may enable useful applications. Two recent\nexamples are the concepts of quantum artificial life, as well as of quantum\nmemristors. In this Perspective, we give an overview of these topics,\ndescribing the related research carried out by the scientific community.",
          "arxiv_id": "2004.12076v2"
        },
        {
          "title": "Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers",
          "year": "2024-07",
          "abstract": "When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.",
          "arxiv_id": "2407.14055v2"
        }
      ],
      "13": [
        {
          "title": "Continuous Time Bandits With Sampling Costs",
          "year": "2021-07",
          "abstract": "We consider a continuous-time multi-arm bandit problem (CTMAB), where the\nlearner can sample arms any number of times in a given interval and obtain a\nrandom reward from each sample, however, increasing the frequency of sampling\nincurs an additive penalty/cost. Thus, there is a tradeoff between obtaining\nlarge reward and incurring sampling cost as a function of the sampling\nfrequency. The goal is to design a learning algorithm that minimizes regret,\nthat is defined as the difference of the payoff of the oracle policy and that\nof the learning algorithm. CTMAB is fundamentally different than the usual\nmulti-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial\nin CTMAB, since the optimal sampling frequency depends on the mean of the arm,\nwhich needs to be estimated. We first establish lower bounds on the regret\nachievable with any algorithm and then propose algorithms that achieve the\nlower bound up to logarithmic factors. For the single-arm case, we show that\nthe lower bound on the regret is $\\Omega((\\log T)^2/\\mu)$, where $\\mu$ is the\nmean of the arm, and $T$ is the time horizon. For the multiple arms case, we\nshow that the lower bound on the regret is $\\Omega((\\log T)^2 \\mu/\\Delta^2)$,\nwhere $\\mu$ now represents the mean of the best arm, and $\\Delta$ is the\ndifference of the mean of the best and the second-best arm. We then propose an\nalgorithm that achieves the bound up to constant terms.",
          "arxiv_id": "2107.05289v2"
        },
        {
          "title": "Context-lumpable stochastic bandits",
          "year": "2023-06",
          "abstract": "We consider a contextual bandit problem with $S$ contexts and $K$ actions. In\neach round $t=1,2,\\dots$, the learner observes a random context and chooses an\naction based on its past experience. The learner then observes a random reward\nwhose mean is a function of the context and the action for the round. Under the\nassumption that the contexts can be lumped into $r\\le \\min\\{S,K\\}$ groups such\nthat the mean reward for the various actions is the same for any two contexts\nthat are in the same group, we give an algorithm that outputs an\n$\\epsilon$-optimal policy after using at most $\\widetilde O(r (S +K\n)/\\epsilon^2)$ samples with high probability and provide a matching\n$\\Omega(r(S+K)/\\epsilon^2)$ lower bound. In the regret minimization setting, we\ngive an algorithm whose cumulative regret up to time $T$ is bounded by\n$\\widetilde O(\\sqrt{r^3(S+K)T})$. To the best of our knowledge, we are the\nfirst to show the near-optimal sample complexity in the PAC setting and\n$\\widetilde O(\\sqrt{{poly}(r)(S+K)T})$ minimax regret in the online setting for\nthis problem. We also show our algorithms can be applied to more general\nlow-rank bandits and get improved regret bounds in some scenarios.",
          "arxiv_id": "2306.13053v2"
        },
        {
          "title": "p-Mean Regret for Stochastic Bandits",
          "year": "2024-12",
          "abstract": "In this work, we extend the concept of the $p$-mean welfare objective from\nsocial choice theory (Moulin 2004) to study $p$-mean regret in stochastic\nmulti-armed bandit problems. The $p$-mean regret, defined as the difference\nbetween the optimal mean among the arms and the $p$-mean of the expected\nrewards, offers a flexible framework for evaluating bandit algorithms, enabling\nalgorithm designers to balance fairness and efficiency by adjusting the\nparameter $p$. Our framework encompasses both average cumulative regret and\nNash regret as special cases.\n  We introduce a simple, unified UCB-based algorithm (Explore-Then-UCB) that\nachieves novel $p$-mean regret bounds. Our algorithm consists of two phases: a\ncarefully calibrated uniform exploration phase to initialize sample means,\nfollowed by the UCB1 algorithm of Auer, Cesa-Bianchi, and Fischer (2002). Under\nmild assumptions, we prove that our algorithm achieves a $p$-mean regret bound\nof $\\tilde{O}\\left(\\sqrt{\\frac{k}{T^{\\frac{1}{2|p|}}}}\\right)$ for all $p \\leq\n-1$, where $k$ represents the number of arms and $T$ the time horizon. When\n$-1<p<0$, we achieve a regret bound of\n$\\tilde{O}\\left(\\sqrt{\\frac{k^{1.5}}{T^{\\frac{1}{2}}}}\\right)$. For the range\n$0< p \\leq 1$, we achieve a $p$-mean regret scaling as\n$\\tilde{O}\\left(\\sqrt{\\frac{k}{T}}\\right)$, which matches the previously\nestablished lower bound up to logarithmic factors (Auer et al. 1995). This\nresult stems from the fact that the $p$-mean regret of any algorithm is at\nleast its average cumulative regret for $p \\leq 1$.\n  In the case of Nash regret (the limit as $p$ approaches zero), our unified\napproach differs from prior work (Barman et al. 2023), which requires a new\nNash Confidence Bound algorithm. Notably, we achieve the same regret bound up\nto constant factors using our more general method.",
          "arxiv_id": "2412.10751v1"
        }
      ],
      "14": [
        {
          "title": "Multi-Behavior Sequential Recommendation with Temporal Graph Transformer",
          "year": "2022-06",
          "abstract": "Modeling time-evolving preferences of users with their sequential item\ninteractions, has attracted increasing attention in many online applications.\nHence, sequential recommender systems have been developed to learn the dynamic\nuser interests from the historical interactions for suggesting items. However,\nthe interaction pattern encoding functions in most existing sequential\nrecommender systems have focused on single type of user-item interactions. In\nmany real-life online platforms, user-item interactive behaviors are often\nmulti-typed (e.g., click, add-to-favorite, purchase) with complex cross-type\nbehavior inter-dependencies. Learning from informative representations of users\nand items based on their multi-typed interaction data, is of great importance\nto accurately characterize the time-evolving user preference. In this work, we\ntackle the dynamic user-item relation learning with the awareness of\nmulti-behavior interactive patterns. Towards this end, we propose a new\nTemporal Graph Transformer (TGT) recommendation framework to jointly capture\ndynamic short-term and long-range user-item interactive patterns, by exploring\nthe evolving correlations across different types of behaviors. The new TGT\nmethod endows the sequential recommendation architecture to distill dedicated\nknowledge for type-specific behavior relational context and the implicit\nbehavior dependencies. Experiments on the real-world datasets indicate that our\nmethod TGT consistently outperforms various state-of-the-art recommendation\nmethods. Our model implementation codes are available at\nhttps://github.com/akaxlh/TGT.",
          "arxiv_id": "2206.02687v1"
        },
        {
          "title": "Graph Collaborative Signals Denoising and Augmentation for Recommendation",
          "year": "2023-04",
          "abstract": "Graph collaborative filtering (GCF) is a popular technique for capturing\nhigh-order collaborative signals in recommendation systems. However, GCF's\nbipartite adjacency matrix, which defines the neighbors being aggregated based\non user-item interactions, can be noisy for users/items with abundant\ninteractions and insufficient for users/items with scarce interactions.\nAdditionally, the adjacency matrix ignores user-user and item-item\ncorrelations, which can limit the scope of beneficial neighbors being\naggregated.\n  In this work, we propose a new graph adjacency matrix that incorporates\nuser-user and item-item correlations, as well as a properly designed user-item\ninteraction matrix that balances the number of interactions across all users.\nTo achieve this, we pre-train a graph-based recommendation method to obtain\nusers/items embeddings, and then enhance the user-item interaction matrix via\ntop-K sampling. We also augment the symmetric user-user and item-item\ncorrelation components to the adjacency matrix. Our experiments demonstrate\nthat the enhanced user-item interaction matrix with improved neighbors and\nlower density leads to significant benefits in graph-based recommendation.\nMoreover, we show that the inclusion of user-user and item-item correlations\ncan improve recommendations for users with both abundant and insufficient\ninteractions. The code is in \\url{https://github.com/zfan20/GraphDA}.",
          "arxiv_id": "2304.03344v2"
        },
        {
          "title": "RecXplainer: Amortized Attribute-based Personalized Explanations for Recommender Systems",
          "year": "2022-11",
          "abstract": "Recommender systems influence many of our interactions in the digital world\n-- impacting how we shop for clothes, sorting what we see when browsing YouTube\nor TikTok, and determining which restaurants and hotels we are shown when using\nhospitality platforms. Modern recommender systems are large, opaque models\ntrained on a mixture of proprietary and open-source datasets. Naturally, issues\nof trust arise on both the developer and user side: is the system working\ncorrectly, and why did a user receive (or not receive) a particular\nrecommendation? Providing an explanation alongside a recommendation alleviates\nsome of these concerns. The status quo for auxiliary recommender system\nfeedback is either user-specific explanations (e.g., \"users who bought item B\nalso bought item A\") or item-specific explanations (e.g., \"we are recommending\nitem A because you watched/bought item B\"). However, users bring personalized\ncontext into their search experience, valuing an item as a function of that\nitem's attributes and their own personal preferences. In this work, we propose\nRecXplainer, a novel method for generating fine-grained explanations based on a\nuser's preferences over the attributes of recommended items. We evaluate\nRecXplainer on five real-world and large-scale recommendation datasets using\nfive different kinds of recommender systems to demonstrate the efficacy of\nRecXplainer in capturing users' preferences over item attributes and using them\nto explain recommendations. We also compare RecXplainer to five baselines and\nshow RecXplainer's exceptional performance on ten metrics.",
          "arxiv_id": "2211.14935v2"
        }
      ],
      "15": [
        {
          "title": "A Ladder of Causal Distances",
          "year": "2020-05",
          "abstract": "Causal discovery, the task of automatically constructing a causal model from\ndata, is of major significance across the sciences. Evaluating the performance\nof causal discovery algorithms should ideally involve comparing the inferred\nmodels to ground-truth models available for benchmark datasets, which in turn\nrequires a notion of distance between causal models. While such distances have\nbeen proposed previously, they are limited by focusing on graphical properties\nof the causal models being compared. Here, we overcome this limitation by\ndefining distances derived from the causal distributions induced by the models,\nrather than exclusively from their graphical structure. Pearl and Mackenzie\n(2018) have arranged the properties of causal models in a hierarchy called the\n\"ladder of causation\" spanning three rungs: observational, interventional, and\ncounterfactual. Following this organization, we introduce a hierarchy of three\ndistances, one for each rung of the ladder. Our definitions are intuitively\nappealing as well as efficient to compute approximately. We put our causal\ndistances to use by benchmarking standard causal discovery systems on both\nsynthetic and real-world datasets for which ground-truth causal models are\navailable. Finally, we highlight the usefulness of our causal distances by\nbriefly discussing further applications beyond the evaluation of causal\ndiscovery techniques.",
          "arxiv_id": "2005.02480v2"
        },
        {
          "title": "Dynamic Causal Structure Discovery and Causal Effect Estimation",
          "year": "2025-01",
          "abstract": "To represent the causal relationships between variables, a directed acyclic\ngraph (DAG) is widely utilized in many areas, such as social sciences,\nepidemics, and genetics. Many causal structure learning approaches are\ndeveloped to learn the hidden causal structure utilizing deep-learning\napproaches. However, these approaches have a hidden assumption that the causal\nrelationship remains unchanged over time, which may not hold in real life. In\nthis paper, we develop a new framework to model the dynamic causal graph where\nthe causal relations are allowed to be time-varying. We incorporate the basis\napproximation method into the score-based causal discovery approach to capture\nthe dynamic pattern of the causal graphs. Utilizing the autoregressive model\nstructure, we could capture both contemporaneous and time-lagged causal\nrelationships while allowing them to vary with time. We propose an algorithm\nthat could provide both past-time estimates and future-time predictions on the\ncausal graphs, and conduct simulations to demonstrate the usefulness of the\nproposed method. We also apply the proposed method for the covid-data analysis,\nand provide causal estimates on how policy restriction's effect changes.",
          "arxiv_id": "2501.06534v1"
        },
        {
          "title": "Combining Interventional and Observational Data Using Causal Reductions",
          "year": "2021-03",
          "abstract": "Unobserved confounding is one of the main challenges when estimating causal\neffects. We propose a causal reduction method that, given a causal model,\nreplaces an arbitrary number of possibly high-dimensional latent confounders\nwith a single latent confounder that takes values in the same space as the\ntreatment variable, without changing the observational and interventional\ndistributions the causal model entails. This allows us to estimate the causal\neffect in a principled way from combined data without relying on the common but\noften unrealistic assumption that all confounders have been observed. We apply\nour causal reduction in three different settings. In the first setting, we\nassume the treatment and outcome to be discrete. The causal reduction then\nimplies bounds between the observational and interventional distributions that\ncan be exploited for estimation purposes. In certain cases with highly\nunbalanced observational samples, the accuracy of the causal effect estimate\ncan be improved by incorporating observational data. Second, for continuous\nvariables and assuming a linear-Gaussian model, we derive equality constraints\nfor the parameters of the observational and interventional distributions.\nThird, for the general continuous setting (possibly nonlinear and\nnon-Gaussian), we parameterize the reduced causal model using normalizing\nflows, a flexible class of easily invertible nonlinear transformations. We\nperform a series of experiments on synthetic data and find that in several\ncases the number of interventional samples can be reduced when adding\nobservational training samples without sacrificing accuracy.",
          "arxiv_id": "2103.04786v3"
        }
      ],
      "16": [
        {
          "title": "A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge",
          "year": "2024-02",
          "abstract": "In recent years, both online and offline deep learning models have been\ndeveloped for time series forecasting. However, offline deep forecasting models\nfail to adapt effectively to changes in time-series data, while online deep\nforecasting models are often expensive and have complex training procedures. In\nthis paper, we reframe the online nonlinear time-series forecasting problem as\none of linear hyperdimensional time-series forecasting. Nonlinear\nlow-dimensional time-series data is mapped to high-dimensional\n(hyperdimensional) spaces for linear hyperdimensional prediction, allowing\nfast, efficient and lightweight online time-series forecasting. Our framework,\nTSF-HD, adapts to time-series distribution shifts using a novel co-training\nframework for its hyperdimensional mapping and its linear hyperdimensional\npredictor. TSF-HD is shown to outperform the state of the art, while having\nreduced inference latency, for both short-term and long-term time series\nforecasting. Our code is publicly available at\nhttp://github.com/tsfhd2024/tsf-hd.git",
          "arxiv_id": "2402.01999v1"
        },
        {
          "title": "Times2D: Multi-Period Decomposition and Derivative Mapping for General Time Series Forecasting",
          "year": "2025-03",
          "abstract": "Time series forecasting is an important application in various domains such\nas energy management, traffic planning, financial markets, meteorology, and\nmedicine. However, real-time series data often present intricate temporal\nvariability and sharp fluctuations, which pose significant challenges for time\nseries forecasting. Previous models that rely on 1D time series representations\nusually struggle with complex temporal variations. To address the limitations\nof 1D time series, this study introduces the Times2D method that transforms the\n1D time series into 2D space. Times2D consists of three main parts: first, a\nPeriodic Decomposition Block (PDB) that captures temporal variations within a\nperiod and between the same periods by converting the time series into a 2D\ntensor in the frequency domain. Second, the First and Second Derivative\nHeatmaps (FSDH) capture sharp changes and turning points, respectively.\nFinally, an Aggregation Forecasting Block (AFB) integrates the output tensors\nfrom PDB and FSDH for accurate forecasting. This 2D transformation enables the\nutilization of 2D convolutional operations to effectively capture long and\nshort characteristics of the time series. Comprehensive experimental results\nacross large-scale data in the literature demonstrate that the proposed Times2D\nmodel achieves state-of-the-art performance in both short-term and long-term\nforecasting. The code is available in this repository:\nhttps://github.com/Tims2D/Times2D.",
          "arxiv_id": "2504.00118v1"
        },
        {
          "title": "Monitoring Time Series With Missing Values: a Deep Probabilistic Approach",
          "year": "2022-03",
          "abstract": "Systems are commonly monitored for health and security through collection and\nstreaming of multivariate time series. Advances in time series forecasting due\nto adoption of multilayer recurrent neural network architectures make it\npossible to forecast in high-dimensional time series, and identify and classify\nnovelties early, based on subtle changes in the trends. However, mainstream\napproaches to multi-variate time series predictions do not handle well cases\nwhen the ongoing forecast must include uncertainty, nor they are robust to\nmissing data. We introduce a new architecture for time series monitoring based\non combination of state-of-the-art methods of forecasting in high-dimensional\ntime series with full probabilistic handling of uncertainty. We demonstrate\nadvantage of the architecture for time series forecasting and novelty\ndetection, in particular with partially missing data, and empirically evaluate\nand compare the architecture to state-of-the-art approaches on a real-world\ndata set.",
          "arxiv_id": "2203.04916v1"
        }
      ],
      "17": [
        {
          "title": "Improved Alignment of Modalities in Large Vision Language Models",
          "year": "2025-03",
          "abstract": "Recent advancements in vision-language models have achieved remarkable\nresults in making language models understand vision inputs. However, a unified\napproach to align these models across diverse tasks such as image captioning\nand visual question answering remains a challenge. Existing methods either\nrequire very big language models or very big datasets which is not efficient in\nutilizing existing models. This paper addresses this gap and devises a training\nstrategy of auto-regressive vision-language models, to unify vision-language\ntasks like image-captioning and visual question answering. We propose four\ntraining stages for aligning the vision model with the language model, in other\nwords, the language model is given an ability to process visual inputs. We also\ndevise different attention masks for training transformer-based language models\nthat improve the quality of visual features. Further, we introduce some\nfindings, 1) the attention mask should not be applied on visual inputs, 2) the\nLanguage model converges faster on AI- generated data, 3) More work should be\ndone in the alignment stage during the pre-training of the model, 4) the model\ncan easily adapt to any downstream tasks like visual question answering on\nhealthcare datasets like PathVQA. After training the model for one epoch for\nall the stages, it outperforms large models like VILA-13 billion models on\ncommon benchmarks like CIDEr scores on COCO and Flickr30k datasets and achieves\nvery close scores to GIT-2 on the same dataset despite being a much smaller\nmodel trained on a much smaller dataset. All of the training is done using best\npractices available like multi- GPU parallel training, lower-precision training\nwith 16-bit float numbers, faster attention (SDPA), and gradient accumulation,\nand completed the training within 12 hours.",
          "arxiv_id": "2503.19508v1"
        },
        {
          "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
          "year": "2021-02",
          "abstract": "Pre-trained representations are becoming crucial for many NLP and perception\ntasks. While representation learning in NLP has transitioned to training on raw\ntext without human annotations, visual and vision-language representations\nstill rely heavily on curated training datasets that are expensive or require\nexpert knowledge. For vision applications, representations are mostly learned\nusing datasets with explicit class labels such as ImageNet or OpenImages. For\nvision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all\ninvolve a non-trivial data collection (and cleaning) process. This costly\ncuration process limits the size of datasets and hence hinders the scaling of\ntrained models. In this paper, we leverage a noisy dataset of over one billion\nimage alt-text pairs, obtained without expensive filtering or post-processing\nsteps in the Conceptual Captions dataset. A simple dual-encoder architecture\nlearns to align visual and language representations of the image and text pairs\nusing a contrastive loss. We show that the scale of our corpus can make up for\nits noise and leads to state-of-the-art representations even with such a simple\nlearning scheme. Our visual representation achieves strong performance when\ntransferred to classification tasks such as ImageNet and VTAB. The aligned\nvisual and language representations enables zero-shot image classification and\nalso set new state-of-the-art results on Flickr30K and MSCOCO image-text\nretrieval benchmarks, even when compared with more sophisticated\ncross-attention models. The representations also enable cross-modality search\nwith complex text and text + image queries.",
          "arxiv_id": "2102.05918v2"
        },
        {
          "title": "Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval",
          "year": "2024-12",
          "abstract": "Cross-modal (e.g. image-text, video-text) retrieval is an important task in\ninformation retrieval and multimodal vision-language understanding field.\nTemporal understanding makes video-text retrieval more challenging than\nimage-text retrieval. However, we find that the widely used video-text\nbenchmarks have shortcomings in comprehensively assessing abilities of models,\nespecially in temporal understanding, causing large-scale image-text\npre-trained models can already achieve comparable zero-shot performance with\nvideo-text pre-trained models. In this paper, we introduce RTime, a novel\ntemporal-emphasized video-text retrieval dataset. We first obtain videos of\nactions or events with significant temporality, and then reverse these videos\nto create harder negative samples. We then recruit annotators to judge the\nsignificance and reversibility of candidate videos, and write captions for\nqualified videos. We further adopt GPT-4 to extend more captions based on\nhuman-written captions. Our RTime dataset currently consists of 21k videos with\n10 captions per video, totalling about 122 hours. Based on RTime, we propose\nthree retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We\nfurther enhance the use of harder-negatives in model training, and benchmark a\nvariety of video-text models on RTime. Extensive experiment analysis proves\nthat RTime indeed poses new and higher challenges to video-text retrieval. We\nrelease our RTime\ndataset\\footnote{\\url{https://github.com/qyr0403/Reversed-in-Time}} to further\nadvance video-text retrieval and multimodal understanding research.",
          "arxiv_id": "2412.19178v1"
        }
      ],
      "18": [
        {
          "title": "Contrastive Credibility Propagation for Reliable Semi-Supervised Learning",
          "year": "2022-11",
          "abstract": "Producing labels for unlabeled data is error-prone, making semi-supervised\nlearning (SSL) troublesome. Often, little is known about when and why an\nalgorithm fails to outperform a supervised baseline. Using benchmark datasets,\nwe craft five common real-world SSL data scenarios: few-label, open-set,\nnoisy-label, and class distribution imbalance/misalignment in the labeled and\nunlabeled sets. We propose a novel algorithm called Contrastive Credibility\nPropagation (CCP) for deep SSL via iterative transductive pseudo-label\nrefinement. CCP unifies semi-supervised learning and noisy label learning for\nthe goal of reliably outperforming a supervised baseline in any data scenario.\nCompared to prior methods which focus on a subset of scenarios, CCP uniquely\noutperforms the supervised baseline in all scenarios, supporting practitioners\nwhen the qualities of labeled or unlabeled data are unknown.",
          "arxiv_id": "2211.09929v4"
        },
        {
          "title": "Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial Label Learning",
          "year": "2024-12",
          "abstract": "In partial label learning (PLL), every sample is associated with a candidate\nlabel set comprising the ground-truth label and several noisy labels. The\nconventional PLL assumes the noisy labels are randomly generated\n(instance-independent), while in practical scenarios, the noisy labels are\nalways instance-dependent and are highly related to the sample features,\nleading to the instance-dependent partial label learning (IDPLL) problem.\nInstance-dependent noisy label is a double-edged sword. On one side, it may\npromote model training as the noisy labels can depict the sample to some\nextent. On the other side, it brings high label ambiguity as the noisy labels\nare quite undistinguishable from the ground-truth label. To leverage the\nnuances of IDPLL effectively, for the first time we create class-wise\nembeddings for each sample, which allow us to explore the relationship of\ninstance-dependent noisy labels, i.e., the class-wise embeddings in the\ncandidate label set should have high similarity, while the class-wise\nembeddings between the candidate label set and the non-candidate label set\nshould have high dissimilarity. Moreover, to reduce the high label ambiguity,\nwe introduce the concept of class prototypes containing global feature\ninformation to disambiguate the candidate label set. Extensive experimental\ncomparisons with twelve methods on six benchmark data sets, including four\nfine-grained data sets, demonstrate the effectiveness of the proposed method.\nThe code implementation is publicly available at\nhttps://github.com/Yangfc-ML/CEL.",
          "arxiv_id": "2412.05029v1"
        },
        {
          "title": "Contrastive Learning Improves Model Robustness Under Label Noise",
          "year": "2021-04",
          "abstract": "Deep neural network-based classifiers trained with the categorical\ncross-entropy (CCE) loss are sensitive to label noise in the training data. One\ncommon type of method that can mitigate the impact of label noise can be viewed\nas supervised robust methods; one can simply replace the CCE loss with a loss\nthat is robust to label noise, or re-weight training samples and down-weight\nthose with higher loss values. Recently, another type of method using\nsemi-supervised learning (SSL) has been proposed, which augments these\nsupervised robust methods to exploit (possibly) noisy samples more effectively.\nAlthough supervised robust methods perform well across different data types,\nthey have been shown to be inferior to the SSL methods on image classification\ntasks under label noise. Therefore, it remains to be seen that whether these\nsupervised robust methods can also perform well if they can utilize the\nunlabeled samples more effectively. In this paper, we show that by initializing\nsupervised robust methods using representations learned through contrastive\nlearning leads to significantly improved performance under label noise.\nSurprisingly, even the simplest method (training a classifier with the CCE\nloss) can outperform the state-of-the-art SSL method by more than 50\\% under\nhigh label noise when initialized with contrastive learning. Our implementation\nwill be publicly available at\n{\\url{https://github.com/arghosh/noisy_label_pretrain}}.",
          "arxiv_id": "2104.08984v1"
        }
      ],
      "19": [
        {
          "title": "Machine Learning-Based Prediction of ICU Readmissions in Intracerebral Hemorrhage Patients: Insights from the MIMIC Databases",
          "year": "2025-01",
          "abstract": "Intracerebral hemorrhage (ICH) is a life-risking condition characterized by\nbleeding within the brain parenchyma. ICU readmission in ICH patients is a\ncritical outcome, reflecting both clinical severity and resource utilization.\nAccurate prediction of ICU readmission risk is crucial for guiding clinical\ndecision-making and optimizing healthcare resources. This study utilized the\nMedical Information Mart for Intensive Care (MIMIC-III and MIMIC-IV) databases,\nwhich contain comprehensive clinical and demographic data on ICU patients.\nPatients with ICH were identified from both databases. Various clinical,\nlaboratory, and demographic features were extracted for analysis based on both\noverview literature and experts' opinions. Preprocessing methods like imputing\nand sampling were applied to improve the performance of our models. Machine\nlearning techniques, such as Artificial Neural Network (ANN), XGBoost, and\nRandom Forest, were employed to develop predictive models for ICU readmission\nrisk. Model performance was evaluated using metrics such as AUROC, accuracy,\nsensitivity, and specificity. The developed models demonstrated robust\npredictive accuracy for ICU readmission in ICH patients, with key predictors\nincluding demographic information, clinical parameters, and laboratory\nmeasurements. Our study provides a predictive framework for ICU readmission\nrisk in ICH patients, which can aid in clinical decision-making and improve\nresource allocation in intensive care settings.",
          "arxiv_id": "2501.01183v1"
        },
        {
          "title": "A Study of Social and Behavioral Determinants of Health in Lung Cancer Patients Using Transformers-based Natural Language Processing Models",
          "year": "2021-08",
          "abstract": "Social and behavioral determinants of health (SBDoH) have important roles in\nshaping people's health. In clinical research studies, especially comparative\neffectiveness studies, failure to adjust for SBDoH factors will potentially\ncause confounding issues and misclassification errors in either statistical\nanalyses and machine learning-based models. However, there are limited studies\nto examine SBDoH factors in clinical outcomes due to the lack of structured\nSBDoH information in current electronic health record (EHR) systems, while much\nof the SBDoH information is documented in clinical narratives. Natural language\nprocessing (NLP) is thus the key technology to extract such information from\nunstructured clinical text. However, there is not a mature clinical NLP system\nfocusing on SBDoH. In this study, we examined two state-of-the-art\ntransformer-based NLP models, including BERT and RoBERTa, to extract SBDoH\nconcepts from clinical narratives, applied the best performing model to extract\nSBDoH concepts on a lung cancer screening patient cohort, and examined the\ndifference of SBDoH information between NLP extracted results and structured\nEHRs (SBDoH information captured in standard vocabularies such as the\nInternational Classification of Diseases codes). The experimental results show\nthat the BERT-based NLP model achieved the best strict/lenient F1-score of\n0.8791 and 0.8999, respectively. The comparison between NLP extracted SBDoH\ninformation and structured EHRs in the lung cancer patient cohort of 864\npatients with 161,933 various types of clinical notes showed that much more\ndetailed information about smoking, education, and employment were only\ncaptured in clinical narratives and that it is necessary to use both clinical\nnarratives and structured EHRs to construct a more complete picture of\npatients' SBDoH factors.",
          "arxiv_id": "2108.04949v1"
        },
        {
          "title": "Real-time Prediction of COVID-19 related Mortality using Electronic Health Records",
          "year": "2020-08",
          "abstract": "Coronavirus Disease 2019 (COVID-19) is an emerging respiratory disease caused\nby the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) with rapid\nhuman-to-human transmission and a high case fatality rate particularly in older\npatients. Due to the exponential growth of infections, many healthcare systems\nacross the world are under pressure to care for increasing amounts of at-risk\npatients. Given the high number of infected patients, identifying patients with\nthe highest mortality risk early is critical to enable effective intervention\nand optimal prioritisation of care. Here, we present the COVID-19 Early Warning\nSystem (CovEWS), a clinical risk scoring system for assessing COVID-19 related\nmortality risk. CovEWS provides continuous real-time risk scores for individual\npatients with clinically meaningful predictive performance up to 192 hours (8\ndays) in advance, and is automatically derived from patients' electronic health\nrecords (EHRs) using machine learning. We trained and evaluated CovEWS using\nde-identified data from a cohort of 66430 COVID-19 positive patients seen at\nover 69 healthcare institutions in the United States (US), Australia, Malaysia\nand India amounting to an aggregated total of over 2863 years of patient\nobservation time. On an external test cohort of 5005 patients, CovEWS predicts\nCOVID-19 related mortality from $78.8\\%$ ($95\\%$ confidence interval [CI]:\n$76.0$, $84.7\\%$) to $69.4\\%$ ($95\\%$ CI: $57.6, 75.2\\%$) specificity at a\nsensitivity greater than $95\\%$ between respectively 1 and 192 hours prior to\nobserved mortality events - significantly outperforming existing generic and\nCOVID-19 specific clinical risk scores. CovEWS could enable clinicians to\nintervene at an earlier stage, and may therefore help in preventing or\nmitigating COVID-19 related mortality.",
          "arxiv_id": "2008.13412v1"
        }
      ],
      "20": [
        {
          "title": "Topologically-Aware Deformation Fields for Single-View 3D Reconstruction",
          "year": "2022-05",
          "abstract": "We present a framework for learning 3D object shapes and dense cross-object\n3D correspondences from just an unaligned category-specific image collection.\nThe 3D shapes are generated implicitly as deformations to a category-specific\nsigned distance field and are learned in an unsupervised manner solely from\nunaligned image collections and their poses without any 3D supervision.\nGenerally, image collections on the internet contain several intra-category\ngeometric and topological variations, for example, different chairs can have\ndifferent topologies, which makes the task of joint shape and correspondence\nestimation much more challenging. Because of this, prior works either focus on\nlearning each 3D object shape individually without modeling cross-instance\ncorrespondences or perform joint shape and correspondence estimation on\ncategories with minimal intra-category topological variations. We overcome\nthese restrictions by learning a topologically-aware implicit deformation field\nthat maps a 3D point in the object space to a higher dimensional point in the\ncategory-specific canonical space. At inference time, given a single image, we\nreconstruct the underlying 3D shape by first implicitly deforming each 3D point\nin the object space to the learned category-specific canonical space using the\ntopologically-aware deformation field and then reconstructing the 3D shape as a\ncanonical signed distance field. Both canonical shape and deformation field are\nlearned end-to-end in an inverse-graphics fashion using a learned recurrent ray\nmarcher (SRN) as a differentiable rendering module. Our approach, dubbed TARS,\nachieves state-of-the-art reconstruction fidelity on several datasets:\nShapeNet, Pascal3D+, CUB, and Pix3D chairs. Result videos and code at\nhttps://shivamduggal4.github.io/tars-3D/",
          "arxiv_id": "2205.06267v2"
        },
        {
          "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
          "year": "2025-04",
          "abstract": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera movement is\neffectively canceled out. Within this stabilized 3D representation, TAPIP3D\niteratively refines multi-frame motion estimates, enabling robust point\ntracking over long time horizons. To handle the irregular structure of 3D point\ndistributions, we propose a 3D Neighborhood-to-Neighborhood (N2N) attention\nmechanism - a 3D-aware contextualization strategy that builds informative,\nspatially coherent feature neighborhoods to support precise trajectory\nestimation. Our 3D-centric formulation significantly improves performance over\nexisting 3D point tracking methods and even surpasses state-of-the-art 2D pixel\ntrackers in accuracy when reliable depth is available. The model supports\ninference in both camera-centric (unstabilized) and world-centric (stabilized)\ncoordinates, with experiments showing that compensating for camera motion leads\nto substantial gains in tracking robustness. By replacing the conventional 2D\nsquare correlation windows used in prior 2D and 3D trackers with a spatially\ngrounded 3D attention mechanism, TAPIP3D achieves strong and consistent results\nacross multiple 3D point tracking benchmarks. Project Page:\nhttps://tapip3d.github.io",
          "arxiv_id": "2504.14717v2"
        },
        {
          "title": "Neural Correspondence Field for Object Pose Estimation",
          "year": "2022-07",
          "abstract": "We propose a method for estimating the 6DoF pose of a rigid object with an\navailable 3D model from a single RGB image. Unlike classical\ncorrespondence-based methods which predict 3D object coordinates at pixels of\nthe input image, the proposed method predicts 3D object coordinates at 3D query\npoints sampled in the camera frustum. The move from pixels to 3D points, which\nis inspired by recent PIFu-style methods for 3D reconstruction, enables\nreasoning about the whole object, including its (self-)occluded parts. For a 3D\nquery point associated with a pixel-aligned image feature, we train a\nfully-connected neural network to predict: (i) the corresponding 3D object\ncoordinates, and (ii) the signed distance to the object surface, with the first\ndefined only for query points in the surface vicinity. We call the mapping\nrealized by this network as Neural Correspondence Field. The object pose is\nthen robustly estimated from the predicted 3D-3D correspondences by the\nKabsch-RANSAC algorithm. The proposed method achieves state-of-the-art results\non three BOP datasets and is shown superior especially in challenging cases\nwith occlusion. The project website is at: linhuang17.github.io/NCF.",
          "arxiv_id": "2208.00113v1"
        }
      ],
      "21": [
        {
          "title": "ConLUX: Concept-Based Local Unified Explanations",
          "year": "2024-10",
          "abstract": "With the rapid advancements of various machine learning models, there is a\nsignificant demand for model-agnostic explanation techniques, which can explain\nthese models across different architectures. Mainstream model-agnostic\nexplanation techniques generate local explanations based on basic features\n(e.g., words for text models and (super-)pixels for image models). However,\nthese explanations often do not align with the decision-making processes of the\ntarget models and end-users, resulting in explanations that are unfaithful and\ndifficult for users to understand. On the other hand, concept-based techniques\nprovide explanations based on high-level features (e.g., topics for text models\nand objects for image models), but most are model-specific or require\nadditional pre-defined external concept knowledge. To address this limitation,\nwe propose \\toolname, a general framework to provide concept-based local\nexplanations for any machine learning models. Our key insight is that we can\nautomatically extract high-level concepts from large pre-trained models, and\nuniformly extend existing local model-agnostic techniques to provide unified\nconcept-based explanations. We have instantiated \\toolname on four different\ntypes of explanation techniques: LIME, Kernel SHAP, Anchor, and LORE, and\napplied these techniques to text and image models. Our evaluation results\ndemonstrate that 1) compared to the vanilla versions, \\toolname offers more\nfaithful explanations and makes them more understandable to users, and 2) by\noffering multiple forms of explanations, \\toolname outperforms state-of-the-art\nconcept-based explanation techniques specifically designed for text and image\nmodels, respectively.",
          "arxiv_id": "2410.12439v1"
        },
        {
          "title": "Aligning Explanations with Human Communication",
          "year": "2025-05",
          "abstract": "Machine learning explainability aims to make the decision-making process of\nblack-box models more transparent by finding the most important input features\nfor a given prediction task. Recent works have proposed composing explanations\nfrom semantic concepts (e.g., colors, patterns, shapes) that are inherently\ninterpretable to the user of a model. However, these methods generally ignore\nthe communicative context of explanation-the ability of the user to understand\nthe prediction of the model from the explanation. For example, while a medical\ndoctor might understand an explanation in terms of clinical markers, a patient\nmay need a more accessible explanation to make sense of the same diagnosis. In\nthis paper, we address this gap with listener-adaptive explanations. We propose\nan iterative procedure grounded in principles of pragmatic reasoning and the\nrational speech act to generate explanations that maximize communicative\nutility. Our procedure only needs access to pairwise preferences between\ncandidate explanations, relevant in real-world scenarios where a listener model\nmay not be available. We evaluate our method in image classification tasks,\ndemonstrating improved alignment between explanations and listener preferences\nacross three datasets. Furthermore, we perform a user study that demonstrates\nour explanations increase communicative utility.",
          "arxiv_id": "2505.15626v1"
        },
        {
          "title": "Evaluating Robustness of Counterfactual Explanations",
          "year": "2021-03",
          "abstract": "Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system's behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions -- as changes to the input\n-- that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput -- i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n  In this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.",
          "arxiv_id": "2103.02354v3"
        }
      ],
      "22": [
        {
          "title": "Are You Misinformed? A Study of Covid-Related Fake News in Bengali on Facebook",
          "year": "2022-03",
          "abstract": "Our opinions and views of life can be shaped by how we perceive the opinions\nof others on social media like Facebook. This dependence has increased during\nCOVID-19 periods when we have fewer means to connect with others. However, fake\nnews related to COVID-19 has become a significant problem on Facebook. Bengali\nis the seventh most spoken language worldwide, yet we are aware of no previous\nresearch that studied the prevalence of COVID-19 related fake news in Bengali\non Facebook. In this paper, we develop machine learning models to detect fake\nnews in Bengali automatically. The best performing model is BERT, with an\nF1-score of 0.97. We apply BERT on all Facebook Bengali posts related to\nCOVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into\nthree categories: System (e.g., medical system), belief (e.g., religious\nrituals), and social (e.g., scientific awareness).",
          "arxiv_id": "2203.11669v1"
        },
        {
          "title": "Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking",
          "year": "2020-11",
          "abstract": "The rapid advancement of technology in online communication via social media\nplatforms has led to a prolific rise in the spread of misinformation and fake\nnews. Fake news is especially rampant in the current COVID-19 pandemic, leading\nto people believing in false and potentially harmful claims and stories.\nDetecting fake news quickly can alleviate the spread of panic, chaos and\npotential health hazards. We developed a two stage automated pipeline for\nCOVID-19 fake news detection using state of the art machine learning models for\nnatural language processing. The first model leverages a novel fact checking\nalgorithm that retrieves the most relevant facts concerning user claims about\nparticular COVID-19 claims. The second model verifies the level of truth in the\nclaim by computing the textual entailment between the claim and the true facts\nretrieved from a manually curated COVID-19 dataset. The dataset is based on a\npublicly available knowledge source consisting of more than 5000 COVID-19 false\nclaims and verified explanations, a subset of which was internally annotated\nand cross-validated to train and evaluate our models. We evaluate a series of\nmodels based on classical text-based features to more contextual Transformer\nbased models and observe that a model pipeline based on BERT and ALBERT for the\ntwo stages respectively yields the best results.",
          "arxiv_id": "2011.13253v1"
        },
        {
          "title": "Modeling Political Orientation of Social Media Posts: An Extended Analysis",
          "year": "2023-11",
          "abstract": "Developing machine learning models to characterize political polarization on\nonline social media presents significant challenges. These challenges mainly\nstem from various factors such as the lack of annotated data, presence of noise\nin social media datasets, and the sheer volume of data. The common research\npractice typically examines the biased structure of online user communities for\na given topic or qualitatively measuring the impacts of polarized topics on\nsocial media. However, there is limited work focusing on analyzing polarization\nat the ground-level, specifically in the social media posts themselves. Such\nexisting analysis heavily relies on annotated data, which often requires\nlaborious human labeling, offers labels only to specific problems, and lacks\nthe ability to determine the near-future bias state of a social media\nconversations. Understanding the degree of political orientation conveyed in\nsocial media posts is crucial for quantifying the bias of online user\ncommunities and investigating the spread of polarized content. In this work, we\nfirst introduce two heuristic methods that leverage on news media bias and post\ncontent to label social media posts. Next, we compare the efficacy and quality\nof heuristically labeled dataset with a randomly sampled human-annotated\ndataset. Additionally, we demonstrate that current machine learning models can\nexhibit improved performance in predicting political orientation of social\nmedia posts, employing both traditional supervised learning and few-shot\nlearning setups. We conduct experiments using the proposed heuristic methods\nand machine learning approaches to predict the political orientation of posts\ncollected from two social media forums with diverse political ideologies: Gab\nand Twitter.",
          "arxiv_id": "2311.12323v1"
        }
      ],
      "23": [
        {
          "title": "OmniFair: A Declarative System for Model-Agnostic Group Fairness in Machine Learning",
          "year": "2021-03",
          "abstract": "Machine learning (ML) is increasingly being used to make decisions in our\nsociety. ML models, however, can be unfair to certain demographic groups (e.g.,\nAfrican Americans or females) according to various fairness metrics. Existing\ntechniques for producing fair ML models either are limited to the type of\nfairness constraints they can handle (e.g., preprocessing) or require\nnontrivial modifications to downstream ML training algorithms (e.g.,\nin-processing).\n  We propose a declarative system OmniFair for supporting group fairness in ML.\nOmniFair features a declarative interface for users to specify desired group\nfairness constraints and supports all commonly used group fairness notions,\nincluding statistical parity, equalized odds, and predictive parity. OmniFair\nis also model-agnostic in the sense that it does not require modifications to a\nchosen ML algorithm. OmniFair also supports enforcing multiple user declared\nfairness constraints simultaneously while most previous techniques cannot. The\nalgorithms in OmniFair maximize model accuracy while meeting the specified\nfairness constraints, and their efficiency is optimized based on the\ntheoretically provable monotonicity property regarding the trade-off between\naccuracy and fairness that is unique to our system.\n  We conduct experiments on commonly used datasets that exhibit bias against\nminority groups in the fairness literature. We show that OmniFair is more\nversatile than existing algorithmic fairness approaches in terms of both\nsupported fairness constraints and downstream ML models. OmniFair reduces the\naccuracy loss by up to $94.8\\%$ compared with the second best method. OmniFair\nalso achieves similar running time to preprocessing methods, and is up to\n$270\\times$ faster than in-processing methods.",
          "arxiv_id": "2103.09055v1"
        },
        {
          "title": "Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness",
          "year": "2023-10",
          "abstract": "Counterfactual fairness requires that a person would have been classified in\nthe same way by an AI or other algorithmic system if they had a different\nprotected class, such as a different race or gender. This is an intuitive\nstandard, as reflected in the U.S. legal system, but its use is limited because\ncounterfactuals cannot be directly observed in real-world data. On the other\nhand, group fairness metrics (e.g., demographic parity or equalized odds) are\nless intuitive but more readily observed. In this paper, we use $\\textit{causal\ncontext}$ to bridge the gaps between counterfactual fairness, robust\nprediction, and group fairness. First, we motivate counterfactual fairness by\nshowing that there is not necessarily a fundamental trade-off between fairness\nand accuracy because, under plausible conditions, the counterfactually fair\npredictor is in fact accuracy-optimal in an unbiased target distribution.\nSecond, we develop a correspondence between the causal graph of the\ndata-generating process and which, if any, group fairness metrics are\nequivalent to counterfactual fairness. Third, we show that in three common\nfairness contexts$\\unicode{x2013}$measurement error, selection on label, and\nselection on predictors$\\unicode{x2013}$counterfactual fairness is equivalent\nto demographic parity, equalized odds, and calibration, respectively.\nCounterfactual fairness can sometimes be tested by measuring relatively simple\ngroup fairness metrics.",
          "arxiv_id": "2310.19691v1"
        },
        {
          "title": "Accurate Fairness: Improving Individual Fairness without Trading Accuracy",
          "year": "2022-05",
          "abstract": "Accuracy and individual fairness are both crucial for trustworthy machine\nlearning, but these two aspects are often incompatible with each other so that\nenhancing one aspect may sacrifice the other inevitably with side effects of\ntrue bias or false fairness. We propose in this paper a new fairness criterion,\naccurate fairness, to align individual fairness with accuracy. Informally, it\nrequires the treatments of an individual and the individual's similar\ncounterparts to conform to a uniform target, i.e., the ground truth of the\nindividual. We prove that accurate fairness also implies typical group fairness\ncriteria over a union of similar sub-populations. We then present a Siamese\nfairness in-processing approach to minimize the accuracy and fairness losses of\na machine learning model under the accurate fairness constraints. To the best\nof our knowledge, this is the first time that a Siamese approach is adapted for\nbias mitigation. We also propose fairness confusion matrix-based metrics,\nfair-precision, fair-recall, and fair-F1 score, to quantify a trade-off between\naccuracy and individual fairness. Comparative case studies with popular\nfairness datasets show that our Siamese fairness approach can achieve on\naverage 1.02%-8.78% higher individual fairness (in terms of fairness through\nawareness) and 8.38%-13.69% higher accuracy, as well as 10.09%-20.57% higher\ntrue fair rate, and 5.43%-10.01% higher fair-F1 score, than the\nstate-of-the-art bias mitigation techniques. This demonstrates that our Siamese\nfairness approach can indeed improve individual fairness without trading\naccuracy. Finally, the accurate fairness criterion and Siamese fairness\napproach are applied to mitigate the possible service discrimination with a\nreal Ctrip dataset, by on average fairly serving 112.33% more customers\n(specifically, 81.29% more customers in an accurately fair way) than baseline\nmodels.",
          "arxiv_id": "2205.08704v2"
        }
      ],
      "24": [
        {
          "title": "Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits",
          "year": "2023-08",
          "abstract": "With the growing prevalence of electric vehicles (EVs) and advancements in EV\nelectronics, vehicle-to-grid (V2G) techniques and large-scale scheduling\nstrategies have emerged to promote renewable energy utilization and power grid\nstability. This study proposes a multi-stakeholder hierarchical V2G\ncoordination based on deep reinforcement learning (DRL) and the Proof of Stake\nalgorithm. Furthermore, the multi-stakeholders include the power grid, EV\naggregators (EVAs), and users, and the proposed strategy can achieve\nmulti-stakeholder benefits. On the grid side, load fluctuations and renewable\nenergy consumption are considered, while on the EVA side, energy constraints\nand charging costs are considered. The three critical battery conditioning\nparameters of battery SOX are considered on the user side, including state of\ncharge, state of power, and state of health. Compared with four typical\nbaselines, the multi-stakeholder hierarchical coordination strategy can enhance\nrenewable energy consumption, mitigate load fluctuations, meet the energy\ndemands of EVA, and reduce charging costs and battery degradation under\nrealistic operating conditions.",
          "arxiv_id": "2308.00218v1"
        },
        {
          "title": "Benchmarks and Custom Package for Energy Forecasting",
          "year": "2023-07",
          "abstract": "Energy (load, wind, photovoltaic) forecasting is significant in the power\nindustry as it can provide a reference for subsequent tasks such as power grid\ndispatch, thus bringing huge economic benefits. However, there are many\ndifferences between energy forecasting and traditional time series forecasting.\nOn the one hand, traditional time series mainly focus on capturing\ncharacteristics like trends and cycles. In contrast, the energy series is\nlargely influenced by many external factors, such as meteorological and\ncalendar variables. On the other hand, energy forecasting aims to minimize the\ncost of subsequent tasks such as power grid dispatch, rather than simply\npursuing prediction accuracy. In addition, the scale of energy data can also\nsignificantly impact the predicted results. In this paper, we collected\nlarge-scale load datasets and released a new renewable energy dataset that\ncontains both station-level and region-level renewable generation data with\nmeteorological data. For load data, we also included load domain-specific\nfeature engineering and provided a method to customize the loss function and\nlink the forecasting error to requirements related to subsequent tasks (such as\npower grid dispatching costs), integrating it into our forecasting framework.\nBased on such a situation, we conducted extensive experiments with 21\nforecasting methods in these energy datasets at different levels under 11\nevaluation metrics, providing a comprehensive reference for researchers to\ncompare different energy forecasting models.",
          "arxiv_id": "2307.07191v2"
        },
        {
          "title": "Predicting Short Term Energy Demand in Smart Grid: A Deep Learning Approach for Integrating Renewable Energy Sources in Line with SDGs 7, 9, and 13",
          "year": "2023-04",
          "abstract": "Integrating renewable energy sources into the power grid is becoming\nincreasingly important as the world moves towards a more sustainable energy\nfuture in line with SDG 7. However, the intermittent nature of renewable energy\nsources can make it challenging to manage the power grid and ensure a stable\nsupply of electricity, which is crucial for achieving SDG 9. In this paper, we\npropose a deep learning model for predicting energy demand in a smart power\ngrid, which can improve the integration of renewable energy sources by\nproviding accurate predictions of energy demand. Our approach aligns with SDG\n13 on climate action, enabling more efficient management of renewable energy\nresources. We use long short-term memory networks, well-suited for time series\ndata, to capture complex patterns and dependencies in energy demand data. The\nproposed approach is evaluated using four historical short-term energy demand\ndata datasets from different energy distribution companies, including American\nElectric Power, Commonwealth Edison, Dayton Power and Light, and\nPennsylvania-New Jersey-Maryland Interconnection. The proposed model is\ncompared with three other state-of-the-art forecasting algorithms: Facebook\nProphet, Support Vector Regression, and Random Forest Regression. The\nexperimental results show that the proposed REDf model can accurately predict\nenergy demand with a mean absolute error of 1.4%, indicating its potential to\nenhance the stability and efficiency of the power grid and contribute to\nachieving SDGs 7, 9, and 13. The proposed model also has the potential to\nmanage the integration of renewable energy sources effectively.",
          "arxiv_id": "2304.03997v4"
        }
      ],
      "25": [
        {
          "title": "Space Meets Time: Local Spacetime Neural Network For Traffic Flow Forecasting",
          "year": "2021-09",
          "abstract": "Traffic flow forecasting is a crucial task in urban computing. The challenge\narises as traffic flows often exhibit intrinsic and latent spatio-temporal\ncorrelations that cannot be identified by extracting the spatial and temporal\npatterns of traffic data separately. We argue that such correlations are\nuniversal and play a pivotal role in traffic flow. We put forward {spacetime\ninterval learning} as a paradigm to explicitly capture these correlations\nthrough a unified analysis of both spatial and temporal features. Unlike the\nstate-of-the-art methods, which are restricted to a particular road network, we\nmodel the universal spatio-temporal correlations that are transferable from\ncities to cities. To this end, we propose a new spacetime interval learning\nframework that constructs a local-spacetime context of a traffic sensor\ncomprising the data from its neighbors within close time points. Based on this\nidea, we introduce local spacetime neural network (STNN), which employs novel\nspacetime convolution and attention mechanism to learn the universal\nspatio-temporal correlations. The proposed STNN captures local traffic\npatterns, which does not depend on a specific network structure. As a result, a\ntrained STNN model can be applied on any unseen traffic networks. We evaluate\nthe proposed STNN on two public real-world traffic datasets and a simulated\ndataset on dynamic networks. The experiment results show that STNN not only\nimproves prediction accuracy by 4% over state-of-the-art methods, but is also\neffective in handling the case when the traffic network undergoes dynamic\nchanges as well as the superior generalization capability.",
          "arxiv_id": "2109.05225v2"
        },
        {
          "title": "A Graph and Attentive Multi-Path Convolutional Network for Traffic Prediction",
          "year": "2022-05",
          "abstract": "Traffic prediction is an important and yet highly challenging problem due to\nthe complexity and constantly changing nature of traffic systems. To address\nthe challenges, we propose a graph and attentive multi-path convolutional\nnetwork (GAMCN) model to predict traffic conditions such as traffic speed\nacross a given road network into the future. Our model focuses on the spatial\nand temporal factors that impact traffic conditions. To model the spatial\nfactors, we propose a variant of the graph convolutional network (GCN) named\nLPGCN to embed road network graph vertices into a latent space, where vertices\nwith correlated traffic conditions are close to each other. To model the\ntemporal factors, we use a multi-path convolutional neural network (CNN) to\nlearn the joint impact of different combinations of past traffic conditions on\nthe future traffic conditions. Such a joint impact is further modulated by an\nattention} generated from an embedding of the prediction time, which encodes\nthe periodic patterns of traffic conditions. We evaluate our model on\nreal-world road networks and traffic data. The experimental results show that\nour model outperforms state-of-art traffic prediction models by up to 18.9% in\nterms of prediction errors and 23.4% in terms of prediction efficiency.",
          "arxiv_id": "2205.15218v1"
        },
        {
          "title": "Fusion Matrix Prompt Enhanced Self-Attention Spatial-Temporal Interactive Traffic Forecasting Framework",
          "year": "2024-10",
          "abstract": "Recently, spatial-temporal forecasting technology has been rapidly developed\ndue to the increasing demand for traffic management and travel planning.\nHowever, existing traffic forecasting models still face the following\nlimitations. On one hand, most previous studies either focus too much on\nreal-world geographic information, neglecting the potential traffic correlation\nbetween different regions, or overlook geographical position and only model the\ntraffic flow relationship. On the other hand, the importance of different time\nslices is ignored in time modeling. Therefore, we propose a Fusion Matrix\nPrompt Enhanced Self-Attention Spatial-Temporal Interactive Traffic Forecasting\nFramework (FMPESTF), which is composed of spatial and temporal modules for\ndown-sampling traffic data. The network is designed to establish a traffic\nfusion matrix considering spatial-temporal heterogeneity as a query to\nreconstruct a data-driven dynamic traffic data structure, which accurately\nreveal the flow relationship of nodes in the traffic network. In addition, we\nintroduce attention mechanism in time modeling, and design hierarchical\nspatial-temporal interactive learning to help the model adapt to various\ntraffic scenarios. Through extensive experimental on six real-world traffic\ndatasets, our method is significantly superior to other baseline models,\ndemonstrating its efficiency and accuracy in dealing with traffic forecasting\nproblems.",
          "arxiv_id": "2410.09356v1"
        }
      ],
      "26": [
        {
          "title": "The Nondecreasing Rank",
          "year": "2025-08",
          "abstract": "In this article the notion of the nondecreasing (ND) rank of a matrix or\ntensor is introduced. A tensor has an ND rank of r if it can be represented as\na sum of r outer products of vectors, with each vector satisfying a\nmonotonicity constraint. It is shown that for certain poset orderings finding\nan ND factorization of rank $r$ is equivalent to finding a nonnegative rank-r\nfactorization of a transformed tensor. However, not every tensor that is\nmonotonic has a finite ND rank. Theory is developed describing the properties\nof the ND rank, including typical, maximum, and border ND ranks. Highlighted\nalso are the special settings where a matrix or tensor has an ND rank of one or\ntwo. As a means of finding low ND rank approximations to a data tensor we\nintroduce a variant of the hierarchical alternating least squares algorithm.\nLow ND rank factorizations are found and interpreted for two datasets\nconcerning the weight of pigs and a mental health survey during the COVID-19\npandemic.",
          "arxiv_id": "2509.00265v1"
        },
        {
          "title": "Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay",
          "year": "2022-06",
          "abstract": "We study the tensor-on-tensor regression, where the goal is to connect tensor\nresponses to tensor covariates with a low Tucker rank parameter tensor/matrix\nwithout the prior knowledge of its intrinsic rank. We propose the Riemannian\ngradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with\nthe challenge of unknown rank by studying the effect of rank\nover-parameterization. We provide the first convergence guarantee for the\ngeneral tensor-on-tensor regression by showing that RGD and RGN respectively\nconverge linearly and quadratically to a statistically optimal estimate in both\nrank correctly-parameterized and over-parameterized settings. Our theory\nreveals an intriguing phenomenon: Riemannian optimization methods naturally\nadapt to over-parameterization without modifications to their implementation.\nWe also prove the statistical-computational gap in scalar-on-tensor regression\nby a direct low-degree polynomial argument. Our theory demonstrates a \"blessing\nof statistical-computational gap\" phenomenon: in a wide range of scenarios in\ntensor-on-tensor regression for tensors of order three or higher, the\ncomputationally required sample size matches what is needed by moderate rank\nover-parameterization when considering computationally feasible estimators,\nwhile there are no such benefits in the matrix settings. This shows moderate\nrank over-parameterization is essentially \"cost-free\" in terms of sample size\nin tensor-on-tensor regression of order three or higher. Finally, we conduct\nsimulation studies to show the advantages of our proposed methods and to\ncorroborate our theoretical findings.",
          "arxiv_id": "2206.08756v3"
        },
        {
          "title": "Robust Tensor Principal Component Analysis: Exact Recovery via Deterministic Model",
          "year": "2020-08",
          "abstract": "Tensor, also known as multi-dimensional array, arises from many applications\nin signal processing, manufacturing processes, healthcare, among others. As one\nof the most popular methods in tensor literature, Robust tensor principal\ncomponent analysis (RTPCA) is a very effective tool to extract the low rank and\nsparse components in tensors. In this paper, a new method to analyze RTPCA is\nproposed based on the recently developed tensor-tensor product and tensor\nsingular value decomposition (t-SVD). Specifically, it aims to solve a convex\noptimization problem whose objective function is a weighted combination of the\ntensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact\nrecovery is built on the tensor incoherence conditions and the assumption of a\nuniform model on the sparse support. Unlike this conventional way, in this\npaper, without any assumption of randomness, the exact recovery can be achieved\nin a completely deterministic fashion by characterizing the tensor\nrank-sparsity incoherence, which is an uncertainty principle between the\nlow-rank tensor spaces and the pattern of sparse tensor.",
          "arxiv_id": "2008.02211v1"
        }
      ],
      "27": [
        {
          "title": "From Imitation to Exploration: End-to-end Autonomous Driving based on World Model",
          "year": "2024-10",
          "abstract": "In recent years, end-to-end autonomous driving architectures have gained\nincreasing attention due to their advantage in avoiding error accumulation.\nMost existing end-to-end autonomous driving methods are based on Imitation\nLearning (IL), which can quickly derive driving strategies by mimicking expert\nbehaviors. However, IL often struggles to handle scenarios outside the training\ndataset, especially in high-dynamic and interaction-intensive traffic\nenvironments. In contrast, Reinforcement Learning (RL)-based driving models can\noptimize driving decisions through interaction with the environment, improving\nadaptability and robustness.\n  To leverage the strengths of both IL and RL, we propose RAMBLE, an end-to-end\nworld model-based RL method for driving decision-making. RAMBLE extracts\nenvironmental context information from RGB images and LiDAR data through an\nasymmetrical variational autoencoder. A transformer-based architecture is then\nused to capture the dynamic transitions of traffic participants. Next, an\nactor-critic structure reinforcement learning algorithm is applied to derive\ndriving strategies based on the latent features of the current state and\ndynamics. To accelerate policy convergence and ensure stable training, we\nintroduce a training scheme that initializes the policy network using IL, and\nemploys KL loss and soft update mechanisms to smoothly transition the model\nfrom IL to RL.\n  RAMBLE achieves state-of-the-art performance in route completion rate on the\nCARLA Leaderboard 1.0 and completes all 38 scenarios on the CARLA Leaderboard\n2.0, demonstrating its effectiveness in handling complex and dynamic traffic\nscenarios. The model will be open-sourced upon paper acceptance at\nhttps://github.com/SCP-CN-001/ramble to support further research and\ndevelopment in autonomous driving.",
          "arxiv_id": "2410.02253v2"
        },
        {
          "title": "DISC: Dataset for Analyzing Driving Styles In Simulated Crashes for Mixed Autonomy",
          "year": "2025-01",
          "abstract": "Handling pre-crash scenarios is still a major challenge for self-driving cars\ndue to limited practical data and human-driving behavior datasets. We introduce\nDISC (Driving Styles In Simulated Crashes), one of the first datasets designed\nto capture various driving styles and behaviors in pre-crash scenarios for\nmixed autonomy analysis. DISC includes over 8 classes of driving\nstyles/behaviors from hundreds of drivers navigating a simulated vehicle\nthrough a virtual city, encountering rare-event traffic scenarios. This dataset\nenables the classification of pre-crash human driving behaviors in unsafe\nconditions, supporting individualized trajectory prediction based on observed\ndriving patterns. By utilizing a custom-designed VR-based in-house driving\nsimulator, TRAVERSE, data was collected through a driver-centric study\ninvolving human drivers encountering twelve simulated accident scenarios. This\ndataset fills a critical gap in human-centric driving data for rare events\ninvolving interactions with autonomous vehicles. It enables autonomous systems\nto better react to human drivers and optimize trajectory prediction in mixed\nautonomy environments involving both human-driven and self-driving cars. In\naddition, individual driving behaviors are classified through a set of\nstandardized questionnaires, carefully designed to identify and categorize\ndriving behavior traits. We correlate data features with driving behaviors,\nshowing that the simulated environment reflects real-world driving styles. DISC\nis the first dataset to capture how various driving styles respond to accident\nscenarios, offering significant potential to enhance autonomous vehicle safety\nand driving behavior analysis in mixed autonomy environments.",
          "arxiv_id": "2502.00050v1"
        },
        {
          "title": "Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving",
          "year": "2024-02",
          "abstract": "In recent years, the expansion of internet technology and advancements in\nautomation have brought significant attention to autonomous driving technology.\nMajor automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have\nprogressively introduced products ranging from assisted-driving vehicles to\nsemi-autonomous vehicles. However, this period has also witnessed several\ntraffic safety incidents involving self-driving vehicles. For instance, in\nMarch 2016, a Google self-driving car was involved in a minor collision with a\nbus. At the time of the accident, the autonomous vehicle was attempting to\nmerge into the right lane but failed to dynamically respond to the real-time\nenvironmental information during the lane change. It incorrectly assumed that\nthe approaching bus would slow down to avoid it, leading to a low-speed\ncollision with the bus. This incident highlights the current technological\nshortcomings and safety concerns associated with autonomous lane-changing\nbehavior, despite the rapid advancements in autonomous driving technology.\nLane-changing is among the most common and hazardous behaviors in highway\ndriving, significantly impacting traffic safety and flow. Therefore,\nlane-changing is crucial for traffic safety, and accurately predicting drivers'\nlane change intentions can markedly enhance driving safety. This paper\nintroduces a deep learning-based prediction method for autonomous driving lane\nchange behavior, aiming to facilitate safe lane changes and thereby improve\nroad safety.",
          "arxiv_id": "2402.16036v1"
        }
      ],
      "28": [
        {
          "title": "Multiband VAE: Latent Space Alignment for Knowledge Consolidation in Continual Learning",
          "year": "2021-06",
          "abstract": "We propose a new method for unsupervised generative continual learning\nthrough realignment of Variational Autoencoder's latent space. Deep generative\nmodels suffer from catastrophic forgetting in the same way as other neural\nstructures. Recent generative continual learning works approach this problem\nand try to learn from new data without forgetting previous knowledge. However,\nthose methods usually focus on artificial scenarios where examples share almost\nno similarity between subsequent portions of data - an assumption not realistic\nin the real-life applications of continual learning. In this work, we identify\nthis limitation and posit the goal of generative continual learning as a\nknowledge accumulation task. We solve it by continuously aligning latent\nrepresentations of new data that we call bands in additional latent space where\nexamples are encoded independently of their source task. In addition, we\nintroduce a method for controlled forgetting of past data that simplifies this\nprocess. On top of the standard continual learning benchmarks, we propose a\nnovel challenging knowledge consolidation scenario and show that the proposed\napproach outperforms state-of-the-art by up to twofold across all experiments\nand the additional real-life evaluation. To our knowledge, Multiband VAE is the\nfirst method to show forward and backward knowledge transfer in generative\ncontinual learning.",
          "arxiv_id": "2106.12196v2"
        },
        {
          "title": "Saliency-Augmented Memory Completion for Continual Learning",
          "year": "2022-12",
          "abstract": "Continual Learning is considered a key step toward next-generation Artificial\nIntelligence. Among various methods, replay-based approaches that maintain and\nreplay a small episodic memory of previous samples are one of the most\nsuccessful strategies against catastrophic forgetting. However, since\nforgetting is inevitable given bounded memory and unbounded tasks, how to\nforget is a problem continual learning must address. Therefore, beyond simply\navoiding catastrophic forgetting, an under-explored issue is how to reasonably\nforget while ensuring the merits of human memory, including 1. storage\nefficiency, 2. generalizability, and 3. some interpretability. To achieve these\nsimultaneously, our paper proposes a new saliency-augmented memory completion\nframework for continual learning, inspired by recent discoveries in memory\ncompletion separation in cognitive neuroscience. Specifically, we innovatively\npropose to store the part of the image most important to the tasks in episodic\nmemory by saliency map extraction and memory encoding. When learning new tasks,\nprevious data from memory are inpainted by an adaptive data generation module,\nwhich is inspired by how humans complete episodic memory. The module's\nparameters are shared across all tasks and it can be jointly trained with a\ncontinual learning classifier as bilevel optimization. Extensive experiments on\nseveral continual learning and image classification benchmarks demonstrate the\nproposed method's effectiveness and efficiency.",
          "arxiv_id": "2212.13242v1"
        },
        {
          "title": "Metalearning Continual Learning Algorithms",
          "year": "2023-12",
          "abstract": "General-purpose learning systems should improve themselves in open-ended\nfashion in ever-changing environments. Conventional learning algorithms for\nneural networks, however, suffer from catastrophic forgetting (CF), i.e.,\npreviously acquired skills are forgotten when a new task is learned. Instead of\nhand-crafting new algorithms for avoiding CF, we propose Automated Continual\nLearning (ACL) to train self-referential neural networks to metalearn their own\nin-context continual (meta)learning algorithms. ACL encodes continual learning\n(CL) desiderata -- good performance on both old and new tasks -- into its\nmetalearning objectives. Our experiments demonstrate that ACL effectively\nresolves \"in-context catastrophic forgetting,\" a problem that naive in-context\nlearning algorithms suffer from; ACL-learned algorithms outperform both\nhand-crafted learning algorithms and popular meta-continual learning methods on\nthe Split-MNIST benchmark in the replay-free setting, and enables continual\nlearning of diverse tasks consisting of multiple standard image classification\ndatasets. We also discuss the current limitations of in-context CL by comparing\nACL with state-of-the-art CL methods that leverage pre-trained models. Overall,\nwe bring several novel perspectives into the long-standing problem of CL.",
          "arxiv_id": "2312.00276v3"
        }
      ],
      "29": [
        {
          "title": "Do Weibo platform experts perform better at predicting stock market?",
          "year": "2024-02",
          "abstract": "Sentiment analysis can be used for stock market prediction. However, existing\nresearch has not studied the impact of a user's financial background on\nsentiment-based forecasting of the stock market using artificial neural\nnetworks. In this work, a novel combination of neural networks is used for the\nassessment of sentiment-based stock market prediction, based on the financial\nbackground of the population that generated the sentiment. The state-of-the-art\nlanguage processing model Bidirectional Encoder Representations from\nTransformers (BERT) is used to classify the sentiment and a Long-Short Term\nMemory (LSTM) model is used for time-series based stock market prediction. For\nevaluation, the Weibo social networking platform is used as a sentiment data\ncollection source. Weibo users (and their comments respectively) are divided\ninto Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor\n(UFA) groups according to their background information, as collected by Weibo.\nThe Hong Kong Hang Seng index is used to extract historical stock market change\ndata. The results indicate that stock market prediction learned from the AFA\ngroup users is 39.67% more precise than that learned from the UFA group users\nand shows the highest accuracy (87%) when compared to existing approaches.",
          "arxiv_id": "2403.00772v1"
        },
        {
          "title": "Identifying Trades Using Technical Analysis and ML/DL Models",
          "year": "2023-04",
          "abstract": "The importance of predicting stock market prices cannot be overstated. It is\na pivotal task for investors and financial institutions as it enables them to\nmake informed investment decisions, manage risks, and ensure the stability of\nthe financial system. Accurate stock market predictions can help investors\nmaximize their returns and minimize their losses, while financial institutions\ncan use this information to develop effective risk management policies.\nHowever, stock market prediction is a challenging task due to the complex\nnature of the stock market and the multitude of factors that can affect stock\nprices. As a result, advanced technologies such as deep learning are being\nincreasingly utilized to analyze vast amounts of data and provide valuable\ninsights into the behavior of the stock market. While deep learning has shown\npromise in accurately predicting stock prices, there is still much research to\nbe done in this area.",
          "arxiv_id": "2304.09936v1"
        },
        {
          "title": "Learning Universal Multi-level Market Irrationality Factors to Improve Stock Return Forecasting",
          "year": "2025-02",
          "abstract": "Recent years have witnessed the perfect encounter of deep learning and\nquantitative trading has achieved great success in stock investment. Numerous\ndeep learning-based models have been developed for forecasting stock returns,\nleveraging the powerful representation capabilities of neural networks to\nidentify patterns and factors influencing stock prices. These models can\neffectively capture general patterns in the market, such as stock price trends,\nvolume-price relationships, and time variations. However, the impact of special\nirrationality factors -- such as market sentiment, speculative behavior, market\nmanipulation, and psychological biases -- have not been fully considered in\nexisting deep stock forecasting models due to their relative abstraction as\nwell as lack of explicit labels and data description. To fill this gap, we\npropose UMI, a Universal multi-level Market Irrationality factor model to\nenhance stock return forecasting. The UMI model learns factors that can reflect\nirrational behaviors in market from both individual stock and overall market\nlevels. For the stock-level, UMI construct an estimated rational price for each\nstock, which is cointegrated with the stock's actual price. The discrepancy\nbetween the actual and the rational prices serves as a factor to indicate\nstock-level irrational events. Additionally, we define market-level irrational\nbehaviors as anomalous synchronous fluctuations of stocks within a market.\nUsing two self-supervised representation learning tasks, i.e., sub-market\ncomparative learning and market synchronism prediction, the UMI model\nincorporates market-level irrationalities into a market representation vector,\nwhich is then used as the market-level irrationality factor.",
          "arxiv_id": "2502.04737v1"
        }
      ],
      "30": [
        {
          "title": "FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection",
          "year": "2025-04",
          "abstract": "The proliferation of Internet of Things (IoT) devices has expanded the attack\nsurface, necessitating efficient intrusion detection systems (IDSs) for network\nprotection. This paper presents FLARE, a feature-based lightweight aggregation\nfor robust evaluation of IoT intrusion detection to address the challenges of\nsecuring IoT environments through feature aggregation techniques. FLARE\nutilizes a multilayered processing approach, incorporating session, flow, and\ntime-based sliding-window data aggregation to analyze network behavior and\ncapture vital features from IoT network traffic data. We perform extensive\nevaluations on IoT data generated from our laboratory experimental setup to\nassess the effectiveness of the proposed aggregation technique. To classify\nattacks in IoT IDS, we employ four supervised learning models and two deep\nlearning models. We validate the performance of these models in terms of\naccuracy, precision, recall, and F1-score. Our results reveal that\nincorporating the FLARE aggregation technique as a foundational step in feature\nengineering, helps lay a structured representation, and enhances the\nperformance of complex end-to-end models, making it a crucial step in IoT IDS\npipeline. Our findings highlight the potential of FLARE as a valuable technique\nto improve performance and reduce computational costs of end-to-end IDS\nimplementations, thereby fostering more robust IoT intrusion detection systems.",
          "arxiv_id": "2504.15375v1"
        },
        {
          "title": "Unsupervised Ensemble Based Deep Learning Approach for Attack Detection in IoT Network",
          "year": "2022-07",
          "abstract": "The Internet of Things (IoT) has altered living by controlling devices/things\nover the Internet. IoT has specified many smart solutions for daily problems,\ntransforming cyber-physical systems (CPS) and other classical fields into smart\nregions. Most of the edge devices that make up the Internet of Things have very\nminimal processing power. To bring down the IoT network, attackers can utilise\nthese devices to conduct a variety of network attacks. In addition, as more and\nmore IoT devices are added, the potential for new and unknown threats grows\nexponentially. For this reason, an intelligent security framework for IoT\nnetworks must be developed that can identify such threats. In this paper, we\nhave developed an unsupervised ensemble learning model that is able to detect\nnew or unknown attacks in an IoT network from an unlabelled dataset. The\nsystem-generated labelled dataset is used to train a deep learning model to\ndetect IoT network attacks. Additionally, the research presents a feature\nselection mechanism for identifying the most relevant aspects in the dataset\nfor detecting attacks. The study shows that the suggested model is able to\nidentify the unlabelled IoT network datasets and DBN (Deep Belief Network)\noutperform the other models with a detection accuracy of 97.5% and a false\nalarm rate of 2.3% when trained using labelled dataset supplied by the proposed\napproach.",
          "arxiv_id": "2207.07903v1"
        },
        {
          "title": "Machine Learning-Assisted Intrusion Detection for Enhancing Internet of Things Security",
          "year": "2024-10",
          "abstract": "Attacks against the Internet of Things (IoT) are rising as devices,\napplications, and interactions become more networked and integrated. The\nincrease in cyber-attacks that target IoT networks poses a considerable\nvulnerability and threat to the privacy, security, functionality, and\navailability of critical systems, which leads to operational disruptions,\nfinancial losses, identity thefts, and data breaches. To efficiently secure IoT\ndevices, real-time detection of intrusion systems is critical, especially those\nusing machine learning to identify threats and mitigate risks and\nvulnerabilities. This paper investigates the latest research on machine\nlearning-based intrusion detection strategies for IoT security, concentrating\non real-time responsiveness, detection accuracy, and algorithm efficiency. Key\nstudies were reviewed from all well-known academic databases, and a taxonomy\nwas provided for the existing approaches. This review also highlights existing\nresearch gaps and outlines the limitations of current IoT security frameworks\nto offer practical insights for future research directions and developments.",
          "arxiv_id": "2410.01016v2"
        }
      ],
      "31": [
        {
          "title": "Performance evaluation results of evolutionary clustering algorithm star for clustering heterogeneous datasets",
          "year": "2021-04",
          "abstract": "This article presents the data used to evaluate the performance of\nevolutionary clustering algorithm star (ECA*) compared to five traditional and\nmodern clustering algorithms. Two experimental methods are employed to examine\nthe performance of ECA* against genetic algorithm for clustering++\n(GENCLUST++), learning vector quantisation (LVQ) , expectation maximisation\n(EM) , K-means++ (KM++) and K-means (KM). These algorithms are applied to 32\nheterogenous and multi-featured datasets to determine which one performs well\non the three tests. For one, ther paper examines the efficiency of ECA* in\ncontradiction of its corresponding algorithms using clustering evaluation\nmeasures. These validation criteria are objective function and cluster quality\nmeasures. For another, it suggests a performance rating framework to measurethe\nthe performance sensitivity of these algorithms on varos dataset features\n(cluster dimensionality, number of clusters, cluster overlap, cluster shape and\ncluster structure). The contributions of these experiments are two-folds: (i)\nECA* exceeds its counterpart aloriths in ability to find out the right cluster\nnumber; (ii) ECA* is less sensitive towards dataset features compared to its\ncompetitive techniques. Nonetheless, the results of the experiments performed\ndemonstrate some limitations in the ECA*: (i) ECA* is not fully applied based\non the premise that no prior knowledge exists; (ii) Adapting and utilising ECA*\non several real applications has not been achieved yet.",
          "arxiv_id": "2105.02810v1"
        },
        {
          "title": "Exponentially Consistent Nonparametric Linkage-Based Clustering of Data Sequences",
          "year": "2024-11",
          "abstract": "In this paper, we consider nonparametric clustering of $M$ independent and\nidentically distributed (i.i.d.) data sequences generated from {\\em unknown}\ndistributions. The distributions of the $M$ data sequences belong to $K$\nunderlying distribution clusters. Existing results on exponentially consistent\nnonparametric clustering algorithms, like single linkage-based (SLINK)\nclustering and $k$-medoids distribution clustering, assume that the maximum\nintra-cluster distance ($d_L$) is smaller than the minimum inter-cluster\ndistance ($d_H$). First, in the fixed sample size (FSS) setting, we show that\nexponential consistency can be achieved for SLINK clustering under a less\nstrict assumption, $d_I < d_H$, where $d_I$ is the maximum distance between any\ntwo sub-clusters of a cluster that partition the cluster. Note that $d_I < d_L$\nin general. Thus, our results show that SLINK is exponentially consistent for a\nlarger class of problems than previously known. In our simulations, we also\nidentify examples where $k$-medoids clustering is unable to find the true\nclusters, but SLINK is exponentially consistent. Then, we propose a sequential\nclustering algorithm, named SLINK-SEQ, based on SLINK and prove that it is also\nexponentially consistent. Simulation results show that the SLINK-SEQ algorithm\nrequires fewer expected number of samples than the FSS SLINK algorithm for the\nsame probability of error.",
          "arxiv_id": "2411.13922v4"
        },
        {
          "title": "From A-to-Z Review of Clustering Validation Indices",
          "year": "2024-07",
          "abstract": "Data clustering involves identifying latent similarities within a dataset and\norganizing them into clusters or groups. The outcomes of various clustering\nalgorithms differ as they are susceptible to the intrinsic characteristics of\nthe original dataset, including noise and dimensionality. The effectiveness of\nsuch clustering procedures directly impacts the homogeneity of clusters,\nunderscoring the significance of evaluating algorithmic outcomes. Consequently,\nthe assessment of clustering quality presents a significant and complex\nendeavor. A pivotal aspect affecting clustering validation is the cluster\nvalidity metric, which aids in determining the optimal number of clusters. The\nmain goal of this study is to comprehensively review and explain the\nmathematical operation of internal and external cluster validity indices, but\nnot all, to categorize these indices and to brainstorm suggestions for future\nadvancement of clustering validation research. In addition, we review and\nevaluate the performance of internal and external clustering validation indices\non the most common clustering algorithms, such as the evolutionary clustering\nalgorithm star (ECA*). Finally, we suggest a classification framework for\nexamining the functionality of both internal and external clustering validation\nmeasures regarding their ideal values, user-friendliness, responsiveness to\ninput data, and appropriateness across various fields. This classification aids\nresearchers in selecting the appropriate clustering validation measure to suit\ntheir specific requirements.",
          "arxiv_id": "2407.20246v1"
        }
      ],
      "32": [
        {
          "title": "On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning",
          "year": "2023-11",
          "abstract": "Bayesian deep learning and conformal prediction are two methods that have\nbeen used to convey uncertainty and increase safety in machine learning\nsystems. We focus on combining Bayesian deep learning with split conformal\nprediction and how this combination effects out-of-distribution coverage;\nparticularly in the case of multiclass image classification. We suggest that if\nthe model is generally underconfident on the calibration set, then the\nresultant conformal sets may exhibit worse out-of-distribution coverage\ncompared to simple predictive credible sets. Conversely, if the model is\noverconfident on the calibration set, the use of conformal prediction may\nimprove out-of-distribution coverage. We evaluate prediction sets as a result\nof combining split conformal methods and neural networks trained with (i)\nstochastic gradient descent, (ii) deep ensembles, and (iii) mean-field\nvariational inference. Our results suggest that combining Bayesian deep\nlearning models with split conformal prediction can, in some cases, cause\nunintended consequences such as reducing out-of-distribution coverage.",
          "arxiv_id": "2311.12688v2"
        },
        {
          "title": "When Can We Reuse a Calibration Set for Multiple Conformal Predictions?",
          "year": "2025-06",
          "abstract": "Reliable uncertainty quantification is crucial for the trustworthiness of\nmachine learning applications. Inductive Conformal Prediction (ICP) offers a\ndistribution-free framework for generating prediction sets or intervals with\nuser-specified confidence. However, standard ICP guarantees are marginal and\ntypically require a fresh calibration set for each new prediction to maintain\ntheir validity. This paper addresses this practical limitation by demonstrating\nhow e-conformal prediction, in conjunction with Hoeffding's inequality, can\nenable the repeated use of a single calibration set with a high probability of\npreserving the desired coverage. Through a case study on the CIFAR-10 dataset,\nwe train a deep neural network and utilise a calibration set to estimate a\nHoeffding correction. This correction allows us to apply a modified Markov's\ninequality, leading to the construction of prediction sets with quantifiable\nconfidence. Our results illustrate the feasibility of maintaining provable\nperformance in conformal prediction while enhancing its practicality by\nreducing the need for repeated calibration. The code for this work is publicly\navailable.",
          "arxiv_id": "2506.19689v1"
        },
        {
          "title": "Conformal Prediction and Human Decision Making",
          "year": "2025-03",
          "abstract": "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
          "arxiv_id": "2503.11709v2"
        }
      ],
      "33": [
        {
          "title": "Deep Learning for EEG Seizure Detection in Preterm Infants",
          "year": "2021-05",
          "abstract": "EEG is the gold standard for seizure detection in the newborn infant, but EEG\ninterpretation in the preterm group is particularly challenging; trained\nexperts are scarce and the task of interpreting EEG in real-time is arduous.\nPreterm infants are reported to have a higher incidence of seizures compared to\nterm infants. Preterm EEG morphology differs from that of term infants, which\nimplies that seizure detection algorithms trained on term EEG may not be\nappropriate. The task of developing preterm specific algorithms becomes\nextra-challenging given the limited amount of annotated preterm EEG data\navailable. This paper explores novel deep learning (DL) architectures for the\ntask of neonatal seizure detection in preterm infants. The study tests and\ncompares several approaches to address the problem: training on data from\nfull-term infants; training on data from preterm infants; training on\nage-specific preterm data and transfer learning. The system performance is\nassessed on a large database of continuous EEG recordings of 575h in duration.\nIt is shown that the accuracy of a validated term-trained EEG seizure detection\nalgorithm, based on a support vector machine classifier, when tested on preterm\ninfants falls well short of the performance achieved for full-term infants. An\nAUC of 88.3% was obtained when tested on preterm EEG as compared to 96.6%\nobtained when tested on term EEG. When re-trained on preterm EEG, the\nperformance marginally increases to 89.7%. An alternative DL approach shows a\nmore stable trend when tested on the preterm cohort, starting with an AUC of\n93.3% for the term-trained algorithm and reaching 95.0% by transfer learning\nfrom the term model using available preterm data.",
          "arxiv_id": "2106.00611v1"
        },
        {
          "title": "Subject-Adaptive Transfer Learning Using Resting State EEG Signals for Cross-Subject EEG Motor Imagery Classification",
          "year": "2024-05",
          "abstract": "Electroencephalography (EEG) motor imagery (MI) classification is a\nfundamental, yet challenging task due to the variation of signals between\nindividuals i.e., inter-subject variability. Previous approaches try to\nmitigate this using task-specific (TS) EEG signals from the target subject in\ntraining. However, recording TS EEG signals requires time and limits its\napplicability in various fields. In contrast, resting state (RS) EEG signals\nare a viable alternative due to ease of acquisition with rich subject\ninformation. In this paper, we propose a novel subject-adaptive transfer\nlearning strategy that utilizes RS EEG signals to adapt models on unseen\nsubject data. Specifically, we disentangle extracted features into task- and\nsubject-dependent features and use them to calibrate RS EEG signals for\nobtaining task information while preserving subject characteristics. The\ncalibrated signals are then used to adapt the model to the target subject,\nenabling the model to simulate processing TS EEG signals of the target subject.\nThe proposed method achieves state-of-the-art accuracy on three public\nbenchmarks, demonstrating the effectiveness of our method in cross-subject EEG\nMI classification. Our findings highlight the potential of leveraging RS EEG\nsignals to advance practical brain-computer interface systems. The code is\navailable at https://github.com/SionAn/MICCAI2024-ResTL.",
          "arxiv_id": "2405.19346v2"
        },
        {
          "title": "hvEEGNet: exploiting hierarchical VAEs on EEG data for neuroscience applications",
          "year": "2023-11",
          "abstract": "With the recent success of artificial intelligence in neuroscience, a number\nof deep learning (DL) models were proposed for classification, anomaly\ndetection, and pattern recognition tasks in electroencephalography (EEG). EEG\nis a multi-channel time-series that provides information about the individual\nbrain activity for diagnostics, neuro-rehabilitation, and other applications\n(including emotions recognition). Two main issues challenge the existing\nDL-based modeling methods for EEG: the high variability between subjects and\nthe low signal-to-noise ratio making it difficult to ensure a good quality in\nthe EEG data. In this paper, we propose two variational autoencoder models,\nnamely vEEGNet-ver3 and hvEEGNet, to target the problem of high-fidelity EEG\nreconstruction. We properly designed their architectures using the blocks of\nthe well-known EEGNet as the encoder, and proposed a loss function based on\ndynamic time warping. We tested the models on the public Dataset 2a - BCI\nCompetition IV, where EEG was collected from 9 subjects and 22 channels.\nhvEEGNet was found to reconstruct the EEG data with very high-fidelity,\noutperforming most previous solutions (including our vEEGNet-ver3 ).\nFurthermore, this was consistent across all subjects. Interestingly, hvEEGNet\nmade it possible to discover that this popular dataset includes a number of\ncorrupted EEG recordings that might have influenced previous literature\nresults. We also investigated the training behaviour of our models and related\nit with the quality and the size of the input EEG dataset, aiming at opening a\nnew research debate on this relationship. In the future, hvEEGNet could be used\nas anomaly (e.g., artefact) detector in large EEG datasets to support the\ndomain experts, but also the latent representations it provides could be used\nin other classification problems and EEG data generation.",
          "arxiv_id": "2312.00799v1"
        }
      ],
      "34": [
        {
          "title": "FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting",
          "year": "2024-05",
          "abstract": "Ensemble forecasting is crucial for improving weather predictions, especially\nfor forecasts of extreme events. Constructing an ensemble prediction system\n(EPS) based on conventional NWP models is highly computationally expensive. ML\nmodels have emerged as valuable tools for deterministic weather forecasts,\nproviding forecasts with significantly reduced computational requirements and\neven surpassing the forecast performance of traditional NWP models. However,\nchallenges arise when applying ML models to ensemble forecasting. Recent ML\nmodels, such as GenCast and SEEDS model, rely on the ERA5 EDA or operational\nNWP ensemble members for forecast generation. Their spatial resolution is also\nconsidered too coarse for many applications. To overcome these limitations, we\nintroduce FuXi-ENS, an advanced ML model designed to deliver 6-hourly global\nensemble weather forecasts up to 15 days. This model runs at a significantly\nincreased spatial resolution of 0.25\\textdegree, incorporating 5 atmospheric\nvariables at 13 pressure levels, along with 13 surface variables. By leveraging\nthe inherent probabilistic nature of Variational AutoEncoder (VAE), FuXi-ENS\noptimizes a loss function that combines the CRPS and the KL divergence between\nthe predicted and target distribution, facilitating the incorporation of\nflow-dependent perturbations in both initial conditions and forecast. This\ninnovative approach makes FuXi-ENS an advancement over the traditional ones\nthat use L1 loss combined with the KL loss in standard VAE models for ensemble\nweather forecasting. Results demonstrate that FuXi-ENS outperforms ensemble\nforecasts from the ECMWF, a world leading NWP model, in the CRPS of 98.1% of\n360 variable and forecast lead time combinations. This achievement underscores\nthe potential of the FuXi-ENS model to enhance ensemble weather forecasts,\noffering a promising direction for further development in this field.",
          "arxiv_id": "2405.05925v3"
        },
        {
          "title": "Robustness of AI-based weather forecasts in a changing climate",
          "year": "2024-09",
          "abstract": "Data-driven machine learning models for weather forecasting have made\ntransformational progress in the last 1-2 years, with state-of-the-art ones now\noutperforming the best physics-based models for a wide range of skill scores.\nGiven the strong links between weather and climate modelling, this raises the\nquestion whether machine learning models could also revolutionize climate\nscience, for example by informing mitigation and adaptation to climate change\nor to generate larger ensembles for more robust uncertainty estimates. Here, we\nshow that current state-of-the-art machine learning models trained for weather\nforecasting in present-day climate produce skillful forecasts across different\nclimate states corresponding to pre-industrial, present-day, and future 2.9K\nwarmer climates. This indicates that the dynamics shaping the weather on short\ntimescales may not differ fundamentally in a changing climate. It also\ndemonstrates out-of-distribution generalization capabilities of the machine\nlearning models that are a critical prerequisite for climate applications.\nNonetheless, two of the models show a global-mean cold bias in the forecasts\nfor the future warmer climate state, i.e. they drift towards the colder\npresent-day climate they have been trained for. A similar result is obtained\nfor the pre-industrial case where two out of three models show a warming. We\ndiscuss possible remedies for these biases and analyze their spatial\ndistribution, revealing complex warming and cooling patterns that are partly\nrelated to missing ocean-sea ice and land surface information in the training\ndata. Despite these current limitations, our results suggest that data-driven\nmachine learning models will provide powerful tools for climate science and\ntransform established approaches by complementing conventional physics-based\nmodels.",
          "arxiv_id": "2409.18529v1"
        },
        {
          "title": "FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting",
          "year": "2024-01",
          "abstract": "Kilometer-scale modeling of global atmosphere dynamics enables fine-grained\nweather forecasting and decreases the risk of disastrous weather and climate\nactivity. Therefore, building a kilometer-scale global forecast model is a\npersistent pursuit in the meteorology domain. Active international efforts have\nbeen made in past decades to improve the spatial resolution of numerical\nweather models. Nonetheless, developing the higher resolution numerical model\nremains a long-standing challenge due to the substantial consumption of\ncomputational resources. Recent advances in data-driven global weather\nforecasting models utilize reanalysis data for model training and have\ndemonstrated comparable or even higher forecasting skills than numerical\nmodels. However, they are all limited by the resolution of reanalysis data and\nincapable of generating higher-resolution forecasts. This work presents\nFengWu-GHR, the first data-driven global weather forecasting model running at\nthe 0.09$^{\\circ}$ horizontal resolution. FengWu-GHR introduces a novel\napproach that opens the door for operating ML-based high-resolution forecasts\nby inheriting prior knowledge from a pretrained low-resolution model. The\nhindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to\nthe IFS-HRES. Furthermore, evaluations on station observations and case studies\nof extreme events support the competitive operational forecasting skill of\nFengWu-GHR at the high resolution.",
          "arxiv_id": "2402.00059v1"
        }
      ],
      "35": [
        {
          "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
          "year": "2023-10",
          "abstract": "Large language models (LLMs) are susceptible to red teaming attacks, which\ncan induce LLMs to generate harmful content. Previous research constructs\nattack prompts via manual or automatic methods, which have their own\nlimitations on construction cost and quality. To address these issues, we\npropose an integrated approach that combines manual and automatic methods to\neconomically generate high-quality attack prompts. Specifically, considering\nthe impressive capabilities of newly emerged LLMs, we propose an attack\nframework to instruct LLMs to mimic human-generated prompts through in-context\nlearning. Furthermore, we propose a defense framework that fine-tunes victim\nLLMs through iterative interactions with the attack framework to enhance their\nsafety against red teaming attacks. Extensive experiments on different LLMs\nvalidate the effectiveness of our proposed attack and defense frameworks.\nAdditionally, we release a series of attack prompts datasets named SAP with\nvarying sizes, facilitating the safety evaluation and enhancement of more LLMs.\nOur code and dataset is available on https://github.com/Aatrox103/SAP .",
          "arxiv_id": "2310.12505v1"
        },
        {
          "title": "AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models",
          "year": "2025-04",
          "abstract": "Large Language Models (LLMs) continue to exhibit vulnerabilities to\njailbreaking attacks: carefully crafted malicious inputs intended to circumvent\nsafety guardrails and elicit harmful responses. As such, we present AutoAdv, a\nnovel framework that automates adversarial prompt generation to systematically\nevaluate and expose vulnerabilities in LLM safety mechanisms. Our approach\nleverages a parametric attacker LLM to produce semantically disguised malicious\nprompts through strategic rewriting techniques, specialized system prompts, and\noptimized hyperparameter configurations. The primary contribution of our work\nis a dynamic, multi-turn attack methodology that analyzes failed jailbreak\nattempts and iteratively generates refined follow-up prompts, leveraging\ntechniques such as roleplaying, misdirection, and contextual manipulation. We\nquantitatively evaluate attack success rate (ASR) using the StrongREJECT\n(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.\nThrough extensive empirical evaluation of state-of-the-art models--including\nChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our\nautomated attacks achieving jailbreak success rates of up to 86% for harmful\ncontent generation. Our findings reveal that current safety mechanisms remain\nsusceptible to sophisticated multi-turn attacks, emphasizing the urgent need\nfor more robust defense strategies.",
          "arxiv_id": "2507.01020v1"
        },
        {
          "title": "EnJa: Ensemble Jailbreak on Large Language Models",
          "year": "2024-08",
          "abstract": "As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.",
          "arxiv_id": "2408.03603v1"
        }
      ],
      "36": [
        {
          "title": "TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction",
          "year": "2022-04",
          "abstract": "Knowledge graph embedding methods are important for the knowledge graph\ncompletion (or link prediction) task. One existing efficient method, PairRE,\nleverages two separate vectors to model complex relations (i.e., 1-to-N,\nN-to-1, and N-to-N) in knowledge graphs. However, such a method strictly\nrestricts entities on the hyper-ellipsoid surfaces which limits the\noptimization of entity distribution, leading to suboptimal performance of\nknowledge graph completion. To address this issue, we propose a novel score\nfunction TranSHER, which leverages relation-specific translations between head\nand tail entities to relax the constraint of hyper-ellipsoid restrictions. By\nintroducing an intuitive and simple relation-specific translation, TranSHER can\nprovide more direct guidance on optimization and capture more semantic\ncharacteristics of entities with complex relations. Experimental results show\nthat TranSHER achieves significant performance improvements on link prediction\nand generalizes well to datasets in different domains and scales. Our codes are\npublic available at https://github.com/yizhilll/TranSHER.",
          "arxiv_id": "2204.13221v2"
        },
        {
          "title": "Inference over Unseen Entities, Relations and Literals on Knowledge Graphs",
          "year": "2024-10",
          "abstract": "In recent years, knowledge graph embedding models have been successfully\napplied in the transductive setting to tackle various challenging tasks\nincluding link prediction, and query answering. Yet, the transductive setting\ndoes not allow for reasoning over unseen entities, relations, let alone\nnumerical or non-numerical literals. Although increasing efforts are put into\nexploring inductive scenarios, inference over unseen entities, relations, and\nliterals has yet to come. This limitation prohibits the existing methods from\nhandling real-world dynamic knowledge graphs involving heterogeneous\ninformation about the world. Here, we propose a remedy to this limitation. We\npropose the attentive byte-pair encoding layer (BytE) to construct a triple\nembedding from a sequence of byte-pair encoded subword units of entities and\nrelations. Compared to the conventional setting, BytE leads to massive feature\nreuse via weight tying, since it forces a knowledge graph embedding model to\nlearn embeddings for subword units instead of entities and relations directly.\nConsequently, the size of the embedding matrices are not anymore bound to the\nunique number of entities and relations of a knowledge graph. Experimental\nresults show that BytE improves the link prediction performance of 4 knowledge\ngraph embedding models on datasets where the syntactic representations of\ntriples are semantically meaningful. However, benefits of training a knowledge\ngraph embedding model with BytE dissipate on knowledge graphs where entities\nand relations are represented with plain numbers or URIs. We provide an open\nsource implementation of BytE to foster reproducible research.",
          "arxiv_id": "2410.06742v1"
        },
        {
          "title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding",
          "year": "2021-10",
          "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become\nwidespread recently, and many knowledge graph embedding (KGE) methods are\nproposed to embed entities and relations of a KG into continuous vector spaces.\nSuch embedding methods simplify the operations of conducting various in-KG\ntasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering).\nThey can be viewed as general solutions for representing KGs. However, existing\nKGE methods are not applicable to inductive settings, where a model trained on\nsource KGs will be tested on target KGs with entities unseen during model\ntraining. Existing works focusing on KGs in inductive settings can only solve\nthe inductive relation prediction task. They can not handle other out-of-KG\ntasks as general as KGE methods since they don't produce embeddings for\nentities. In this paper, to achieve inductive knowledge graph embedding, we\npropose a model MorsE, which does not learn embeddings for entities but learns\ntransferable meta-knowledge that can be used to produce entity embeddings. Such\nmeta-knowledge is modeled by entity-independent modules and learned by\nmeta-learning. Experimental results show that our model significantly\noutperforms corresponding baselines for in-KG and out-of-KG tasks in inductive\nsettings.",
          "arxiv_id": "2110.14170v3"
        }
      ],
      "37": [
        {
          "title": "Segmenter: Transformer for Semantic Segmentation",
          "year": "2021-05",
          "abstract": "Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution-based methods, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on both ADE20K and\nPascal Context datasets and is competitive on Cityscapes.",
          "arxiv_id": "2105.05633v3"
        },
        {
          "title": "A Survey on Instance Segmentation: State of the art",
          "year": "2020-06",
          "abstract": "Object detection or localization is an incremental step in progression from\ncoarse to fine digital image inference. It not only provides the classes of the\nimage objects, but also provides the location of the image objects which have\nbeen classified. The location is given in the form of bounding boxes or\ncentroids. Semantic segmentation gives fine inference by predicting labels for\nevery pixel in the input image. Each pixel is labelled according to the object\nclass within which it is enclosed. Furthering this evolution, instance\nsegmentation gives different labels for separate instances of objects belonging\nto the same class. Hence, instance segmentation may be defined as the technique\nof simultaneously solving the problem of object detection as well as that of\nsemantic segmentation. In this survey paper on instance segmentation -- its\nbackground, issues, techniques, evolution, popular datasets, related work up to\nthe state of the art and future scope have been discussed. The paper provides\nvaluable information for those who want to do research in the field of instance\nsegmentation.",
          "arxiv_id": "2007.00047v1"
        },
        {
          "title": "IFSeg: Image-free Semantic Segmentation via Vision-Language Model",
          "year": "2023-03",
          "abstract": "Vision-language (VL) pre-training has recently gained much attention for its\ntransferability and flexibility in novel concepts (e.g., cross-modality\ntransfer) across various visual tasks. However, VL-driven segmentation has been\nunder-explored, and the existing approaches still have the burden of acquiring\nadditional training images or even segmentation annotations to adapt a VL model\nto downstream segmentation tasks. In this paper, we introduce a novel\nimage-free segmentation task where the goal is to perform semantic segmentation\ngiven only a set of the target semantic categories, but without any\ntask-specific images and annotations. To tackle this challenging task, our\nproposed method, coined IFSeg, generates VL-driven artificial\nimage-segmentation pairs and updates a pre-trained VL model to a segmentation\ntask. We construct this artificial training data by creating a 2D map of random\nsemantic categories and another map of their corresponding word tokens. Given\nthat a pre-trained VL model projects visual and text tokens into a common space\nwhere tokens that share the semantics are located closely, this artificially\ngenerated word map can replace the real image inputs for such a VL model.\nThrough an extensive set of experiments, our model not only establishes an\neffective baseline for this novel task but also demonstrates strong\nperformances compared to existing methods that rely on stronger supervision,\nsuch as task-specific images and segmentation masks. Code is available at\nhttps://github.com/alinlab/ifseg.",
          "arxiv_id": "2303.14396v1"
        }
      ],
      "38": [
        {
          "title": "NatGen: Generative pre-training by \"Naturalizing\" source code",
          "year": "2022-06",
          "abstract": "Pre-trained Generative Language models (e.g. PLBART, CodeT5, SPT-Code) for\nsource code yielded strong results on several tasks in the past few years,\nincluding code generation and translation. These models have adopted varying\npre-training objectives to learn statistics of code construction from very\nlarge-scale corpora in a self-supervised fashion; the success of pre-trained\nmodels largely hinges on these pre-training objectives. This paper proposes a\nnew pre-training objective, \"Naturalizing\" of source code, exploiting code's\nbimodal, dual-channel (formal & natural channels) nature. Unlike natural\nlanguage, code's bimodal, dual-channel nature allows us to generate\nsemantically equivalent code at scale. We introduce six classes of semantic\npreserving transformations to introduce un-natural forms of code, and then\nforce our model to produce more natural original programs written by\ndevelopers. Learning to generate equivalent, but more natural code, at scale,\nover large corpora of open-source code, without explicit manual supervision,\nhelps the model learn to both ingest & generate code. We fine-tune our model in\nthree generative Software Engineering tasks: code generation, code translation,\nand code refinement with limited human-curated labeled data and achieve\nstate-of-the-art performance rivaling CodeT5. We show that our pre-trained\nmodel is especially competitive at zero-shot and few-shot learning, and better\nat learning code properties (e.g., syntax, data flow).",
          "arxiv_id": "2206.07585v2"
        },
        {
          "title": "Function-constrained Program Synthesis",
          "year": "2023-11",
          "abstract": "This work introduces (1) a technique that allows large language models (LLMs)\nto leverage user-provided code when solving programming tasks and (2) a method\nto iteratively generate modular sub-functions that can aid future code\ngeneration attempts when the initial code generated by the LLM is inadequate.\nGenerating computer programs in general-purpose programming languages like\nPython poses a challenge for LLMs when instructed to use code provided in the\nprompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code\ncompletions in real-time by drawing on all code available in a development\nenvironment. However, restricting code-specific LLMs to use only in-context\ncode is not straightforward, as the model is not explicitly instructed to use\nthe user-provided code and users cannot highlight precisely which snippets of\ncode the model should incorporate into its context. Moreover, current systems\nlack effective recovery methods, forcing users to iteratively re-prompt the\nmodel with modified prompts until a sufficient solution is reached. Our method\ndiffers from traditional LLM-powered code-generation by constraining\ncode-generation to an explicit function set and enabling recovery from failed\nattempts through automatically generated sub-functions. When the LLM cannot\nproduce working code, we generate modular sub-functions to aid subsequent\nattempts at generating functional code. A by-product of our method is a library\nof reusable sub-functions that can solve related tasks, imitating a software\nteam where efficiency scales with experience. We also introduce a new\n\"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding\nabilities compared to traditional zero-shot evaluation. Our proposed evaluation\nmethod encourages models to output solutions in a structured format, decreasing\nsyntax errors that can be mistaken for poor coding ability.",
          "arxiv_id": "2311.15500v2"
        },
        {
          "title": "How Accurately Do Large Language Models Understand Code?",
          "year": "2025-04",
          "abstract": "Large Language Models (LLMs) are increasingly used in post-development tasks\nsuch as code repair and testing. A key factor in these tasks' success is the\nmodel's deep understanding of code. However, the extent to which LLMs truly\nunderstand code remains largely unevaluated. Quantifying code comprehension is\nchallenging due to its abstract nature and the lack of a standardized metric.\nPreviously, this was assessed through developer surveys, which are not feasible\nfor evaluating LLMs. Existing LLM benchmarks focus primarily on code\ngeneration, fundamentally different from code comprehension. Additionally,\nfixed benchmarks quickly become obsolete as they become part of the training\ndata. This paper presents the first large-scale empirical investigation into\nLLMs' ability to understand code. Inspired by mutation testing, we use an LLM's\nfault-finding ability as a proxy for its deep code understanding. This approach\nis based on the insight that a model capable of identifying subtle functional\ndiscrepancies must understand the code well. We inject faults in real-world\nprograms and ask the LLM to localize them, ensuring the specifications suffice\nfor fault localization. Next, we apply semantic-preserving code mutations\n(SPMs) to the faulty programs and test whether the LLMs still locate the\nfaults, verifying their confidence in code understanding. We evaluate nine\npopular LLMs on 600,010 debugging tasks from 670 Java and 637 Python programs.\nWe find that LLMs lose the ability to debug the same bug in 78% of faulty\nprograms when SPMs are applied, indicating a shallow understanding of code and\nreliance on features irrelevant to semantics. We also find that LLMs understand\ncode earlier in the program better than later. This suggests that LLMs' code\ncomprehension remains tied to lexical and syntactic features due to\ntokenization designed for natural languages, which overlooks code semantics.",
          "arxiv_id": "2504.04372v2"
        }
      ],
      "39": [
        {
          "title": "UAV and Machine Learning Based Refinement of a Satellite-Driven Vegetation Index for Precision Agriculture",
          "year": "2020-04",
          "abstract": "Precision agriculture is considered to be a fundamental approach in pursuing\na low-input, high-efficiency, and sustainable kind of agriculture when\nperforming site-specific management practices. To achieve this objective, a\nreliable and updated description of the local status of crops is required.\nRemote sensing, and in particular satellite-based imagery, proved to be a\nvaluable tool in crop mapping, monitoring, and diseases assessment. However,\nfreely available satellite imagery with low or moderate resolutions showed some\nlimits in specific agricultural applications, e.g., where crops are grown by\nrows. Indeed, in this framework, the satellite's output could be biased by\nintra-row covering, giving inaccurate information about crop status. This paper\npresents a novel satellite imagery refinement framework, based on a deep\nlearning technique which exploits information properly derived from high\nresolution images acquired by unmanned aerial vehicle (UAV) airborne\nmultispectral sensors. To train the convolutional neural network, only a single\nUAV-driven dataset is required, making the proposed approach simple and\ncost-effective. A vineyard in Serralunga d'Alba (Northern Italy) was chosen as\na case study for validation purposes. Refined satellite-driven normalized\ndifference vegetation index (NDVI) maps, acquired in four different periods\nduring the vine growing season, were shown to better describe crop status with\nrespect to raw datasets by correlation analysis and ANOVA. In addition, using a\nK-means based classifier, 3-class vineyard vigor maps were profitably derived\nfrom the NDVI maps, which are a valuable tool for growers.",
          "arxiv_id": "2004.14421v1"
        },
        {
          "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping",
          "year": "2025-05",
          "abstract": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nhttps://github.com/flyakon/AgriFM.",
          "arxiv_id": "2505.21357v2"
        },
        {
          "title": "Deep Learning for Satellite Image Time Series Analysis: A Review",
          "year": "2024-04",
          "abstract": "Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.",
          "arxiv_id": "2404.03936v2"
        }
      ],
      "40": [
        {
          "title": "Bayesian autoregression to optimize temporal Matrn kernel Gaussian process hyperparameters",
          "year": "2025-08",
          "abstract": "Gaussian processes are important models in the field of probabilistic\nnumerics. We present a procedure for optimizing Mat\\'ern kernel temporal\nGaussian processes with respect to the kernel covariance function's\nhyperparameters. It is based on casting the optimization problem as a recursive\nBayesian estimation procedure for the parameters of an autoregressive model. We\ndemonstrate that the proposed procedure outperforms maximizing the marginal\nlikelihood as well as Hamiltonian Monte Carlo sampling, both in terms of\nruntime and ultimate root mean square error in Gaussian process regression.",
          "arxiv_id": "2508.09792v1"
        },
        {
          "title": "Amortized Variational Inference for Deep Gaussian Processes",
          "year": "2024-09",
          "abstract": "Gaussian processes (GPs) are Bayesian nonparametric models for function\napproximation with principled predictive uncertainty estimates. Deep Gaussian\nprocesses (DGPs) are multilayer generalizations of GPs that can represent\ncomplex marginal densities as well as complex mappings. As exact inference is\neither computationally prohibitive or analytically intractable in GPs and\nextensions thereof, some existing methods resort to variational inference (VI)\ntechniques for tractable approximations. However, the expressivity of\nconventional approximate GP models critically relies on independent inducing\nvariables that might not be informative enough for some problems. In this work\nwe introduce amortized variational inference for DGPs, which learns an\ninference function that maps each observation to variational parameters. The\nresulting method enjoys a more expressive prior conditioned on fewer input\ndependent inducing variables and a flexible amortized marginal posterior that\nis able to model more complicated functions. We show with theoretical reasoning\nand experimental results that our method performs similarly or better than\nprevious approaches at less computational cost.",
          "arxiv_id": "2409.12301v1"
        },
        {
          "title": "Stein Variational Gaussian Processes",
          "year": "2020-09",
          "abstract": "We show how to use Stein variational gradient descent (SVGD) to carry out\ninference in Gaussian process (GP) models with non-Gaussian likelihoods and\nlarge data volumes. Markov chain Monte Carlo (MCMC) is extremely\ncomputationally intensive for these situations, but the parametric assumptions\nrequired for efficient variational inference (VI) result in incorrect inference\nwhen they encounter the multi-modal posterior distributions that are common for\nsuch models. SVGD provides a non-parametric alternative to variational\ninference which is substantially faster than MCMC. We prove that for GP models\nwith Lipschitz gradients the SVGD algorithm monotonically decreases the\nKullback-Leibler divergence from the sampling distribution to the true\nposterior. Our method is demonstrated on benchmark problems in both regression\nand classification, a multimodal posterior, and an air quality example with\n550,134 spatiotemporal observations, showing substantial performance\nimprovements over MCMC and VI.",
          "arxiv_id": "2009.12141v3"
        }
      ],
      "41": [
        {
          "title": "Learning to Discover Knowledge: A Weakly-Supervised Partial Domain Adaptation Approach",
          "year": "2024-06",
          "abstract": "Domain adaptation has shown appealing performance by leveraging knowledge\nfrom a source domain with rich annotations. However, for a specific target\ntask, it is cumbersome to collect related and high-quality source domains. In\nreal-world scenarios, large-scale datasets corrupted with noisy labels are easy\nto collect, stimulating a great demand for automatic recognition in a\ngeneralized setting, i.e., weakly-supervised partial domain adaptation\n(WS-PDA), which transfers a classifier from a large source domain with noises\nin labels to a small unlabeled target domain. As such, the key issues of WS-PDA\nare: 1) how to sufficiently discover the knowledge from the noisy labeled\nsource domain and the unlabeled target domain, and 2) how to successfully adapt\nthe knowledge across domains. In this paper, we propose a simple yet effective\ndomain adaptation approach, termed as self-paced transfer classifier learning\n(SP-TCL), to address the above issues, which could be regarded as a\nwell-performing baseline for several generalized domain adaptation tasks. The\nproposed model is established upon the self-paced learning scheme, seeking a\npreferable classifier for the target domain. Specifically, SP-TCL learns to\ndiscover faithful knowledge via a carefully designed prudent loss function and\nsimultaneously adapts the learned knowledge to the target domain by iteratively\nexcluding source examples from training under the self-paced fashion. Extensive\nevaluations on several benchmark datasets demonstrate that SP-TCL significantly\noutperforms state-of-the-art approaches on several generalized domain\nadaptation tasks.",
          "arxiv_id": "2406.14274v1"
        },
        {
          "title": "Generation, augmentation, and alignment: A pseudo-source domain based method for source-free domain adaptation",
          "year": "2021-09",
          "abstract": "Conventional unsupervised domain adaptation (UDA) methods need to access both\nlabeled source samples and unlabeled target samples simultaneously to train the\nmodel. While in some scenarios, the source samples are not available for the\ntarget domain due to data privacy and safety. To overcome this challenge,\nrecently, source-free domain adaptation (SFDA) has attracted the attention of\nresearchers, where both a trained source model and unlabeled target samples are\ngiven. Existing SFDA methods either adopt a pseudo-label based strategy or\ngenerate more samples. However, these methods do not explicitly reduce the\ndistribution shift across domains, which is the key to a good adaptation.\nAlthough there are no source samples available, fortunately, we find that some\ntarget samples are very similar to the source domain and can be used to\napproximate the source domain. This approximated domain is denoted as the\npseudo-source domain. In this paper, inspired by this observation, we propose a\nnovel method based on the pseudo-source domain. The proposed method firstly\ngenerates and augments the pseudo-source domain, and then employs distribution\nalignment with four novel losses based on pseudo-label based strategy. Among\nthem, a domain adversarial loss is introduced between the pseudo-source domain\nthe remaining target domain to reduce the distribution shift. The results on\nthree real-world datasets verify the effectiveness of the proposed method.",
          "arxiv_id": "2109.04015v1"
        },
        {
          "title": "Unsupervised Domain Adaptation for Extra Features in the Target Domain Using Optimal Transport",
          "year": "2022-09",
          "abstract": "Domain adaptation aims to transfer knowledge of labeled instances obtained\nfrom a source domain to a target domain to fill the gap between the domains.\nMost domain adaptation methods assume that the source and target domains have\nthe same dimensionality. Methods that are applicable when the number of\nfeatures is different in each domain have rarely been studied, especially when\nno label information is given for the test data obtained from the target\ndomain. In this paper, it is assumed that common features exist in both domains\nand that extra (new additional) features are observed in the target domain;\nhence, the dimensionality of the target domain is higher than that of the\nsource domain. To leverage the homogeneity of the common features, the\nadaptation between these source and target domains is formulated as an optimal\ntransport (OT) problem. In addition, a learning bound in the target domain for\nthe proposed OT-based method is derived. The proposed algorithm is validated\nusing both simulated and real-world data.",
          "arxiv_id": "2209.04594v1"
        }
      ],
      "42": [
        {
          "title": "Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set",
          "year": "2022-06",
          "abstract": "The recent work ``Combinatorial Optimization with Physics-Inspired Graph\nNeural Networks'' [Nat Mach Intell 4 (2022) 367] introduces a physics-inspired\nunsupervised Graph Neural Network (GNN) to solve combinatorial optimization\nproblems on sparse graphs. To test the performances of these GNNs, the authors\nof the work show numerical results for two fundamental problems: maximum cut\nand maximum independent set (MIS). They conclude that \"the graph neural network\noptimizer performs on par or outperforms existing solvers, with the ability to\nscale beyond the state of the art to problems with millions of variables.\"\n  In this comment, we show that a simple greedy algorithm, running in almost\nlinear time, can find solutions for the MIS problem of much better quality than\nthe GNN. The greedy algorithm is faster by a factor of $10^4$ with respect to\nthe GNN for problems with a million variables. We do not see any good reason\nfor solving the MIS with these GNN, as well as for using a sledgehammer to\ncrack nuts.\n  In general, many claims of superiority of neural networks in solving\ncombinatorial problems are at risk of being not solid enough, since we lack\nstandard benchmarks based on really hard problems. We propose one of such hard\nbenchmarks, and we hope to see future neural network optimizers tested on these\nproblems before any claim of superiority is made.",
          "arxiv_id": "2206.13211v2"
        },
        {
          "title": "Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement Learning",
          "year": "2024-03",
          "abstract": "Quadratic Assignment Problem (QAP) is a practical combinatorial optimization\nproblems that has been studied for several years. Since it is NP-hard, solving\nlarge problem instances of QAP is challenging. Although heuristics can find\nsemi-optimal solutions, the execution time significantly increases as the\nproblem size increases. Recently, solving combinatorial optimization problems\nby deep learning has been attracting attention as a faster solver than\nheuristics. Even with deep learning, however, solving large QAP is still\nchallenging. In this paper, we propose the deep reinforcement learning model\ncalled the two-stage graph pointer network (GPN) for solving QAP. Two-stage GPN\nrelies on GPN, which has been proposed for Euclidean Traveling Salesman Problem\n(TSP). First, we extend GPN for general TSP, and then we add new algorithms to\nthat model for solving QAP. Our experimental results show that our two-stage\nGPN provides semi-optimal solutions for benchmark problem instances from TSPlib\nand QAPLIB.",
          "arxiv_id": "2404.00539v1"
        },
        {
          "title": "Solving Dynamic Graph Problems with Multi-Attention Deep Reinforcement Learning",
          "year": "2022-01",
          "abstract": "Graph problems such as traveling salesman problem, or finding minimal Steiner\ntrees are widely studied and used in data engineering and computer science.\nTypically, in real-world applications, the features of the graph tend to change\nover time, thus, finding a solution to the problem becomes challenging. The\ndynamic version of many graph problems are the key for a plethora of real-world\nproblems in transportation, telecommunication, and social networks. In recent\nyears, using deep learning techniques to find heuristic solutions for NP-hard\ngraph combinatorial problems has gained much interest as these learned\nheuristics can find near-optimal solutions efficiently. However, most of the\nexisting methods for learning heuristics focus on static graph problems. The\ndynamic nature makes NP-hard graph problems much more challenging to learn, and\nthe existing methods fail to find reasonable solutions.\n  In this paper, we propose a novel architecture named Graph Temporal Attention\nwith Reinforcement Learning (GTA-RL) to learn heuristic solutions for\ngraph-based dynamic combinatorial optimization problems. The GTA-RL\narchitecture consists of an encoder capable of embedding temporal features of a\ncombinatorial problem instance and a decoder capable of dynamically focusing on\nthe embedded features to find a solution to a given combinatorial problem\ninstance. We then extend our architecture to learn heuristics for the real-time\nversion of combinatorial optimization problems where all input features of a\nproblem are not known a prior, but rather learned in real-time. Our\nexperimental results against several state-of-the-art learning-based algorithms\nand optimal solvers demonstrate that our approach outperforms the\nstate-of-the-art learning-based approaches in terms of effectiveness and\noptimal solvers in terms of efficiency on dynamic and real-time graph\ncombinatorial optimization.",
          "arxiv_id": "2201.04895v1"
        }
      ],
      "43": [
        {
          "title": "Exoplanet Detection using Machine Learning",
          "year": "2020-11",
          "abstract": "We introduce a new machine learning based technique to detect exoplanets\nusing the transit method. Machine learning and deep learning techniques have\nproven to be broadly applicable in various scientific research areas. We aim to\nexploit some of these methods to improve the conventional algorithm based\napproaches presently used in astrophysics to detect exoplanets. Using the\ntime-series analysis library TSFresh to analyse light curves, we extracted 789\nfeatures from each curve, which capture the information about the\ncharacteristics of a light curve. We then used these features to train a\ngradient boosting classifier using the machine learning tool lightgbm. This\napproach was tested on simulated data, which showed that is more effective than\nthe conventional box least squares fitting (BLS) method. We further found that\nour method produced comparable results to existing state-of-the-art deep\nlearning models, while being much more computationally efficient and without\nneeding folded and secondary views of the light curves. For Kepler data, the\nmethod is able to predict a planet with an AUC of 0.948, so that 94.8 per cent\nof the true planet signals are ranked higher than non-planet signals. The\nresulting recall is 0.96, so that 96 per cent of real planets are classified as\nplanets. For the Transiting Exoplanet Survey Satellite (TESS) data, we found\nour method can classify light curves with an accuracy of 0.98, and is able to\nidentify planets with a recall of 0.82 at a precision of 0.63.",
          "arxiv_id": "2011.14135v2"
        },
        {
          "title": "Field Level Neural Network Emulator for Cosmological N-body Simulations",
          "year": "2022-06",
          "abstract": "We build a field level emulator for cosmic structure formation that is\naccurate in the nonlinear regime. Our emulator consists of two convolutional\nneural networks trained to output the nonlinear displacements and velocities of\nN-body simulation particles based on their linear inputs. Cosmology dependence\nis encoded in the form of style parameters at each layer of the neural network,\nenabling the emulator to effectively interpolate the outcomes of structure\nformation between different flat $\\Lambda$CDM cosmologies over a wide range of\nbackground matter densities. The neural network architecture makes the model\ndifferentiable by construction, providing a powerful tool for fast field level\ninference. We test the accuracy of our method by considering several summary\nstatistics, including the density power spectrum with and without redshift\nspace distortions, the displacement power spectrum, the momentum power\nspectrum, the density bispectrum, halo abundances, and halo profiles with and\nwithout redshift space distortions. We compare these statistics from our\nemulator with the full N-body results, the COLA method, and a fiducial neural\nnetwork with no cosmological dependence. We find our emulator gives accurate\nresults down to scales of $k \\sim 1\\ \\mathrm{Mpc}^{-1}\\, h$, representing a\nconsiderable improvement over both COLA and the fiducial neural network. We\nalso demonstrate that our emulator generalizes well to initial conditions\ncontaining primordial non-Gaussianity, without the need for any additional\nstyle parameters or retraining.",
          "arxiv_id": "2206.04594v2"
        },
        {
          "title": "syren-new: Precise formulae for the linear and nonlinear matter power spectra with massive neutrinos and dynamical dark energy",
          "year": "2024-10",
          "abstract": "Current and future large scale structure surveys aim to constrain the\nneutrino mass and the equation of state of dark energy. We aim to construct\naccurate and interpretable symbolic approximations to the linear and nonlinear\nmatter power spectra as a function of cosmological parameters in extended\n$\\Lambda$CDM models which contain massive neutrinos and non-constant equations\nof state for dark energy. This constitutes an extension of the syren-halofit\nemulators to incorporate these two effects, which we call syren-new\n(SYmbolic-Regression-ENhanced power spectrum emulator with NEutrinos and\n$W_0-w_a$). We also obtain a simple approximation to the derived parameter\n$\\sigma_8$ as a function of the cosmological parameters for these models. Our\nresults for the linear power spectrum are designed to emulate CLASS, whereas\nfor the nonlinear case we aim to match the results of EuclidEmulator2. We\ncompare our results to existing emulators and $N$-body simulations. Our\nanalytic emulators for $\\sigma_8$, the linear and nonlinear power spectra\nachieve root mean squared errors of 0.1%, 0.3% and 1.3%, respectively, across a\nwide range of cosmological parameters, redshifts and wavenumbers. We verify\nthat emulator-related discrepancies are subdominant compared to observational\nerrors and other modelling uncertainties when computing shear power spectra for\nLSST-like surveys. Our expressions have similar accuracy to existing\n(numerical) emulators, but are at least an order of magnitude faster, both on a\nCPU and GPU. Our work greatly improves the accuracy, speed and range of\napplicability of current symbolic approximations to the linear and nonlinear\nmatter power spectra. We provide publicly available code for all symbolic\napproximations found.",
          "arxiv_id": "2410.14623v1"
        }
      ],
      "44": [
        {
          "title": "Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes",
          "year": "2023-08",
          "abstract": "We characterize learnability for quantum measurement classes by establishing\nmatching necessary and sufficient conditions for their PAC learnability, along\nwith corresponding sample complexity bounds, in the setting where the learner\nis given access only to prepared quantum states. We first probe the results\nfrom previous works on this setting. We show that the empirical risk defined in\nprevious works and matching the definition in the classical theory fails to\nsatisfy the uniform convergence property enjoyed in the classical setting for\nsome learnable classes. Moreover, we show that VC dimension generalization\nupper bounds in previous work are frequently infinite, even for\nfinite-dimensional POVM classes. To surmount the failure of the standard ERM to\nsatisfy uniform convergence, we define a new learning rule -- denoised ERM. We\nshow this to be a universal learning rule for POVM and probabilistically\nobserved concept classes, and the condition for it to satisfy uniform\nconvergence is finite fat shattering dimension of the class. We give\nquantitative sample complexity upper and lower bounds for learnability in terms\nof finite fat-shattering dimension and a notion of approximate finite\npartitionability into approximately jointly measurable subsets, which allow for\nsample reuse. We then show that finite fat shattering dimension implies finite\ncoverability by approximately jointly measurable subsets, leading to our\nmatching conditions. We also show that every measurement class defined on a\nfinite-dimensional Hilbert space is PAC learnable. We illustrate our results on\nseveral example POVM classes.",
          "arxiv_id": "2308.12304v1"
        },
        {
          "title": "PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes",
          "year": "2021-06",
          "abstract": "We give a novel, unified derivation of conditional PAC-Bayesian and mutual\ninformation (MI) generalization bounds. We derive conditional MI bounds as an\ninstance, with special choice of prior, of conditional MAC-Bayesian (Mean\nApproximately Correct) bounds, itself derived from conditional PAC-Bayesian\nbounds, where `conditional' means that one can use priors conditioned on a\njoint training and ghost sample. This allows us to get nontrivial PAC-Bayes and\nMI-style bounds for general VC classes, something recently shown to be\nimpossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get\nfaster rates of order $O \\left(({\\text{KL}}/n)^{\\gamma}\\right)$ for $\\gamma >\n1/2$ if a Bernstein condition holds and for exp-concave losses (with\n$\\gamma=1$), which is impossible with both standard PAC-Bayes generalization\nand MI bounds. Our work extends the recent work by Steinke and Zakynthinou\n[2020] who handle MI with VC but neither PAC-Bayes nor fast rates, the recent\nwork of Hellstr\\\"om and Durisi [2020] who extend the latter to the PAC-Bayes\nsetting via a unifying exponential inequality, and Mhammedi et al. [2019] who\ninitiated fast rate PAC-Bayes generalization error bounds but handle neither MI\nnor general VC classes.",
          "arxiv_id": "2106.09683v1"
        },
        {
          "title": "Proper Learnability and the Role of Unlabeled Data",
          "year": "2025-02",
          "abstract": "Proper learning refers to the setting in which learners must emit predictors\nin the underlying hypothesis class $H$, and often leads to learners with simple\nalgorithmic forms (e.g. empirical risk minimization (ERM), structural risk\nminimization (SRM)). The limitation of proper learning, however, is that there\nexist problems which can only be learned improperly, e.g. in multiclass\nclassification. Thus, we ask: Under what assumptions on the hypothesis class or\nthe information provided to the learner is a problem properly learnable? We\nfirst demonstrate that when the unlabeled data distribution is given, there\nalways exists an optimal proper learner governed by distributional\nregularization, a randomized generalization of regularization. We refer to this\nsetting as the distribution-fixed PAC model, and continue to evaluate the\nlearner on its worst-case performance over all distributions. Our result holds\nfor all metric loss functions and any finite learning problem (with no\ndependence on its size). Further, we demonstrate that sample complexities in\nthe distribution-fixed PAC model can shrink by only a logarithmic factor from\nthe classic PAC model, strongly refuting the role of unlabeled data in PAC\nlearning (from a worst-case perspective).\n  We complement this with impossibility results which obstruct any\ncharacterization of proper learnability in the realizable PAC model. First, we\nobserve that there are problems whose proper learnability is logically\nundecidable, i.e., independent of the ZFC axioms. We then show that proper\nlearnability is not a monotone property of the underlying hypothesis class, and\nthat it is not a local property (in a precise sense). Our impossibility results\nall hold even for the fundamental setting of multiclass classification, and go\nthrough a reduction of EMX learning (Ben-David et al., 2019) to proper\nclassification which may be of independent interest.",
          "arxiv_id": "2502.10359v1"
        }
      ],
      "45": [
        {
          "title": "BioGrad: Biologically Plausible Gradient-Based Learning for Spiking Neural Networks",
          "year": "2021-10",
          "abstract": "Spiking neural networks (SNN) are delivering energy-efficient, massively\nparallel, and low-latency solutions to AI problems, facilitated by the emerging\nneuromorphic chips. To harness these computational benefits, SNN need to be\ntrained by learning algorithms that adhere to brain-inspired neuromorphic\nprinciples, namely event-based, local, and online computations. Yet, the\nstate-of-the-art SNN training algorithms are based on backprop that does not\nfollow the above principles. Due to its limited biological plausibility, the\napplication of backprop to SNN requires non-local feedback pathways for\ntransmitting continuous-valued errors, and relies on gradients from future\ntimesteps. The introduction of biologically plausible modifications to backprop\nhas helped overcome several of its limitations, but limits the degree to which\nbackprop is approximated, which hinders its performance. We propose a\nbiologically plausible gradient-based learning algorithm for SNN that is\nfunctionally equivalent to backprop, while adhering to all three neuromorphic\nprinciples. We introduced multi-compartment spiking neurons with local\neligibility traces to compute the gradients required for learning, and a\nperiodic \"sleep\" phase to further improve the approximation to backprop during\nwhich a local Hebbian rule aligns the feedback and feedforward weights. Our\nmethod achieved the same level of performance as backprop with multi-layer\nfully connected SNN on MNIST (98.13%) and the event-based N-MNIST (97.59%)\ndatasets. We deployed our learning algorithm on Intel's Loihi to train a\n1-hidden-layer network for MNIST, and obtained 93.32% test accuracy while\nconsuming 400 times less energy per training sample than BioGrad on GPU. Our\nwork shows that optimal learning is feasible in neuromorphic computing, and\nfurther pursuing its biological plausibility can better capture the benefits of\nthis emerging computing paradigm.",
          "arxiv_id": "2110.14092v1"
        },
        {
          "title": "Co-learning synaptic delays, weights and adaptation in spiking neural networks",
          "year": "2023-09",
          "abstract": "Spiking neural networks (SNN) distinguish themselves from artificial neural\nnetworks (ANN) because of their inherent temporal processing and spike-based\ncomputations, enabling a power-efficient implementation in neuromorphic\nhardware. In this paper, we demonstrate that data processing with spiking\nneurons can be enhanced by co-learning the connection weights with two other\nbiologically inspired neuronal features: 1) a set of parameters describing\nneuronal adaptation processes and 2) synaptic propagation delays. The former\nallows the spiking neuron to learn how to specifically react to incoming spikes\nbased on its past. The trained adaptation parameters result in neuronal\nheterogeneity, which is found in the brain and also leads to a greater variety\nin available spike patterns. The latter enables to learn to explicitly\ncorrelate patterns that are temporally distanced. Synaptic delays reflect the\ntime an action potential requires to travel from one neuron to another. We show\nthat each of the co-learned features separately leads to an improvement over\nthe baseline SNN and that the combination of both leads to state-of-the-art SNN\nresults on all speech recognition datasets investigated with a simple 2-hidden\nlayer feed-forward network. Our SNN outperforms the ANN on the neuromorpic\ndatasets (Spiking Heidelberg Digits and Spiking Speech Commands), even with\nfewer trainable parameters. On the 35-class Google Speech Commands dataset, our\nSNN also outperforms a GRU of similar size. Our work presents brain-inspired\nimprovements to SNN that enable them to excel over an equivalent ANN of similar\nsize on tasks with rich temporal dynamics.",
          "arxiv_id": "2311.16112v1"
        },
        {
          "title": "Boost Event-Driven Tactile Learning with Location Spiking Neurons",
          "year": "2022-10",
          "abstract": "Tactile sensing is essential for a variety of daily tasks. And recent\nadvances in event-driven tactile sensors and Spiking Neural Networks (SNNs)\nspur the research in related fields. However, SNN-enabled event-driven tactile\nlearning is still in its infancy due to the limited representation abilities of\nexisting spiking neurons and high spatio-temporal complexity in the\nevent-driven tactile data. In this paper, to improve the representation\ncapability of existing spiking neurons, we propose a novel neuron model called\n\"location spiking neuron\", which enables us to extract features of event-based\ndata in a novel way. Specifically, based on the classical Time Spike Response\nModel (TSRM), we develop the Location Spike Response Model (LSRM). In addition,\nbased on the most commonly-used Time Leaky Integrate-and-Fire (TLIF) model, we\ndevelop the Location Leaky Integrate-and-Fire (LLIF) model. Moreover, to\ndemonstrate the representation effectiveness of our proposed neurons and\ncapture the complex spatio-temporal dependencies in the event-driven tactile\ndata, we exploit the location spiking neurons to propose two hybrid models for\nevent-driven tactile learning. Specifically, the first hybrid model combines a\nfully-connected SNN with TSRM neurons and a fully-connected SNN with LSRM\nneurons. And the second hybrid model fuses the spatial spiking graph neural\nnetwork with TLIF neurons and the temporal spiking graph neural network with\nLLIF neurons. Extensive experiments demonstrate the significant improvements of\nour models over the state-of-the-art methods on event-driven tactile learning.\nMoreover, compared to the counterpart artificial neural networks (ANNs), our\nSNN models are 10x to 100x energy-efficient, which shows the superior energy\nefficiency of our models and may bring new opportunities to the spike-based\nlearning community and neuromorphic engineering.",
          "arxiv_id": "2210.04277v3"
        }
      ],
      "46": [
        {
          "title": "Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition",
          "year": "2024-01",
          "abstract": "WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nenables contactless, long-range sensing in spatially constrained environments\nwhile preserving visual privacy. However, despite the presence of numerous\nWiFi-enabled devices around us, few expose CSI to users, resulting in a lack of\nsensing hardware options. Variants of the Espressif ESP32 have emerged as\npotential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this\nwork, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for\ntheir ability to facilitate long-range through-wall HAR. Two promising systems\nare proposed, one of which combines the ESP32-S3 with a directional biquad\nantenna. This combination represents, to the best of our knowledge, the first\ndemonstration of such a system in WiFi-based HAR. The second system relies on\nthe built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves\ndirectionality through a plane reflector. In a comprehensive evaluation of\nline-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems\nare deployed in an office environment spanning a distance of 18 meters across\nfive rooms. In this experimental setup, the Wallhack1.8k dataset, comprising\n1806 CSI amplitude spectrograms of human activities, is collected and made\npublicly available. Based on Wallhack1.8k, we train activity recognition models\nusing the EfficientNetV2 architecture to assess system performance in LOS and\nNLOS scenarios. For the core NLOS activity recognition problem, the biquad\nantenna and PIFA-based systems achieve accuracies of 92.0$\\pm$3.5 and\n86.8$\\pm$4.7, respectively, demonstrating the feasibility of long-range\nthrough-wall HAR with the proposed systems.",
          "arxiv_id": "2401.01388v1"
        },
        {
          "title": "HAR-GCNN: Deep Graph CNNs for Human Activity Recognition From Highly Unlabeled Mobile Sensor Data",
          "year": "2022-03",
          "abstract": "The problem of human activity recognition from mobile sensor data applies to\nmultiple domains, such as health monitoring, personal fitness, daily life\nlogging, and senior care. A critical challenge for training human activity\nrecognition models is data quality. Acquiring balanced datasets containing\naccurate activity labels requires humans to correctly annotate and potentially\ninterfere with the subjects' normal activities in real-time. Despite the\nlikelihood of incorrect annotation or lack thereof, there is often an inherent\nchronology to human behavior. For example, we take a shower after we exercise.\nThis implicit chronology can be used to learn unknown labels and classify\nfuture activities. In this work, we propose HAR-GCCN, a deep graph CNN model\nthat leverages the correlation between chronologically adjacent sensor\nmeasurements to predict the correct labels for unclassified activities that\nhave at least one activity label. We propose a new training strategy enforcing\nthat the model predicts the missing activity labels by leveraging the known\nones. HAR-GCCN shows superior performance relative to previously used baseline\nmethods, improving classification accuracy by about 25% and up to 68% on\ndifferent datasets. Code is available at\n\\url{https://github.com/abduallahmohamed/HAR-GCNN}.",
          "arxiv_id": "2203.03087v1"
        },
        {
          "title": "Overview of Human Activity Recognition Using Sensor Data",
          "year": "2023-09",
          "abstract": "Human activity recognition (HAR) is an essential research field that has been\nused in different applications including home and workplace automation,\nsecurity and surveillance as well as healthcare. Starting from conventional\nmachine learning methods to the recently developing deep learning techniques\nand the Internet of things, significant contributions have been shown in the\nHAR area in the last decade. Even though several review and survey studies have\nbeen published, there is a lack of sensor-based HAR overview studies focusing\non summarising the usage of wearable sensors and smart home sensors data as\nwell as applications of HAR and deep learning techniques. Hence, we overview\nsensor-based HAR, discuss several important applications that rely on HAR, and\nhighlight the most common machine learning methods that have been used for HAR.\nFinally, several challenges of HAR are explored that should be addressed to\nfurther improve the robustness of HAR.",
          "arxiv_id": "2309.07170v1"
        }
      ],
      "47": [
        {
          "title": "TSRNet: Simple Framework for Real-time ECG Anomaly Detection with Multimodal Time and Spectrogram Restoration Network",
          "year": "2023-12",
          "abstract": "The electrocardiogram (ECG) is a valuable signal used to assess various\naspects of heart health, such as heart rate and rhythm. It plays a crucial role\nin identifying cardiac conditions and detecting anomalies in ECG data. However,\ndistinguishing between normal and abnormal ECG signals can be a challenging\ntask. In this paper, we propose an approach that leverages anomaly detection to\nidentify unhealthy conditions using solely normal ECG data for training.\nFurthermore, to enhance the information available and build a robust system, we\nsuggest considering both the time series and time-frequency domain aspects of\nthe ECG signal. As a result, we introduce a specialized network called the\nMultimodal Time and Spectrogram Restoration Network (TSRNet) designed\nspecifically for detecting anomalies in ECG signals. TSRNet falls into the\ncategory of restoration-based anomaly detection and draws inspiration from both\nthe time series and spectrogram domains. By extracting representations from\nboth domains, TSRNet effectively captures the comprehensive characteristics of\nthe ECG signal. This approach enables the network to learn robust\nrepresentations with superior discrimination abilities, allowing it to\ndistinguish between normal and abnormal ECG patterns more effectively.\nFurthermore, we introduce a novel inference method, termed Peak-based Error,\nthat specifically focuses on ECG peaks, a critical component in detecting\nabnormalities. The experimental result on the large-scale dataset PTB-XL has\ndemonstrated the effectiveness of our approach in ECG anomaly detection, while\nalso prioritizing efficiency by minimizing the number of trainable parameters.\nOur code is available at https://github.com/UARK-AICV/TSRNet.",
          "arxiv_id": "2312.10187v2"
        },
        {
          "title": "ETP: Learning Transferable ECG Representations via ECG-Text Pre-training",
          "year": "2023-09",
          "abstract": "In the domain of cardiovascular healthcare, the Electrocardiogram (ECG)\nserves as a critical, non-invasive diagnostic tool. Although recent strides in\nself-supervised learning (SSL) have been promising for ECG representation\nlearning, these techniques often require annotated samples and struggle with\nclasses not present in the fine-tuning stages. To address these limitations, we\nintroduce ECG-Text Pre-training (ETP), an innovative framework designed to\nlearn cross-modal representations that link ECG signals with textual reports.\nFor the first time, this framework leverages the zero-shot classification task\nin the ECG domain. ETP employs an ECG encoder along with a pre-trained language\nmodel to align ECG signals with their corresponding textual reports. The\nproposed framework excels in both linear evaluation and zero-shot\nclassification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets,\nshowcasing its ability for robust and generalizable cross-modal ECG feature\nlearning.",
          "arxiv_id": "2309.07145v1"
        },
        {
          "title": "Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model",
          "year": "2025-02",
          "abstract": "Electrocardiogram (ECG) is essential for the clinical diagnosis of\narrhythmias and other heart diseases, but deep learning methods based on ECG\noften face limitations due to the need for high-quality annotations. Although\nprevious ECG self-supervised learning (eSSL) methods have made significant\nprogress in representation learning from unannotated ECG data, they typically\ntreat ECG signals as ordinary time-series data, segmenting the signals using\nfixed-size and fixed-step time windows, which often ignore the form and rhythm\ncharacteristics and latent semantic relationships in ECG signals. In this work,\nwe introduce a novel perspective on ECG signals, treating heartbeats as words\nand rhythms as sentences. Based on this perspective, we first designed the\nQRS-Tokenizer, which generates semantically meaningful ECG sentences from the\nraw ECG signals. Building on these, we then propose HeartLang, a novel\nself-supervised learning framework for ECG language processing, learning\ngeneral representations at form and rhythm levels. Additionally, we construct\nthe largest heartbeat-based ECG vocabulary to date, which will further advance\nthe development of ECG language processing. We evaluated HeartLang across six\npublic ECG datasets, where it demonstrated robust competitiveness against other\neSSL methods. Our data and code are publicly available at\nhttps://github.com/PKUDigitalHealth/HeartLang.",
          "arxiv_id": "2502.10707v1"
        }
      ],
      "48": [
        {
          "title": "EMO: Episodic Memory Optimization for Few-Shot Meta-Learning",
          "year": "2023-06",
          "abstract": "Few-shot meta-learning presents a challenge for gradient descent optimization\ndue to the limited number of training samples per task. To address this issue,\nwe propose an episodic memory optimization for meta-learning, we call EMO,\nwhich is inspired by the human ability to recall past learning experiences from\nthe brain's memory. EMO retains the gradient history of past experienced tasks\nin external memory, enabling few-shot learning in a memory-augmented way. By\nlearning to retain and recall the learning process of past training tasks, EMO\nnudges parameter updates in the right direction, even when the gradients\nprovided by a limited number of examples are uninformative. We prove\ntheoretically that our algorithm converges for smooth, strongly convex\nobjectives. EMO is generic, flexible, and model-agnostic, making it a simple\nplug-and-play optimizer that can be seamlessly embedded into existing\noptimization-based few-shot meta-learning approaches. Empirical results show\nthat EMO scales well with most few-shot classification benchmarks and improves\nthe performance of optimization-based meta-learning methods, resulting in\naccelerated convergence.",
          "arxiv_id": "2306.05189v3"
        },
        {
          "title": "Variable-Shot Adaptation for Online Meta-Learning",
          "year": "2020-12",
          "abstract": "Few-shot meta-learning methods consider the problem of learning new tasks\nfrom a small, fixed number of examples, by meta-learning across static data\nfrom a set of previous tasks. However, in many real world settings, it is more\nnatural to view the problem as one of minimizing the total amount of\nsupervision --- both the number of examples needed to learn a new task and the\namount of data needed for meta-learning. Such a formulation can be studied in a\nsequential learning setting, where tasks are presented in sequence. When\nstudying meta-learning in this online setting, a critical question arises: can\nmeta-learning improve over the sample complexity and regret of standard\nempirical risk minimization methods, when considering both meta-training and\nadaptation together? The answer is particularly non-obvious for meta-learning\nalgorithms with complex bi-level optimizations that may demand large amounts of\nmeta-training data. To answer this question, we extend previous meta-learning\nalgorithms to handle the variable-shot settings that naturally arise in\nsequential learning: from many-shot learning at the start, to zero-shot\nlearning towards the end. On sequential learning problems, we find that\nmeta-learning solves the full task set with fewer overall labels and achieves\ngreater cumulative performance, compared to standard supervised methods. These\nresults suggest that meta-learning is an important ingredient for building\nlearning systems that continuously learn and improve over a sequence of\nproblems.",
          "arxiv_id": "2012.07769v1"
        },
        {
          "title": "Few-Shot Learning with a Strong Teacher",
          "year": "2021-07",
          "abstract": "Few-shot learning (FSL) aims to generate a classifier using limited labeled\nexamples. Many existing works take the meta-learning approach, constructing a\nfew-shot learner that can learn from few-shot examples to generate a\nclassifier. Typically, the few-shot learner is constructed or meta-trained by\nsampling multiple few-shot tasks in turn and optimizing the few-shot learner's\nperformance in generating classifiers for those tasks. The performance is\nmeasured by how well the resulting classifiers classify the test (i.e., query)\nexamples of those tasks. In this paper, we point out two potential weaknesses\nof this approach. First, the sampled query examples may not provide sufficient\nsupervision for meta-training the few-shot learner. Second, the effectiveness\nof meta-learning diminishes sharply with the increasing number of shots. To\nresolve these issues, we propose a novel meta-training objective for the\nfew-shot learner, which is to encourage the few-shot learner to generate\nclassifiers that perform like strong classifiers. Concretely, we associate each\nsampled few-shot task with a strong classifier, which is trained with ample\nlabeled examples. The strong classifiers can be seen as the target classifiers\nthat we hope the few-shot learner to generate given few-shot examples, and we\nuse the strong classifiers to supervise the few-shot learner. We present an\nefficient way to construct the strong classifier, making our proposed objective\nan easily plug-and-play term to existing meta-learning based FSL methods. We\nvalidate our approach, LastShot, in combinations with many representative\nmeta-learning methods. On several benchmark datasets, our approach leads to a\nnotable improvement across a variety of tasks. More importantly, with our\napproach, meta-learning based FSL methods can outperform non-meta-learning\nbased methods at different numbers of shots.",
          "arxiv_id": "2107.00197v2"
        }
      ],
      "49": [
        {
          "title": "BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation",
          "year": "2025-08",
          "abstract": "Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely\nused to study human brain activity. fMRI signals in areas across the brain\ntransiently synchronise and desynchronise their activity in a highly structured\nmanner, even when an individual is at rest. These functional connectivity\ndynamics may be related to behaviour and neuropsychiatric disease. To model\nthese dynamics, temporal brain connectivity representations are essential, as\nthey reflect evolving interactions between brain regions and provide insight\ninto transient neural states and network reconfigurations. However,\nconventional graph neural networks (GNNs) often struggle to capture long-range\ntemporal dependencies in dynamic fMRI data. To address this challenge, we\npropose BrainATCL, an unsupervised, nonparametric framework for adaptive\ntemporal brain connectivity learning, enabling functional link prediction and\nage estimation. Our method dynamically adjusts the lookback window for each\nsnapshot based on the rate of newly added edges. Graph sequences are\nsubsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal\nrepresentations of dynamic functional connectivity in resting-state fMRI data\nof 1,000 participants from the Human Connectome Project. To further improve\nspatial modeling, we incorporate brain structure and function-informed edge\nattributes, i.e., the left/right hemispheric identity and subnetwork membership\nof brain regions, enabling the model to capture biologically meaningful\ntopological patterns. We evaluate our BrainATCL on two tasks: functional link\nprediction and age estimation. The experimental results demonstrate superior\nperformance and strong generalization, including in cross-session prediction\nscenarios.",
          "arxiv_id": "2508.07106v1"
        },
        {
          "title": "Recurrent Brain Graph Mapper for Predicting Time-Dependent Brain Graph Evaluation Trajectory",
          "year": "2021-10",
          "abstract": "Several brain disorders can be detected by observing alterations in the\nbrain's structural and functional connectivities. Neurological findings suggest\nthat early diagnosis of brain disorders, such as mild cognitive impairment\n(MCI), can prevent and even reverse its development into Alzheimer's disease\n(AD). In this context, recent studies aimed to predict the evolution of brain\nconnectivities over time by proposing machine learning models that work on\nbrain images. However, such an approach is costly and time-consuming. Here, we\npropose to use brain connectivities as a more efficient alternative for\ntime-dependent brain disorder diagnosis by regarding the brain as instead a\nlarge interconnected graph characterizing the interconnectivity scheme between\nseveral brain regions. We term our proposed method Recurrent Brain Graph Mapper\n(RBGM), a novel efficient edge-based recurrent graph neural network that\npredicts the time-dependent evaluation trajectory of a brain graph from a\nsingle baseline. Our RBGM contains a set of recurrent neural network-inspired\nmappers for each time point, where each mapper aims to project the ground-truth\nbrain graph onto its next time point. We leverage the teacher forcing method to\nboost training and improve the evolved brain graph quality. To maintain the\ntopological consistency between the predicted brain graphs and their\ncorresponding ground-truth brain graphs at each time point, we further\nintegrate a topological loss. We also use l1 loss to capture time-dependency\nand minimize the distance between the brain graph at consecutive time points\nfor regularization. Benchmarks against several variants of RBGM and\nstate-of-the-art methods prove that we can achieve the same accuracy in\npredicting brain graph evolution more efficiently, paving the way for novel\ngraph neural network architecture and a highly efficient training scheme.",
          "arxiv_id": "2110.11237v1"
        },
        {
          "title": "Multi-Resolution Graph Analysis of Dynamic Brain Network for Classification of Alzheimer's Disease and Mild Cognitive Impairment",
          "year": "2024-09",
          "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder marked by memory\nloss and cognitive decline, making early detection vital for timely\nintervention. However, early diagnosis is challenging due to the heterogeneous\npresentation of symptoms. Resting-state functional magnetic resonance imaging\n(rs-fMRI) captures spontaneous brain activity and functional connectivity,\nwhich are known to be disrupted in AD and mild cognitive impairment (MCI).\nTraditional methods, such as Pearson's correlation, have been used to calculate\nassociation matrices, but these approaches often overlook the dynamic and\nnon-stationary nature of brain activity. In this study, we introduce a novel\nmethod that integrates discrete wavelet transform (DWT) and graph theory to\nmodel the dynamic behavior of brain networks. Our approach captures the\ntime-frequency representation of brain activity, allowing for a more nuanced\nanalysis of the underlying network dynamics. Machine learning was employed to\nautomate the discrimination of different stages of AD based on learned patterns\nfrom brain network at different frequency bands. We applied our method to a\ndataset of rs-fMRI images from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI) database, demonstrating its potential as an early diagnostic tool for AD\nand for monitoring disease progression. Our statistical analysis identifies\nspecific brain regions and connections that are affected in AD and MCI, at\ndifferent frequency bands, offering deeper insights into the disease's impact\non brain function.",
          "arxiv_id": "2409.04072v2"
        }
      ],
      "50": [
        {
          "title": "Active ML for 6G: Towards Efficient Data Generation, Acquisition, and Annotation",
          "year": "2024-06",
          "abstract": "This paper explores the integration of active machine learning (ML) for 6G\nnetworks, an area that remains under-explored yet holds potential. Unlike\npassive ML systems, active ML can be made to interact with the network\nenvironment. It actively selects informative and representative data points for\ntraining, thereby reducing the volume of data needed while accelerating the\nlearning process. While active learning research mainly focuses on data\nannotation, we call for a network-centric active learning framework that\nconsiders both annotation (i.e., what is the label) and data acquisition (i.e.,\nwhich and how many samples to collect). Moreover, we explore the synergy\nbetween generative artificial intelligence (AI) and active learning to overcome\nexisting limitations in both active learning and generative AI. This paper also\nfeatures a case study on a mmWave throughput prediction problem to demonstrate\nthe practical benefits and improved performance of active learning for 6G\nnetworks. Furthermore, we discuss how the implications of active learning\nextend to numerous 6G network use cases. We highlight the potential of active\nlearning based 6G networks to enhance computational efficiency, data annotation\nand acquisition efficiency, adaptability, and overall network intelligence. We\nconclude with a discussion on challenges and future research directions for\nactive learning in 6G networks, including development of novel query\nstrategies, distributed learning integration, and inclusion of human- and\nmachine-in-the-loop learning.",
          "arxiv_id": "2406.03630v1"
        },
        {
          "title": "Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets",
          "year": "2023-07",
          "abstract": "Active learning improves the performance of machine learning methods by\njudiciously selecting a limited number of unlabeled data points to query for\nlabels, with the aim of maximally improving the underlying classifier's\nperformance. Recent gains have been made using sequential active learning for\nsynthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration,\nsequential active learning selects a query set of size one while batch active\nlearning selects a query set of multiple datapoints. While batch active\nlearning methods exhibit greater efficiency, the challenge lies in maintaining\nmodel accuracy relative to sequential active learning methods. We developed a\nnovel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set\n(DAC) for core-set generation and LocalMax for batch sampling. The batch active\nlearning process that combines DAC and LocalMax achieves nearly identical\naccuracy as sequential active learning but is more efficient, proportional to\nthe batch size. As an application, a pipeline is built based on transfer\nlearning feature embedding, graph learning, DAC, and LocalMax to classify the\nFUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the\nstate-of-the-art CNN-based methods.",
          "arxiv_id": "2307.10495v1"
        },
        {
          "title": "Deep Active Learning for Computer Vision: Past and Future",
          "year": "2022-11",
          "abstract": "As an important data selection schema, active learning emerges as the\nessential component when iterating an Artificial Intelligence (AI) model. It\nbecomes even more critical given the dominance of deep neural network based\nmodels, which are composed of a large number of parameters and data hungry, in\napplication. Despite its indispensable role for developing AI models, research\non active learning is not as intensive as other research directions. In this\npaper, we present a review of active learning through deep active learning\napproaches from the following perspectives: 1) technical advancements in active\nlearning, 2) applications of active learning in computer vision, 3) industrial\nsystems leveraging or with potential to leverage active learning for data\niteration, 4) current limitations and future research directions. We expect\nthis paper to clarify the significance of active learning in a modern AI model\nmanufacturing process and to bring additional research attention to active\nlearning. By addressing data automation challenges and coping with automated\nmachine learning systems, active learning will facilitate democratization of AI\ntechnologies by boosting model production at scale.",
          "arxiv_id": "2211.14819v2"
        }
      ],
      "51": [
        {
          "title": "SCCAM: Supervised Contrastive Convolutional Attention Mechanism for Ante-hoc Interpretable Fault Diagnosis with Limited Fault Samples",
          "year": "2023-02",
          "abstract": "In real industrial processes, fault diagnosis methods are required to learn\nfrom limited fault samples since the procedures are mainly under normal\nconditions and the faults rarely occur. Although attention mechanisms have\nbecome popular in the field of fault diagnosis, the existing attention-based\nmethods are still unsatisfying for the above practical applications. First,\npure attention-based architectures like transformers need a large number of\nfault samples to offset the lack of inductive biases thus performing poorly\nunder limited fault samples. Moreover, the poor fault classification dilemma\nfurther leads to the failure of the existing attention-based methods to\nidentify the root causes. To address the aforementioned issues, we innovatively\npropose a supervised contrastive convolutional attention mechanism (SCCAM) with\nante-hoc interpretability, which solves the root cause analysis problem under\nlimited fault samples for the first time. The proposed SCCAM method is tested\non a continuous stirred tank heater and the Tennessee Eastman industrial\nprocess benchmark. Three common fault diagnosis scenarios are covered,\nincluding a balanced scenario for additional verification and two scenarios\nwith limited fault samples (i.e., imbalanced scenario and long-tail scenario).\nThe comprehensive results demonstrate that the proposed SCCAM method can\nachieve better performance compared with the state-of-the-art methods on fault\nclassification and root cause analysis.",
          "arxiv_id": "2302.01599v2"
        },
        {
          "title": "Synthesizing Rolling Bearing Fault Samples in New Conditions: A framework based on a modified CGAN",
          "year": "2022-06",
          "abstract": "Bearings are one of the vital components of rotating machines that are prone\nto unexpected faults. Therefore, bearing fault diagnosis and condition\nmonitoring is essential for reducing operational costs and downtime in numerous\nindustries. In various production conditions, bearings can be operated under a\nrange of loads and speeds, which causes different vibration patterns associated\nwith each fault type. Normal data is ample as systems usually work in desired\nconditions. On the other hand, fault data is rare, and in many conditions,\nthere is no data recorded for the fault classes. Accessing fault data is\ncrucial for developing data-driven fault diagnosis tools that can improve both\nthe performance and safety of operations. To this end, a novel algorithm based\non Conditional Generative Adversarial Networks (CGANs) is introduced. Trained\non the normal and fault data on any actual fault conditions, this algorithm\ngenerates fault data from normal data of target conditions. The proposed method\nis validated on a real-world bearing dataset, and fault data are generated for\ndifferent conditions. Several state-of-the-art classifiers and visualization\nmodels are implemented to evaluate the quality of the synthesized data. The\nresults demonstrate the efficacy of the proposed algorithm.",
          "arxiv_id": "2206.12076v3"
        },
        {
          "title": "Data-Driven Fault Diagnosis Analysis and Open-Set Classification of Time-Series Data",
          "year": "2020-09",
          "abstract": "Fault diagnosis of dynamic systems is done by detecting changes in\ntime-series data, for example residuals, caused by system degradation and\nfaulty components. The use of general-purpose multi-class classification\nmethods for fault diagnosis is complicated by imbalanced training data and\nunknown fault classes. Another complicating factor is that different fault\nclasses can result in similar residual outputs, especially for small faults,\nwhich causes classification ambiguities. In this work, a framework for\ndata-driven analysis and open-set classification is developed for fault\ndiagnosis applications using the Kullback-Leibler divergence. A data-driven\nfault classification algorithm is proposed which can handle imbalanced\ndatasets, class overlapping, and unknown faults. In addition, an algorithm is\nproposed to estimate the size of the fault when training data contains\ninformation from known fault realizations. An advantage of the proposed\nframework is that it can also be used for quantitative analysis of fault\ndiagnosis performance, for example, to analyze how easy it is to classify\nfaults of different magnitudes. To evaluate the usefulness of the proposed\nmethods, multiple datasets from different fault scenarios have been collected\nfrom an internal combustion engine test bench to illustrate the design process\nof a data-driven diagnosis system, including quantitative fault diagnosis\nanalysis and evaluation of the developed open set fault classification\nalgorithm.",
          "arxiv_id": "2009.04756v2"
        }
      ],
      "52": [
        {
          "title": "High Dimensional Bayesian Optimization with Kernel Principal Component Analysis",
          "year": "2022-04",
          "abstract": "Bayesian Optimization (BO) is a surrogate-based global optimization strategy\nthat relies on a Gaussian Process regression (GPR) model to approximate the\nobjective function and an acquisition function to suggest candidate points. It\nis well-known that BO does not scale well for high-dimensional problems because\nthe GPR model requires substantially more data points to achieve sufficient\naccuracy and acquisition optimization becomes computationally expensive in high\ndimensions. Several recent works aim at addressing these issues, e.g., methods\nthat implement online variable selection or conduct the search on a\nlower-dimensional sub-manifold of the original search space. Advancing our\nprevious work of PCA-BO that learns a linear sub-manifold, this paper proposes\na novel kernel PCA-assisted BO (KPCA-BO) algorithm, which embeds a non-linear\nsub-manifold in the search space and performs BO on this sub-manifold.\nIntuitively, constructing the GPR model on a lower-dimensional sub-manifold\nhelps improve the modeling accuracy without requiring much more data from the\nobjective function. Also, our approach defines the acquisition function on the\nlower-dimensional sub-manifold, making the acquisition optimization more\nmanageable.\n  We compare the performance of KPCA-BO to a vanilla BO and to PCA-BO on the\nmulti-modal problems of the COCO/BBOB benchmark suite. Empirical results show\nthat KPCA-BO outperforms BO in terms of convergence speed on most test\nproblems, and this benefit becomes more significant when the dimensionality\nincreases. For the 60D functions, KPCA-BO achieves better results than PCA-BO\nfor many test cases. Compared to the vanilla BO, it efficiently reduces the CPU\ntime required to train the GPR model and to optimize the acquisition function\ncompared to the vanilla BO.",
          "arxiv_id": "2204.13753v2"
        },
        {
          "title": "Expected Coordinate Improvement for High-Dimensional Bayesian Optimization",
          "year": "2024-04",
          "abstract": "Bayesian optimization (BO) algorithm is very popular for solving\nlow-dimensional expensive optimization problems. Extending Bayesian\noptimization to high dimension is a meaningful but challenging task. One of the\nmajor challenges is that it is difficult to find good infill solutions as the\nacquisition functions are also high-dimensional. In this work, we propose the\nexpected coordinate improvement (ECI) criterion for high-dimensional Bayesian\noptimization. The proposed ECI criterion measures the potential improvement we\ncan get by moving the current best solution along one coordinate. The proposed\napproach selects the coordinate with the highest ECI value to refine in each\niteration and covers all the coordinates gradually by iterating over the\ncoordinates. The greatest advantage of the proposed ECI-BO (expected coordinate\nimprovement based Bayesian optimization) algorithm over the standard BO\nalgorithm is that the infill selection problem of the proposed algorithm is\nalways a one-dimensional problem thus can be easily solved. Numerical\nexperiments show that the proposed algorithm can achieve significantly better\nresults than the standard BO algorithm and competitive results when compared\nwith five state-of-the-art high-dimensional BOs. This work provides a simple\nbut efficient approach for high-dimensional Bayesian optimization.",
          "arxiv_id": "2404.11917v2"
        },
        {
          "title": "Batched Energy-Entropy acquisition for Bayesian Optimization",
          "year": "2024-10",
          "abstract": "Bayesian optimization (BO) is an attractive machine learning framework for\nperforming sample-efficient global optimization of black-box functions. The\noptimization process is guided by an acquisition function that selects points\nto acquire in each round of BO. In batched BO, when multiple points are\nacquired in parallel, commonly used acquisition functions are often\nhigh-dimensional and intractable, leading to the use of sampling-based\nalternatives. We propose a statistical physics inspired acquisition function\nfor BO with Gaussian processes that can natively handle batches. Batched\nEnergy-Entropy acquisition for BO (BEEBO) enables tight control of the\nexplore-exploit trade-off of the optimization process and generalizes to\nheteroskedastic black-box problems. We demonstrate the applicability of BEEBO\non a range of problems, showing competitive performance to existing methods.",
          "arxiv_id": "2410.08804v1"
        }
      ],
      "53": [
        {
          "title": "X-Risk Analysis for AI Research",
          "year": "2022-06",
          "abstract": "Artificial intelligence (AI) has the potential to greatly improve society,\nbut as with any powerful technology, it comes with heightened risks and\nresponsibilities. Current AI research lacks a systematic discussion of how to\nmanage long-tail risks from AI systems, including speculative long-term risks.\nKeeping in mind the potential benefits of AI, there is some concern that\nbuilding ever more intelligent and powerful AI systems could eventually result\nin systems that are more powerful than us; some say this is like playing with\nfire and speculate that this could create existential risks (x-risks). To add\nprecision and ground these discussions, we provide a guide for how to analyze\nAI x-risk, which consists of three parts: First, we review how systems can be\nmade safer today, drawing on time-tested concepts from hazard analysis and\nsystems safety that have been designed to steer large processes in safer\ndirections. Next, we discuss strategies for having long-term impacts on the\nsafety of future systems. Finally, we discuss a crucial concept in making AI\nsystems safer by improving the balance between safety and general capabilities.\nWe hope this document and the presented concepts and tools serve as a useful\nguide for understanding how to analyze AI x-risk.",
          "arxiv_id": "2206.05862v7"
        },
        {
          "title": "Response by the Montreal AI Ethics Institute to the European Commission's Whitepaper on AI",
          "year": "2020-06",
          "abstract": "In February 2020, the European Commission (EC) published a white paper\nentitled, On Artificial Intelligence - A European approach to excellence and\ntrust. This paper outlines the EC's policy options for the promotion and\nadoption of artificial intelligence (AI) in the European Union. The Montreal AI\nEthics Institute (MAIEI) reviewed this paper and published a response\naddressing the EC's plans to build an \"ecosystem of excellence\" and an\n\"ecosystem of trust,\" as well as the safety and liability implications of AI,\nthe internet of things (IoT), and robotics.\n  MAIEI provides 15 recommendations in relation to the sections outlined above,\nincluding: 1) focus efforts on the research and innovation community, member\nstates, and the private sector; 2) create alignment between trading partners'\npolicies and EU policies; 3) analyze the gaps in the ecosystem between\ntheoretical frameworks and approaches to building trustworthy AI; 4) focus on\ncoordination and policy alignment; 5) focus on mechanisms that promote private\nand secure sharing of data; 6) create a network of AI research excellence\ncentres to strengthen the research and innovation community; 7) promote\nknowledge transfer and develop AI expertise through Digital Innovation Hubs; 8)\nadd nuance to the discussion regarding the opacity of AI systems; 9) create a\nprocess for individuals to appeal an AI system's decision or output; 10)\nimplement new rules and strengthen existing regulations; 11) ban the use of\nfacial recognition technology; 12) hold all AI systems to similar standards and\ncompulsory requirements; 13) ensure biometric identification systems fulfill\nthe purpose for which they are implemented; 14) implement a voluntary labelling\nsystem for systems that are not considered high-risk; 15) appoint individuals\nto the oversight process who understand AI systems well and are able to\ncommunicate potential risks.",
          "arxiv_id": "2006.09428v1"
        },
        {
          "title": "Responsible Design Patterns for Machine Learning Pipelines",
          "year": "2023-05",
          "abstract": "Integrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
          "arxiv_id": "2306.01788v3"
        }
      ],
      "54": [
        {
          "title": "AD-MERCS: Modeling Normality and Abnormality in Unsupervised Anomaly Detection",
          "year": "2023-05",
          "abstract": "Most anomaly detection systems try to model normal behavior and assume\nanomalies deviate from it in diverse manners. However, there may be patterns in\nthe anomalies as well. Ideally, an anomaly detection system can exploit\npatterns in both normal and anomalous behavior. In this paper, we present\nAD-MERCS, an unsupervised approach to anomaly detection that explicitly aims at\ndoing both. AD-MERCS identifies multiple subspaces of the instance space within\nwhich patterns exist, and identifies conditions (possibly in other subspaces)\nthat characterize instances that deviate from these patterns. Experiments show\nthat this modeling of both normality and abnormality makes the anomaly detector\nperformant on a wide range of types of anomalies. Moreover, by identifying\npatterns and conditions in (low-dimensional) subspaces, the anomaly detector\ncan provide simple explanations of why something is considered an anomaly.\nThese explanations can be both negative (deviation from some pattern) as\npositive (meeting some condition that is typical for anomalies).",
          "arxiv_id": "2305.12958v1"
        },
        {
          "title": "Deep Positive-Unlabeled Anomaly Detection for Contaminated Unlabeled Data",
          "year": "2024-05",
          "abstract": "Semi-supervised anomaly detection, which aims to improve the anomaly\ndetection performance by using a small amount of labeled anomaly data in\naddition to unlabeled data, has attracted attention. Existing semi-supervised\napproaches assume that most unlabeled data are normal, and train anomaly\ndetectors by minimizing the anomaly scores for the unlabeled data while\nmaximizing those for the labeled anomaly data. However, in practice, the\nunlabeled data are often contaminated with anomalies. This weakens the effect\nof maximizing the anomaly scores for anomalies, and prevents us from improving\nthe detection performance. To solve this problem, we propose the deep\npositive-unlabeled anomaly detection framework, which integrates\npositive-unlabeled learning with deep anomaly detection models such as\nautoencoders and deep support vector data descriptions. Our approach enables\nthe approximation of anomaly scores for normal data using the unlabeled data\nand the labeled anomaly data. Therefore, without labeled normal data, our\napproach can train anomaly detectors by minimizing the anomaly scores for\nnormal data while maximizing those for the labeled anomaly data. Experiments on\nvarious datasets show that our approach achieves better detection performance\nthan existing approaches.",
          "arxiv_id": "2405.18929v2"
        },
        {
          "title": "AGAD: Adversarial Generative Anomaly Detection",
          "year": "2023-04",
          "abstract": "Anomaly detection suffered from the lack of anomalies due to the diversity of\nabnormalities and the difficulties of obtaining large-scale anomaly data.\nSemi-supervised anomaly detection methods are often used to solely leverage\nnormal data to detect abnormalities that deviated from the learnt normality\ndistributions. Meanwhile, given the fact that limited anomaly data can be\nobtained with a minor cost in practice, some researches also investigated\nanomaly detection methods under supervised scenarios with limited anomaly data.\nIn order to address the lack of abnormal data for robust anomaly detection, we\npropose Adversarial Generative Anomaly Detection (AGAD), a self-contrast-based\nanomaly detection paradigm that learns to detect anomalies by generating\n\\textit{contextual adversarial information} from the massive normal examples.\nEssentially, our method generates pseudo-anomaly data for both supervised and\nsemi-supervised anomaly detection scenarios. Extensive experiments are carried\nout on multiple benchmark datasets and real-world datasets, the results show\nsignificant improvement in both supervised and semi-supervised scenarios.\nImportantly, our approach is data-efficient that can boost up the detection\naccuracy with no more than 5% anomalous training data.",
          "arxiv_id": "2304.04211v1"
        }
      ],
      "55": [
        {
          "title": "Experiments with Optimal Model Trees",
          "year": "2025-03",
          "abstract": "Model trees provide an appealing way to perform interpretable machine\nlearning for both classification and regression problems. In contrast to\n``classic'' decision trees with constant values in their leaves, model trees\ncan use linear combinations of predictor variables in their leaf nodes to form\npredictions, which can help achieve higher accuracy and smaller trees. Typical\nalgorithms for learning model trees from training data work in a greedy\nfashion, growing the tree in a top-down manner by recursively splitting the\ndata into smaller and smaller subsets. Crucially, the selected splits are only\nlocally optimal, potentially rendering the tree overly complex and less\naccurate than a tree whose structure is globally optimal for the training data.\nIn this paper, we empirically investigate the effect of constructing globally\noptimal model trees for classification and regression with linear support\nvector machines at the leaf nodes. To this end, we present mixed-integer linear\nprogramming formulations to learn optimal trees, compute such trees for a large\ncollection of benchmark data sets, and compare their performance against\ngreedily grown model trees in terms of interpretability and accuracy. We also\ncompare to classic optimal and greedily grown decision trees, random forests,\nand support vector machines. Our results show that optimal model trees can\nachieve competitive accuracy with very small trees. We also investigate the\neffect on the accuracy of replacing axis-parallel splits with multivariate\nones, foregoing interpretability while potentially obtaining greater accuracy.",
          "arxiv_id": "2503.12902v1"
        },
        {
          "title": "On Computing Optimal Tree Ensembles",
          "year": "2023-06",
          "abstract": "Random forests and, more generally, (decision\\nobreakdash-)tree ensembles are\nwidely used methods for classification and regression. Recent algorithmic\nadvances allow to compute decision trees that are optimal for various measures\nsuch as their size or depth. We are not aware of such research for tree\nensembles and aim to contribute to this area. Mainly, we provide two novel\nalgorithms and corresponding lower bounds. First, we are able to carry over and\nsubstantially improve on tractability results for decision trees: We obtain an\nalgorithm that, given a training-data set and an size bound $S \\in \\mathbb{R}$,\ncomputes a tree ensemble of size at most $S$ that classifies the data\ncorrectly. The algorithm runs in $(4\\delta D S)^S \\cdot poly$-time, where $D$\nthe largest domain size, $\\delta$ is the largest number of features in which\ntwo examples differ, $n$ the number of input examples, and $poly$ a polynomial\nof the input size. For decision trees, that is, ensembles of size 1, we obtain\na running time of $(\\delta D s)^s \\cdot poly$, where $s$ is the size of the\ntree. To obtain these algorithms, we introduce the witness-tree technique,\nwhich seems promising for practical implementations. Secondly, we show that\ndynamic programming, which has been applied successfully to computing decision\ntrees, may also be viable for tree ensembles, providing an $\\ell^n \\cdot\npoly$-time algorithm, where $\\ell$ is the number of trees. Finally, we compare\nthe number of cuts necessary to classify training data sets for decision trees\nand tree ensembles, showing that ensembles may need exponentially fewer cuts\nfor increasing number of trees.",
          "arxiv_id": "2306.04423v2"
        },
        {
          "title": "Boosting-Based Sequential Meta-Tree Ensemble Construction for Improved Decision Trees",
          "year": "2024-02",
          "abstract": "A decision tree is one of the most popular approaches in machine learning\nfields. However, it suffers from the problem of overfitting caused by overly\ndeepened trees. Then, a meta-tree is recently proposed. It solves the problem\nof overfitting caused by overly deepened trees. Moreover, the meta-tree\nguarantees statistical optimality based on Bayes decision theory. Therefore,\nthe meta-tree is expected to perform better than the decision tree. In contrast\nto a single decision tree, it is known that ensembles of decision trees, which\nare typically constructed boosting algorithms, are more effective in improving\npredictive performance. Thus, it is expected that ensembles of meta-trees are\nmore effective in improving predictive performance than a single meta-tree, and\nthere are no previous studies that construct multiple meta-trees in boosting.\nTherefore, in this study, we propose a method to construct multiple meta-trees\nusing a boosting approach. Through experiments with synthetic and benchmark\ndatasets, we conduct a performance comparison between the proposed methods and\nthe conventional methods using ensembles of decision trees. Furthermore, while\nensembles of decision trees can cause overfitting as well as a single decision\ntree, experiments confirmed that ensembles of meta-trees can prevent\noverfitting due to the tree depth.",
          "arxiv_id": "2402.06386v1"
        }
      ],
      "56": [
        {
          "title": "Modeling time evolving COVID-19 uncertainties with density dependent asymptomatic infections and social reinforcement",
          "year": "2021-08",
          "abstract": "The COVID-19 pandemic has posed significant challenges in modeling its\ncomplex epidemic transmissions, infection and contagion, which are very\ndifferent from known epidemics. The challenges in quantifying COVID-19\ncomplexities include effectively modeling its process and data uncertainties.\nThe uncertainties are embedded in implicit and high-proportional undocumented\ninfections, asymptomatic contagion, social reinforcement of infections, and\nvarious quality issues in the reported data. These uncertainties become even\nmore apparent in the first two months of the COVID-19 pandemic, when the\nrelevant knowledge, case reporting and testing were all limited. Here we\nintroduce a novel hybrid approach Susceptible-Undocumented infected-Documented\ninfected-Recovered (SUDR) model. First, SUDR (1) characterizes and\ndistinguishes Undocumented (U) and Documented (D) infections commonly seen\nduring COVID-19 incubation periods and asymptomatic infections. Second, SUDR\ncharacterizes the probabilistic density of infections by capturing exogenous\nprocesses. Lastly, SUDR approximates the density likelihood of COVID-19\nprevalence over time by incorporating Bayesian inference into SUDR. Different\nfrom existing COVID-19 models, SUDR characterizes the undocumented infections\nduring unknown transmission processes. To capture the uncertainties of temporal\ntransmission and social reinforcement during COVID-19 contagion, the\ntransmission rate is modeled by a time-varying density function of undocumented\ninfectious cases. By sampling from the mean-field posterior distribution with\nreasonable priors, SUDR handles the randomness, noise and sparsity of COVID-19\nobservations widely seen in the public COVID-19 case data. The results\ndemonstrate a deeper quantitative understanding of the above uncertainties, in\ncomparison with classic SIR, time-dependent SIR, and probabilistic SIR models.",
          "arxiv_id": "2108.10029v2"
        },
        {
          "title": "Examining Deep Learning Models with Multiple Data Sources for COVID-19 Forecasting",
          "year": "2020-10",
          "abstract": "The COVID-19 pandemic represents the most significant public health disaster\nsince the 1918 influenza pandemic. During pandemics such as COVID-19, timely\nand reliable spatio-temporal forecasting of epidemic dynamics is crucial. Deep\nlearning-based time series models for forecasting have recently gained\npopularity and have been successfully used for epidemic forecasting. Here we\nfocus on the design and analysis of deep learning-based models for COVID-19\nforecasting. We implement multiple recurrent neural network-based deep learning\nmodels and combine them using the stacking ensemble technique. In order to\nincorporate the effects of multiple factors in COVID-19 spread, we consider\nmultiple sources such as COVID-19 confirmed and death case count data and\ntesting data for better predictions. To overcome the sparsity of training data\nand to address the dynamic correlation of the disease, we propose\nclustering-based training for high-resolution forecasting. The methods help us\nto identify the similar trends of certain groups of regions due to various\nspatio-temporal effects. We examine the proposed method for forecasting weekly\nCOVID-19 new confirmed cases at county-, state-, and country-level. A\ncomprehensive comparison between different time series models in COVID-19\ncontext is conducted and analyzed. The results show that simple deep learning\nmodels can achieve comparable or better performance when compared with more\ncomplicated models. We are currently integrating our methods as a part of our\nweekly forecasts that we provide state and federal authorities.",
          "arxiv_id": "2010.14491v2"
        },
        {
          "title": "COVID-19 Hospitalizations Forecasts Using Internet Search Data",
          "year": "2022-02",
          "abstract": "As the COVID-19 spread over the globe and new variants of COVID-19 keep\noccurring, reliable real-time forecasts of COVID-19 hospitalizations are\ncritical for public health decision on medical resources allocations such as\nICU beds, ventilators, and personnel to prepare for the surge of COVID-19\npandemics. Inspired by the strong association between public search behavior\nand hospitalization admission, we extended previously-proposed influenza\ntracking model, ARGO (AutoRegression with GOogle search data), to predict\nfuture 2-week national and state-level COVID-19 new hospital admissions.\nLeveraging the COVID-19 related time series information and Google search data,\nour method is able to robustly capture new COVID-19 variants' surges, and\nself-correct at both national and state level. Based on our retrospective\nout-of-sample evaluation over 12-month comparison period, our method achieves\non average 15\\% error reduction over the best alternative models collected from\nCOVID-19 forecast hub. Overall, we showed that our method is flexible,\nself-correcting, robust, accurate, and interpretable, making it a potentially\npowerful tool to assist health-care officials and decision making for the\ncurrent and future infectious disease outbreak.",
          "arxiv_id": "2202.03869v1"
        }
      ],
      "57": [
        {
          "title": "YAHPO Gym -- An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization",
          "year": "2021-09",
          "abstract": "When developing and analyzing new hyperparameter optimization methods, it is\nvital to empirically evaluate and compare them on well-curated benchmark\nsuites. In this work, we propose a new set of challenging and relevant\nbenchmark problems motivated by desirable properties and requirements for such\nbenchmarks. Our new surrogate-based benchmark collection consists of 14\nscenarios that in total constitute over 700 multi-fidelity hyperparameter\noptimization problems, which all enable multi-objective hyperparameter\noptimization. Furthermore, we empirically compare surrogate-based benchmarks to\nthe more widely-used tabular benchmarks, and demonstrate that the latter may\nproduce unfaithful results regarding the performance ranking of HPO methods. We\nexamine and compare our benchmark collection with respect to defined\nrequirements and propose a single-objective as well as a multi-objective\nbenchmark suite on which we compare 7 single-objective and 7 multi-objective\noptimizers in a benchmark experiment. Our software is available at\n[https://github.com/slds-lmu/yahpo_gym].",
          "arxiv_id": "2109.03670v4"
        },
        {
          "title": "HPO X ELA: Investigating Hyperparameter Optimization Landscapes by Means of Exploratory Landscape Analysis",
          "year": "2022-07",
          "abstract": "Hyperparameter optimization (HPO) is a key component of machine learning\nmodels for achieving peak predictive performance. While numerous methods and\nalgorithms for HPO have been proposed over the last years, little progress has\nbeen made in illuminating and examining the actual structure of these black-box\noptimization problems. Exploratory landscape analysis (ELA) subsumes a set of\ntechniques that can be used to gain knowledge about properties of unknown\noptimization problems. In this paper, we evaluate the performance of five\ndifferent black-box optimizers on 30 HPO problems, which consist of two-,\nthree- and five-dimensional continuous search spaces of the XGBoost learner\ntrained on 10 different data sets. This is contrasted with the performance of\nthe same optimizers evaluated on 360 problem instances from the black-box\noptimization benchmark (BBOB). We then compute ELA features on the HPO and BBOB\nproblems and examine similarities and differences. A cluster analysis of the\nHPO and BBOB problems in ELA feature space allows us to identify how the HPO\nproblems compare to the BBOB problems on a structural meta-level. We identify a\nsubset of BBOB problems that are close to the HPO problems in ELA feature space\nand show that optimizer performance is comparably similar on these two sets of\nbenchmark problems. We highlight open challenges of ELA for HPO and discuss\npotential directions of future research and applications.",
          "arxiv_id": "2208.00220v1"
        },
        {
          "title": "Fair and Green Hyperparameter Optimization via Multi-objective and Multiple Information Source Bayesian Optimization",
          "year": "2022-05",
          "abstract": "There is a consensus that focusing only on accuracy in searching for optimal\nmachine learning models amplifies biases contained in the data, leading to\nunfair predictions and decision supports. Recently, multi-objective\nhyperparameter optimization has been proposed to search for machine learning\nmodels which offer equally Pareto-efficient trade-offs between accuracy and\nfairness. Although these approaches proved to be more versatile than\nfairness-aware machine learning algorithms -- which optimize accuracy\nconstrained to some threshold on fairness -- they could drastically increase\nthe energy consumption in the case of large datasets. In this paper we propose\nFanG-HPO, a Fair and Green Hyperparameter Optimization (HPO) approach based on\nboth multi-objective and multiple information source Bayesian optimization.\nFanG-HPO uses subsets of the large dataset (aka information sources) to obtain\ncheap approximations of both accuracy and fairness, and multi-objective\nBayesian Optimization to efficiently identify Pareto-efficient machine learning\nmodels. Experiments consider two benchmark (fairness) datasets and two machine\nlearning algorithms (XGBoost and Multi-Layer Perceptron), and provide an\nassessment of FanG-HPO against both fairness-aware machine learning algorithms\nand hyperparameter optimization via a multi-objective single-source\noptimization algorithm in BoTorch, a state-of-the-art platform for Bayesian\nOptimization.",
          "arxiv_id": "2205.08835v1"
        }
      ],
      "58": [
        {
          "title": "Leveraging Previous Facial Action Units Knowledge for Emotion Recognition on Faces",
          "year": "2023-11",
          "abstract": "People naturally understand emotions, thus permitting a machine to do the\nsame could open new paths for human-computer interaction. Facial expressions\ncan be very useful for emotion recognition techniques, as these are the biggest\ntransmitters of non-verbal cues capable of being correlated with emotions.\nSeveral techniques are based on Convolutional Neural Networks (CNNs) to extract\ninformation in a machine learning process. However, simple CNNs are not always\nsufficient to locate points of interest on the face that can be correlated with\nemotions. In this work, we intend to expand the capacity of emotion recognition\ntechniques by proposing the usage of Facial Action Units (AUs) recognition\ntechniques to recognize emotions. This recognition will be based on the Facial\nAction Coding System (FACS) and computed by a machine learning system. In\nparticular, our method expands over EmotiRAM, an approach for multi-cue emotion\nrecognition, in which we improve over their facial encoding module.",
          "arxiv_id": "2311.11980v1"
        },
        {
          "title": "Real-time Facial Expression Recognition \"In The Wild'' by Disentangling 3D Expression from Identity",
          "year": "2020-05",
          "abstract": "Human emotions analysis has been the focus of many studies, especially in the\nfield of Affective Computing, and is important for many applications, e.g.\nhuman-computer intelligent interaction, stress analysis, interactive games,\nanimations, etc. Solutions for automatic emotion analysis have also benefited\nfrom the development of deep learning approaches and the availability of vast\namount of visual facial data on the internet. This paper proposes a novel\nmethod for human emotion recognition from a single RGB image. We construct a\nlarge-scale dataset of facial videos (\\textbf{FaceVid}), rich in facial\ndynamics, identities, expressions, appearance and 3D pose variations. We use\nthis dataset to train a deep Convolutional Neural Network for estimating\nexpression parameters of a 3D Morphable Model and combine it with an effective\nback-end emotion classifier. Our proposed framework runs at 50 frames per\nsecond and is capable of robustly estimating parameters of 3D expression\nvariation and accurately recognizing facial expressions from in-the-wild\nimages. We present extensive experimental evaluation that shows that the\nproposed method outperforms the compared techniques in estimating the 3D\nexpression parameters and achieves state-of-the-art performance in recognising\nthe basic emotions from facial images, as well as recognising stress from\nfacial videos. %compared to the current state of the art in emotion recognition\nfrom facial images.",
          "arxiv_id": "2005.05509v1"
        },
        {
          "title": "Interpretable Image Emotion Recognition: A Domain Adaptation Approach Using Facial Expressions",
          "year": "2020-11",
          "abstract": "This paper proposes a feature-based domain adaptation technique for\nidentifying emotions in generic images, encompassing both facial and non-facial\nobjects, as well as non-human components. This approach addresses the challenge\nof the limited availability of pre-trained models and well-annotated datasets\nfor Image Emotion Recognition (IER). Initially, a deep-learning-based Facial\nExpression Recognition (FER) system is developed, classifying facial images\ninto discrete emotion classes. Maintaining the same network architecture, this\nFER system is then adapted to recognize emotions in generic images through the\napplication of discrepancy loss, enabling the model to effectively learn IER\nfeatures while classifying emotions into categories such as 'happy,' 'sad,'\n'hate,' and 'anger.' Additionally, a novel interpretability method, Divide and\nConquer based Shap (DnCShap), is introduced to elucidate the visual features\nmost relevant for emotion recognition. The proposed IER system demonstrated\nemotion classification accuracies of 61.86% for the IAPSa dataset, 62.47 for\nthe ArtPhoto dataset, 70.78% for the FI dataset, and 59.72% for the EMOTIC\ndataset. The system effectively identifies the important visual features that\nlead to specific emotion classifications and also provides detailed embedding\nplots explaining the predictions, enhancing the understanding and trust in\nAI-driven emotion recognition systems.",
          "arxiv_id": "2011.08388v4"
        }
      ],
      "59": [
        {
          "title": "Transferable Graph Backdoor Attack",
          "year": "2022-06",
          "abstract": "Graph Neural Networks (GNNs) have achieved tremendous success in many graph\nmining tasks benefitting from the message passing strategy that fuses the local\nstructure and node features for better graph representation learning. Despite\nthe success of GNNs, and similar to other types of deep neural networks, GNNs\nare found to be vulnerable to unnoticeable perturbations on both graph\nstructure and node features. Many adversarial attacks have been proposed to\ndisclose the fragility of GNNs under different perturbation strategies to\ncreate adversarial examples. However, vulnerability of GNNs to successful\nbackdoor attacks was only shown recently. In this paper, we disclose the TRAP\nattack, a Transferable GRAPh backdoor attack. The core attack principle is to\npoison the training dataset with perturbation-based triggers that can lead to\nan effective and transferable backdoor attack. The perturbation trigger for a\ngraph is generated by performing the perturbation actions on the graph\nstructure via a gradient based score matrix from a surrogate model. Compared\nwith prior works, TRAP attack is different in several ways: i) it exploits a\nsurrogate Graph Convolutional Network (GCN) model to generate perturbation\ntriggers for a blackbox based backdoor attack; ii) it generates sample-specific\nperturbation triggers which do not have a fixed pattern; and iii) the attack\ntransfers, for the first time in the context of GNNs, to different GNN models\nwhen trained with the forged poisoned training dataset. Through extensive\nevaluations on four real-world datasets, we demonstrate the effectiveness of\nthe TRAP attack to build transferable backdoors in four different popular GNNs\nusing four real-world datasets.",
          "arxiv_id": "2207.00425v3"
        },
        {
          "title": "Graph Structure Learning for Robust Graph Neural Networks",
          "year": "2020-05",
          "abstract": "Graph Neural Networks (GNNs) are powerful tools in representation learning\nfor graphs. However, recent studies show that GNNs are vulnerable to\ncarefully-crafted perturbations, called adversarial attacks. Adversarial\nattacks can easily fool GNNs in making predictions for downstream tasks. The\nvulnerability to adversarial attacks has raised increasing concerns for\napplying GNNs in safety-critical applications. Therefore, developing robust\nalgorithms to defend adversarial attacks is of great significance. A natural\nidea to defend adversarial attacks is to clean the perturbed graph. It is\nevident that real-world graphs share some intrinsic properties. For example,\nmany real-world graphs are low-rank and sparse, and the features of two\nadjacent nodes tend to be similar. In fact, we find that adversarial attacks\nare likely to violate these graph properties. Therefore, in this paper, we\nexplore these properties to defend adversarial attacks on graphs. In\nparticular, we propose a general framework Pro-GNN, which can jointly learn a\nstructural graph and a robust graph neural network model from the perturbed\ngraph guided by these properties. Extensive experiments on real-world graphs\ndemonstrate that the proposed framework achieves significantly better\nperformance compared with the state-of-the-art defense methods, even when the\ngraph is heavily perturbed. We release the implementation of Pro-GNN to our\nDeepRobust repository for adversarial attacks and defenses (footnote:\nhttps://github.com/DSE-MSU/DeepRobust). The specific experimental settings to\nreproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",
          "arxiv_id": "2005.10203v3"
        },
        {
          "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
          "year": "2021-08",
          "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph structure related tasks such as node classification and graph\nclassification. However, GNNs are vulnerable to adversarial attacks. Existing\nworks mainly focus on attacking GNNs for node classification; nevertheless, the\nattacks against GNNs for graph classification have not been well explored.\n  In this work, we conduct a systematic study on adversarial attacks against\nGNNs for graph classification via perturbing the graph structure. In\nparticular, we focus on the most challenging attack, i.e., hard label black-box\nattack, where an attacker has no knowledge about the target GNN model and can\nonly obtain predicted labels through querying the target model.To achieve this\ngoal, we formulate our attack as an optimization problem, whose objective is to\nminimize the number of edges to be perturbed in a graph while maintaining the\nhigh attack success rate. The original optimization problem is intractable to\nsolve, and we relax the optimization problem to be a tractable one, which is\nsolved with theoretical convergence guarantee. We also design a coarse-grained\nsearching algorithm and a query-efficient gradient computation algorithm to\ndecrease the number of queries to the target GNN model. Our experimental\nresults on three real-world datasets demonstrate that our attack can\neffectively attack representative GNNs for graph classification with less\nqueries and perturbations. We also evaluate the effectiveness of our attack\nunder two defenses: one is well-designed adversarial graph detector and the\nother is that the target GNN model itself is equipped with a defense to prevent\nadversarial graph generation. Our experimental results show that such defenses\nare not effective enough, which highlights more advanced defenses.",
          "arxiv_id": "2108.09513v2"
        }
      ],
      "60": [
        {
          "title": "Transparency and Privacy: The Role of Explainable AI and Federated Learning in Financial Fraud Detection",
          "year": "2023-12",
          "abstract": "Fraudulent transactions and how to detect them remain a significant problem\nfor financial institutions around the world. The need for advanced fraud\ndetection systems to safeguard assets and maintain customer trust is paramount\nfor financial institutions, but some factors make the development of effective\nand efficient fraud detection systems a challenge. One of such factors is the\nfact that fraudulent transactions are rare and that many transaction datasets\nare imbalanced; that is, there are fewer significant samples of fraudulent\ntransactions than legitimate ones. This data imbalance can affect the\nperformance or reliability of the fraud detection model. Moreover, due to the\ndata privacy laws that all financial institutions are subject to follow,\nsharing customer data to facilitate a higher-performing centralized model is\nimpossible. Furthermore, the fraud detection technique should be transparent so\nthat it does not affect the user experience. Hence, this research introduces a\nnovel approach using Federated Learning (FL) and Explainable AI (XAI) to\naddress these challenges. FL enables financial institutions to collaboratively\ntrain a model to detect fraudulent transactions without directly sharing\ncustomer data, thereby preserving data privacy and confidentiality. Meanwhile,\nthe integration of XAI ensures that the predictions made by the model can be\nunderstood and interpreted by human experts, adding a layer of transparency and\ntrust to the system. Experimental results, based on realistic transaction\ndatasets, reveal that the FL-based fraud detection system consistently\ndemonstrates high performance metrics. This study grounds FL's potential as an\neffective and privacy-preserving tool in the fight against fraud.",
          "arxiv_id": "2312.13334v1"
        },
        {
          "title": "Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation",
          "year": "2024-12",
          "abstract": "Credit card fraud incurs a considerable cost for both cardholders and issuing\nbanks. Contemporary methods apply machine learning-based classifiers to detect\nfraudulent behavior from labeled transaction records. But labeled data are\nusually a small proportion of billions of real transactions due to expensive\nlabeling costs, which implies that they do not well exploit many natural\nfeatures from unlabeled data. Therefore, we propose a semi-supervised graph\nneural network for fraud detection. Specifically, we leverage transaction\nrecords to construct a temporal transaction graph, which is composed of\ntemporal transactions (nodes) and interactions (edges) among them. Then we pass\nmessages among the nodes through a Gated Temporal Attention Network (GTAN) to\nlearn the transaction representation. We further model the fraud patterns\nthrough risk propagation among transactions. The extensive experiments are\nconducted on a real-world transaction dataset and two publicly available fraud\ndetection datasets. The result shows that our proposed method, namely GTAN,\noutperforms other state-of-the-art baselines on three fraud detection datasets.\nSemi-supervised experiments demonstrate the excellent fraud detection\nperformance of our model with only a tiny proportion of labeled data.",
          "arxiv_id": "2412.18287v1"
        },
        {
          "title": "Heterogeneous Graph Auto-Encoder for CreditCard Fraud Detection",
          "year": "2024-10",
          "abstract": "The digital revolution has significantly impacted financial transactions,\nleading to a notable increase in credit card usage. However, this convenience\ncomes with a trade-off: a substantial rise in fraudulent activities.\nTraditional machine learning methods for fraud detection often struggle to\ncapture the inherent interconnectedness within financial data. This paper\nproposes a novel approach for credit card fraud detection that leverages Graph\nNeural Networks (GNNs) with attention mechanisms applied to heterogeneous graph\nrepresentations of financial data. Unlike homogeneous graphs, heterogeneous\ngraphs capture intricate relationships between various entities in the\nfinancial ecosystem, such as cardholders, merchants, and transactions,\nproviding a richer and more comprehensive data representation for fraud\nanalysis. To address the inherent class imbalance in fraud data, where genuine\ntransactions significantly outnumber fraudulent ones, the proposed approach\nintegrates an autoencoder. This autoencoder, trained on genuine transactions,\nlearns a latent representation and flags deviations during reconstruction as\npotential fraud. This research investigates two key questions: (1) How\neffectively can a GNN with an attention mechanism detect and prevent credit\ncard fraud when applied to a heterogeneous graph? (2) How does the efficacy of\nthe autoencoder with attention approach compare to traditional methods? The\nresults are promising, demonstrating that the proposed model outperforms\nbenchmark algorithms such as Graph Sage and FI-GRL, achieving a superior AUC-PR\nof 0.89 and an F1-score of 0.81. This research significantly advances fraud\ndetection systems and the overall security of financial transactions by\nleveraging GNNs with attention mechanisms and addressing class imbalance\nthrough an autoencoder.",
          "arxiv_id": "2410.08121v1"
        }
      ],
      "61": [
        {
          "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective",
          "year": "2021-02",
          "abstract": "Neural Architecture Search (NAS) has been explosively studied to automate the\ndiscovery of top-performer neural networks. Current works require heavy\ntraining of supernet or intensive architecture evaluations, thus suffering from\nheavy resource consumption and often incurring search bias due to truncated\ntraining or approximations. Can we select the best neural architectures without\ninvolving any training and eliminate a drastic portion of the search cost? We\nprovide an affirmative answer, by proposing a novel framework called\ntraining-free neural architecture search (TE-NAS). TE-NAS ranks architectures\nby analyzing the spectrum of the neural tangent kernel (NTK) and the number of\nlinear regions in the input space. Both are motivated by recent theory advances\nin deep networks and can be computed without any training and any label. We\nshow that: (1) these two measurements imply the trainability and expressivity\nof a neural network; (2) they strongly correlate with the network's test\naccuracy. Further on, we design a pruning-based NAS mechanism to achieve a more\nflexible and superior trade-off between the trainability and expressivity\nduring the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes\nhigh-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on\nCIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in\nbridging the theoretical findings of deep networks and practical impacts in\nreal NAS applications. Code is available at:\nhttps://github.com/VITA-Group/TENAS.",
          "arxiv_id": "2102.11535v4"
        },
        {
          "title": "A General-Purpose Transferable Predictor for Neural Architecture Search",
          "year": "2023-02",
          "abstract": "Understanding and modelling the performance of neural architectures is key to\nNeural Architecture Search (NAS). Performance predictors have seen widespread\nuse in low-cost NAS and achieve high ranking correlations between predicted and\nground truth performance in several NAS benchmarks. However, existing\npredictors are often designed based on network encodings specific to a\npredefined search space and are therefore not generalizable to other search\nspaces or new architecture families. In this paper, we propose a\ngeneral-purpose neural predictor for NAS that can transfer across search\nspaces, by representing any given candidate Convolutional Neural Network (CNN)\nwith a Computation Graph (CG) that consists of primitive operators. We further\ncombine our CG network representation with Contrastive Learning (CL) and\npropose a graph representation learning procedure that leverages the structural\ninformation of unlabeled architectures from multiple families to train CG\nembeddings for our performance predictor. Experimental results on\nNAS-Bench-101, 201 and 301 demonstrate the efficacy of our scheme as we achieve\nstrong positive Spearman Rank Correlation Coefficient (SRCC) on every search\nspace, outperforming several Zero-Cost Proxies, including Synflow and Jacov,\nwhich are also generalizable predictors across search spaces. Moreover, when\nusing our proposed general-purpose predictor in an evolutionary neural\narchitecture search algorithm, we can find high-performance architectures on\nNAS-Bench-101 and find a MobileNetV3 architecture that attains 79.2% top-1\naccuracy on ImageNet.",
          "arxiv_id": "2302.10835v1"
        },
        {
          "title": "Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation",
          "year": "2025-08",
          "abstract": "Neural Architecture Search (NAS) automates the design of high-performing\nneural networks but typically targets a single predefined task, thereby\nrestricting its real-world applicability. To address this, Meta Neural\nArchitecture Search (Meta-NAS) has emerged as a promising paradigm that\nleverages prior knowledge across tasks to enable rapid adaptation to new ones.\nNevertheless, existing Meta-NAS methods often struggle with poor\ngeneralization, limited search spaces, or high computational costs. In this\npaper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS\nfirst models neural architectures as graphs, and then a hybrid search strategy\nis developed to find and generate new graphs that lead to promising neural\narchitectures. The search strategy combines global architecture search via\nBayesian Optimization in the search space with local exploration for novel\nneural networks via gradient ascent in the latent space. Such a hybrid search\nstrategy allows GraB-NAS to discover task-aware architectures with strong\nperformance, even beyond the predefined search space. Extensive experiments\ndemonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,\nachieving better generalization and search effectiveness.",
          "arxiv_id": "2508.09467v1"
        }
      ],
      "62": [
        {
          "title": "In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation",
          "year": "2023-06",
          "abstract": "Out-of-distribution (OOD) detection is the problem of identifying inputs\nwhich are unrelated to the in-distribution task. The OOD detection performance\nwhen the in-distribution (ID) is ImageNet-1K is commonly being tested on a\nsmall range of test OOD datasets. We find that most of the currently used test\nOOD datasets, including datasets from the open set recognition (OSR)\nliterature, have severe issues: In some cases more than 50$\\%$ of the dataset\ncontains objects belonging to one of the ID classes. These erroneous samples\nheavily distort the evaluation of OOD detectors. As a solution, we introduce\nwith NINCO a novel test OOD dataset, each sample checked to be ID free, which\nwith its fine-grained range of OOD classes allows for a detailed analysis of an\nOOD detector's strengths and failure modes, particularly when paired with a\nnumber of synthetic \"OOD unit-tests\". We provide detailed evaluations across a\nlarge set of architectures and OOD detection methods on NINCO and the\nunit-tests, revealing new insights about model weaknesses and the effects of\npretraining on OOD detection performance. We provide code and data at\nhttps://github.com/j-cb/NINCO.",
          "arxiv_id": "2306.00826v1"
        },
        {
          "title": "Meta OOD Learning for Continuously Adaptive OOD Detection",
          "year": "2023-09",
          "abstract": "Out-of-distribution (OOD) detection is crucial to modern deep learning\napplications by identifying and alerting about the OOD samples that should not\nbe tested or used for making predictions. Current OOD detection methods have\nmade significant progress when in-distribution (ID) and OOD samples are drawn\nfrom static distributions. However, this can be unrealistic when applied to\nreal-world systems which often undergo continuous variations and shifts in ID\nand OOD distributions over time. Therefore, for an effective application in\nreal-world systems, the development of OOD detection methods that can adapt to\nthese dynamic and evolving distributions is essential. In this paper, we\npropose a novel and more realistic setting called continuously adaptive\nout-of-distribution (CAOOD) detection which targets on developing an OOD\ndetection model that enables dynamic and quick adaptation to a new arriving\ndistribution, with insufficient ID samples during deployment time. To address\nCAOOD, we develop meta OOD learning (MOL) by designing a learning-to-adapt\ndiagram such that a good initialized OOD detection model is learned during the\ntraining process. In the testing process, MOL ensures OOD detection performance\nover shifting distributions by quickly adapting to new distributions with a few\nadaptations. Extensive experiments on several OOD benchmarks endorse the\neffectiveness of our method in preserving both ID classification accuracy and\nOOD detection performance on continuously shifting distributions.",
          "arxiv_id": "2309.11705v1"
        },
        {
          "title": "Can We Ignore Labels In Out of Distribution Detection?",
          "year": "2025-04",
          "abstract": "Out-of-distribution (OOD) detection methods have recently become more\nprominent, serving as a core element in safety-critical autonomous systems. One\nmajor purpose of OOD detection is to reject invalid inputs that could lead to\nunpredictable errors and compromise safety. Due to the cost of labeled data,\nrecent works have investigated the feasibility of self-supervised learning\n(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In\nthis work, we identify a set of conditions for a theoretical guarantee of\nfailure in unlabeled OOD detection algorithms from an information-theoretic\nperspective. These conditions are present in all OOD tasks dealing with\nreal-world data: I) we provide theoretical proof of unlabeled OOD detection\nfailure when there exists zero mutual information between the learning\nobjective and the in-distribution labels, a.k.a. 'label blindness', II) we\ndefine a new OOD task - Adjacent OOD detection - that tests for label blindness\nand accounts for a previously ignored safety gap in all OOD detection\nbenchmarks, and III) we perform experiments demonstrating that existing\nunlabeled OOD methods fail under conditions suggested by our label blindness\ntheory and analyze the implications for future research in unlabeled OOD\nmethods.",
          "arxiv_id": "2504.14704v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T19:40:04Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}