{
  "topics": {
    "data": {
      "0": {
        "name": "0_blockchain_Blockchain_smart_contracts",
        "keywords": [
          [
            "blockchain",
            0.031288422326568636
          ],
          [
            "Blockchain",
            0.014666115693072178
          ],
          [
            "smart",
            0.013323761119807377
          ],
          [
            "contracts",
            0.01169118485713966
          ],
          [
            "transactions",
            0.011507207477921992
          ],
          [
            "chain",
            0.010981903936070915
          ],
          [
            "transaction",
            0.010844888632684865
          ],
          [
            "Ethereum",
            0.010757246240862939
          ],
          [
            "data",
            0.009819661006369702
          ],
          [
            "decentralized",
            0.00924932217249653
          ]
        ],
        "count": 3134
      },
      "1": {
        "name": "1_quantum_key_Quantum_encryption",
        "keywords": [
          [
            "quantum",
            0.03644430555355466
          ],
          [
            "key",
            0.014610040268680088
          ],
          [
            "Quantum",
            0.012423835728372018
          ],
          [
            "encryption",
            0.011497337407510896
          ],
          [
            "security",
            0.01100894691539314
          ],
          [
            "scheme",
            0.010380713221757052
          ],
          [
            "secure",
            0.01027436613621669
          ],
          [
            "classical",
            0.010272733601897188
          ],
          [
            "cryptographic",
            0.009229440582071096
          ],
          [
            "cryptography",
            0.008965309987549777
          ]
        ],
        "count": 3080
      },
      "2": {
        "name": "2_LLMs_LLM_models_AI",
        "keywords": [
          [
            "LLMs",
            0.026766972850754158
          ],
          [
            "LLM",
            0.018304333401254567
          ],
          [
            "models",
            0.01474795828290163
          ],
          [
            "AI",
            0.013077947858154424
          ],
          [
            "language",
            0.012818167076359892
          ],
          [
            "Language",
            0.012371875271050874
          ],
          [
            "Large",
            0.012062400401457712
          ],
          [
            "model",
            0.010566767882720469
          ],
          [
            "Models",
            0.010531415762045334
          ],
          [
            "safety",
            0.010423623541269345
          ]
        ],
        "count": 2182
      },
      "3": {
        "name": "3_privacy_DP_data_private",
        "keywords": [
          [
            "privacy",
            0.03435389734838399
          ],
          [
            "DP",
            0.02436074185760147
          ],
          [
            "data",
            0.018543041660662046
          ],
          [
            "private",
            0.018204068906336798
          ],
          [
            "differential privacy",
            0.015433046180566026
          ],
          [
            "differential",
            0.015377547626980102
          ],
          [
            "utility",
            0.011721157145877969
          ],
          [
            "Privacy",
            0.011391436902663105
          ],
          [
            "Differential",
            0.009648834293940624
          ],
          [
            "model",
            0.009477653051154727
          ]
        ],
        "count": 1775
      },
      "4": {
        "name": "4_FL_Federated_learning_model",
        "keywords": [
          [
            "FL",
            0.03236214429307436
          ],
          [
            "Federated",
            0.02669149413800849
          ],
          [
            "learning",
            0.02235669975394854
          ],
          [
            "model",
            0.020751452895011898
          ],
          [
            "privacy",
            0.020392684918188964
          ],
          [
            "federated",
            0.019493408727776065
          ],
          [
            "clients",
            0.019492205847408562
          ],
          [
            "Learning",
            0.018142405265073065
          ],
          [
            "data",
            0.01772625718248584
          ],
          [
            "federated learning",
            0.016270368536225335
          ]
        ],
        "count": 1276
      },
      "5": {
        "name": "5_adversarial_attacks_robustness_Adversarial",
        "keywords": [
          [
            "adversarial",
            0.044728439890807534
          ],
          [
            "attacks",
            0.020016812646769775
          ],
          [
            "robustness",
            0.019245582640828693
          ],
          [
            "Adversarial",
            0.019015802487708077
          ],
          [
            "examples",
            0.018076345391297165
          ],
          [
            "adversarial examples",
            0.016600893177817683
          ],
          [
            "attack",
            0.015082121644053103
          ],
          [
            "models",
            0.014298356336081383
          ],
          [
            "adversarial attacks",
            0.01377191243858265
          ],
          [
            "perturbations",
            0.012593652824717394
          ]
        ],
        "count": 1050
      },
      "6": {
        "name": "6_code_software_vulnerability_vulnerabilities",
        "keywords": [
          [
            "code",
            0.03393599935849493
          ],
          [
            "software",
            0.020836176456015112
          ],
          [
            "vulnerability",
            0.01921634345485931
          ],
          [
            "vulnerabilities",
            0.01874761947602034
          ],
          [
            "security",
            0.014346051875557821
          ],
          [
            "source",
            0.010907783608586701
          ],
          [
            "fuzzing",
            0.010437641444330879
          ],
          [
            "analysis",
            0.009251850228547995
          ],
          [
            "detection",
            0.009087146473103805
          ],
          [
            "vulnerability detection",
            0.008789602026882224
          ]
        ],
        "count": 1013
      },
      "7": {
        "name": "7_detection_network_traffic_intrusion",
        "keywords": [
          [
            "detection",
            0.022891792054260708
          ],
          [
            "network",
            0.021173485710045226
          ],
          [
            "traffic",
            0.019718556669299347
          ],
          [
            "intrusion",
            0.014178443927153063
          ],
          [
            "Detection",
            0.01350240112715079
          ],
          [
            "learning",
            0.01299813777960371
          ],
          [
            "intrusion detection",
            0.011997846028674312
          ],
          [
            "Intrusion",
            0.011645541324091358
          ],
          [
            "attacks",
            0.011004074344138134
          ],
          [
            "IDS",
            0.010781871320689786
          ]
        ],
        "count": 1004
      },
      "8": {
        "name": "8_cyber_security_cybersecurity_Cyber",
        "keywords": [
          [
            "cyber",
            0.027506174618556634
          ],
          [
            "security",
            0.01796512681702177
          ],
          [
            "cybersecurity",
            0.01622200086254131
          ],
          [
            "Cyber",
            0.012657802984541795
          ],
          [
            "threats",
            0.009374238845846573
          ],
          [
            "systems",
            0.009284962129182518
          ],
          [
            "research",
            0.008798899532851975
          ],
          [
            "risk",
            0.00824085916588931
          ],
          [
            "network",
            0.00804001343612776
          ],
          [
            "attack",
            0.007705926593384345
          ]
        ],
        "count": 874
      },
      "9": {
        "name": "9_memory_execution_hardware_cache",
        "keywords": [
          [
            "memory",
            0.022572151286276716
          ],
          [
            "execution",
            0.015955539078428973
          ],
          [
            "hardware",
            0.015557324739498302
          ],
          [
            "cache",
            0.014679446373355479
          ],
          [
            "security",
            0.01389762592649756
          ],
          [
            "software",
            0.012638581417838076
          ],
          [
            "attacks",
            0.012547064899508758
          ],
          [
            "performance",
            0.010985382673662027
          ],
          [
            "channel",
            0.010974438883319183
          ],
          [
            "code",
            0.01095740971286764
          ]
        ],
        "count": 731
      },
      "10": {
        "name": "10_malware_detection_Malware_malware detection",
        "keywords": [
          [
            "malware",
            0.06556518304784126
          ],
          [
            "detection",
            0.023023337911162994
          ],
          [
            "Malware",
            0.021697464053261797
          ],
          [
            "malware detection",
            0.0189443834321278
          ],
          [
            "Android",
            0.016109678445014684
          ],
          [
            "ransomware",
            0.01593736058991791
          ],
          [
            "learning",
            0.012929796171891338
          ],
          [
            "classification",
            0.010922594135761472
          ],
          [
            "analysis",
            0.010153194009585666
          ],
          [
            "features",
            0.010122863827056336
          ]
        ],
        "count": 720
      },
      "11": {
        "name": "11_password_DNS_users_user",
        "keywords": [
          [
            "password",
            0.020754778665232426
          ],
          [
            "DNS",
            0.017851880718744897
          ],
          [
            "users",
            0.015392788744067973
          ],
          [
            "user",
            0.014097248090665114
          ],
          [
            "web",
            0.013043492252603083
          ],
          [
            "authentication",
            0.012892290329492285
          ],
          [
            "passwords",
            0.012332534025361505
          ],
          [
            "websites",
            0.011657217475728551
          ],
          [
            "security",
            0.011161935526631176
          ],
          [
            "traffic",
            0.010598879480577311
          ]
        ],
        "count": 713
      },
      "12": {
        "name": "12_watermarking_watermark_image_watermarks",
        "keywords": [
          [
            "watermarking",
            0.03954605290125658
          ],
          [
            "watermark",
            0.03148298696465609
          ],
          [
            "image",
            0.020935036547295855
          ],
          [
            "watermarks",
            0.017028635212458017
          ],
          [
            "model",
            0.01485108808330175
          ],
          [
            "embedding",
            0.014675889428474854
          ],
          [
            "Watermarking",
            0.014344153196112652
          ],
          [
            "steganography",
            0.014173814240999224
          ],
          [
            "images",
            0.013238426112780468
          ],
          [
            "text",
            0.012490288356759423
          ]
        ],
        "count": 625
      },
      "13": {
        "name": "13_face_biometric_recognition_authentication",
        "keywords": [
          [
            "face",
            0.023379967110774974
          ],
          [
            "biometric",
            0.023154786552524082
          ],
          [
            "recognition",
            0.01736837352487745
          ],
          [
            "authentication",
            0.017144167302351503
          ],
          [
            "facial",
            0.013696899294173432
          ],
          [
            "images",
            0.012959666703739542
          ],
          [
            "user",
            0.012703631636954546
          ],
          [
            "privacy",
            0.01094898875573799
          ],
          [
            "Face",
            0.009701522709603502
          ],
          [
            "systems",
            0.009700080069922854
          ]
        ],
        "count": 583
      },
      "14": {
        "name": "14_backdoor_attacks_backdoor attacks_model",
        "keywords": [
          [
            "backdoor",
            0.045723364348648564
          ],
          [
            "attacks",
            0.022957147420291323
          ],
          [
            "backdoor attacks",
            0.019403874378993352
          ],
          [
            "model",
            0.01923032273368679
          ],
          [
            "trigger",
            0.017835348736563783
          ],
          [
            "training",
            0.017805130124998177
          ],
          [
            "Backdoor",
            0.01734414134398131
          ],
          [
            "attack",
            0.01723983565108212
          ],
          [
            "poisoning",
            0.0160694615049236
          ],
          [
            "clean",
            0.014300021387692288
          ]
        ],
        "count": 527
      },
      "15": {
        "name": "15_vehicles_vehicle_CAN_attacks",
        "keywords": [
          [
            "vehicles",
            0.02288835062691294
          ],
          [
            "vehicle",
            0.02191043273592077
          ],
          [
            "CAN",
            0.016446914794496063
          ],
          [
            "attacks",
            0.014160906324523288
          ],
          [
            "security",
            0.013295544141276082
          ],
          [
            "communication",
            0.010772024477440096
          ],
          [
            "autonomous",
            0.00999010691088342
          ],
          [
            "attack",
            0.009937542271120467
          ],
          [
            "systems",
            0.009559544245606523
          ],
          [
            "detection",
            0.009292028120059777
          ]
        ],
        "count": 512
      },
      "16": {
        "name": "16_wireless_security_networks_network",
        "keywords": [
          [
            "wireless",
            0.017977105561431176
          ],
          [
            "security",
            0.016801108597941733
          ],
          [
            "networks",
            0.01503564980547009
          ],
          [
            "network",
            0.014985256715510787
          ],
          [
            "jamming",
            0.012508594248086613
          ],
          [
            "channel",
            0.01250196545978747
          ],
          [
            "communication",
            0.011689798396211313
          ],
          [
            "layer",
            0.008881552688132097
          ],
          [
            "signal",
            0.008585577848983247
          ],
          [
            "authentication",
            0.008578945082212712
          ]
        ],
        "count": 435
      },
      "17": {
        "name": "17_speech_audio_voice_speaker",
        "keywords": [
          [
            "speech",
            0.042567720780591504
          ],
          [
            "audio",
            0.03910424556172619
          ],
          [
            "voice",
            0.02866301876245888
          ],
          [
            "speaker",
            0.02354832548950197
          ],
          [
            "ASR",
            0.015043543900353786
          ],
          [
            "attacks",
            0.013690897780408539
          ],
          [
            "adversarial",
            0.012427604843481422
          ],
          [
            "Speech",
            0.012419967713614255
          ],
          [
            "Audio",
            0.01193979776893578
          ],
          [
            "systems",
            0.011604398174351419
          ]
        ],
        "count": 386
      },
      "18": {
        "name": "18_inference_HE_privacy_computation",
        "keywords": [
          [
            "inference",
            0.026207277641285776
          ],
          [
            "HE",
            0.016412794761581927
          ],
          [
            "privacy",
            0.014894964296118385
          ],
          [
            "computation",
            0.014676124092149854
          ],
          [
            "data",
            0.01391392214730305
          ],
          [
            "MPC",
            0.013465940395354445
          ],
          [
            "secure",
            0.013444629054940675
          ],
          [
            "model",
            0.01236171436726141
          ],
          [
            "neural",
            0.011971114498906782
          ],
          [
            "learning",
            0.011385243606482472
          ]
        ],
        "count": 330
      },
      "19": {
        "name": "19_IoT_devices_security_Things",
        "keywords": [
          [
            "IoT",
            0.05930106799410459
          ],
          [
            "devices",
            0.02979899173233774
          ],
          [
            "security",
            0.02166642054093513
          ],
          [
            "Things",
            0.020385757097658802
          ],
          [
            "Internet",
            0.020287892318613786
          ],
          [
            "IoT devices",
            0.015872820943115214
          ],
          [
            "smart",
            0.01311823771684419
          ],
          [
            "home",
            0.012299335194135131
          ],
          [
            "device",
            0.01102193436598553
          ],
          [
            "data",
            0.009991028004206258
          ]
        ],
        "count": 329
      },
      "20": {
        "name": "20_hardware_design_locking_logic",
        "keywords": [
          [
            "hardware",
            0.02347785182008975
          ],
          [
            "design",
            0.021662696569434346
          ],
          [
            "locking",
            0.019521786461997443
          ],
          [
            "logic",
            0.01650858628601975
          ],
          [
            "security",
            0.0163385490327623
          ],
          [
            "circuit",
            0.014559300707120488
          ],
          [
            "circuits",
            0.013706364240350906
          ],
          [
            "Trojan",
            0.01353641949099009
          ],
          [
            "HT",
            0.013001789160768577
          ],
          [
            "Trojans",
            0.01281499711544809
          ]
        ],
        "count": 326
      },
      "21": {
        "name": "21_graph_GNNs_Graph_node",
        "keywords": [
          [
            "graph",
            0.06164795504400072
          ],
          [
            "GNNs",
            0.03332272727535518
          ],
          [
            "Graph",
            0.0328299246399243
          ],
          [
            "node",
            0.023028507951279337
          ],
          [
            "GNN",
            0.02205783222010126
          ],
          [
            "attack",
            0.01669292885161409
          ],
          [
            "graphs",
            0.01611040917838524
          ],
          [
            "attacks",
            0.014605273692624458
          ],
          [
            "nodes",
            0.014040535123819895
          ],
          [
            "Neural",
            0.013824022537978854
          ]
        ],
        "count": 304
      },
      "22": {
        "name": "22_phishing_Phishing_emails_detection",
        "keywords": [
          [
            "phishing",
            0.07118833595152056
          ],
          [
            "Phishing",
            0.027001049266255378
          ],
          [
            "emails",
            0.02209931906082489
          ],
          [
            "detection",
            0.020453429759168848
          ],
          [
            "email",
            0.017445519374067447
          ],
          [
            "URL",
            0.01597709169723227
          ],
          [
            "URLs",
            0.013921267849026423
          ],
          [
            "phishing emails",
            0.011584308317581338
          ],
          [
            "attacks",
            0.011222848389035686
          ],
          [
            "phishing detection",
            0.011197671113349386
          ]
        ],
        "count": 300
      },
      "23": {
        "name": "23_grid_power_smart_grids",
        "keywords": [
          [
            "grid",
            0.0380474025508874
          ],
          [
            "power",
            0.027723786703194288
          ],
          [
            "smart",
            0.023014329294523204
          ],
          [
            "grids",
            0.017306786781155856
          ],
          [
            "cyber",
            0.017154430276378283
          ],
          [
            "energy",
            0.016337315432858105
          ],
          [
            "smart grid",
            0.015510519796128741
          ],
          [
            "data",
            0.014979194441628488
          ],
          [
            "Smart",
            0.014340009544714665
          ],
          [
            "power grid",
            0.012798770664966595
          ]
        ],
        "count": 295
      },
      "24": {
        "name": "24_apps_Android_app_mobile",
        "keywords": [
          [
            "apps",
            0.06298927373892424
          ],
          [
            "Android",
            0.044924944508715384
          ],
          [
            "app",
            0.03945492550497279
          ],
          [
            "mobile",
            0.020098547506513503
          ],
          [
            "privacy",
            0.01937182122711974
          ],
          [
            "user",
            0.014846933735795997
          ],
          [
            "users",
            0.014259319077355528
          ],
          [
            "security",
            0.013699423448622705
          ],
          [
            "analysis",
            0.013466755529630073
          ],
          [
            "data",
            0.013262744147752855
          ]
        ],
        "count": 263
      },
      "25": {
        "name": "25_models_image_images_diffusion",
        "keywords": [
          [
            "models",
            0.03142291718251421
          ],
          [
            "image",
            0.031225032326900545
          ],
          [
            "images",
            0.025824373975869872
          ],
          [
            "diffusion",
            0.0250199087379112
          ],
          [
            "diffusion models",
            0.019647102110791563
          ],
          [
            "Diffusion",
            0.019134388198942087
          ],
          [
            "text",
            0.018136979431037405
          ],
          [
            "generation",
            0.016405747487353177
          ],
          [
            "generative",
            0.015753514415391093
          ],
          [
            "prompts",
            0.013067869335685581
          ]
        ],
        "count": 259
      },
      "26": {
        "name": "26_synthetic_data_synthetic data_privacy",
        "keywords": [
          [
            "synthetic",
            0.046561584791697655
          ],
          [
            "data",
            0.0441587973428374
          ],
          [
            "synthetic data",
            0.04008873086120226
          ],
          [
            "privacy",
            0.030326510167870552
          ],
          [
            "Synthetic",
            0.0187496531819411
          ],
          [
            "private",
            0.018348840370922145
          ],
          [
            "DP",
            0.016522004050669524
          ],
          [
            "generative",
            0.015950391217263417
          ],
          [
            "data generation",
            0.015252172675471225
          ],
          [
            "Data",
            0.013869158715160498
          ]
        ],
        "count": 237
      },
      "27": {
        "name": "27_DNN_model_DNNs_neural",
        "keywords": [
          [
            "DNN",
            0.034385920264894634
          ],
          [
            "model",
            0.01800798613217091
          ],
          [
            "DNNs",
            0.01591012658078552
          ],
          [
            "neural",
            0.01576802063006046
          ],
          [
            "Neural",
            0.01484652732240093
          ],
          [
            "hardware",
            0.014548732348679412
          ],
          [
            "accelerators",
            0.012527880429795886
          ],
          [
            "attack",
            0.012394857565338788
          ],
          [
            "models",
            0.01211960596470007
          ],
          [
            "inference",
            0.011853521645441208
          ]
        ],
        "count": 189
      }
    },
    "correlations": [
      [
        1.0,
        -0.7503380952991283,
        -0.7523181055803488,
        -0.7382160440917303,
        -0.7204094156098311,
        -0.7494251977590074,
        -0.7146166207769096,
        -0.7335669311934587,
        -0.734477493087077,
        -0.7529415613849512,
        -0.7613924645542244,
        -0.718435807775961,
        -0.7632828689231941,
        -0.7493849457571097,
        -0.7436024922680445,
        -0.739223883153297,
        -0.7147640626007159,
        -0.764490586541237,
        -0.7535951413997437,
        -0.7028108405573561,
        -0.720859932713142,
        -0.7378879055017877,
        -0.7566487342185553,
        -0.7076098858877344,
        -0.7592406366832021,
        -0.7587583010360348,
        -0.7303808590866453,
        -0.7535278610984
      ],
      [
        -0.7503380952991283,
        1.0,
        -0.7624520958983148,
        -0.7500087648277494,
        -0.7440961644016708,
        -0.7508698193624569,
        -0.729959857368874,
        -0.7546310783022615,
        -0.727818971810645,
        -0.7379989979572907,
        -0.760807955585086,
        -0.7351302252296993,
        -0.7606199206719642,
        -0.7545856882221117,
        -0.7434732606614027,
        -0.7481009753357397,
        -0.7162436565890189,
        -0.764377622911619,
        -0.7448519643250284,
        -0.7312519669757074,
        -0.7165725238979944,
        -0.7564554733867503,
        -0.7613371694606541,
        -0.7482842442112151,
        -0.7634778870537982,
        -0.7571908044492455,
        -0.7513584071524291,
        -0.7539728724778898
      ],
      [
        -0.7523181055803488,
        -0.7624520958983148,
        1.0,
        -0.7241384064498129,
        -0.7195458523678135,
        -0.6789031455527732,
        -0.6486507413592616,
        -0.7368757207793908,
        -0.7340492298174448,
        -0.7552490875290889,
        -0.7394214658086732,
        -0.7299153982653812,
        -0.7104297771029715,
        -0.7526083426710322,
        -0.6684437935464792,
        -0.732117290934678,
        -0.7455480491771425,
        -0.7541619756676603,
        -0.7314733601668608,
        -0.7475499485349926,
        -0.738089815024233,
        -0.7453131708702361,
        -0.7330278159017867,
        -0.7544026350948391,
        -0.7556886021835973,
        -0.5842315216399706,
        -0.6998389879390394,
        -0.725412818178006
      ],
      [
        -0.7382160440917303,
        -0.7500087648277494,
        -0.7241384064498129,
        1.0,
        -0.397996155856018,
        -0.7350482491754762,
        -0.7373787607447115,
        -0.7281128969317117,
        -0.7441356458927992,
        -0.752981942002257,
        -0.7577959544532054,
        -0.678671129404776,
        -0.7589513073255328,
        -0.7423569882983219,
        -0.6967024750910861,
        -0.7422649553528994,
        -0.7341231277252792,
        -0.7542738881179213,
        -0.6281611710956065,
        -0.7300856454992288,
        -0.7326149827973054,
        -0.7227624491193076,
        -0.760333490489477,
        -0.7377112993745876,
        -0.7496240000370405,
        -0.7166141778971789,
        -0.08934058143181006,
        -0.7050780083422719
      ],
      [
        -0.7204094156098311,
        -0.7440961644016708,
        -0.7195458523678135,
        -0.397996155856018,
        1.0,
        -0.6939105678552593,
        -0.7192985692018778,
        -0.6586692776426433,
        -0.7259396151558495,
        -0.7521383150297505,
        -0.742026413649733,
        -0.699890751938384,
        -0.7436060084473081,
        -0.7420819962002947,
        -0.5343960501472214,
        -0.7171945551178587,
        -0.7179115672425384,
        -0.7519197977447227,
        -0.5968027594898282,
        -0.7037362903362592,
        -0.7269932976038143,
        -0.7330833596684083,
        -0.7533956998601369,
        -0.7362974411729586,
        -0.7491108920058245,
        -0.7067440213773728,
        -0.3424026431293872,
        -0.5597320770108489
      ],
      [
        -0.7494251977590074,
        -0.7508698193624569,
        -0.6789031455527732,
        -0.7350482491754762,
        -0.6939105678552593,
        1.0,
        -0.7035835223852471,
        -0.7005135441801245,
        -0.7287533104536175,
        -0.7465555689584635,
        -0.7239422612638955,
        -0.7125710208658272,
        -0.7267787237979171,
        -0.7326313851685109,
        -0.48724748746487023,
        -0.4944540870127372,
        -0.7171437753600998,
        -0.7212550911641951,
        -0.7331735999386129,
        -0.7324276857912407,
        -0.7295280033768532,
        -0.7223500185135091,
        -0.7437882907022642,
        -0.7358906988733118,
        -0.7568213704913399,
        -0.6460638106900278,
        -0.7076812214482957,
        -0.5575558796409501
      ],
      [
        -0.7146166207769096,
        -0.729959857368874,
        -0.6486507413592616,
        -0.7373787607447115,
        -0.7192985692018778,
        -0.7035835223852471,
        1.0,
        -0.7283580643897973,
        -0.5880187256912623,
        -0.7064854781186043,
        -0.7290632260943555,
        -0.7000433759978093,
        -0.7472238500972368,
        -0.7449117527670048,
        -0.6880303486201402,
        -0.7193438137568829,
        -0.5881028397450457,
        -0.7548840872825757,
        -0.7382239473829411,
        -0.6300738023828469,
        -0.5951111758202716,
        -0.7195195877584551,
        -0.7453798256625659,
        -0.7373714270964957,
        -0.7383601632407394,
        -0.7187622925952407,
        -0.7182941939676604,
        -0.7227386301560876
      ],
      [
        -0.7335669311934587,
        -0.7546310783022615,
        -0.7368757207793908,
        -0.7281128969317117,
        -0.6586692776426433,
        -0.7005135441801245,
        -0.7283580643897973,
        1.0,
        -0.6890457681508211,
        -0.7530898111682192,
        -0.6546823561043236,
        -0.7247867767336471,
        -0.7575802288025454,
        -0.7469660174568142,
        -0.6967386371461828,
        -0.6805952424778134,
        -0.575004371383476,
        -0.7516313138454616,
        -0.651013691847343,
        -0.6015176622398261,
        -0.7303030446437754,
        -0.7084865113736731,
        -0.6669873806184934,
        -0.7230377219654462,
        -0.7517754929505134,
        -0.7304937816017573,
        -0.7066708623691504,
        -0.703177933425814
      ],
      [
        -0.734477493087077,
        -0.727818971810645,
        -0.7340492298174448,
        -0.7441356458927992,
        -0.7259396151558495,
        -0.7287533104536175,
        -0.5880187256912623,
        -0.6890457681508211,
        1.0,
        -0.7436630547028795,
        -0.7270602333108332,
        -0.7267907871629951,
        -0.7585911918892551,
        -0.7382653452420784,
        -0.7161129498541925,
        -0.7009631821678104,
        -0.48086285229309833,
        -0.7599737264406764,
        -0.7464284154977536,
        -0.5609791464274623,
        -0.5242855232471463,
        -0.7346513542750961,
        -0.7403917563328508,
        -0.6007463621725124,
        -0.7545053359361236,
        -0.7430675672956588,
        -0.732227890368923,
        -0.7404819783435703
      ],
      [
        -0.7529415613849512,
        -0.7379989979572907,
        -0.7552490875290889,
        -0.752981942002257,
        -0.7521383150297505,
        -0.7465555689584635,
        -0.7064854781186043,
        -0.7530898111682192,
        -0.7436630547028795,
        1.0,
        -0.7533131397275572,
        -0.7283511619969889,
        -0.764232053641053,
        -0.756236998933947,
        -0.7378218285160347,
        -0.7423806240305966,
        -0.7442566789758487,
        -0.765263861598739,
        -0.7438227389631168,
        -0.7354628754463662,
        -0.6490359651938203,
        -0.7574152777232526,
        -0.761894832554921,
        -0.749039436117432,
        -0.7594131070221228,
        -0.7606187811050371,
        -0.7467831291468401,
        -0.746347077588416
      ],
      [
        -0.7613924645542244,
        -0.760807955585086,
        -0.7394214658086732,
        -0.7577959544532054,
        -0.742026413649733,
        -0.7239422612638955,
        -0.7290632260943555,
        -0.6546823561043236,
        -0.7270602333108332,
        -0.7533131397275572,
        1.0,
        -0.7479026683892169,
        -0.7600013459843826,
        -0.7562246002176269,
        -0.7365910177228496,
        -0.7474521474401865,
        -0.7513991446251669,
        -0.7597847607518212,
        -0.7487016123734684,
        -0.7257835378505536,
        -0.7490519145975171,
        -0.7305436469378181,
        -0.6571420128333518,
        -0.7525396305611576,
        -0.6656444086499586,
        -0.7397608792666004,
        -0.7471500595763833,
        -0.7408436233973252
      ],
      [
        -0.718435807775961,
        -0.7351302252296993,
        -0.7299153982653812,
        -0.678671129404776,
        -0.699890751938384,
        -0.7125710208658272,
        -0.7000433759978093,
        -0.7247867767336471,
        -0.7267907871629951,
        -0.7283511619969889,
        -0.7479026683892169,
        1.0,
        -0.7536301080781002,
        -0.7198466769419694,
        -0.6961142894332281,
        -0.725547768098391,
        -0.717399179342577,
        -0.7483001846316361,
        -0.7363122850503055,
        -0.7203204826309404,
        -0.7094863761658726,
        -0.7435226844202482,
        -0.7449334216478575,
        -0.7378471652158324,
        -0.7374288424978457,
        -0.7268737595658501,
        -0.6990486798161806,
        -0.72348161225507
      ],
      [
        -0.7632828689231941,
        -0.7606199206719642,
        -0.7104297771029715,
        -0.7589513073255328,
        -0.7436060084473081,
        -0.7267787237979171,
        -0.7472238500972368,
        -0.7575802288025454,
        -0.7585911918892551,
        -0.764232053641053,
        -0.7600013459843826,
        -0.7536301080781002,
        1.0,
        -0.7488192294011986,
        -0.728215681862818,
        -0.7555636561192669,
        -0.7557623285997497,
        -0.7294620230665615,
        -0.7598112926493623,
        -0.7607770407341221,
        -0.7516243280593676,
        -0.754483828230528,
        -0.7592801486759191,
        -0.758434238945912,
        -0.7637313572376692,
        -0.561421678162173,
        -0.743910415200557,
        -0.6794364606698307
      ],
      [
        -0.7493849457571097,
        -0.7545856882221117,
        -0.7526083426710322,
        -0.7423569882983219,
        -0.7420819962002947,
        -0.7326313851685109,
        -0.7449117527670048,
        -0.7469660174568142,
        -0.7382653452420784,
        -0.756236998933947,
        -0.7562246002176269,
        -0.7198466769419694,
        -0.7488192294011986,
        1.0,
        -0.7325504612395835,
        -0.7468729964794412,
        -0.740000262608671,
        -0.727652486427822,
        -0.7441559486972602,
        -0.7408953122061064,
        -0.7460051008718931,
        -0.7609322381391896,
        -0.752196887899425,
        -0.7544740912328385,
        -0.7532823688570207,
        -0.7210660543572212,
        -0.7324920510422253,
        -0.7378400240494534
      ],
      [
        -0.7436024922680445,
        -0.7434732606614027,
        -0.6684437935464792,
        -0.6967024750910861,
        -0.5343960501472214,
        -0.48724748746487023,
        -0.6880303486201402,
        -0.6967386371461828,
        -0.7161129498541925,
        -0.7378218285160347,
        -0.7365910177228496,
        -0.6961142894332281,
        -0.728215681862818,
        -0.7325504612395835,
        1.0,
        -0.4625755890577016,
        -0.7100130639969823,
        -0.733555441892755,
        -0.7152670989273611,
        -0.7224471567980273,
        -0.7117610191465626,
        -0.7171029850320623,
        -0.7431959085694423,
        -0.7299419720052235,
        -0.7528627873592033,
        -0.6613189521190537,
        -0.620106591057359,
        -0.46680170095005974
      ],
      [
        -0.739223883153297,
        -0.7481009753357397,
        -0.732117290934678,
        -0.7422649553528994,
        -0.7171945551178587,
        -0.4944540870127372,
        -0.7193438137568829,
        -0.6805952424778134,
        -0.7009631821678104,
        -0.7423806240305966,
        -0.7474521474401865,
        -0.725547768098391,
        -0.7555636561192669,
        -0.7468729964794412,
        -0.4625755890577016,
        1.0,
        -0.7098905620758208,
        -0.7501943211718549,
        -0.7461191184832379,
        -0.7218566398228824,
        -0.7259806202794903,
        -0.7483838120946105,
        -0.7456503790639164,
        -0.7242158002046775,
        -0.7543154205568369,
        -0.733705227067796,
        -0.7283521225816281,
        -0.7170013780445421
      ],
      [
        -0.7147640626007159,
        -0.7162436565890189,
        -0.7455480491771425,
        -0.7341231277252792,
        -0.7179115672425384,
        -0.7171437753600998,
        -0.5881028397450457,
        -0.575004371383476,
        -0.48086285229309833,
        -0.7442566789758487,
        -0.7513991446251669,
        -0.717399179342577,
        -0.7557623285997497,
        -0.740000262608671,
        -0.7100130639969823,
        -0.7098905620758208,
        1.0,
        -0.756528459682725,
        -0.7439911221519553,
        -0.5413056163094111,
        -0.513543126461918,
        -0.7428201733234079,
        -0.7570931526716873,
        -0.7309825730476662,
        -0.7505675823358249,
        -0.7403015205977177,
        -0.7249876546209879,
        -0.7090059151561883
      ],
      [
        -0.764490586541237,
        -0.764377622911619,
        -0.7541619756676603,
        -0.7542738881179213,
        -0.7519197977447227,
        -0.7212550911641951,
        -0.7548840872825757,
        -0.7516313138454616,
        -0.7599737264406764,
        -0.765263861598739,
        -0.7597847607518212,
        -0.7483001846316361,
        -0.7294620230665615,
        -0.727652486427822,
        -0.733555441892755,
        -0.7501943211718549,
        -0.756528459682725,
        1.0,
        -0.7573344983694368,
        -0.752322216093053,
        -0.7577347359628359,
        -0.7629657006623034,
        -0.7579001428345911,
        -0.7624298370772227,
        -0.7537324186320686,
        -0.7302574088022993,
        -0.7416954267635838,
        -0.7428908647626616
      ],
      [
        -0.7535951413997437,
        -0.7448519643250284,
        -0.7314733601668608,
        -0.6281611710956065,
        -0.5968027594898282,
        -0.7331735999386129,
        -0.7382239473829411,
        -0.651013691847343,
        -0.7464284154977536,
        -0.7438227389631168,
        -0.7487016123734684,
        -0.7363122850503055,
        -0.7598112926493623,
        -0.7441559486972602,
        -0.7152670989273611,
        -0.7461191184832379,
        -0.7439911221519553,
        -0.7573344983694368,
        1.0,
        -0.7461403100182626,
        -0.7350398907697089,
        -0.7367091039907663,
        -0.7566977737086675,
        -0.7437285855589878,
        -0.7568731449720603,
        -0.7295686729846695,
        -0.6689389917913411,
        -0.6884387889983921
      ],
      [
        -0.7028108405573561,
        -0.7312519669757074,
        -0.7475499485349926,
        -0.7300856454992288,
        -0.7037362903362592,
        -0.7324276857912407,
        -0.6300738023828469,
        -0.6015176622398261,
        -0.5609791464274623,
        -0.7354628754463662,
        -0.7257835378505536,
        -0.7203204826309404,
        -0.7607770407341221,
        -0.7408953122061064,
        -0.7224471567980273,
        -0.7218566398228824,
        -0.5413056163094111,
        -0.752322216093053,
        -0.7461403100182626,
        1.0,
        -0.587691690320366,
        -0.7504839967187984,
        -0.755833240402493,
        -0.7137885220842568,
        -0.7385094284104494,
        -0.7470927438929174,
        -0.715595776832741,
        -0.7368347818704808
      ],
      [
        -0.720859932713142,
        -0.7165725238979944,
        -0.738089815024233,
        -0.7326149827973054,
        -0.7269932976038143,
        -0.7295280033768532,
        -0.5951111758202716,
        -0.7303030446437754,
        -0.5242855232471463,
        -0.6490359651938203,
        -0.7490519145975171,
        -0.7094863761658726,
        -0.7516243280593676,
        -0.7460051008718931,
        -0.7117610191465626,
        -0.7259806202794903,
        -0.513543126461918,
        -0.7577347359628359,
        -0.7350398907697089,
        -0.587691690320366,
        1.0,
        -0.7367078171144783,
        -0.7564924617037678,
        -0.7284948600332073,
        -0.7516117255961995,
        -0.7375279288798644,
        -0.7305738004378248,
        -0.7309972481125615
      ],
      [
        -0.7378879055017877,
        -0.7564554733867503,
        -0.7453131708702361,
        -0.7227624491193076,
        -0.7330833596684083,
        -0.7223500185135091,
        -0.7195195877584551,
        -0.7084865113736731,
        -0.7346513542750961,
        -0.7574152777232526,
        -0.7305436469378181,
        -0.7435226844202482,
        -0.754483828230528,
        -0.7609322381391896,
        -0.7171029850320623,
        -0.7483838120946105,
        -0.7428201733234079,
        -0.7629657006623034,
        -0.7367091039907663,
        -0.7504839967187984,
        -0.7367078171144783,
        1.0,
        -0.7468374916552813,
        -0.7499303411133934,
        -0.757498234974384,
        -0.743324347105699,
        -0.7288266103849519,
        -0.7201698690138006
      ],
      [
        -0.7566487342185553,
        -0.7613371694606541,
        -0.7330278159017867,
        -0.760333490489477,
        -0.7533956998601369,
        -0.7437882907022642,
        -0.7453798256625659,
        -0.6669873806184934,
        -0.7403917563328508,
        -0.761894832554921,
        -0.6571420128333518,
        -0.7449334216478575,
        -0.7592801486759191,
        -0.752196887899425,
        -0.7431959085694423,
        -0.7456503790639164,
        -0.7570931526716873,
        -0.7579001428345911,
        -0.7566977737086675,
        -0.755833240402493,
        -0.7564924617037678,
        -0.7468374916552813,
        1.0,
        -0.7566146527789174,
        -0.7544589374809898,
        -0.7482981648300082,
        -0.7542321648601198,
        -0.756411138610964
      ],
      [
        -0.7076098858877344,
        -0.7482842442112151,
        -0.7544026350948391,
        -0.7377112993745876,
        -0.7362974411729586,
        -0.7358906988733118,
        -0.7373714270964957,
        -0.7230377219654462,
        -0.6007463621725124,
        -0.749039436117432,
        -0.7525396305611576,
        -0.7378471652158324,
        -0.758434238945912,
        -0.7544740912328385,
        -0.7299419720052235,
        -0.7242158002046775,
        -0.7309825730476662,
        -0.7624298370772227,
        -0.7437285855589878,
        -0.7137885220842568,
        -0.7284948600332073,
        -0.7499303411133934,
        -0.7566146527789174,
        1.0,
        -0.7607898990036128,
        -0.7537277576538222,
        -0.7363593386265767,
        -0.743059147574676
      ],
      [
        -0.7592406366832021,
        -0.7634778870537982,
        -0.7556886021835973,
        -0.7496240000370405,
        -0.7491108920058245,
        -0.7568213704913399,
        -0.7383601632407394,
        -0.7517754929505134,
        -0.7545053359361236,
        -0.7594131070221228,
        -0.6656444086499586,
        -0.7374288424978457,
        -0.7637313572376692,
        -0.7532823688570207,
        -0.7528627873592033,
        -0.7543154205568369,
        -0.7505675823358249,
        -0.7537324186320686,
        -0.7568731449720603,
        -0.7385094284104494,
        -0.7516117255961995,
        -0.757498234974384,
        -0.7544589374809898,
        -0.7607898990036128,
        1.0,
        -0.7572513211155558,
        -0.7452700213104559,
        -0.7571513453683107
      ],
      [
        -0.7587583010360348,
        -0.7571908044492455,
        -0.5842315216399706,
        -0.7166141778971789,
        -0.7067440213773728,
        -0.6460638106900278,
        -0.7187622925952407,
        -0.7304937816017573,
        -0.7430675672956588,
        -0.7606187811050371,
        -0.7397608792666004,
        -0.7268737595658501,
        -0.561421678162173,
        -0.7210660543572212,
        -0.6613189521190537,
        -0.733705227067796,
        -0.7403015205977177,
        -0.7302574088022993,
        -0.7295686729846695,
        -0.7470927438929174,
        -0.7375279288798644,
        -0.743324347105699,
        -0.7482981648300082,
        -0.7537277576538222,
        -0.7572513211155558,
        1.0,
        -0.6739197172498099,
        -0.6823873681504252
      ],
      [
        -0.7303808590866453,
        -0.7513584071524291,
        -0.6998389879390394,
        -0.08934058143181006,
        -0.3424026431293872,
        -0.7076812214482957,
        -0.7182941939676604,
        -0.7066708623691504,
        -0.732227890368923,
        -0.7467831291468401,
        -0.7471500595763833,
        -0.6990486798161806,
        -0.743910415200557,
        -0.7324920510422253,
        -0.620106591057359,
        -0.7283521225816281,
        -0.7249876546209879,
        -0.7416954267635838,
        -0.6689389917913411,
        -0.715595776832741,
        -0.7305738004378248,
        -0.7288266103849519,
        -0.7542321648601198,
        -0.7363593386265767,
        -0.7452700213104559,
        -0.6739197172498099,
        1.0,
        -0.6920677982395609
      ],
      [
        -0.7535278610984,
        -0.7539728724778898,
        -0.725412818178006,
        -0.7050780083422719,
        -0.5597320770108489,
        -0.5575558796409501,
        -0.7227386301560876,
        -0.703177933425814,
        -0.7404819783435703,
        -0.746347077588416,
        -0.7408436233973252,
        -0.72348161225507,
        -0.6794364606698307,
        -0.7378400240494534,
        -0.46680170095005974,
        -0.7170013780445421,
        -0.7090059151561883,
        -0.7428908647626616,
        -0.6884387889983921,
        -0.7368347818704808,
        -0.7309972481125615,
        -0.7201698690138006,
        -0.756411138610964,
        -0.743059147574676,
        -0.7571513453683107,
        -0.6823873681504252,
        -0.6920677982395609,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        57,
        5,
        1,
        21,
        13,
        14,
        7,
        10,
        8,
        0,
        5,
        8,
        0,
        8,
        7,
        7,
        3,
        1,
        2,
        17,
        13,
        4,
        1,
        7,
        4,
        2,
        15,
        2
      ],
      "2020-02": [
        66,
        18,
        0,
        20,
        15,
        38,
        11,
        11,
        9,
        1,
        11,
        8,
        1,
        4,
        20,
        1,
        2,
        1,
        3,
        15,
        8,
        7,
        0,
        5,
        5,
        6,
        11,
        3
      ],
      "2020-03": [
        53,
        15,
        0,
        20,
        15,
        25,
        7,
        13,
        9,
        4,
        6,
        15,
        0,
        5,
        15,
        3,
        3,
        3,
        2,
        19,
        9,
        4,
        0,
        6,
        4,
        5,
        19,
        7
      ],
      "2020-04": [
        64,
        11,
        0,
        17,
        16,
        21,
        11,
        13,
        14,
        0,
        13,
        13,
        2,
        4,
        16,
        4,
        6,
        3,
        5,
        21,
        4,
        6,
        5,
        6,
        9,
        7,
        15,
        5
      ],
      "2020-05": [
        84,
        16,
        0,
        19,
        11,
        20,
        14,
        4,
        16,
        7,
        10,
        14,
        2,
        8,
        21,
        1,
        4,
        4,
        4,
        21,
        10,
        11,
        4,
        7,
        13,
        7,
        11,
        8
      ],
      "2020-06": [
        53,
        11,
        0,
        31,
        10,
        25,
        18,
        9,
        8,
        0,
        9,
        17,
        1,
        7,
        19,
        5,
        4,
        1,
        4,
        24,
        12,
        11,
        5,
        6,
        19,
        4,
        21,
        9
      ],
      "2020-07": [
        69,
        9,
        0,
        29,
        21,
        29,
        14,
        6,
        9,
        5,
        5,
        15,
        1,
        7,
        20,
        3,
        6,
        6,
        5,
        29,
        11,
        5,
        5,
        4,
        15,
        5,
        21,
        8
      ],
      "2020-08": [
        45,
        10,
        1,
        24,
        7,
        8,
        12,
        11,
        4,
        4,
        8,
        12,
        4,
        2,
        8,
        4,
        0,
        3,
        6,
        15,
        6,
        9,
        1,
        5,
        16,
        4,
        19,
        3
      ],
      "2020-09": [
        64,
        9,
        0,
        23,
        21,
        24,
        8,
        14,
        14,
        4,
        7,
        11,
        3,
        6,
        6,
        2,
        2,
        3,
        4,
        10,
        4,
        12,
        1,
        12,
        4,
        10,
        14,
        11
      ],
      "2020-10": [
        53,
        10,
        0,
        37,
        18,
        32,
        13,
        10,
        10,
        5,
        14,
        23,
        3,
        4,
        19,
        5,
        6,
        5,
        3,
        21,
        8,
        6,
        2,
        5,
        5,
        9,
        24,
        6
      ],
      "2020-11": [
        41,
        16,
        0,
        21,
        19,
        25,
        11,
        7,
        9,
        2,
        7,
        17,
        1,
        3,
        19,
        5,
        2,
        3,
        7,
        20,
        4,
        5,
        2,
        7,
        9,
        2,
        14,
        10
      ],
      "2020-12": [
        46,
        11,
        1,
        23,
        19,
        26,
        15,
        16,
        8,
        3,
        10,
        13,
        2,
        9,
        14,
        3,
        6,
        3,
        2,
        21,
        6,
        3,
        2,
        4,
        7,
        7,
        21,
        7
      ],
      "2021-01": [
        66,
        8,
        0,
        17,
        12,
        19,
        8,
        8,
        11,
        1,
        14,
        6,
        0,
        6,
        11,
        3,
        4,
        6,
        3,
        29,
        2,
        7,
        2,
        4,
        9,
        6,
        15,
        4
      ],
      "2021-02": [
        66,
        6,
        0,
        24,
        14,
        30,
        14,
        6,
        11,
        0,
        13,
        9,
        1,
        5,
        21,
        4,
        3,
        7,
        3,
        24,
        5,
        8,
        0,
        7,
        10,
        5,
        14,
        9
      ],
      "2021-03": [
        82,
        14,
        0,
        27,
        14,
        23,
        16,
        16,
        15,
        2,
        20,
        16,
        6,
        9,
        20,
        2,
        2,
        5,
        6,
        29,
        7,
        5,
        2,
        5,
        8,
        5,
        18,
        11
      ],
      "2021-04": [
        65,
        8,
        0,
        12,
        21,
        21,
        12,
        14,
        13,
        3,
        10,
        12,
        3,
        3,
        18,
        4,
        5,
        5,
        5,
        21,
        5,
        9,
        4,
        5,
        7,
        4,
        13,
        9
      ],
      "2021-05": [
        76,
        22,
        0,
        27,
        17,
        23,
        16,
        10,
        15,
        4,
        7,
        16,
        1,
        3,
        19,
        5,
        7,
        0,
        5,
        28,
        6,
        8,
        3,
        7,
        12,
        6,
        11,
        11
      ],
      "2021-06": [
        70,
        13,
        0,
        41,
        13,
        36,
        14,
        13,
        21,
        3,
        8,
        11,
        3,
        5,
        12,
        5,
        5,
        2,
        9,
        16,
        6,
        10,
        4,
        6,
        8,
        11,
        13,
        5
      ],
      "2021-07": [
        58,
        11,
        0,
        23,
        14,
        22,
        15,
        14,
        13,
        1,
        14,
        8,
        1,
        6,
        10,
        2,
        4,
        4,
        5,
        19,
        10,
        10,
        0,
        5,
        3,
        1,
        14,
        6
      ],
      "2021-08": [
        49,
        8,
        1,
        20,
        20,
        23,
        13,
        15,
        15,
        3,
        9,
        11,
        4,
        2,
        12,
        4,
        8,
        2,
        1,
        18,
        3,
        7,
        6,
        3,
        3,
        6,
        12,
        4
      ],
      "2021-09": [
        78,
        20,
        1,
        18,
        19,
        17,
        8,
        5,
        12,
        3,
        20,
        13,
        2,
        5,
        15,
        3,
        1,
        12,
        7,
        38,
        8,
        9,
        3,
        4,
        7,
        8,
        11,
        5
      ],
      "2021-10": [
        54,
        17,
        1,
        39,
        21,
        32,
        13,
        8,
        15,
        2,
        9,
        11,
        4,
        6,
        24,
        0,
        3,
        10,
        11,
        22,
        3,
        10,
        3,
        10,
        3,
        13,
        14,
        7
      ],
      "2021-11": [
        58,
        11,
        0,
        28,
        26,
        22,
        13,
        14,
        11,
        7,
        17,
        10,
        1,
        5,
        16,
        5,
        2,
        4,
        3,
        16,
        6,
        6,
        4,
        5,
        6,
        4,
        9,
        8
      ],
      "2021-12": [
        64,
        21,
        1,
        27,
        17,
        22,
        24,
        10,
        12,
        2,
        12,
        8,
        4,
        4,
        14,
        3,
        6,
        6,
        6,
        15,
        13,
        14,
        2,
        3,
        9,
        4,
        9,
        7
      ],
      "2022-01": [
        71,
        8,
        0,
        28,
        21,
        19,
        14,
        7,
        7,
        1,
        9,
        15,
        3,
        6,
        23,
        4,
        8,
        7,
        5,
        22,
        6,
        7,
        2,
        3,
        7,
        7,
        25,
        3
      ],
      "2022-02": [
        74,
        16,
        2,
        30,
        22,
        19,
        14,
        10,
        11,
        4,
        8,
        17,
        2,
        10,
        20,
        7,
        2,
        12,
        4,
        20,
        4,
        16,
        4,
        7,
        4,
        5,
        19,
        7
      ],
      "2022-03": [
        81,
        13,
        0,
        24,
        22,
        33,
        16,
        12,
        10,
        1,
        9,
        12,
        4,
        7,
        23,
        3,
        2,
        9,
        6,
        26,
        9,
        10,
        5,
        7,
        7,
        6,
        14,
        7
      ],
      "2022-04": [
        52,
        16,
        3,
        19,
        25,
        29,
        13,
        8,
        8,
        1,
        10,
        12,
        5,
        8,
        11,
        3,
        4,
        9,
        2,
        11,
        9,
        12,
        5,
        6,
        11,
        5,
        17,
        6
      ],
      "2022-05": [
        68,
        18,
        1,
        25,
        29,
        29,
        10,
        16,
        15,
        2,
        15,
        19,
        3,
        7,
        25,
        6,
        4,
        3,
        5,
        15,
        5,
        11,
        4,
        7,
        6,
        12,
        20,
        9
      ],
      "2022-06": [
        59,
        13,
        1,
        39,
        35,
        45,
        9,
        7,
        13,
        3,
        14,
        10,
        0,
        6,
        21,
        2,
        4,
        5,
        4,
        24,
        4,
        21,
        1,
        7,
        5,
        8,
        21,
        9
      ],
      "2022-07": [
        69,
        11,
        0,
        32,
        34,
        25,
        14,
        11,
        12,
        0,
        7,
        13,
        2,
        8,
        11,
        4,
        6,
        6,
        5,
        14,
        5,
        7,
        5,
        6,
        6,
        2,
        15,
        8
      ],
      "2022-08": [
        75,
        11,
        2,
        30,
        34,
        27,
        11,
        14,
        14,
        3,
        17,
        19,
        6,
        6,
        38,
        10,
        3,
        4,
        6,
        29,
        8,
        12,
        2,
        4,
        7,
        8,
        16,
        9
      ],
      "2022-09": [
        62,
        15,
        1,
        35,
        27,
        24,
        22,
        12,
        12,
        5,
        12,
        7,
        5,
        7,
        22,
        2,
        3,
        5,
        5,
        17,
        10,
        8,
        4,
        8,
        9,
        7,
        16,
        3
      ],
      "2022-10": [
        66,
        16,
        2,
        39,
        32,
        38,
        15,
        12,
        21,
        2,
        12,
        15,
        5,
        4,
        34,
        4,
        8,
        9,
        12,
        22,
        3,
        10,
        2,
        6,
        5,
        13,
        22,
        7
      ],
      "2022-11": [
        63,
        20,
        0,
        39,
        25,
        25,
        22,
        9,
        12,
        3,
        14,
        11,
        1,
        6,
        27,
        3,
        4,
        8,
        6,
        15,
        8,
        10,
        0,
        6,
        4,
        13,
        27,
        8
      ],
      "2022-12": [
        81,
        17,
        0,
        28,
        25,
        15,
        13,
        11,
        14,
        5,
        17,
        15,
        3,
        4,
        21,
        4,
        10,
        2,
        3,
        17,
        3,
        9,
        6,
        2,
        7,
        8,
        23,
        11
      ],
      "2023-01": [
        62,
        12,
        0,
        25,
        18,
        17,
        19,
        7,
        5,
        1,
        9,
        13,
        3,
        7,
        22,
        4,
        6,
        6,
        5,
        21,
        9,
        13,
        2,
        1,
        7,
        7,
        12,
        3
      ],
      "2023-02": [
        56,
        22,
        5,
        30,
        23,
        18,
        17,
        7,
        22,
        3,
        12,
        11,
        1,
        6,
        34,
        1,
        2,
        3,
        7,
        13,
        3,
        9,
        0,
        12,
        9,
        13,
        19,
        9
      ],
      "2023-03": [
        84,
        24,
        7,
        22,
        32,
        35,
        17,
        8,
        12,
        2,
        10,
        8,
        6,
        6,
        26,
        5,
        4,
        4,
        6,
        28,
        9,
        10,
        3,
        4,
        3,
        9,
        14,
        3
      ],
      "2023-04": [
        78,
        22,
        4,
        26,
        26,
        25,
        20,
        13,
        12,
        1,
        9,
        8,
        2,
        4,
        19,
        1,
        7,
        6,
        6,
        27,
        10,
        18,
        1,
        5,
        2,
        9,
        17,
        15
      ],
      "2023-05": [
        83,
        13,
        16,
        39,
        35,
        41,
        15,
        4,
        15,
        2,
        14,
        13,
        9,
        5,
        26,
        5,
        8,
        5,
        5,
        33,
        8,
        14,
        5,
        10,
        5,
        11,
        28,
        8
      ],
      "2023-06": [
        102,
        19,
        15,
        46,
        35,
        34,
        20,
        20,
        10,
        4,
        13,
        15,
        6,
        2,
        28,
        4,
        3,
        5,
        11,
        24,
        9,
        11,
        2,
        10,
        12,
        15,
        25,
        6
      ],
      "2023-07": [
        77,
        14,
        17,
        33,
        21,
        28,
        14,
        10,
        21,
        1,
        14,
        12,
        4,
        9,
        23,
        6,
        6,
        6,
        6,
        13,
        9,
        8,
        2,
        11,
        4,
        11,
        22,
        9
      ],
      "2023-08": [
        87,
        16,
        13,
        32,
        40,
        29,
        22,
        11,
        17,
        7,
        17,
        10,
        5,
        9,
        26,
        5,
        4,
        8,
        7,
        17,
        8,
        15,
        10,
        5,
        6,
        15,
        8,
        11
      ],
      "2023-09": [
        94,
        27,
        15,
        29,
        32,
        29,
        16,
        11,
        15,
        3,
        13,
        15,
        8,
        4,
        18,
        4,
        4,
        7,
        7,
        20,
        14,
        16,
        6,
        6,
        3,
        13,
        17,
        10
      ],
      "2023-10": [
        65,
        18,
        39,
        27,
        30,
        21,
        26,
        12,
        11,
        5,
        11,
        12,
        6,
        8,
        29,
        2,
        8,
        1,
        8,
        21,
        6,
        16,
        9,
        4,
        4,
        9,
        26,
        6
      ],
      "2023-11": [
        75,
        28,
        23,
        20,
        29,
        35,
        21,
        17,
        15,
        3,
        7,
        4,
        7,
        8,
        17,
        5,
        6,
        2,
        4,
        23,
        8,
        10,
        3,
        6,
        3,
        15,
        31,
        8
      ],
      "2023-12": [
        77,
        20,
        33,
        34,
        39,
        41,
        24,
        10,
        18,
        1,
        11,
        12,
        11,
        7,
        28,
        4,
        6,
        6,
        10,
        28,
        4,
        20,
        1,
        10,
        1,
        11,
        20,
        8
      ],
      "2024-01": [
        79,
        25,
        29,
        22,
        37,
        22,
        23,
        10,
        15,
        0,
        7,
        8,
        6,
        5,
        19,
        11,
        4,
        9,
        7,
        24,
        10,
        12,
        9,
        5,
        2,
        8,
        16,
        8
      ],
      "2024-02": [
        103,
        20,
        67,
        41,
        39,
        36,
        19,
        15,
        20,
        6,
        9,
        15,
        13,
        8,
        35,
        4,
        8,
        2,
        9,
        24,
        10,
        16,
        11,
        5,
        11,
        16,
        25,
        7
      ],
      "2024-03": [
        86,
        16,
        48,
        31,
        41,
        33,
        22,
        17,
        13,
        2,
        13,
        17,
        13,
        5,
        31,
        3,
        8,
        3,
        10,
        31,
        4,
        17,
        5,
        8,
        6,
        19,
        28,
        9
      ],
      "2024-04": [
        87,
        23,
        43,
        21,
        39,
        26,
        22,
        9,
        18,
        2,
        16,
        13,
        7,
        10,
        26,
        0,
        4,
        11,
        4,
        21,
        4,
        11,
        3,
        6,
        4,
        11,
        20,
        4
      ],
      "2024-05": [
        111,
        23,
        61,
        36,
        41,
        29,
        24,
        13,
        16,
        4,
        22,
        15,
        16,
        5,
        33,
        5,
        8,
        4,
        7,
        35,
        10,
        24,
        6,
        7,
        6,
        17,
        23,
        7
      ],
      "2024-06": [
        89,
        20,
        64,
        42,
        39,
        23,
        25,
        15,
        12,
        2,
        7,
        9,
        7,
        3,
        37,
        8,
        8,
        6,
        3,
        28,
        7,
        16,
        5,
        4,
        1,
        27,
        29,
        2
      ],
      "2024-07": [
        84,
        14,
        55,
        40,
        37,
        28,
        37,
        18,
        8,
        5,
        8,
        16,
        10,
        4,
        29,
        2,
        7,
        4,
        4,
        26,
        2,
        9,
        4,
        8,
        8,
        16,
        34,
        5
      ],
      "2024-08": [
        88,
        14,
        58,
        25,
        25,
        31,
        33,
        20,
        12,
        5,
        14,
        6,
        5,
        5,
        22,
        5,
        4,
        7,
        5,
        19,
        3,
        10,
        11,
        4,
        6,
        15,
        20,
        4
      ],
      "2024-09": [
        94,
        22,
        66,
        24,
        33,
        25,
        32,
        15,
        17,
        4,
        17,
        11,
        10,
        15,
        33,
        7,
        7,
        14,
        5,
        20,
        13,
        12,
        9,
        8,
        10,
        20,
        28,
        3
      ],
      "2024-10": [
        103,
        36,
        106,
        41,
        42,
        35,
        22,
        13,
        21,
        4,
        12,
        11,
        23,
        4,
        34,
        5,
        2,
        6,
        8,
        19,
        5,
        10,
        7,
        4,
        8,
        18,
        29,
        15
      ],
      "2024-11": [
        85,
        29,
        55,
        26,
        39,
        23,
        20,
        12,
        11,
        4,
        11,
        13,
        21,
        7,
        33,
        3,
        7,
        10,
        6,
        17,
        8,
        16,
        8,
        8,
        3,
        13,
        26,
        6
      ],
      "2024-12": [
        99,
        19,
        73,
        31,
        29,
        33,
        17,
        22,
        14,
        2,
        10,
        16,
        7,
        8,
        27,
        1,
        5,
        2,
        9,
        18,
        6,
        16,
        13,
        9,
        4,
        25,
        17,
        3
      ],
      "2025-01": [
        94,
        28,
        68,
        20,
        31,
        29,
        20,
        20,
        19,
        2,
        12,
        14,
        9,
        8,
        25,
        6,
        6,
        4,
        6,
        23,
        10,
        24,
        3,
        9,
        5,
        13,
        23,
        5
      ],
      "2025-02": [
        94,
        22,
        115,
        31,
        36,
        26,
        31,
        20,
        17,
        1,
        15,
        16,
        17,
        8,
        35,
        2,
        6,
        10,
        6,
        18,
        13,
        15,
        11,
        3,
        5,
        19,
        30,
        7
      ],
      "2025-03": [
        105,
        32,
        71,
        24,
        40,
        24,
        34,
        15,
        24,
        1,
        14,
        9,
        12,
        10,
        25,
        7,
        2,
        7,
        5,
        20,
        4,
        9,
        11,
        5,
        8,
        16,
        21,
        6
      ],
      "2025-04": [
        139,
        29,
        97,
        22,
        20,
        20,
        30,
        23,
        16,
        8,
        17,
        11,
        9,
        6,
        33,
        8,
        1,
        12,
        7,
        26,
        7,
        12,
        5,
        7,
        7,
        23,
        19,
        3
      ],
      "2025-05": [
        133,
        31,
        160,
        37,
        39,
        34,
        35,
        20,
        22,
        5,
        24,
        9,
        25,
        3,
        33,
        8,
        7,
        16,
        5,
        25,
        11,
        19,
        11,
        4,
        4,
        19,
        28,
        8
      ],
      "2025-06": [
        124,
        30,
        118,
        49,
        29,
        39,
        28,
        23,
        12,
        0,
        22,
        13,
        16,
        7,
        22,
        7,
        7,
        5,
        9,
        21,
        7,
        19,
        10,
        2,
        6,
        18,
        36,
        5
      ],
      "2025-07": [
        118,
        43,
        91,
        27,
        27,
        19,
        29,
        11,
        14,
        1,
        24,
        9,
        15,
        7,
        26,
        7,
        8,
        13,
        2,
        20,
        2,
        17,
        12,
        7,
        8,
        21,
        19,
        1
      ],
      "2025-08": [
        104,
        23,
        118,
        18,
        32,
        27,
        27,
        14,
        22,
        4,
        16,
        5,
        14,
        11,
        31,
        6,
        4,
        6,
        10,
        27,
        7,
        22,
        8,
        13,
        11,
        25,
        16,
        2
      ],
      "2025-09": [
        64,
        27,
        55,
        18,
        18,
        8,
        20,
        10,
        13,
        3,
        12,
        8,
        9,
        5,
        13,
        3,
        7,
        2,
        4,
        17,
        3,
        7,
        9,
        5,
        5,
        6,
        10,
        2
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Data Storage in the Decentralized World: Blockchain and Derivatives",
          "year": "2020-12",
          "abstract": "We have entered an era where the importance of decentralized solutions has\nbecome more obvious. Blockchain technology and its derivatives are distributed\nledger technologies that keep the registry of data between peers of a network.\nThis ledger is secured within a successive over looping cryptographic chain.\nThe accomplishment of the Bitcoin cryptocurrency proved that blockchain\ntechnology and its derivatives could be used to eliminate intermediaries and\nprovide security for cyberspace. However, there are some challenges in the\nimplementation of blockchain technology. This chapter first explains the\nconcept of blockchain technology and the data that we can store therein. The\nmain advantage of blockchain is the security services that it provides. This\nsection continues by describing these services.. The challenges of blockchain;\nblockchain anomalies, energy consumption, speed, scalability, interoperability,\nprivacy and cryptology in the age of quantum computing are described. Selected\nsolutions for these challenges are given. Remarkable derivatives of blockchain,\nwhich use different solutions (directed acyclic graph, distributed hash table,\ngossip consensus protocol) to solve some of these challenges are described.\nThen the data storage in blockchain and evolving data solutions are explained.\nThe comparison of decentralized solutions with the lcentralized database\nsystems is given. A multi-platform interoperable scalable architecture (MPISA)\nis proposed. In the conclusion we include the evolution assumptions of data\nstorage in a decentralized world.",
          "arxiv_id": "2012.10253v1"
        },
        {
          "title": "A Survey of Blockchain Data Management Systems",
          "year": "2021-11",
          "abstract": "Blockchain has been widely deployed in various sectors, such as finance,\neducation, and public services. Since blockchain runs as an immutable\ndistributed ledger, it has decentralized mechanisms with persistency,\nanonymity, and auditability, where transactions are jointly performed through\ncryptocurrency-based consensus algorithms by worldwide distributed nodes. There\nhave been many survey papers reviewing the blockchain technologies from\ndifferent perspectives, e.g., digital currencies, consensus algorithms, and\nsmart contracts. However, none of them have focused on the blockchain data\nmanagement systems. To fill in this gap, we have conducted a comprehensive\nsurvey on the data management systems, based on three typical types of\nblockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed\nAcyclic Graph)-based blockchain. We categorize their data management mechanisms\ninto three layers: blockchain architecture, blockchain data structure, and\nblockchain storage engine, where block architecture indicates how to record\ntransactions on a distributed ledger, blockchain data structure refers to the\ninternal structure of each block, and blockchain storage engine specifies the\nstorage form of data on the blockchain system. For each layer, the works\nadvancing the state-of-the-art are discussed together with technical\nchallenges. Furthermore, we lay out the future research directions for the\nblockchain data management systems.",
          "arxiv_id": "2111.13683v1"
        },
        {
          "title": "Blockchain Oracle Design Patterns",
          "year": "2021-06",
          "abstract": "Blockchain is a form of distributed ledger technology (DLT) where data is\nshared among users connected over the internet. Transactions are data state\nchanges on the blockchain that are permanently recorded in a secure and\ntransparent way without the need of a third party. Besides, the introduction of\nsmart contracts to the blockchain has added programmability to the blockchain\nand revolutionized the software ecosystem leading toward decentralized\napplications (DApps) attracting businesses and organizations to employ this\ntechnology. Although promising, blockchains and smart contracts have no access\nto the external systems (i.e., off-chain) where real-world data and events\nresides; consequently, the usability of smart contracts in terms of performance\nand programmability would be limited to the on-chain data. Hence,\n\\emph{blockchain oracles} are introduced to mitigate the issue and are defined\nas trusted third-party services that send and verify the external information\n(i.e., feedback) and submit it to smart contracts for triggering state changes\nin the blockchain. In this paper, we will study and analyze blockchain oracles\nwith regard to how they provide feedback to the blockchain and smart contracts.\nWe classify the blockchain oracle techniques into two major groups such as\nvoting-based strategies and reputation-based ones. The former mainly relies on\nparticipants' stakes for outcome finalization while the latter considers\nreputation in conjunction with authenticity proof mechanisms for data\ncorrectness and integrity. We then provide a structured description of patterns\nin detail for each classification and discuss research directions in the end.",
          "arxiv_id": "2106.09349v1"
        }
      ],
      "1": [
        {
          "title": "Pseudorandom quantum authentication",
          "year": "2025-01",
          "abstract": "We introduce the pseudorandom quantum authentication scheme (PQAS), an\nefficient method for encrypting quantum states that relies solely on the\nexistence of pseudorandom unitaries (PRUs). The scheme guarantees that for any\neavesdropper with quantum polynomial-time (QPT) computational power, the\nencrypted states are indistinguishable from the maximally mixed state.\nFurthermore, the receiver can verify that the state has not been tampered with\nand recover the original state with asymptotically unit fidelity. Our scheme is\ncost-effective, requiring only polylogarithmic circuit depth and a single\nshared key to encrypt a polynomial number of states. Notably, the PQAS can\npotentially exist even without quantum-secure one-way functions, requiring\nfundamentally weaker computational assumptions than semantic classical\ncryptography. Additionally, PQAS is secure against attacks that plague\nprotocols based on QPT indistinguishability from Haar random states, such as\nchosen-plaintext attacks (CPAs) and attacks that reveal meta-information such\nas quantum resources. We relate the amount of meta-information that is leaked\nto quantum pseudoresources, giving the concept a practical meaning. As an\napplication, we construct important cryptographic primitives, such as\nverifiable pseudorandom density matrices (VPRDMs), which are\nQPT-indistinguishable from random mixed states while being efficiently\nverifiable via a secret key, as well as verifiable noise-robust EFI pairs and\none-way state generators (OWSGs). Our results establish a new paradigm of\nquantum information processing with weaker computational assumptions.",
          "arxiv_id": "2501.00951v1"
        },
        {
          "title": "Founding Quantum Cryptography on Quantum Advantage, or, Towards Cryptography from $\\mathsf{\\#P}$-Hardness",
          "year": "2024-09",
          "abstract": "Recent oracle separations [Kretschmer, TQC'21, Kretschmer et. al., STOC'23]\nhave raised the tantalizing possibility of building quantum cryptography from\nsources of hardness that persist even if the polynomial hierarchy collapses. We\nrealize this possibility by building quantum bit commitments and secure\ncomputation from unrelativized, well-studied mathematical problems that are\nconjectured to be hard for $\\mathsf{P^{\\#P}}$ -- such as approximating the\npermanents of complex Gaussian matrices, or approximating the output\nprobabilities of random quantum circuits. Indeed, we show that as long as any\none of the conjectures underlying sampling-based quantum advantage (e.g.,\nBosonSampling, Random Circuit Sampling, IQP, etc.) is true, quantum\ncryptography can be based on the extremely mild assumption that\n$\\mathsf{P^{\\#P}} \\not\\subseteq \\mathsf{(io)BQP/qpoly}$. We prove that the\nfollowing hardness assumptions are equivalent. (1) The hardness of\napproximating the probability assigned to a randomly chosen string in the\nsupport of certain efficiently sampleable distributions (upto inverse\npolynomial multiplicative error).(2) The existence of one-way puzzles, where a\nquantum sampler outputs a pair of classical strings -- a puzzle and its key --\nand where the hardness lies in finding the key corresponding to a random\npuzzle. These are known to imply quantum bit commitments [Khurana and Tomer,\nSTOC'24]. (3) The existence of state puzzles, or one-way state synthesis, where\nit is hard to synthesize a secret quantum state given a public classical\nidentifier. These capture the hardness of search problems with quantum inputs\n(secrets) and classical outputs (challenges). These are the first constructions\nof quantum cryptographic primitives (one-way puzzles, quantum bit commitments,\nstate puzzles) from concrete, well-founded mathematical assumptions that do not\nimply the existence of classical cryptography.",
          "arxiv_id": "2409.15248v2"
        },
        {
          "title": "Efficient Quantum Pseudorandomness from Hamiltonian Phase States",
          "year": "2024-10",
          "abstract": "Quantum pseudorandomness has found applications in many areas of quantum\ninformation, ranging from entanglement theory, to models of scrambling\nphenomena in chaotic quantum systems, and, more recently, in the foundations of\nquantum cryptography. Kretschmer (TQC '21) showed that both pseudorandom states\nand pseudorandom unitaries exist even in a world without classical one-way\nfunctions. To this day, however, all known constructions require classical\ncryptographic building blocks which are themselves synonymous with the\nexistence of one-way functions, and which are also challenging to realize on\nrealistic quantum hardware.\n  In this work, we seek to make progress on both of these fronts simultaneously\n-- by decoupling quantum pseudorandomness from classical cryptography\naltogether. We introduce a quantum hardness assumption called the Hamiltonian\nPhase State (HPS) problem, which is the task of decoding output states of a\nrandom instantaneous quantum polynomial-time (IQP) circuit. Hamiltonian phase\nstates can be generated very efficiently using only Hadamard gates,\nsingle-qubit Z-rotations and CNOT circuits. We show that the hardness of our\nproblem reduces to a worst-case version of the problem, and we provide evidence\nthat our assumption is plausibly fully quantum; meaning, it cannot be used to\nconstruct one-way functions. We also show information-theoretic hardness when\nonly few copies of HPS are available by proving an approximate $t$-design\nproperty of our ensemble. Finally, we show that our HPS assumption and its\nvariants allow us to efficiently construct many pseudorandom quantum\nprimitives, ranging from pseudorandom states, to quantum pseudoentanglement, to\npseudorandom unitaries, and even primitives such as public-key encryption with\nquantum keys.",
          "arxiv_id": "2410.08073v3"
        }
      ],
      "2": [
        {
          "title": "PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs",
          "year": "2024-09",
          "abstract": "Large Language Models (LLMs) have gained widespread use in various\napplications due to their powerful capability to generate human-like text.\nHowever, prompt injection attacks, which involve overwriting a model's original\ninstructions with malicious prompts to manipulate the generated text, have\nraised significant concerns about the security and reliability of LLMs.\nEnsuring that LLMs are robust against such attacks is crucial for their\ndeployment in real-world applications, particularly in critical tasks.\n  In this paper, we propose PROMPTFUZZ, a novel testing framework that\nleverages fuzzing techniques to systematically assess the robustness of LLMs\nagainst prompt injection attacks. Inspired by software fuzzing, PROMPTFUZZ\nselects promising seed prompts and generates a diverse set of prompt injections\nto evaluate the target LLM's resilience. PROMPTFUZZ operates in two stages: the\nprepare phase, which involves selecting promising initial seeds and collecting\nfew-shot examples, and the focus phase, which uses the collected examples to\ngenerate diverse, high-quality prompt injections. Using PROMPTFUZZ, we can\nuncover more vulnerabilities in LLMs, even those with strong defense prompts.\n  By deploying the generated attack prompts from PROMPTFUZZ in a real-world\ncompetition, we achieved the 7th ranking out of over 4000 participants (top\n0.14%) within 2 hours. Additionally, we construct a dataset to fine-tune LLMs\nfor enhanced robustness against prompt injection attacks. While the fine-tuned\nmodel shows improved robustness, PROMPTFUZZ continues to identify\nvulnerabilities, highlighting the importance of robust testing for LLMs. Our\nwork emphasizes the critical need for effective testing tools and provides a\npractical framework for evaluating and improving the robustness of LLMs against\nprompt injection attacks.",
          "arxiv_id": "2409.14729v2"
        },
        {
          "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
          "year": "2023-10",
          "abstract": "Large language models (LLMs) are susceptible to red teaming attacks, which\ncan induce LLMs to generate harmful content. Previous research constructs\nattack prompts via manual or automatic methods, which have their own\nlimitations on construction cost and quality. To address these issues, we\npropose an integrated approach that combines manual and automatic methods to\neconomically generate high-quality attack prompts. Specifically, considering\nthe impressive capabilities of newly emerged LLMs, we propose an attack\nframework to instruct LLMs to mimic human-generated prompts through in-context\nlearning. Furthermore, we propose a defense framework that fine-tunes victim\nLLMs through iterative interactions with the attack framework to enhance their\nsafety against red teaming attacks. Extensive experiments on different LLMs\nvalidate the effectiveness of our proposed attack and defense frameworks.\nAdditionally, we release a series of attack prompts datasets named SAP with\nvarying sizes, facilitating the safety evaluation and enhancement of more LLMs.\nOur code and dataset is available on https://github.com/Aatrox103/SAP .",
          "arxiv_id": "2310.12505v1"
        },
        {
          "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
          "year": "2024-04",
          "abstract": "Large Language Models (LLMs) have gained widespread adoption across various\ndomains, including chatbots and auto-task completion agents. However, these\nmodels are susceptible to safety vulnerabilities such as jailbreaking, prompt\ninjection, and privacy leakage attacks. These vulnerabilities can lead to the\ngeneration of malicious content, unauthorized actions, or the disclosure of\nconfidential information. While foundational LLMs undergo alignment training\nand incorporate safety measures, they are often subject to fine-tuning, or\ndoing quantization resource-constrained environments. This study investigates\nthe impact of these modifications on LLM safety, a critical consideration for\nbuilding reliable and secure AI systems. We evaluate foundational models\nincluding Mistral, Llama series, Qwen, and MosaicML, along with their\nfine-tuned variants. Our comprehensive analysis reveals that fine-tuning\ngenerally increases the success rates of jailbreak attacks, while quantization\nhas variable effects on attack success rates. Importantly, we find that\nproperly implemented guardrails significantly enhance resistance to jailbreak\nattempts. These findings contribute to our understanding of LLM vulnerabilities\nand provide insights for developing more robust safety strategies in the\ndeployment of language models.",
          "arxiv_id": "2404.04392v3"
        }
      ],
      "3": [
        {
          "title": "Universal Optimality and Robust Utility Bounds for Metric Differential Privacy",
          "year": "2022-05",
          "abstract": "We study the privacy-utility trade-off in the context of metric differential\nprivacy. Ghosh et al. introduced the idea of universal optimality to\ncharacterise the best mechanism for a certain query that simultaneously\nsatisfies (a fixed) $\\epsilon$-differential privacy constraint whilst at the\nsame time providing better utility compared to any other\n$\\epsilon$-differentially private mechanism for the same query. They showed\nthat the Geometric mechanism is \"universally optimal\" for the class of counting\nqueries. On the other hand, Brenner and Nissim showed that outside the space of\ncounting queries, and for the Bayes risk loss function, no such universally\noptimal mechanisms exist. In this paper we use metric differential privacy and\nquantitative information flow as the fundamental principle for studying\nuniversal optimality. Metric differential privacy is a generalisation of both\nstandard (i.e., central) differential privacy and local differential privacy,\nand it is increasingly being used in various application domains, for instance\nin location privacy and in privacy preserving machine learning. Using this\nframework we are able to clarify Nissim and Brenner's negative results, showing\n(a) that in fact all privacy types contain optimal mechanisms relative to\ncertain kinds of non-trivial loss functions, and (b) extending and generalising\ntheir negative results beyond Bayes risk specifically to a wide class of\nnon-trivial loss functions. We also propose weaker universal benchmarks of\nutility called \"privacy type capacities\". We show that such capacities always\nexist and can be computed using a convex optimisation algorithm.",
          "arxiv_id": "2205.01258v1"
        },
        {
          "title": "Bounding, Concentrating, and Truncating: Unifying Privacy Loss Composition for Data Analytics",
          "year": "2020-04",
          "abstract": "Differential privacy (DP) provides rigorous privacy guarantees on\nindividual's data while also allowing for accurate statistics to be conducted\non the overall, sensitive dataset. To design a private system, first private\nalgorithms must be designed that can quantify the privacy loss of each outcome\nthat is released. However, private algorithms that inject noise into the\ncomputation are not sufficient to ensure individuals' data is protected due to\nmany noisy results ultimately concentrating to the true, non-privatized result.\nHence there have been several works providing precise formulas for how the\nprivacy loss accumulates over multiple interactions with private algorithms.\nHowever, these formulas either provide very general bounds on the privacy loss,\nat the cost of being overly pessimistic for certain types of private\nalgorithms, or they can be too narrow in scope to apply to general privacy\nsystems. In this work, we unify existing privacy loss composition bounds for\nspecial classes of differentially private (DP) algorithms along with general DP\ncomposition bounds. In particular, we provide strong privacy loss bounds when\nan analyst may select pure DP, bounded range (e.g. exponential mechanisms), or\nconcentrated DP mechanisms in any order. We also provide optimal privacy loss\nbounds that apply when an analyst can select pure DP and bounded range\nmechanisms in a batch, i.e. non-adaptively. Further, when an analyst selects\nmechanisms within each class adaptively, we show a difference in privacy loss\nbetween different, predetermined orderings of pure DP and bounded range\nmechanisms. Lastly, we compare the composition bounds of Laplace and Gaussian\nmechanisms based on histogram datasets.",
          "arxiv_id": "2004.07223v3"
        },
        {
          "title": "Differentially Private Prototypes for Imbalanced Transfer Learning",
          "year": "2024-06",
          "abstract": "Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\n\\textit{pure DP}. We additionally show that privacy-utility trade-offs can be\nfurther improved when leveraging the public data beyond pre-training of the\nencoder: in particular, we can privately sample our DP prototypes from the\npublicly available data points used to train the encoder. Our experimental\nevaluation with four state-of-the-art encoders, four vision datasets, and under\ndifferent data and imbalancedness regimes demonstrate DPPL's high performance\nunder strong privacy guarantees in challenging private learning setups",
          "arxiv_id": "2406.08039v3"
        }
      ],
      "4": [
        {
          "title": "Byzantine-Robust Decentralized Federated Learning",
          "year": "2024-06",
          "abstract": "Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without revealing their private training data. In\nconventional FL, the system follows the server-assisted architecture\n(server-assisted FL), where the training process is coordinated by a central\nserver. However, the server-assisted FL framework suffers from poor scalability\ndue to a communication bottleneck at the server, and trust dependency issues.\nTo address challenges, decentralized federated learning (DFL) architecture has\nbeen proposed to allow clients to train models collaboratively in a serverless\nand peer-to-peer manner. However, due to its fully decentralized nature, DFL is\nhighly vulnerable to poisoning attacks, where malicious clients could\nmanipulate the system by sending carefully-crafted local models to their\nneighboring clients. To date, only a limited number of Byzantine-robust DFL\nmethods have been proposed, most of which are either communication-inefficient\nor remain vulnerable to advanced poisoning attacks. In this paper, we propose a\nnew algorithm called BALANCE (Byzantine-robust averaging through local\nsimilarity in decentralization) to defend against poisoning attacks in DFL. In\nBALANCE, each client leverages its own local model as a similarity reference to\ndetermine if the received model is malicious or benign. We establish the\ntheoretical convergence guarantee for BALANCE under poisoning attacks in both\nstrongly convex and non-convex settings. Furthermore, the convergence rate of\nBALANCE under poisoning attacks matches those of the state-of-the-art\ncounterparts in Byzantine-free settings. Extensive experiments also demonstrate\nthat BALANCE outperforms existing DFL methods and effectively defends against\npoisoning attacks.",
          "arxiv_id": "2406.10416v4"
        },
        {
          "title": "BlindFL: Segmented Federated Learning with Fully Homomorphic Encryption",
          "year": "2025-01",
          "abstract": "Federated learning (FL) is a popular privacy-preserving edge-to-cloud\ntechnique used for training and deploying artificial intelligence (AI) models\non edge devices. FL aims to secure local client data while also collaboratively\ntraining a global model. Under standard FL, clients within the federation send\nmodel updates, derived from local data, to a central server for aggregation\ninto a global model. However, extensive research has demonstrated that private\ndata can be reliably reconstructed from these model updates using gradient\ninversion attacks (GIAs). To protect client data from server-side GIAs,\nprevious FL schemes have employed fully homomorphic encryption (FHE) to secure\nmodel updates while still enabling popular aggregation methods. However,\ncurrent FHE-based FL schemes either incur substantial computational overhead or\ntrade security and/or model accuracy for efficiency. We introduce BlindFL, a\nframework for global model aggregation in which clients encrypt and send a\nsubset of their local model update. With choice over the subset size, BlindFL\noffers flexible efficiency gains while preserving full encryption of aggregated\nupdates. Moreover, we demonstrate that implementing BlindFL can substantially\nlower space and time transmission costs per client, compared with plain FL with\nFHE, while maintaining global model accuracy. BlindFL also offers additional\ndepth of security. While current single-key, FHE-based FL schemes explicitly\ndefend against server-side adversaries, they do not address the realistic\nthreat of malicious clients within the federation. By contrast, we\ntheoretically and experimentally demonstrate that BlindFL significantly impedes\nclient-side model poisoning attacks, a first for single-key, FHE-based FL\nschemes.",
          "arxiv_id": "2501.11659v1"
        },
        {
          "title": "Robust Quantity-Aware Aggregation for Federated Learning",
          "year": "2022-05",
          "abstract": "Federated learning (FL) enables multiple clients to collaboratively train\nmodels without sharing their local data, and becomes an important\nprivacy-preserving machine learning framework. However, classical FL faces\nserious security and robustness problem, e.g., malicious clients can poison\nmodel updates and at the same time claim large quantities to amplify the impact\nof their model updates in the model aggregation. Existing defense methods for\nFL, while all handling malicious model updates, either treat all quantities\nbenign or simply ignore/truncate the quantities of all clients. The former is\nvulnerable to quantity-enhanced attack, while the latter leads to sub-optimal\nperformance since the local data on different clients is usually in\nsignificantly different sizes. In this paper, we propose a robust\nquantity-aware aggregation algorithm for federated learning, called FedRA, to\nperform the aggregation with awareness of local data quantities while being\nable to defend against quantity-enhanced attacks. More specifically, we propose\na method to filter malicious clients by jointly considering the uploaded model\nupdates and data quantities from different clients, and performing\nquantity-aware weighted averaging on model updates from remaining clients.\nMoreover, as the number of malicious clients participating in the federated\nlearning may dynamically change in different rounds, we also propose a\nmalicious client number estimator to predict how many suspicious clients should\nbe filtered in each round. Experiments on four public datasets demonstrate the\neffectiveness of our FedRA method in defending FL against quantity-enhanced\nattacks.",
          "arxiv_id": "2205.10848v2"
        }
      ],
      "5": [
        {
          "title": "Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation",
          "year": "2022-10",
          "abstract": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarial\nexamples, which can produce erroneous predictions by injecting imperceptible\nperturbations. In this work, we study the transferability of adversarial\nexamples, which is significant due to its threat to real-world applications\nwhere model architecture or parameters are usually unknown. Many existing works\nreveal that the adversarial examples are likely to overfit the surrogate model\nthat they are generated from, limiting its transfer attack performance against\ndifferent target models. To mitigate the overfitting of the surrogate model, we\npropose a novel attack method, dubbed reverse adversarial perturbation (RAP).\nSpecifically, instead of minimizing the loss of a single adversarial point, we\nadvocate seeking adversarial example located at a region with unified low loss\nvalue, by injecting the worst-case perturbation (the reverse adversarial\nperturbation) for each step of the optimization procedure. The adversarial\nattack with RAP is formulated as a min-max bi-level optimization problem. By\nintegrating RAP into the iterative process for attacks, our method can find\nmore stable adversarial examples which are less sensitive to the changes of\ndecision boundary, mitigating the overfitting of the surrogate model.\nComprehensive experimental comparisons demonstrate that RAP can significantly\nboost adversarial transferability. Furthermore, RAP can be naturally combined\nwith many existing black-box attack techniques, to further boost the\ntransferability. When attacking a real-world image recognition system, Google\nCloud Vision API, we obtain 22% performance improvement of targeted attacks\nover the compared method. Our codes are available at\nhttps://github.com/SCLBD/Transfer_attack_RAP.",
          "arxiv_id": "2210.05968v1"
        },
        {
          "title": "Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model",
          "year": "2023-04",
          "abstract": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarial\nexamples. Moreover, the transferability of the adversarial examples has\nreceived broad attention in recent years, which means that adversarial examples\ncrafted by a surrogate model can also attack unknown models. This phenomenon\ngave birth to the transfer-based adversarial attacks, which aim to improve the\ntransferability of the generated adversarial examples. In this paper, we\npropose to improve the transferability of adversarial examples in the\ntransfer-based attack via masking unimportant parameters (MUP). The key idea in\nMUP is to refine the pretrained surrogate models to boost the transfer-based\nattack. Based on this idea, a Taylor expansion-based metric is used to evaluate\nthe parameter importance score and the unimportant parameters are masked during\nthe generation of adversarial examples. This process is simple, yet can be\nnaturally combined with various existing gradient-based optimizers for\ngenerating adversarial examples, thus further improving the transferability of\nthe generated adversarial examples. Extensive experiments are conducted to\nvalidate the effectiveness of the proposed MUP-based methods.",
          "arxiv_id": "2304.06908v1"
        },
        {
          "title": "Learning Defense Transformers for Counterattacking Adversarial Examples",
          "year": "2021-03",
          "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples with small\nperturbations. Adversarial defense thus has been an important means which\nimproves the robustness of DNNs by defending against adversarial examples.\nExisting defense methods focus on some specific types of adversarial examples\nand may fail to defend well in real-world applications. In practice, we may\nface many types of attacks where the exact type of adversarial examples in\nreal-world applications can be even unknown. In this paper, motivated by that\nadversarial examples are more likely to appear near the classification\nboundary, we study adversarial examples from a new perspective that whether we\ncan defend against adversarial examples by pulling them back to the original\nclean distribution. We theoretically and empirically verify the existence of\ndefense affine transformations that restore adversarial examples. Relying on\nthis, we learn a defense transformer to counterattack the adversarial examples\nby parameterizing the affine transformations and exploiting the boundary\ninformation of DNNs. Extensive experiments on both toy and real-world datasets\ndemonstrate the effectiveness and generalization of our defense transformer.",
          "arxiv_id": "2103.07595v1"
        }
      ],
      "6": [
        {
          "title": "Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries",
          "year": "2024-11",
          "abstract": "Security experts reverse engineer (decompile) binary code to identify\ncritical security vulnerabilities. The limited access to source code in vital\nsystems - such as firmware, drivers, and proprietary software used in Critical\nInfrastructures (CI) - makes this analysis even more crucial on the binary\nlevel. Even with available source code, a semantic gap persists after\ncompilation between the source and the binary code executed by the processor.\nThis gap may hinder the detection of vulnerabilities in source code. That being\nsaid, current research on Large Language Models (LLMs) overlooks the\nsignificance of decompiled binaries in this area by focusing solely on source\ncode. In this work, we are the first to empirically uncover the substantial\nsemantic limitations of state-of-the-art LLMs when it comes to analyzing\nvulnerabilities in decompiled binaries, largely due to the absence of relevant\ndatasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary\ncode vulnerability dataset. Our dataset is multi-architecture and\nmulti-optimization, focusing on C/C++ due to their wide usage in CI and\nassociation with numerous vulnerabilities. Specifically, we curate 150,872\nsamples of vulnerable and non-vulnerable decompiled binary code for the task of\n(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)\nrecovering function names in the domain of decompiled binaries. Subsequently,\nwe fine-tune state-of-the-art LLMs using DeBinVul and report on a performance\nincrease of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and\nCodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,\nusing DeBinVul, we report a high performance of 80-90% on the vulnerability\nclassification task. Furthermore, we report improved performance in function\nname recovery and vulnerability description tasks.",
          "arxiv_id": "2411.04981v1"
        },
        {
          "title": "An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding",
          "year": "2025-04",
          "abstract": "Binary code analysis plays a pivotal role in the field of software security\nand is widely used in tasks such as software maintenance, malware detection,\nsoftware vulnerability discovery, patch analysis, etc. However, unlike source\ncode, reverse engineers face significant challenges in understanding binary\ncode due to the lack of intuitive semantic information. Although traditional\nreverse tools can convert binary code into C-like pseudo code, the lack of code\ncomments and symbolic information such as function names still makes code\nunderstanding difficult. In recent years, two groups of techniques have shown\npromising prospects: (1) Deep learning-based techniques have demonstrated\ncompetitive results in tasks related to binary code understanding, furthermore,\n(2) Large Language Models (LLMs) have been extensively pre-trained at the\nsource-code level for tasks such as code understanding and generation. This has\nleft participants wondering about the capabilities of LLMs in binary code\nunderstanding. To this end, this work proposes a benchmark to evaluate the\neffectiveness of LLMs in real-world reverse engineering scenarios, which covers\ntwo key binary code understanding tasks, i.e., function name recovery and\nbinary code summarization. To more comprehensively evaluate, we include\nbinaries with multiple target architectures as well as different optimization\noptions. We gain valuable insights into the capabilities and limitations\nthrough extensive empirical studies of popular LLMs using our benchmark. Our\nevaluations reveal that existing LLMs can understand binary code to a certain\nextent, thereby improving the efficiency of binary code analysis. Our results\nhighlight the great potential of the LLMs in advancing the field of binary code\nunderstanding, and provide new directions for binary code analysis techniques.",
          "arxiv_id": "2504.21803v1"
        },
        {
          "title": "Can We Trust Large Language Models Generated Code? A Framework for In-Context Learning, Security Patterns, and Code Evaluations Across Diverse LLMs",
          "year": "2024-06",
          "abstract": "Large Language Models (LLMs) such as ChatGPT and GitHub Copilot have\nrevolutionized automated code generation in software engineering. However, as\nthese models are increasingly utilized for software development, concerns have\narisen regarding the security and quality of the generated code. These concerns\nstem from LLMs being primarily trained on publicly available code repositories\nand internet-based textual data, which may contain insecure code. This presents\na significant risk of perpetuating vulnerabilities in the generated code,\ncreating potential attack vectors for exploitation by malicious actors. Our\nresearch aims to tackle these issues by introducing a framework for secure\nbehavioral learning of LLMs through In-Content Learning (ICL) patterns during\nthe code generation process, followed by rigorous security evaluations. To\nachieve this, we have selected four diverse LLMs for experimentation. We have\nevaluated these coding LLMs across three programming languages and identified\nsecurity vulnerabilities and code smells. The code is generated through ICL\nwith curated problem sets and undergoes rigorous security testing to evaluate\nthe overall quality and trustworthiness of the generated code. Our research\nindicates that ICL-driven one-shot and few-shot learning patterns can enhance\ncode security, reducing vulnerabilities in various programming scenarios.\nDevelopers and researchers should know that LLMs have a limited understanding\nof security principles. This may lead to security breaches when the generated\ncode is deployed in production systems. Our research highlights LLMs are a\npotential source of new vulnerabilities to the software supply chain. It is\nimportant to consider this when using LLMs for code generation. This research\narticle offers insights into improving LLM security and encourages proactive\nuse of LLMs for code generation to ensure software system safety.",
          "arxiv_id": "2406.12513v1"
        }
      ],
      "7": [
        {
          "title": "Autoencoder-based Unsupervised Intrusion Detection using Multi-Scale Convolutional Recurrent Networks",
          "year": "2022-04",
          "abstract": "The massive growth of network traffic data leads to a large volume of\ndatasets. Labeling these datasets for identifying intrusion attacks is very\nlaborious and error-prone. Furthermore, network traffic data have complex\ntime-varying non-linear relationships. The existing state-of-the-art intrusion\ndetection solutions use a combination of various supervised approaches along\nwith fused features subsets based on correlations in traffic data. These\nsolutions often require high computational cost, manual support in fine-tuning\nintrusion detection models, and labeling of data that limit real-time\nprocessing of network traffic. Unsupervised solutions do reduce computational\ncomplexities and manual support for labeling data but current unsupervised\nsolutions do not consider spatio-temporal correlations in traffic data. To\naddress this, we propose a unified Autoencoder based on combining multi-scale\nconvolutional neural network and long short-term memory (MSCNN-LSTM-AE) for\nanomaly detection in network traffic. The model first employs Multiscale\nConvolutional Neural Network Autoencoder (MSCNN-AE) to analyze the spatial\nfeatures of the dataset, and then latent space features learned from MSCNN-AE\nemploys Long Short-Term Memory (LSTM) based Autoencoder Network to process the\ntemporal features. Our model further employs two Isolation Forest algorithms as\nerror correction mechanisms to detect false positives and false negatives to\nimprove detection accuracy. %Additionally, covariance matrices forms a\nRiemannian manifold that is naturally embedded with distance metrices that\nfacilitates descriminative patterns for detecting malicious network traffic. We\nevaluated our model NSL-KDD, UNSW-NB15, and CICDDoS2019 dataset and showed our\nproposed method significantly outperforms the conventional unsupervised methods\nand other existing studies on the dataset.",
          "arxiv_id": "2204.03779v1"
        },
        {
          "title": "Anomal-E: A Self-Supervised Network Intrusion Detection System based on Graph Neural Networks",
          "year": "2022-07",
          "abstract": "This paper investigates Graph Neural Networks (GNNs) application for\nself-supervised network intrusion and anomaly detection. GNNs are a deep\nlearning approach for graph-based data that incorporate graph structures into\nlearning to generalise graph representations and output embeddings. As network\nflows are naturally graph-based, GNNs are a suitable fit for analysing and\nlearning network behaviour. The majority of current implementations of\nGNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled\nnetwork traffic which can not only restrict the amount and structure of input\ntraffic, but also the NIDSs potential to adapt to unseen attacks. To overcome\nthese restrictions, we present Anomal-E, a GNN approach to intrusion and\nanomaly detection that leverages edge features and graph topological structure\nin a self-supervised process. This approach is, to the best our knowledge, the\nfirst successful and practical approach to network intrusion detection that\nutilises network flows in a self-supervised, edge leveraging GNN. Experimental\nresults on two modern benchmark NIDS datasets not only clearly display the\nimprovement of using Anomal-E embeddings rather than raw features, but also the\npotential Anomal-E has for detection on wild network traffic.",
          "arxiv_id": "2207.06819v5"
        },
        {
          "title": "Using EBGAN for Anomaly Intrusion Detection",
          "year": "2022-06",
          "abstract": "As an active network security protection scheme, intrusion detection system\n(IDS) undertakes the important responsibility of detecting network attacks in\nthe form of malicious network traffic. Intrusion detection technology is an\nimportant part of IDS. At present, many scholars have carried out extensive\nresearch on intrusion detection technology. However, developing an efficient\nintrusion detection method for massive network traffic data is still difficult.\nSince Generative Adversarial Networks (GANs) have powerful modeling\ncapabilities for complex high-dimensional data, they provide new ideas for\naddressing this problem. In this paper, we put forward an EBGAN-based intrusion\ndetection method, IDS-EBGAN, that classifies network records as normal traffic\nor malicious traffic. The generator in IDS-EBGAN is responsible for converting\nthe original malicious network traffic in the training set into adversarial\nmalicious examples. This is because we want to use adversarial learning to\nimprove the ability of discriminator to detect malicious traffic. At the same\ntime, the discriminator adopts Autoencoder model. During testing, IDS-EBGAN\nuses reconstruction error of discriminator to classify traffic records.",
          "arxiv_id": "2206.10400v1"
        }
      ],
      "8": [
        {
          "title": "A Survey on Cyber-Resilience Approaches for Cyber-Physical Systems",
          "year": "2023-02",
          "abstract": "Concerns for the resilience of Cyber-Physical Systems (CPS)s in critical\ninfrastructure are growing. CPS integrate sensing, computation, control, and\nnetworking into physical objects and mission-critical services, connecting\ntraditional infrastructure to internet technologies. While this integration\nincreases service efficiency, it has to face the possibility of new threats\nposed by the new functionalities. This leads to cyber-threats, such as\ndenial-of-service, modification of data, information leakage, spreading of\nmalware, and many others. Cyber-resilience refers to the ability of a CPS to\nprepare, absorb, recover, and adapt to the adverse effects associated with\ncyber-threats, e.g., physical degradation of the CPS performance resulting from\na cyber-attack. Cyber-resilience aims at ensuring CPS survival by keeping the\ncore functionalities of the CPS in case of extreme events. The literature on\ncyber-resilience is rapidly increasing, leading to a broad variety of research\nworks addressing this new topic. In this article, we create a systematization\nof knowledge about existing scientific efforts of making CPSs cyber-resilient.\nWe systematically survey recent literature addressing cyber-resilience with a\nfocus on techniques that may be used on CPSs. We first provide preliminaries\nand background on CPSs and threats, and subsequently survey state-of-the-art\napproaches that have been proposed by recent research work applicable to CPSs.\nIn particular, we aim at differentiating research work from traditional risk\nmanagement approaches based on the general acceptance that it is unfeasible to\nprevent and mitigate all possible risks threatening a CPS. We also discuss\nquestions and research challenges, with a focus on the practical aspects of\ncyber-resilience, such as the use of metrics and evaluation methods as well as\ntesting and validation environments.",
          "arxiv_id": "2302.05402v2"
        },
        {
          "title": "knowCC: Knowledge, awareness of computer & cyber ethics between CS/non-CS university students",
          "year": "2023-10",
          "abstract": "Technology has advanced dramatically in the previous several years. There are\nalso cyber assaults. Cyberattacks pose a possible danger to information\nsecurity and the general public. Since data practice and internet consumption\nrates continue to upswing, cyber awareness has become progressively important.\nFurthermore, as businesses pace their digital transformation with mobile\ndevices, cloud services, communal media, and Internet of Things services,\ncybersecurity has appeared as a critical issue in corporate risk management.\nThis research focuses on the relations between cybersecurity awareness, cyber\nknowledge, computer ethics, cyber ethics, and cyber behavior, as well as\nprotective tools, across university students in general. The findings express\nthat while internet users are alert of cyber threats, they only take the most\nelementary and easy-to-implement precautions. Several knowledge and awareness\nhave been proposed to knob the issue of cyber security. It also grants the\nprinciples of cybersecurity in terms of its structure, workforces, and evidence\npertaining to the shield of personal information in the cyber world. The first\nstep is for people to educate themselves about the negative aspects of the\ninternet and to learn more about cyber threats so that they can notice when an\nattack is taking place. To validate the efficiency of the suggested analysis\nbetween CS and non-CS university students, case study along with several\ncomparisons are provided.",
          "arxiv_id": "2310.12684v1"
        },
        {
          "title": "Mitigation Techniques for Cyber Attacks: A Systematic Mapping Study",
          "year": "2023-08",
          "abstract": "In the wake of the arrival of digital media, the Internet, the web, and\nonline social media, a flood of new cyber security research questions have\nemerged. There is a lot of money being lost around the world because of\ncyber-attacks. As a result, cyber security has emerged as one of the world's\nmost complex and pressing issues. Cyber security experts from both industry and\nacademia institutions are now analyzing current cyber-attacks occurring around\nthe world and developing various strategies to defend systems from possible\ncyber-threats and attacks. This paper examines recent cyber security attacks as\nwell as the financial losses incurred as a result of the growing number of\ncyber-attacks. Our findings indicate that the majority of the research chosen\nfor this study focused solely on a small number of widespread security flaws,\nsuch as malware, phishing, and denial-of-service attacks. A total of over 50\nmajor studies that have been published in reputable academic journals and\nconferences have been chosen for additional examination. A taxonomy of\ncyber-attacks elements that is based on the context of use in various\nenvironments has also been suggested, in addition to a review of the most\nrecent studies on countermeasures for cyber-attacks being the state of the art.\nLastly, the research gaps in terms of open issues have been described in order\nto offer potential future directions for the researchers working in the field\nof cyber security.",
          "arxiv_id": "2308.13587v1"
        }
      ],
      "9": [
        {
          "title": "Building Your Own Trusted Execution Environments Using FPGA",
          "year": "2022-03",
          "abstract": "In recent years, we have witnessed unprecedented growth in using\nhardware-assisted Trusted Execution Environments (TEE) or enclaves to protect\nsensitive code and data on commodity devices thanks to new hardware security\nfeatures, such as Intel SGX and Arm TrustZone. Even though the proprietary TEEs\nbring many benefits, they have been criticized for lack of transparency,\nvulnerabilities, and various restrictions. For example, existing TEEs only\nprovide a static and fixed hardware Trusted Computing Base (TCB), which cannot\nbe customized for different applications. Existing TEEs time-share a processor\ncore with the Rich Execution Environment (REE), making execution less efficient\nand vulnerable to cache side-channel attacks. Moreover, TrustZone lacks\nhardware support for multiple TEEs, remote attestation, and memory encryption.\n  In this paper, we present BYOTee (Build Your Own Trusted Execution\nEnvironments), which is an easy-to-use infrastructure for building multiple\nequally secure enclaves by utilizing commodity Field Programmable Gate Arrays\n(FPGA) devices. BYOTee creates enclaves with customized hardware TCBs, which\ninclude softcore CPUs, block RAMs, and peripheral connections, in FPGA on\ndemand. Additionally, BYOTee provides mechanisms to attest the integrity of the\ncustomized enclaves' hardware and software stacks, including bitstream,\nfirmware, and the Security-Sensitive Applications (SSA) along with their inputs\nand outputs to remote verifiers. We implement a BYOTee system for the Xilinx\nSystem-on-Chip (SoC) FPGA. The evaluations on the low-end Zynq-7000 system for\nfour SSAs and 12 benchmark applications demonstrate the usage, security,\neffectiveness, and performance of the BYOTee framework.",
          "arxiv_id": "2203.04214v3"
        },
        {
          "title": "CrypTag: Thwarting Physical and Logical Memory Vulnerabilities using Cryptographically Colored Memory",
          "year": "2020-12",
          "abstract": "Memory vulnerabilities are a major threat to many computing systems. To\neffectively thwart spatial and temporal memory vulnerabilities, full logical\nmemory safety is required. However, current mitigation techniques for memory\nsafety are either too expensive or trade security against efficiency. One\npromising attempt to detect memory safety vulnerabilities in hardware is memory\ncoloring, a security policy deployed on top of tagged memory architectures.\nHowever, due to the memory storage and bandwidth overhead of large tags,\ncommodity tagged memory architectures usually only provide small tag sizes,\nthus limiting their use for security applications. Irrespective of logical\nmemory safety, physical memory safety is a necessity in hostile environments\nprevalent for modern cloud computing and IoT devices. Architectures from Intel\nand AMD already implement transparent memory encryption to maintain\nconfidentiality and integrity of all off-chip data. Surprisingly, the\ncombination of both, logical and physical memory safety, has not yet been\nextensively studied in previous research, and a naive combination of both\nsecurity strategies would accumulate both overheads. In this paper, we propose\nCrypTag, an efficient hardware/software co-design mitigating a large class of\nlogical memory safety issues and providing full physical memory safety. At its\ncore, CrypTag utilizes a transparent memory encryption engine not only for\nphysical memory safety, but also for memory coloring at hardly any additional\ncosts. The design avoids any overhead for tag storage by embedding memory\ncolors in the upper bits of a pointer and using these bits as an additional\ninput for the memory encryption. A custom compiler extension automatically\nleverages CrypTag to detect logical memory safety issues for commodity programs\nand is fully backward compatible.",
          "arxiv_id": "2012.06761v2"
        },
        {
          "title": "Stockade: Hardware Hardening for Distributed Trusted Sandboxes",
          "year": "2021-08",
          "abstract": "The widening availability of hardware-based trusted execution environments\n(TEEs) has been accelerating the adaptation of new applications using TEEs.\nRecent studies showed that a cloud application consists of multiple distributed\nsoftware modules provided by mutually distrustful parties. The applications use\nmultiple TEEs (enclaves) communicating through software-encrypted memory\nchannels. Such execution model requires bi-directional protection: protecting\nthe rest of the system from the enclave module with sandboxing and protecting\nthe enclave module from a third-part module and operating systems. However, the\ncurrent TEE model, such as Intel SGX, cannot efficiently represent such\ndistributed sandbox applications. To overcome the lack of hardware supports for\nsandboxed TEEs, this paper proposes an extended enclave model called Stockade,\nwhich supports distributed sandboxes hardened by hardware. Stockade proposes\nnew three key techniques. First, it extends the hardware-based memory isolation\nin SGX to confine a user software module only within its enclave. Second, it\nproposes a trusted monitor enclave that filters and validates systems calls\nfrom enclaves. Finally, it allows hardware-protected memory sharing between a\npair of enclaves for efficient protected communication without software-based\nencryption. Using an emulated SGX platform with the proposed extensions, this\npaper shows that distributed sandbox applications can be effectively supported\nwith small changes of SGX hardware.",
          "arxiv_id": "2108.13922v2"
        }
      ],
      "10": [
        {
          "title": "Android Malware Detection Based on RGB Images and Multi-feature Fusion",
          "year": "2024-08",
          "abstract": "With the widespread adoption of smartphones, Android malware has become a\nsignificant challenge in the field of mobile device security. Current Android\nmalware detection methods often rely on feature engineering to construct\ndynamic or static features, which are then used for learning. However, static\nfeature-based methods struggle to counter code obfuscation, packing, and\nsigning techniques, while dynamic feature-based methods involve time-consuming\nfeature extraction. Image-based methods for Android malware detection offer\nbetter resilience against malware variants and polymorphic malware. This paper\nproposes an end-to-end Android malware detection technique based on RGB images\nand multi-feature fusion. The approach involves extracting Dalvik Executable\n(DEX) files, AndroidManifest.xml files, and API calls from APK files,\nconverting them into grayscale images, and enhancing their texture features\nusing Canny edge detection, histogram equalization, and adaptive thresholding\ntechniques. These grayscale images are then combined into an RGB image\ncontaining multi-feature fusion information, which is analyzed using mainstream\nimage classification models for Android malware detection. Extensive\nexperiments demonstrate that the proposed method effectively captures Android\nmalware characteristics, achieving an accuracy of up to 97.25%, outperforming\nexisting detection methods that rely solely on DEX files as classification\nfeatures. Additionally, ablation experiments confirm the effectiveness of using\nthe three key files for feature representation in the proposed approach.",
          "arxiv_id": "2408.16555v1"
        },
        {
          "title": "Task-Aware Meta Learning-based Siamese Neural Network for Classifying Obfuscated Malware",
          "year": "2021-10",
          "abstract": "Malware authors apply different techniques of control flow obfuscation, in\norder to create new malware variants to avoid detection. Existing Siamese\nneural network (SNN)-based malware detection methods fail to correctly classify\ndifferent malware families when such obfuscated malware samples are present in\nthe training dataset, resulting in high false-positive rates. To address this\nissue, we propose a novel task-aware few-shot-learning-based Siamese Neural\nNetwork that is resilient against the presence of malware variants affected by\nsuch control flow obfuscation techniques. Using the average entropy features of\neach malware family as inputs, in addition to the image features, our model\ngenerates the parameters for the feature layers, to more accurately adjust the\nfeature embedding for different malware families, each of which has obfuscated\nmalware variants. In addition, our proposed method can classify malware\nclasses, even if there are only one or a few training samples available. Our\nmodel utilizes few-shot learning with the extracted features of a pre-trained\nnetwork (e.g., VGG-16), to avoid the bias typically associated with a model\ntrained with a limited number of training samples. Our proposed approach is\nhighly effective in recognizing unique malware signatures, thus correctly\nclassifying malware samples that belong to the same malware family, even in the\npresence of obfuscated malware variants. Our experimental results, validated by\nN-way on N-shot learning, show that our model is highly effective in\nclassification accuracy, exceeding a rate \\textgreater 91\\%, compared to other\nsimilar methods.",
          "arxiv_id": "2110.13409v3"
        },
        {
          "title": "EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers",
          "year": "2025-06",
          "abstract": "A lack of accessible data has historically restricted malware analysis\nresearch, and practitioners have relied heavily on datasets provided by\nindustry sources to advance. Existing public datasets are limited by narrow\nscope - most include files targeting a single platform, have labels supporting\njust one type of malware classification task, and make no effort to capture the\nevasive files that make malware detection difficult in practice. We present\nEMBER2024, a new dataset that enables holistic evaluation of malware\nclassifiers. Created in collaboration with the authors of EMBER2017 and\nEMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,\nand labels for more than 3.2 million files from six file formats. Our dataset\nsupports the training and evaluation of machine learning models on seven\nmalware classification tasks, including malware detection, malware family\nclassification, and malware behavior identification. EMBER2024 is the first to\ninclude a collection of malicious files that initially went undetected by a set\nof antivirus products, creating a \"challenge\" set to assess classifier\nperformance against evasive malware. This work also introduces EMBER feature\nversion 3, with added support for several new feature types. We are releasing\nthe EMBER2024 dataset to promote reproducibility and empower researchers in the\npursuit of new malware research topics.",
          "arxiv_id": "2506.05074v1"
        }
      ],
      "11": [
        {
          "title": "Bicycle Attacks Considered Harmful: Quantifying the Damage of Widespread Password Length Leakage",
          "year": "2020-02",
          "abstract": "We examine the issue of password length leakage via encrypted traffic i.e.,\nbicycle attacks. We aim to quantify both the prevalence of password length\nleakage bugs as well as the potential harm to users. In an observational study,\nwe find that {\\em most} of the Alexa top 100 rates sites are vulnerable to\nbicycle attacks meaning that an eavesdropping attacker can infer the exact\nlength of a password based on the length the encrypted packet containing the\npassword. We discuss several ways in which an eavesdropping attacker could link\nthis password length with a particular user account e.g., a targeted campaign\nagainst a smaller group of users or via DNS hijacking for larger scale\ncampaigns. We next use a decision-theoretic model to quantify the extent to\nwhich password length leakage might help an attacker to crack user passwords.\nIn our analysis, we consider three different levels of password attackers:\nhacker, criminal and nation-state. In all cases, we find that such an attacker\nwho knows the length of each user password gains a significant advantage over\none without knowing the password length. As part of this analysis, we also\nrelease a new differentially private password frequency dataset from the 2016\nLinkedIn breach using a differentially private algorithm of Blocki et al. (NDSS\n2016) to protect user accounts. The LinkedIn frequency corpus is based on over\n170 million passwords making it the largest frequency corpus publicly available\nto password researchers. While the defense against bicycle attacks is\nstraightforward (i.e., ensure that passwords are always padded before\nencryption), we discuss several practical challenges organizations may face\nwhen attempting to patch this vulnerability. We advocate for a new W3C standard\non how password fields are handled which would effectively eliminate most\ninstances of password length leakage.",
          "arxiv_id": "2002.01513v1"
        },
        {
          "title": "Image Based Password Authentication System",
          "year": "2022-05",
          "abstract": "Preservation of information and computer security is broadly dependent on the\nsecured authentication system which is underpinned by password. Text based\npassword is a commonly used and available system for authentication. But it\nbears many limitations like shoulder surfing, dictionary attack, Phishing,\nguessing the password etc. In order to overwhelm these vulnerabilities of\nancient textual password, many graphical or image based password authentication\nsystem has been introduced form last few years. But none of this graphical\nsystem is considered as enough adventurous to keep pace with these issues. Here\nwe have proposed an image based password authentication system which is more\nmethodical and can cope up with every vulnerability of recent password\nauthentication system. To make our system hassle free and more reliable, we\nwill only take username from an user for registration purpose as our system\nwill generate a unique key number for that particular user and this key will be\nused as password for later login procedure. The user name and key both will be\nencrypted using a cryptography algorithm to prevent database hacking. There\nwill be a randomized clickable image grid in our system. By clicking on this\nimage grid, user will input the password key for login purpose. Here we have\ndeveloped another method namely shoulder surfing resistant password. To prevent\nthe attack of shoulder surfing, if any user wishes to change our system\nprovided password key then he or she is allowed to do so by using this method.\nBesides this method allows user to change the password every single time of\nlogin. A user doesn't need to enter any textual password for authentication in\nour recent module and hence combination of all these features improve the\nsecurity, usability and user friendliness of our system.",
          "arxiv_id": "2205.12352v1"
        },
        {
          "title": "Online Authentication Habits of Indian Users",
          "year": "2025-01",
          "abstract": "Passwords have been long used as the primary authentication method for web\nservices. Weak passwords used by the users have prompted the use of password\nmanagement tools and two-factor authentication to ensure better account\nsecurity. While prior studies have studied their adoption individually, none of\nthese studies focuses particularly on the Indian setting, which is culturally\nand economically different from the countries in which these studies have been\ndone in the past. To this end, we conducted a survey with 90 participants\nresiding in India to better understand the mindset of people on using password\nmanagers and two-factor authentication (2FA).\n  Our findings suggest that a majority of the participants have used 2FA and\npassword managers in some form, although they are sometimes unaware of their\nformal names. While many participants used some form of 2FA across all their\naccounts, browser-integrated and device-default password managers are\npredominantly utilized for less sensitive platforms such as e-commerce and\nsocial media rather than for more critical accounts like banking. The primary\nmotivation for using password managers is the convenience of auto-filling.\nHowever, some participants avoid using password managers due to a lack of trust\nin these tools. Notably, dedicated third-party applications show low adoption\nfor both password manager and 2FA.\n  Despite acknowledging the importance of secure password practices, many\nparticipants still reuse passwords across multiple accounts, prefer shorter\npasswords, and use commonly predictable password patterns. Overall, the study\nsuggests that Indians are more inclined to choose default settings,\nunderscoring the need for tailored strategies to improve user awareness and\nstrengthen password security practices.",
          "arxiv_id": "2501.14330v1"
        }
      ],
      "12": [
        {
          "title": "Watermarking Visual Concepts for Diffusion Models",
          "year": "2024-11",
          "abstract": "The personalization techniques of diffusion models succeed in generating\nimages with specific concepts. This ability also poses great threats to\ncopyright protection and network security since malicious users can generate\nunauthorized content and disinformation relevant to a target concept. Model\nwatermarking is an effective solution to trace the malicious generated images\nand safeguard their copyright. However, existing model watermarking techniques\nmerely achieve image-level tracing without concept traceability. When tracing\ninfringing or harmful concepts, current approaches execute image concept\ndetection and model tracing sequentially, where performance is critically\nconstrained by concept detection accuracy. In this paper, we propose a\nlightweight concept watermarking framework that efficiently binds target\nconcepts to model watermarks, supporting simultaneous concept identification\nand model tracing via single-stage watermark verification. To further enhance\nthe robustness of concept watermarking, we propose an adversarial perturbation\ninjection method collaboratively embedded with watermarks during image\ngeneration, avoiding watermark removal by model purification attacks.\nExperimental results demonstrate that ConceptWM significantly outperforms\nstate-of-the-art watermarking methods, improving detection accuracy by\n6.3%-19.3% across diverse datasets including COCO and StableDiffusionDB.\nAdditionally, ConceptWM possesses a critical capability absent in other\nwatermarking methods: it sustains a 21.7% FID/CLIP degradation under\nadversarial fine-tuning of Stable Diffusion models on WikiArt and CelebA-HQ,\ndemonstrating its capability to mitigate model misuse.",
          "arxiv_id": "2411.11688v3"
        },
        {
          "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain",
          "year": "2025-05",
          "abstract": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy.",
          "arxiv_id": "2505.04977v2"
        },
        {
          "title": "ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization",
          "year": "2024-11",
          "abstract": "Watermarking generative content serves as a vital tool for authentication,\nownership protection, and mitigation of potential misuse. Existing watermarking\nmethods face the challenge of balancing robustness and concealment. They\nempirically inject a watermark that is both invisible and robust and passively\nachieve concealment by limiting the strength of the watermark, thus reducing\nthe robustness. In this paper, we propose to explicitly introduce a watermark\nhiding process to actively achieve concealment, thus allowing the embedding of\nstronger watermarks. To be specific, we implant a robust watermark in an\nintermediate diffusion state and then guide the model to hide the watermark in\nthe final generated image. We employ an adversarial optimization algorithm to\nproduce the optimal hiding prompt guiding signal for each watermark. The prompt\nembedding is optimized to minimize artifacts in the generated image, while the\nwatermark is optimized to achieve maximum strength. The watermark can be\nverified by reversing the generation process. Experiments on various diffusion\nmodels demonstrate the watermark remains verifiable even under significant\nimage tampering and shows superior invisibility compared to other\nstate-of-the-art robust watermarking methods. Code is available at\nhttps://github.com/Hannah1102/ROBIN.",
          "arxiv_id": "2411.03862v2"
        }
      ],
      "13": [
        {
          "title": "CryptoFace: End-to-End Encrypted Face Recognition",
          "year": "2025-08",
          "abstract": "Face recognition is central to many authentication, security, and\npersonalized applications. Yet, it suffers from significant privacy risks,\nparticularly arising from unauthorized access to sensitive biometric data. This\npaper introduces CryptoFace, the first end-to-end encrypted face recognition\nsystem with fully homomorphic encryption (FHE). It enables secure processing of\nfacial data across all stages of a face-recognition process--feature\nextraction, storage, and matching--without exposing raw images or features. We\nintroduce a mixture of shallow patch convolutional networks to support\nhigher-dimensional tensors via patch-based processing while reducing the\nmultiplicative depth and, thus, inference latency. Parallel FHE evaluation of\nthese networks ensures near-resolution-independent latency. On standard face\nrecognition benchmarks, CryptoFace significantly accelerates inference and\nincreases verification accuracy compared to the state-of-the-art FHE neural\nnetworks adapted for face recognition. CryptoFace will facilitate secure face\nrecognition systems requiring robust and provable security. The code is\navailable at https://github.com/human-analysis/CryptoFace.",
          "arxiv_id": "2509.00332v1"
        },
        {
          "title": "Biometric Template Protection for Neural-Network-based Face Recognition Systems: A Survey of Methods and Evaluation Techniques",
          "year": "2021-10",
          "abstract": "As automated face recognition applications tend towards ubiquity, there is a\ngrowing need to secure the sensitive face data used within these systems. This\npaper presents a survey of biometric template protection (BTP) methods proposed\nfor securing face templates (images/features) in neural-network-based face\nrecognition systems. The BTP methods are categorised into two types: Non-NN and\nNN-learned. Non-NN methods use a neural network (NN) as a feature extractor,\nbut the BTP part is based on a non-NN algorithm, whereas NN-learned methods\nemploy a NN to learn a protected template from the unprotected template. We\npresent examples of Non-NN and NN-learned face BTP methods from the literature,\nalong with a discussion of their strengths and weaknesses. We also investigate\nthe techniques used to evaluate these methods in terms of the three most common\nBTP criteria: recognition accuracy, irreversibility, and\nrenewability/unlinkability. The recognition accuracy of protected face\nrecognition systems is generally evaluated using the same (empirical)\ntechniques employed for evaluating standard (unprotected) biometric systems.\nHowever, most irreversibility and renewability/unlinkability evaluations are\nfound to be based on theoretical assumptions/estimates or verbal implications,\nwith a lack of empirical validation in a practical face recognition context.\nSo, we recommend a greater focus on empirical evaluations to provide more\nconcrete insights into the irreversibility and renewability/unlinkability of\nface BTP methods in practice. Additionally, an exploration of the\nreproducibility of the studied BTP works, in terms of the public availability\nof their implementation code and evaluation datasets/procedures, suggests that\nit would be difficult to faithfully replicate most of the reported findings.\nSo, we advocate for a push towards reproducibility, in the hope of advancing\nface BTP research.",
          "arxiv_id": "2110.05044v4"
        },
        {
          "title": "Face Encryption via Frequency-Restricted Identity-Agnostic Attacks",
          "year": "2023-08",
          "abstract": "Billions of people are sharing their daily live images on social media\neveryday. However, malicious collectors use deep face recognition systems to\neasily steal their biometric information (e.g., faces) from these images. Some\nstudies are being conducted to generate encrypted face photos using adversarial\nattacks by introducing imperceptible perturbations to reduce face information\nleakage. However, existing studies need stronger black-box scenario feasibility\nand more natural visual appearances, which challenge the feasibility of privacy\nprotection. To address these problems, we propose a frequency-restricted\nidentity-agnostic (FRIA) framework to encrypt face images from unauthorized\nface recognition without access to personal information. As for the weak\nblack-box scenario feasibility, we obverse that representations of the average\nfeature in multiple face recognition models are similar, thus we propose to\nutilize the average feature via the crawled dataset from the Internet as the\ntarget to guide the generation, which is also agnostic to identities of unknown\nface recognition systems; in nature, the low-frequency perturbations are more\nvisually perceptible by the human vision system. Inspired by this, we restrict\nthe perturbation in the low-frequency facial regions by discrete cosine\ntransform to achieve the visual naturalness guarantee. Extensive experiments on\nseveral face recognition models demonstrate that our FRIA outperforms other\nstate-of-the-art methods in generating more natural encrypted faces while\nattaining high black-box attack success rates of 96%. In addition, we validate\nthe efficacy of FRIA using real-world black-box commercial API, which reveals\nthe potential of FRIA in practice. Our codes can be found in\nhttps://github.com/XinDong10/FRIA.",
          "arxiv_id": "2308.05983v3"
        }
      ],
      "14": [
        {
          "title": "Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks",
          "year": "2021-12",
          "abstract": "Backdoor (Trojan) attacks are emerging threats against deep neural networks\n(DNN). A DNN being attacked will predict to an attacker-desired target class\nwhenever a test sample from any source class is embedded with a backdoor\npattern; while correctly classifying clean (attack-free) test samples. Existing\nbackdoor defenses have shown success in detecting whether a DNN is attacked and\nin reverse-engineering the backdoor pattern in a \"post-training\" regime: the\ndefender has access to the DNN to be inspected and a small, clean dataset\ncollected independently, but has no access to the (possibly poisoned) training\nset of the DNN. However, these defenses neither catch culprits in the act of\ntriggering the backdoor mapping, nor mitigate the backdoor attack at test-time.\nIn this paper, we propose an \"in-flight\" defense against backdoor attacks on\nimage classification that 1) detects use of a backdoor trigger at test-time;\nand 2) infers the class of origin (source class) for a detected trigger\nexample. The effectiveness of our defense is demonstrated experimentally\nagainst different strong backdoor attacks.",
          "arxiv_id": "2112.03350v1"
        },
        {
          "title": "Robust Backdoor Attacks against Deep Neural Networks in Real Physical World",
          "year": "2021-04",
          "abstract": "Deep neural networks (DNN) have been widely deployed in various applications.\nHowever, many researches indicated that DNN is vulnerable to backdoor attacks.\nThe attacker can create a hidden backdoor in target DNN model, and trigger the\nmalicious behaviors by submitting specific backdoor instance. However, almost\nall the existing backdoor works focused on the digital domain, while few\nstudies investigate the backdoor attacks in real physical world. Restricted to\na variety of physical constraints, the performance of backdoor attacks in the\nreal physical world will be severely degraded. In this paper, we propose a\nrobust physical backdoor attack method, PTB (physical transformations for\nbackdoors), to implement the backdoor attacks against deep learning models in\nthe real physical world. Specifically, in the training phase, we perform a\nseries of physical transformations on these injected backdoor instances at each\nround of model training, so as to simulate various transformations that a\nbackdoor may experience in real world, thus improves its physical robustness.\nExperimental results on the state-of-the-art face recognition model show that,\ncompared with the backdoor methods that without PTB, the proposed attack method\ncan significantly improve the performance of backdoor attacks in real physical\nworld. Under various complex physical conditions, by injecting only a very\nsmall ratio (0.5%) of backdoor instances, the attack success rate of physical\nbackdoor attacks with the PTB method on VGGFace is 82%, while the attack\nsuccess rate of backdoor attacks without the proposed PTB method is lower than\n11%. Meanwhile, the normal performance of the target DNN model has not been\naffected.",
          "arxiv_id": "2104.07395v2"
        },
        {
          "title": "BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting",
          "year": "2023-12",
          "abstract": "Deep neural networks (DNNs) are susceptible to backdoor attacks, where\nmalicious functionality is embedded to allow attackers to trigger incorrect\nclassifications. Old-school backdoor attacks use strong trigger features that\ncan easily be learned by victim models. Despite robustness against input\nvariation, the robustness however increases the likelihood of unintentional\ntrigger activations. This leaves traces to existing defenses, which find\napproximate replacements for the original triggers that can activate the\nbackdoor without being identical to the original trigger via, e.g., reverse\nengineering and sample overlay.\n  In this paper, we propose and investigate a new characteristic of backdoor\nattacks, namely, backdoor exclusivity, which measures the ability of backdoor\ntriggers to remain effective in the presence of input variation. Building upon\nthe concept of backdoor exclusivity, we propose Backdoor Exclusivity LifTing\n(BELT), a novel technique which suppresses the association between the backdoor\nand fuzzy triggers to enhance backdoor exclusivity for defense evasion.\nExtensive evaluation on three popular backdoor benchmarks validate, our\napproach substantially enhances the stealthiness of four old-school backdoor\nattacks, which, after backdoor exclusivity lifting, is able to evade seven\nstate-of-the-art backdoor countermeasures, at almost no cost of the attack\nsuccess rate and normal utility. For example, one of the earliest backdoor\nattacks BadNet, enhanced by BELT, evades most of the state-of-the-art defenses\nincluding ABS and MOTH which would otherwise recognize the backdoored model.",
          "arxiv_id": "2312.04902v2"
        }
      ],
      "15": [
        {
          "title": "Automated Fuzzing of Automotive Control Units",
          "year": "2021-02",
          "abstract": "Modern vehicles are governed by a network of Electronic Control Units (ECUs),\nwhich are programmed to sense inputs from the driver and the environment, to\nprocess these inputs, and to control actuators that, e.g., regulate the engine\nor even control the steering system. ECUs within a vehicle communicate via\nautomotive bus systems such as the Controller Area Network (CAN), and beyond\nthe vehicles boundaries through upcoming vehicle-to-vehicle and\nvehicle-to-infrastructure channels. Approaches to manipulate the communication\nbetween ECUs for the purpose of security testing and reverse-engineering of\nvehicular functions have been presented in the past, all of which struggle with\nautomating the detection of system change in response to message injection. In\nthis paper we present our findings with fuzzing CAN networks, in particular\nwhile observing individual ECUs with a sensor harness. The harness detects\nphysical responses, which we then use in a oracle functions to inform the\nfuzzing process. We systematically define fuzzers, fuzzing configurations and\noracle functions for testing ECUs. We evaluate our approach based on case\nstudies of commercial instrument clusters and with an experimental framework\nfor CAN authentication. Our results show that the approach is capable of\nidentifying interesting ECU states with a high level of automation. Our\napproach is applicable in distributed cyber-physical systems beyond automotive\ncomputing.",
          "arxiv_id": "2102.12345v2"
        },
        {
          "title": "X-CANIDS: Signal-Aware Explainable Intrusion Detection System for Controller Area Network-Based In-Vehicle Network",
          "year": "2023-03",
          "abstract": "Controller Area Network (CAN) is an essential networking protocol that\nconnects multiple electronic control units (ECUs) in a vehicle. However,\nCAN-based in-vehicle networks (IVNs) face security risks owing to the CAN\nmechanisms. An adversary can sabotage a vehicle by leveraging the security\nrisks if they can access the CAN bus. Thus, recent actions and cybersecurity\nregulations (e.g., UNR 155) require carmakers to implement intrusion detection\nsystems (IDSs) in their vehicles. The IDS should detect cyberattacks and\nprovide additional information to analyze conducted attacks. Although many IDSs\nhave been proposed, considerations regarding their feasibility and\nexplainability remain lacking. This study proposes X-CANIDS, which is a novel\nIDS for CAN-based IVNs. X-CANIDS dissects the payloads in CAN messages into\nhuman-understandable signals using a CAN database. The signals improve the\nintrusion detection performance compared with the use of bit representations of\nraw payloads. These signals also enable an understanding of which signal or ECU\nis under attack. X-CANIDS can detect zero-day attacks because it does not\nrequire any labeled dataset in the training phase. We confirmed the feasibility\nof the proposed method through a benchmark test on an automotive-grade embedded\ndevice with a GPU. The results of this work will be valuable to carmakers and\nresearchers considering the installation of in-vehicle IDSs for their vehicles.",
          "arxiv_id": "2303.12278v3"
        },
        {
          "title": "Cyberattacks and Countermeasures For In-Vehicle Networks",
          "year": "2020-04",
          "abstract": "As connectivity between and within vehicles increases, so does concern about\nsafety and security. Various automotive serial protocols are used inside\nvehicles such as Controller Area Network (CAN), Local Interconnect Network\n(LIN) and FlexRay. CAN bus is the most used in-vehicle network protocol to\nsupport exchange of vehicle parameters between Electronic Control Units (ECUs).\nThis protocol lacks security mechanisms by design and is therefore vulnerable\nto various attacks. Furthermore, connectivity of vehicles has made the CAN bus\nnot only vulnerable from within the vehicle but also from outside. With the\nrise of connected cars, more entry points and interfaces have been introduced\non board vehicles, thereby also leading to a wider potential attack surface.\nExisting security mechanisms focus on the use of encryption, authentication and\nvehicle Intrusion Detection Systems (IDS), which operate under various\nconstrains such as low bandwidth, small frame size (e.g. in the CAN protocol),\nlimited availability of computational resources and real-time sensitivity. We\nsurvey In-Vehicle Network (IVN) attacks which have been grouped under: direct\ninterfaces-initiated attacks, telematics and infotainment-initiated attacks,\nand sensor-initiated attacks. We survey and classify current cryptographic and\nIDS approaches and compare these approaches based on criteria such as real time\nconstrains, types of hardware used, changes in CAN bus behaviour, types of\nattack mitigation and software/ hardware used to validate these approaches. We\nconclude with potential mitigation strategies and research challenges for the\nfuture.",
          "arxiv_id": "2004.10781v1"
        }
      ],
      "16": [
        {
          "title": "5G Network Security Practices: An Overview and Survey",
          "year": "2024-01",
          "abstract": "This document provides an overview of 5G network security, describing various\ncomponents of the 5G core network architecture and what kind of security\nservices are offered by these 5G components. It also explores the potential\nsecurity risks and vulnerabilities presented by the security architecture in 5G\nand recommends some of the best practices for the 5G network admins to consider\nwhile deploying a secure 5G network, based on the surveyed documents from the\nEuropean government's efforts in commercializing the IoT devices and securing\nsupply chain over 5G networks.",
          "arxiv_id": "2401.14350v1"
        },
        {
          "title": "Advanced Penetration Testing for Enhancing 5G Security",
          "year": "2024-07",
          "abstract": "Advances in fifth-generation (5G) networks enable unprecedented reliability,\nspeed, and connectivity compared to previous mobile networks. These\nadvancements can revolutionize various sectors by supporting applications\nrequiring real-time data processing. However, the rapid deployment and\nintegration of 5G networks bring security concerns that must be addressed to\noperate these infrastructures safely. This paper reviews penetration testing\napproaches for identifying security vulnerabilities in 5G networks. Penetration\ntesting is an ethical hacking technique used to simulate a network's security\nposture in the event of cyberattacks. This review highlights the capabilities,\nadvantages, and limitations of recent 5G-targeting security tools for\npenetration testing. It examines ways adversaries exploit vulnerabilities in 5G\nnetworks, covering tactics and strategies targeted at 5G features. A key topic\nexplored is the comparison of penetration testing methods for 5G and earlier\ngenerations. The article delves into the unique characteristics of 5G,\nincluding massive MIMO, edge computing, and network slicing, and how these\naspects require new penetration testing methods. Understanding these\ndifferences helps develop more effective security solutions tailored to 5G\nnetworks. Our research also indicates that 5G penetration testing should use a\nmultithreaded approach for addressing current security challenges. Furthermore,\nthis paper includes case studies illustrating practical challenges and\nlimitations in real-world applications of penetration testing in 5G networks. A\ncomparative analysis of penetration testing tools for 5G networks highlights\ntheir effectiveness in mitigating vulnerabilities, emphasizing the need for\nadvanced security measures against evolving cyber threats in 5G deployment.",
          "arxiv_id": "2407.17269v1"
        },
        {
          "title": "Towards 5G Zero Trusted Air Interface Architecture",
          "year": "2022-11",
          "abstract": "5G is destined to be supporting large deployment of Industrial IoT (IIoT)\nwith the characteristics of ultra-high densification and low latency. 5G\nutilizes a more intelligent architecture, with Radio Access Networks (RANs) no\nlonger constrained by base station proximity or proprietary infrastructure. The\n3rd Generation Partnership Project (3GPP) covers telecommunication technologies\nincluding RAN, core transport networks and service capabilities. Open RAN\nAlliance (O-RAN) aims to define implementation and deployment architectures,\nfocusing on open-source interfaces and functional units to further reduce the\ncost and complexity. O-RAN based 5G networks could use components from\ndifferent hardware and software vendors, promoting vendor diversity,\ninterchangeability and 5G supply chain resiliency. Both 3GPP and O-RAN 5G have\nto manage the security and privacy challenges that arose from the deployment.\nMany existing research studies have addressed the threats and vulnerabilities\nwithin each system. 5G also has the overwhelming challenges in compliance with\nprivacy regulations and requirements which mandate the user identifiable\ninformation need to be protected.\n  In this paper, we look into the 3GPP and O-RAN 5G security and privacy\ndesigns and the identified threats and vulnerabilities. We also discuss how to\nextend the Zero Trust Model to provide advanced protection over 5G air\ninterfaces and network components.",
          "arxiv_id": "2211.03776v1"
        }
      ],
      "17": [
        {
          "title": "Are disentangled representations all you need to build speaker anonymization systems?",
          "year": "2022-08",
          "abstract": "Speech signals contain a lot of sensitive information, such as the speaker's\nidentity, which raises privacy concerns when speech data get collected. Speaker\nanonymization aims to transform a speech signal to remove the source speaker's\nidentity while leaving the spoken content unchanged. Current methods perform\nthe transformation by relying on content/speaker disentanglement and voice\nconversion. Usually, an acoustic model from an automatic speech recognition\nsystem extracts the content representation while an x-vector system extracts\nthe speaker representation. Prior work has shown that the extracted features\nare not perfectly disentangled. This paper tackles how to improve features\ndisentanglement, and thus the converted anonymized speech. We propose enhancing\nthe disentanglement by removing speaker information from the acoustic model\nusing vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit\nshowed that vector quantization helps conceal the original speaker identity\nwhile maintaining utility for speech recognition.",
          "arxiv_id": "2208.10497v3"
        },
        {
          "title": "FreeTalk:A plug-and-play and black-box defense against speech synthesis attacks",
          "year": "2025-08",
          "abstract": "Recently, speech assistant and speech verification have been used in many\nfields, which brings much benefit and convenience for us. However, when we\nenjoy these speech applications, our speech may be collected by attackers for\nspeech synthesis. For example, an attacker generates some inappropriate\npolitical opinions with the characteristic of the victim's voice by obtaining a\npiece of the victim's speech, which will greatly influence the victim's\nreputation. Specifically, with the appearance of some zero-shot voice\nconversion methods, the cost of speech synthesis attacks has been further\nreduced, which also brings greater challenges to user voice security and\nprivacy. Some researchers have proposed the corresponding privacy-preserving\nmethods. However, the existing approaches have some non-negligible drawbacks:\nlow transferability and robustness, high computational overhead. These\ndeficiencies seriously limit the existing method deployed in practical\nscenarios. Therefore, in this paper, we propose a lightweight, robust,\nplug-and-play privacy preservation method against speech synthesis attacks in a\nblack-box setting. Our method generates and adds a frequency-domain\nperturbation to the original speech to achieve privacy protection and high\nspeech quality. Then, we present a data augmentation strategy and noise\nsmoothing mechanism to improve the robustness of the proposed method. Besides,\nto reduce the user's defense overhead, we also propose a novel identity-wise\nprotection mechanism. It can generate a universal perturbation for one speaker\nand support privacy preservation for speech of any length. Finally, we conduct\nextensive experiments on 5 speech synthesis models, 5 speech verification\nmodels, 1 speech recognition model, and 2 datasets. The experimental results\ndemonstrate that our method has satisfying privacy-preserving performance, high\nspeech quality, and utility.",
          "arxiv_id": "2509.00561v1"
        },
        {
          "title": "ClearMask: Noise-Free and Naturalness-Preserving Protection Against Voice Deepfake Attacks",
          "year": "2025-08",
          "abstract": "Voice deepfake attacks, which artificially impersonate human speech for\nmalicious purposes, have emerged as a severe threat. Existing defenses\ntypically inject noise into human speech to compromise voice encoders in speech\nsynthesis models. However, these methods degrade audio quality and require\nprior knowledge of the attack approaches, limiting their effectiveness in\ndiverse scenarios. Moreover, real-time audios, such as speech in virtual\nmeetings and voice messages, are still exposed to voice deepfake threats. To\novercome these limitations, we propose ClearMask, a noise-free defense\nmechanism against voice deepfake attacks. Unlike traditional approaches,\nClearMask modifies the audio mel-spectrogram by selectively filtering certain\nfrequencies, inducing a transferable voice feature loss without injecting\nnoise. We then apply audio style transfer to further deceive voice decoders\nwhile preserving perceived sound quality. Finally, optimized reverberation is\nintroduced to disrupt the output of voice generation models without affecting\nthe naturalness of the speech. Additionally, we develop LiveMask to protect\nstreaming speech in real-time through a universal frequency filter and\nreverberation generator. Our experimental results show that ClearMask and\nLiveMask effectively prevent voice deepfake attacks from deceiving speaker\nverification models and human listeners, even for unseen voice synthesis models\nand black-box API services. Furthermore, ClearMask demonstrates resilience\nagainst adaptive attackers who attempt to recover the original audio signal\nfrom the protected speech samples.",
          "arxiv_id": "2508.17660v1"
        }
      ],
      "18": [
        {
          "title": "MOFHEI: Model Optimizing Framework for Fast and Efficient Homomorphically Encrypted Neural Network Inference",
          "year": "2024-12",
          "abstract": "Due to the extensive application of machine learning (ML) in a wide range of\nfields and the necessity of data privacy, privacy-preserving machine learning\n(PPML) solutions have recently gained significant traction. One group of\napproaches relies on Homomorphic Encryption (HE), which enables us to perform\nML tasks over encrypted data. However, even with state-of-the-art HE schemes,\nHE operations are still significantly slower compared to their plaintext\ncounterparts and require a considerable amount of memory. Therefore, we propose\nMOFHEI, a framework that optimizes the model to make HE-based neural network\ninference, referred to as private inference (PI), fast and efficient. First,\nour proposed learning-based method automatically transforms a pre-trained ML\nmodel into its compatible version with HE operations, called the HE-friendly\nversion. Then, our iterative block pruning method prunes the model's parameters\nin configurable block shapes in alignment with the data packing method. This\nallows us to drop a significant number of costly HE operations, thereby\nreducing the latency and memory consumption while maintaining the model's\nperformance. We evaluate our framework through extensive experiments on\ndifferent models using various datasets. Our method achieves up to 98% pruning\nratio on LeNet, eliminating up to 93% of the required HE operations for\nperforming PI, reducing latency and the required memory by factors of 9.63 and\n4.04, respectively, with negligible accuracy loss.",
          "arxiv_id": "2412.07954v1"
        },
        {
          "title": "Hyperdimensional Computing as a Rescue for Efficient Privacy-Preserving Machine Learning-as-a-Service",
          "year": "2023-08",
          "abstract": "Machine learning models are often provisioned as a cloud-based service where\nthe clients send their data to the service provider to obtain the result. This\nsetting is commonplace due to the high value of the models, but it requires the\nclients to forfeit the privacy that the query data may contain. Homomorphic\nencryption (HE) is a promising technique to address this adversity. With HE,\nthe service provider can take encrypted data as a query and run the model\nwithout decrypting it. The result remains encrypted, and only the client can\ndecrypt it. All these benefits come at the cost of computational cost because\nHE turns simple floating-point arithmetic into the computation between long\n(degree over 1024) polynomials. Previous work has proposed to tailor deep\nneural networks for efficient computation over encrypted data, but already high\ncomputational cost is again amplified by HE, hindering performance improvement.\nIn this paper we show hyperdimensional computing can be a rescue for\nprivacy-preserving machine learning over encrypted data. We find that the\nadvantage of hyperdimensional computing in performance is amplified when\nworking with HE. This observation led us to design HE-HDC, a machine-learning\ninference system that uses hyperdimensional computing with HE. We carefully\nstructure the machine learning service so that the server will perform only the\nHE-friendly computation. Moreover, we adapt the computation and HE parameters\nto expedite computation while preserving accuracy and security. Our\nexperimental result based on real measurements shows that HE-HDC outperforms\nexisting systems by 26~3000 times with comparable classification accuracy.",
          "arxiv_id": "2310.06840v1"
        },
        {
          "title": "AESPA: Accuracy Preserving Low-degree Polynomial Activation for Fast Private Inference",
          "year": "2022-01",
          "abstract": "Hybrid private inference (PI) protocol, which synergistically utilizes both\nmulti-party computation (MPC) and homomorphic encryption, is one of the most\nprominent techniques for PI. However, even the state-of-the-art PI protocols\nare bottlenecked by the non-linear layers, especially the activation functions.\nAlthough a standard non-linear activation function can generate higher model\naccuracy, it must be processed via a costly garbled-circuit MPC primitive. A\npolynomial activation can be processed via Beaver's multiplication triples MPC\nprimitive but has been incurring severe accuracy drops so far.\n  In this paper, we propose an accuracy preserving low-degree polynomial\nactivation function (AESPA) that exploits the Hermite expansion of the ReLU and\nbasis-wise normalization. We apply AESPA to popular ML models, such as VGGNet,\nResNet, and pre-activation ResNet, to show an inference accuracy comparable to\nthose of the standard models with ReLU activation, achieving superior accuracy\nover prior low-degree polynomial studies. When applied to the all-RELU baseline\non the state-of-the-art Delphi PI protocol, AESPA shows up to 42.1x and 28.3x\nlower online latency and communication cost.",
          "arxiv_id": "2201.06699v2"
        }
      ],
      "19": [
        {
          "title": "A Survey of Analysis Methods for Security and Safety verification in IoT Systems",
          "year": "2022-03",
          "abstract": "Internet of Things (IoT) has been rapidly growing in the past few years in\nall life disciplines. IoT provides automation and smart control to its users in\ndifferent domains such as home automation, healthcare systems, automotive, and\nmany more. Given the tremendous number of connected IoT devices, this growth\nleads to enormous automatic interactions among sizeable IoT apps in their\nenvironment, making IoT apps more intelligent and more enjoyable to their\nusers. But some unforeseen interactions of IoT apps and any potential malicious\nbehaviour can seriously cause insecure and unsafe consequences to its users,\nprimarily non-experts, who lack the required knowledge regarding the potential\nimpact of their IoT automation processes. In this paper, we study the problem\nof security and safety verification of IoT systems. We survey techniques that\nutilize program analysis to verify IoT applications' security and safety\nproperties. The study proposes a set of categorization and classification\nattributes to enhance our understanding of the research landscape in this\ndomain. Moreover, we discuss the main challenges considered in the surveyed\nwork and potential solutions that could be adopted to ensure the security and\nsafety of IoT systems.",
          "arxiv_id": "2203.01464v1"
        },
        {
          "title": "Security and Machine Learning Adoption in IoT: A Preliminary Study of IoT Developer Discussions",
          "year": "2021-04",
          "abstract": "Internet of Things (IoT) is defined as the connection between places and\nphysical objects (i.e., things) over the internet/network via smart computing\ndevices. Traditionally, we learn about the IoT ecosystem/problems by conducting\nsurveys of IoT developers/practitioners. Another way to learn is by analyzing\nIoT developer discussions in popular online developer forums like Stack\nOverflow (SO). However, we are aware of no such studies that focused on IoT\ndevelopers' security and ML-related discussions in SO. This paper offers the\nresults of preliminary study of IoT developer discussions in SO. We find around\n12% of sentences contain security discussions, while around 0.12% sentences\ncontain ML- related discussions. We find that IoT developers discussing\nsecurity issues frequently inquired about how the shared data can be stored,\nshared, and transferred securely across IoT devices and users. We also find\nthat IoT developers are interested to adopt deep neural network-based ML models\ninto their IoT devices, but they find it challenging to accommodate those into\ntheir resource-constrained IoT devices. Our findings offer implications for IoT\nvendors and researchers to develop and design novel techniques for improved\nsecurity and ML adoption into IoT devices.",
          "arxiv_id": "2104.00634v1"
        },
        {
          "title": "Survey on Enterprise Internet-of-Things Systems (E-IoT): A Security Perspective",
          "year": "2021-02",
          "abstract": "As technology becomes more widely available, millions of users worldwide have\ninstalled some form of smart device in their homes or workplaces. These devices\nare often off-the-shelf commodity systems, such as Google Home or Samsung\nSmartThings, that are installed by end-users looking to automate a small\ndeployment. In contrast to these \"plug-and-play\" systems, purpose-built\nEnterprise Internet-of-Things (E-IoT) systems such as Crestron, Control4, RTI,\nSavant offer a smart solution for more sophisticated applications (e.g.,\ncomplete lighting control, A/V management, security). In contrast to commodity\nsystems, E-IoT systems are usually closed source, costly, require certified\ninstallers, and are overall more robust for their use cases. Due to this, E-IoT\nsystems are often found in expensive smart homes, government and academic\nconference rooms, yachts, and smart private offices. However, while there has\nbeen plenty of research on the topic of commodity systems, no current study\nexists that provides a complete picture of E-IoT systems, their components, and\nrelevant threats. As such, lack of knowledge of E-IoT system threats, coupled\nwith the cost of E-IoT systems has led many to assume that E-IoT systems are\nsecure. To address this research gap, raise awareness on E-IoT security, and\nmotivate further research, this work emphasizes E-IoT system components, E-IoT\nvulnerabilities, solutions, and their security implications. In order to\nsystematically analyze the security of E-IoT systems, we divide E-IoT systems\ninto four layers: E-IoT Devices Layer, Communications Layer, Monitoring and\nApplications Layer, and Business Layer. We survey attacks and defense\nmechanisms, considering the E-IoT components at each layer and the associated\nthreats. In addition, we present key observations in state-of-the-art E-IoT\nsecurity and provide a list of open research problems that need further\nresearch.",
          "arxiv_id": "2102.10695v1"
        }
      ],
      "20": [
        {
          "title": "Robust and Attack Resilient Logic Locking with a High Application-Level Impact",
          "year": "2021-01",
          "abstract": "Logic locking is a hardware security technique to intellectual property (IP)\nagainst security threats in the IC supply chain, especially untrusted fabs.\nSuch techniques incorporate additional locking circuitry within an IC that\ninduces incorrect functionality when an incorrect key is provided. The amount\nof error induced is known as the effectiveness of the locking technique. \"SAT\nattacks\" provide a strong mathematical formulation to find the correct key of\nlocked circuits. In order to achieve high SAT resilience(i.e. complexity of SAT\nattacks), many conventional logic locking schemes fail to inject sufficient\nerror into the circuit. For example, in the case of SARLock and Anti-SAT, there\nare usually very few (or only one) input minterms that cause any error at the\ncircuit output. The state-of-the-art stripped functionality logic locking\n(SFLL) technique introduced a trade-off between SAT resilience and\neffectiveness. In this work, we prove that such a trade-off is universal in\nlogic locking. In order to attain high effectiveness of locking without\ncompromising SAT resilience, we propose a novel logic locking scheme, called\nStrong Anti-SAT (SAS). In addition to SAT attacks, removal-based attacks are\nalso popular against logic locking. Based on SAS, we propose Robust SAS (RSAS)\nwhich is resilient to removal attacks and maintains the same SAT resilience and\nas effectiveness as SAS. SAS and RSAS have the following significant\nimprovements over existing techniques. (1) SAT resilience of SAS and RSAS\nagainst SAT attack is not compromised by increase in effectiveness. (2) In\ncontrast to prior work focusing solely on the circuit-level locking impact, we\nintegrate SAS-locked modules into a processor and show that SAS has a high\napplication-level impact. (3) Our experiments show that SAS and RSAS exhibit\nbetter SAT resilience than SFLL and have similar effectiveness.",
          "arxiv_id": "2101.02577v1"
        },
        {
          "title": "Benchmarking at the Frontier of Hardware Security: Lessons from Logic Locking",
          "year": "2020-06",
          "abstract": "Integrated circuits (ICs) are the foundation of all computing systems. They\ncomprise high-value hardware intellectual property (IP) that are at risk of\npiracy, reverse-engineering, and modifications while making their way through\nthe geographically-distributed IC supply chain. On the frontier of hardware\nsecurity are various design-for-trust techniques that claim to protect designs\nfrom untrusted entities across the design flow. Logic locking is one technique\nthat promises protection from the gamut of threats in IC manufacturing. In this\nwork, we perform a critical review of logic locking techniques in the\nliterature, and expose several shortcomings. Taking inspiration from other\ncybersecurity competitions, we devise a community-led benchmarking exercise to\naddress the evaluation deficiencies. In reflecting on this process, we shed new\nlight on deficiencies in evaluation of logic locking and reveal important\nfuture directions. The lessons learned can guide future endeavors in other\nareas of hardware security.",
          "arxiv_id": "2006.06806v1"
        },
        {
          "title": "UNTANGLE: Unlocking Routing and Logic Obfuscation Using Graph Neural Networks-based Link Prediction",
          "year": "2021-11",
          "abstract": "Logic locking aims to prevent intellectual property (IP) piracy and\nunauthorized overproduction of integrated circuits (ICs). However, initial\nlogic locking techniques were vulnerable to the Boolean satisfiability\n(SAT)-based attacks. In response, researchers proposed various SAT-resistant\nlocking techniques such as point function-based locking and symmetric\ninterconnection (SAT-hard) obfuscation. We focus on the latter since point\nfunction-based locking suffers from various structural vulnerabilities. The\nSAT-hard logic locking technique, InterLock [1], achieves a unified logic and\nrouting obfuscation that thwarts state-of-the-art attacks on logic locking. In\nthis work, we propose a novel link prediction-based attack, UNTANGLE, that\nsuccessfully breaks InterLock in an oracle-less setting without having access\nto an activated IC (oracle). Since InterLock hides selected timing paths in\nkey-controlled routing blocks, UNTANGLE reveals the gates and interconnections\nhidden in the routing blocks upon formulating this task as a link prediction\nproblem. The intuition behind our approach is that ICs contain a large amount\nof repetition and reuse cores. Hence, UNTANGLE can infer the hidden timing\npaths by learning the composition of gates in the observed locked netlist or a\ncircuit library leveraging graph neural networks. We show that circuits\nwithstanding SAT-based and other attacks can be unlocked in seconds with 100%\nprecision using UNTANGLE in an oracle-less setting. UNTANGLE is a generic\nattack platform (which we also open source [2]) that applies to multiplexer\n(MUX)-based obfuscation, as demonstrated through our experiments on ISCAS-85\nand ITC-99 benchmarks locked using InterLock and random MUX-based locking.",
          "arxiv_id": "2111.07062v1"
        }
      ],
      "21": [
        {
          "title": "Certified Robustness of Graph Neural Networks against Adversarial Structural Perturbation",
          "year": "2020-08",
          "abstract": "Graph neural networks (GNNs) have recently gained much attention for node and\ngraph classification tasks on graph-structured data. However, multiple recent\nworks showed that an attacker can easily make GNNs predict incorrectly via\nperturbing the graph structure, i.e., adding or deleting edges in the graph. We\naim to defend against such attacks via developing certifiably robust GNNs.\nSpecifically, we prove the certified robustness guarantee of any GNN for both\nnode and graph classifications against structural perturbation. Moreover, we\nshow that our certified robustness guarantee is tight. Our results are based on\na recently proposed technique called randomized smoothing, which we extend to\ngraph data. We also empirically evaluate our method for both node and graph\nclassifications on multiple GNNs and multiple benchmark datasets. For instance,\non the Cora dataset, Graph Convolutional Network with our randomized smoothing\ncan achieve a certified accuracy of 0.49 when the attacker can arbitrarily\nadd/delete at most 15 edges in the graph.",
          "arxiv_id": "2008.10715v3"
        },
        {
          "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
          "year": "2025-03",
          "abstract": "Graph neural networks (GNNs) are becoming the de facto method to learn on the\ngraph data and have achieved the state-of-the-art on node and graph\nclassification tasks. However, recent works show GNNs are vulnerable to\ntraining-time poisoning attacks -- marginally perturbing edges, nodes, or/and\nnode features of training graph(s) can largely degrade GNNs' testing\nperformance. Most previous defenses against graph poisoning attacks are\nempirical and are soon broken by adaptive / stronger ones. A few provable\ndefenses provide robustness guarantees, but have large gaps when applied in\npractice: 1) restrict the attacker on only one type of perturbation; 2) design\nfor a particular GNN architecture or task; and 3) robustness guarantees are not\n100\\% accurate.\n  In this work, we bridge all these gaps by developing PGNNCert, the first\ncertified defense of GNNs against poisoning attacks under arbitrary (edge,\nnode, and node feature) perturbations with deterministic robustness guarantees.\nExtensive evaluations on multiple node and graph classification datasets and\nGNNs demonstrate the effectiveness of PGNNCert to provably defend against\narbitrary poisoning perturbations. PGNNCert is also shown to significantly\noutperform the state-of-the-art certified defenses against edge perturbation or\nnode perturbation during GNN training.",
          "arxiv_id": "2503.18503v1"
        },
        {
          "title": "Inference Attacks Against Graph Neural Networks",
          "year": "2021-10",
          "abstract": "Graph is an important data representation ubiquitously existing in the real\nworld. However, analyzing the graph data is computationally difficult due to\nits non-Euclidean nature. Graph embedding is a powerful tool to solve the graph\nanalytics problem by transforming the graph data into low-dimensional vectors.\nThese vectors could also be shared with third parties to gain additional\ninsights of what is behind the data. While sharing graph embedding is\nintriguing, the associated privacy risks are unexplored. In this paper, we\nsystematically investigate the information leakage of the graph embedding by\nmounting three inference attacks. First, we can successfully infer basic graph\nproperties, such as the number of nodes, the number of edges, and graph\ndensity, of the target graph with up to 0.89 accuracy. Second, given a subgraph\nof interest and the graph embedding, we can determine with high confidence that\nwhether the subgraph is contained in the target graph. For instance, we achieve\n0.98 attack AUC on the DD dataset. Third, we propose a novel graph\nreconstruction attack that can reconstruct a graph that has similar graph\nstructural statistics to the target graph. We further propose an effective\ndefense mechanism based on graph embedding perturbation to mitigate the\ninference attacks without noticeable performance degradation for graph\nclassification tasks. Our code is available at\nhttps://github.com/Zhangzhk0819/GNN-Embedding-Leaks.",
          "arxiv_id": "2110.02631v1"
        }
      ],
      "22": [
        {
          "title": "An Innovative Information Theory-based Approach to Tackle and Enhance The Transparency in Phishing Detection",
          "year": "2024-02",
          "abstract": "Phishing attacks have become a serious and challenging issue for detection,\nexplanation, and defense. Despite more than a decade of research on phishing,\nencompassing both technical and non-technical remedies, phishing continues to\nbe a serious problem. Nowadays, AI-based phishing detection stands out as one\nof the most effective solutions for defending against phishing attacks by\nproviding vulnerability (i.e., phishing or benign) predictions for the data.\nHowever, it lacks explainability in terms of providing comprehensive\ninterpretations for the predictions, such as identifying the specific\ninformation that causes the data to be classified as phishing. To this end, we\npropose an innovative deep learning-based approach for email (the most common\nphishing way) phishing attack localization. Our method can not only predict the\nvulnerability of the email data but also automatically learn and figure out the\nmost important and phishing-relevant information (i.e., sentences) in the\nphishing email data where the selected information indicates useful and concise\nexplanations for the vulnerability. The rigorous experiments on seven\nreal-world diverse email datasets show the effectiveness and advancement of our\nproposed method in selecting crucial information, offering concise explanations\n(by successfully figuring out the most important and phishing-relevant\ninformation) for the vulnerability of the phishing email data. Particularly,\nour method achieves a significantly higher performance, ranging from\napproximately 1.5% to 3.5%, compared to state-of-the-art baselines, as measured\nby the combined average performance of two main metrics Label-Accuracy and\nCognitive-True-Positive.",
          "arxiv_id": "2402.17092v2"
        },
        {
          "title": "Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector",
          "year": "2025-09",
          "abstract": "To combat phishing attacks -- aimed at luring web users to divulge their\nsensitive information -- various phishing detection approaches have been\nproposed. As attackers focus on devising new tactics to bypass existing\ndetection solutions, researchers have adapted by integrating machine learning\nand deep learning into phishing detection. Phishing dataset collection is vital\nto developing effective phishing detection approaches, which highly depend on\nthe diversity of the gathered datasets. The lack of diversity in the dataset\nresults in a biased model. Since phishing websites are often short-lived,\ncollecting them is also a challenge. Consequently, very few phishing webpage\ndataset repositories exist to date. No single repository comprehensively\nconsolidates all phishing elements corresponding to a phishing webpage, namely,\nURL, webpage source code, screenshot, and related webpage resources. This paper\nintroduces a resource collection tool designed to gather various resources\nassociated with a URL, such as CSS, Javascript, favicons, webpage images, and\nscreenshots. Our tool leverages PhishTank as the primary source for obtaining\nactive phishing URLs. Our tool fetches several additional webpage resources\ncompared to PyWebCopy Python library, which provides webpage content for a\ngiven URL. Additionally, we share a sample dataset generated using our tool\ncomprising 4,056 legitimate and 5,666 phishing URLs along with their associated\nresources. We also remark on the top correlated phishing features with their\nassociated class label found in our dataset. Our tool offers a comprehensive\nresource set that can aid researchers in developing effective phishing\ndetection approaches.",
          "arxiv_id": "2509.09592v1"
        },
        {
          "title": "PEEK: Phishing Evolution Framework for Phishing Generation and Evolving Pattern Analysis using Large Language Models",
          "year": "2024-11",
          "abstract": "Phishing remains a pervasive cyber threat, as attackers craft deceptive\nemails to lure victims into revealing sensitive information. While Artificial\nIntelligence (AI), in particular, deep learning, has become a key component in\ndefending against phishing attacks, these approaches face critical limitations.\nThe scarcity of publicly available, diverse, and updated data, largely due to\nprivacy concerns, constrains detection effectiveness. As phishing tactics\nevolve rapidly, models trained on limited, outdated data struggle to detect\nnew, sophisticated deception strategies, leaving systems and people vulnerable\nto an ever-growing array of attacks. We propose the first Phishing Evolution\nFramEworK (PEEK) for augmenting phishing email datasets with respect to quality\nand diversity, and analyzing changing phishing patterns for detection to adapt\nto updated phishing attacks. Specifically, we integrate large language models\n(LLMs) into the process of adversarial training to enhance the performance of\nthe generated dataset and leverage persuasion principles in a recurrent\nframework to facilitate the understanding of changing phishing strategies. PEEK\nraises the proportion of usable phishing samples from 21.4% to 84.8%,\nsurpassing existing works that rely on prompting and fine-tuning LLMs. The\nphishing datasets provided by PEEK, with evolving phishing patterns, outperform\nthe other two available LLM-generated phishing email datasets in improving\ndetection robustness. PEEK phishing boosts detectors' accuracy to over 88% and\nreduces adversarial sensitivity by up to 70%, still maintaining 70% detection\naccuracy against adversarial attacks.",
          "arxiv_id": "2411.11389v2"
        }
      ],
      "23": [
        {
          "title": "A cyber-physical digital twin approach to replicating realistic multi-stage cyberattacks on smart grids",
          "year": "2024-12",
          "abstract": "The integration of information and communication technology in distribution\ngrids presents opportunities for active grid operation management, but also\nincreases the need for security against power outages and cyberattacks. This\npaper examines the impact of cyberattacks on smart grids by replicating the\npower grid in a secure laboratory environment as a cyber-physical digital twin.\nA simulation is used to study communication infrastructures for secure\noperation of smart grids. The cyber-physical digital twin approach combines\ncommunication network emulation and power grid simulation in a common modular\nenvironment, and is demonstrated through laboratory tests and attack\nreplications.",
          "arxiv_id": "2412.04900v1"
        },
        {
          "title": "Cyberattack Detection in Large-Scale Smart Grids using Chebyshev Graph Convolutional Networks",
          "year": "2021-12",
          "abstract": "As a highly complex and integrated cyber-physical system, modern power grids\nare exposed to cyberattacks. False data injection attacks (FDIAs),\nspecifically, represent a major class of cyber threats to smart grids by\ntargeting the measurement data's integrity. Although various solutions have\nbeen proposed to detect those cyberattacks, the vast majority of the works have\nignored the inherent graph structure of the power grid measurements and\nvalidated their detectors only for small test systems with less than a few\nhundred buses. To better exploit the spatial correlations of smart grid\nmeasurements, this paper proposes a deep learning model for cyberattack\ndetection in large-scale AC power grids using Chebyshev Graph Convolutional\nNetworks (CGCN). By reducing the complexity of spectral graph filters and\nmaking them localized, CGCN provides a fast and efficient convolution operation\nto model the graph structural smart grid data. We numerically verify that the\nproposed CGCN based detector surpasses the state-of-the-art model by 7.86 in\ndetection rate and 9.67 in false alarm rate for a large-scale power grid with\n2848 buses. It is notable that the proposed approach detects cyberattacks under\n4 milliseconds for a 2848-bus system, which makes it a good candidate for\nreal-time detection of cyberattacks in large systems.",
          "arxiv_id": "2112.13166v1"
        },
        {
          "title": "Towards Automated Generation of Smart Grid Cyber Range for Cybersecurity Experiments and Training",
          "year": "2024-04",
          "abstract": "Assurance of cybersecurity is crucial to ensure dependability and resilience\nof smart power grid systems. In order to evaluate the impact of potential cyber\nattacks, to assess deployability and effectiveness of cybersecurity measures,\nand to enable hands-on exercise and training of personals, an interactive,\nvirtual environment that emulates the behaviour of a smart grid system, namely\nsmart grid cyber range, has been demanded by industry players as well as\nacademia. A smart grid cyber range is typically implemented as a combination of\ncyber system emulation, which allows interactivity, and physical system (i.e.,\npower grid) simulation that are tightly coupled for consistent cyber and\nphysical behaviours. However, its design and implementation require intensive\nexpertise and efforts in cyber and physical aspects of smart power systems as\nwell as software/system engineering. While many industry players, including\npower grid operators, device vendors, research and education sectors are\ninterested, availability of the smart grid cyber range is limited to a small\nnumber of research labs. To address this challenge, we have developed a\nframework for modelling a smart grid cyber range using an XML-based language,\ncalled SG-ML, and for \"compiling\" the model into an operational cyber range\nwith minimal engineering efforts. The modelling language includes standardized\nschema from IEC 61850 and IEC 61131, which allows industry players to utilize\ntheir existing configurations. The SG-ML framework aims at making a smart grid\ncyber range available to broader user bases to facilitate cybersecurity R\\&D\nand hands-on exercises.",
          "arxiv_id": "2404.00869v1"
        }
      ],
      "24": [
        {
          "title": "ModZoo: A Large-Scale Study of Modded Android Apps and their Markets",
          "year": "2024-02",
          "abstract": "We present the results of the first large-scale study into Android markets\nthat offer modified or modded apps: apps whose features and functionality have\nbeen altered by a third-party. We analyse over 146k (thousand) apps obtained\nfrom 13 of the most popular modded app markets. Around 90% of apps we collect\nare altered in some way when compared to the official counterparts on Google\nPlay. Modifications include games cheats, such as infinite coins or lives;\nmainstream apps with premium features provided for free; and apps with modified\nadvertising identifiers or excluded ads. We find the original app developers\nlose significant potential revenue due to: the provision of paid for apps for\nfree (around 5% of the apps across all markets); the free availability of\npremium features that require payment in the official app; and modified\nadvertising identifiers. While some modded apps have all trackers and ads\nremoved (3%), in general, the installation of these apps is significantly more\nrisky for the user than the official version: modded apps are ten times more\nlikely to be marked as malicious and often request additional permissions.",
          "arxiv_id": "2402.19180v2"
        },
        {
          "title": "Security Apps under the Looking Glass: An Empirical Analysis of Android Security Apps",
          "year": "2020-07",
          "abstract": "Third-party security apps are an integral part of the Android app ecosystem.\nMany users install them as an extra layer of protection for their devices.\nThere are hundreds of such security apps, both free and paid in Google Play\nStore and some of them are downloaded millions of times. By installing security\napps, the smartphone users place a significant amount of trust towards the\nsecurity companies who developed these apps, because a fully functional mobile\nsecurity app requires access to many smartphone resources such as the storage,\ntext messages and email, browser history, and information about other installed\napplications. Often these resources contain highly sensitive personal\ninformation. As such, it is essential to understand the mobile security apps\necosystem to assess whether is it indeed beneficial to install them. To this\nend, in this paper, we present the first empirical study of Android security\napps. We analyse 100 Android security apps from multiple aspects such as\nmetadata, static analysis, and dynamic analysis and presents insights to their\noperations and behaviours. Our results show that 20% of the security apps we\nstudied potentially resell the data they collect from smartphones to third\nparties; in some cases, even without the user consent. Also, our experiments\nshow that around 50% of the security apps fail to identify malware installed on\na smartphone.",
          "arxiv_id": "2007.03905v1"
        },
        {
          "title": "SeMA: Extending and Analyzing Storyboards to Develop Secure Android Apps",
          "year": "2020-01",
          "abstract": "Mobile apps provide various critical services, such as banking,\ncommunication, and healthcare. To this end, they have access to our personal\ninformation and have the ability to perform actions on our behalf. Hence,\nsecuring mobile apps is crucial to ensuring the privacy and safety of its\nusers.\n  Recent research efforts have focused on developing solutions to secure mobile\necosystems (i.e., app platforms, apps, and app stores), specifically in the\ncontext of detecting vulnerabilities in Android apps. Despite this attention,\nknown vulnerabilities are often found in mobile apps, which can be exploited by\nmalicious apps to harm the user. Further, fixing vulnerabilities after\ndeveloping an app has downsides in terms of time, resources, user\ninconvenience, and information loss.\n  In an attempt to address this concern, we have developed SeMA, a mobile app\ndevelopment methodology that builds on existing mobile app design artifacts\nsuch as storyboards. With SeMA, security is a first-class citizen in an app's\ndesign -- app designers and developers can collaborate to specify and reason\nabout the security properties of an app at an abstract level without being\ndistracted by implementation level details. Our realization of SeMA using\nAndroid Studio tooling demonstrates the methodology is complementary to\nexisting design and development practices. An evaluation of the effectiveness\nof SeMA shows the methodology can detect and help prevent 49 vulnerabilities\nknown to occur in Android apps. Further, a usability study of the methodology\ninvolving ten real-world developers shows the methodology is likely to reduce\nthe development time and help developers uncover and prevent known\nvulnerabilities while designing apps.",
          "arxiv_id": "2001.10052v4"
        }
      ],
      "25": [
        {
          "title": "BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models",
          "year": "2023-07",
          "abstract": "The rise in popularity of text-to-image generative artificial intelligence\n(AI) has attracted widespread public interest. We demonstrate that this\ntechnology can be attacked to generate content that subtly manipulates its\nusers. We propose a Backdoor Attack on text-to-image Generative Models (BAGM),\nwhich upon triggering, infuses the generated images with manipulative details\nthat are naturally blended in the content. Our attack is the first to target\nthree popular text-to-image generative models across three stages of the\ngenerative process by modifying the behaviour of the embedded tokenizer, the\nlanguage model or the image generative model. Based on the penetration level,\nBAGM takes the form of a suite of attacks that are referred to as surface,\nshallow and deep attacks in this article. Given the existing gap within this\ndomain, we also contribute a comprehensive set of quantitative metrics designed\nspecifically for assessing the effectiveness of backdoor attacks on\ntext-to-image models. The efficacy of BAGM is established by attacking\nstate-of-the-art generative models, using a marketing scenario as the target\ndomain. To that end, we contribute a dataset of branded product images. Our\nembedded backdoors increase the bias towards the target outputs by more than\nfive times the usual, without compromising the model robustness or the\ngenerated content utility. By exposing generative AI's vulnerabilities, we\nencourage researchers to tackle these challenges and practitioners to exercise\ncaution when using pre-trained models. Relevant code, input prompts and\nsupplementary material can be found at https://github.com/JJ-Vice/BAGM, and the\ndataset is available at:\nhttps://ieee-dataport.org/documents/marketable-foods-mf-dataset.\n  Keywords: Generative Artificial Intelligence, Generative Models,\nText-to-Image generation, Backdoor Attacks, Trojan, Stable Diffusion.",
          "arxiv_id": "2307.16489v2"
        },
        {
          "title": "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning",
          "year": "2023-05",
          "abstract": "With the help of conditioning mechanisms, the state-of-the-art diffusion\nmodels have achieved tremendous success in guided image generation,\nparticularly in text-to-image synthesis. To gain a better understanding of the\ntraining process and potential risks of text-to-image synthesis, we perform a\nsystematic investigation of backdoor attack on text-to-image diffusion models\nand propose BadT2I, a general multimodal backdoor attack framework that tampers\nwith image synthesis in diverse semantic levels. Specifically, we perform\nbackdoor attacks on three levels of the vision semantics: Pixel-Backdoor,\nObject-Backdoor and Style-Backdoor. By utilizing a regularization loss, our\nmethods efficiently inject backdoors into a large-scale text-to-image diffusion\nmodel while preserving its utility with benign inputs. We conduct empirical\nexperiments on Stable Diffusion, the widely-used text-to-image diffusion model,\ndemonstrating that the large-scale diffusion model can be easily backdoored\nwithin a few fine-tuning steps. We conduct additional experiments to explore\nthe impact of different types of textual triggers, as well as the backdoor\npersistence during further training, providing insights for the development of\nbackdoor defense methods. Besides, our investigation may contribute to the\ncopyright protection of text-to-image models in the future.",
          "arxiv_id": "2305.04175v2"
        },
        {
          "title": "DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models",
          "year": "2022-10",
          "abstract": "Text-to-image generation models that generate images based on prompt\ndescriptions have attracted an increasing amount of attention during the past\nfew months. Despite their encouraging performance, these models raise concerns\nabout the misuse of their generated fake images. To tackle this problem, we\npioneer a systematic study on the detection and attribution of fake images\ngenerated by text-to-image generation models. Concretely, we first build a\nmachine learning classifier to detect the fake images generated by various\ntext-to-image generation models. We then attribute these fake images to their\nsource models, such that model owners can be held responsible for their models'\nmisuse. We further investigate how prompts that generate fake images affect\ndetection and attribution. We conduct extensive experiments on four popular\ntext-to-image generation models, including DALL$\\cdot$E 2, Stable Diffusion,\nGLIDE, and Latent Diffusion, and two benchmark prompt-image datasets. Empirical\nresults show that (1) fake images generated by various models can be\ndistinguished from real ones, as there exists a common artifact shared by fake\nimages from different models; (2) fake images can be effectively attributed to\ntheir source models, as different models leave unique fingerprints in their\ngenerated images; (3) prompts with the ``person'' topic or a length between 25\nand 75 enable models to generate fake images with higher authenticity. All\nfindings contribute to the community's insight into the threats caused by\ntext-to-image generation models. We appeal to the community's consideration of\nthe counterpart solutions, like ours, against the rapidly-evolving fake image\ngeneration.",
          "arxiv_id": "2210.06998v2"
        }
      ],
      "26": [
        {
          "title": "Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data Generation and Evaluation in Learning Analytics",
          "year": "2024-01",
          "abstract": "Privacy poses a significant obstacle to the progress of learning analytics\n(LA), presenting challenges like inadequate anonymization and data misuse that\ncurrent solutions struggle to address. Synthetic data emerges as a potential\nremedy, offering robust privacy protection. However, prior LA research on\nsynthetic data lacks thorough evaluation, essential for assessing the delicate\nbalance between privacy and data utility. Synthetic data must not only enhance\nprivacy but also remain practical for data analytics. Moreover, diverse LA\nscenarios come with varying privacy and utility needs, making the selection of\nan appropriate synthetic data approach a pressing challenge. To address these\ngaps, we propose a comprehensive evaluation of synthetic data, which\nencompasses three dimensions of synthetic data quality, namely resemblance,\nutility, and privacy. We apply this evaluation to three distinct LA datasets,\nusing three different synthetic data generation methods. Our results show that\nsynthetic data can maintain similar utility (i.e., predictive performance) as\nreal data, while preserving privacy. Furthermore, considering different privacy\nand data utility requirements in different LA scenarios, we make customized\nrecommendations for synthetic data generation. This paper not only presents a\ncomprehensive evaluation of synthetic data but also illustrates its potential\nin mitigating privacy concerns within the field of LA, thus contributing to a\nwider application of synthetic data in LA and promoting a better practice for\nopen science.",
          "arxiv_id": "2401.06883v1"
        },
        {
          "title": "PreFair: Privately Generating Justifiably Fair Synthetic Data",
          "year": "2022-12",
          "abstract": "When a database is protected by Differential Privacy (DP), its usability is\nlimited in scope. In this scenario, generating a synthetic version of the data\nthat mimics the properties of the private data allows users to perform any\noperation on the synthetic data, while maintaining the privacy of the original\ndata. Therefore, multiple works have been devoted to devising systems for DP\nsynthetic data generation. However, such systems may preserve or even magnify\nproperties of the data that make it unfair, endering the synthetic data unfit\nfor use. In this work, we present PreFair, a system that allows for DP fair\nsynthetic data generation. PreFair extends the state-of-the-art DP data\ngeneration mechanisms by incorporating a causal fairness criterion that ensures\nfair synthetic data. We adapt the notion of justifiable fairness to fit the\nsynthetic data generation scenario. We further study the problem of generating\nDP fair synthetic data, showing its intractability and designing algorithms\nthat are optimal under certain assumptions. We also provide an extensive\nexperimental evaluation, showing that PreFair generates synthetic data that is\nsignificantly fairer than the data generated by leading DP data generation\nmechanisms, while remaining faithful to the private data.",
          "arxiv_id": "2212.10310v2"
        },
        {
          "title": "Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data",
          "year": "2023-10",
          "abstract": "Differentially private (DP) synthetic data sets are a solution for sharing\ndata while preserving the privacy of individual data providers. Understanding\nthe effects of utilizing DP synthetic data in end-to-end machine learning\npipelines impacts areas such as health care and humanitarian action, where data\nis scarce and regulated by restrictive privacy laws. In this work, we\ninvestigate the extent to which synthetic data can replace real, tabular data\nin machine learning pipelines and identify the most effective synthetic data\ngeneration techniques for training and evaluating machine learning models. We\ninvestigate the impacts of differentially private synthetic data on downstream\nclassification tasks from the point of view of utility as well as fairness. Our\nanalysis is comprehensive and includes representatives of the two main types of\nsynthetic data generation algorithms: marginal-based and GAN-based. To the best\nof our knowledge, our work is the first that: (i) proposes a training and\nevaluation framework that does not assume that real data is available for\ntesting the utility and fairness of machine learning models trained on\nsynthetic data; (ii) presents the most extensive analysis of synthetic data set\ngeneration algorithms in terms of utility and fairness when used for training\nmachine learning models; and (iii) encompasses several different definitions of\nfairness. Our findings demonstrate that marginal-based synthetic data\ngenerators surpass GAN-based ones regarding model training utility for tabular\ndata. Indeed, we show that models trained using data generated by\nmarginal-based algorithms can exhibit similar utility to models trained using\nreal data. Our analysis also reveals that the marginal-based synthetic data\ngenerator MWEM PGM can train models that simultaneously achieve utility and\nfairness characteristics close to those obtained by models trained with real\ndata.",
          "arxiv_id": "2310.19250v1"
        }
      ],
      "27": [
        {
          "title": "HASHTAG: Hash Signatures for Online Detection of Fault-Injection Attacks on Deep Neural Networks",
          "year": "2021-11",
          "abstract": "We propose HASHTAG, the first framework that enables high-accuracy detection\nof fault-injection attacks on Deep Neural Networks (DNNs) with provable bounds\non detection performance. Recent literature in fault-injection attacks shows\nthe severe DNN accuracy degradation caused by bit flips. In this scenario, the\nattacker changes a few weight bits during DNN execution by tampering with the\nprogram's DRAM memory. To detect runtime bit flips, HASHTAG extracts a unique\nsignature from the benign DNN prior to deployment. The signature is later used\nto validate the integrity of the DNN and verify the inference output on the\nfly. We propose a novel sensitivity analysis scheme that accurately identifies\nthe most vulnerable DNN layers to the fault-injection attack. The DNN signature\nis then constructed by encoding the underlying weights in the vulnerable layers\nusing a low-collision hash function. When the DNN is deployed, new hashes are\nextracted from the target layers during inference and compared against the\nground-truth signatures. HASHTAG incorporates a lightweight methodology that\nensures a low-overhead and real-time fault detection on embedded platforms.\nExtensive evaluations with the state-of-the-art bit-flip attack on various DNNs\ndemonstrate the competitive advantage of HASHTAG in terms of both attack\ndetection and execution overhead.",
          "arxiv_id": "2111.01932v1"
        },
        {
          "title": "Decompiling x86 Deep Neural Network Executables",
          "year": "2022-10",
          "abstract": "Due to their widespread use on heterogeneous hardware devices, deep learning\n(DL) models are compiled into executables by DL compilers to fully leverage\nlow-level hardware primitives. This approach allows DL computations to be\nundertaken at low cost across a variety of computing platforms, including CPUs,\nGPUs, and various hardware accelerators.\n  We present BTD (Bin to DNN), a decompiler for deep neural network (DNN)\nexecutables. BTD takes DNN executables and outputs full model specifications,\nincluding types of DNN operators, network topology, dimensions, and parameters\nthat are (nearly) identical to those of the input models. BTD delivers a\npractical framework to process DNN executables compiled by different DL\ncompilers and with full optimizations enabled on x86 platforms. It employs\nlearning-based techniques to infer DNN operators, dynamic analysis to reveal\nnetwork architectures, and symbolic execution to facilitate inferring\ndimensions and parameters of DNN operators.\n  Our evaluation reveals that BTD enables accurate recovery of full\nspecifications of complex DNNs with millions of parameters (e.g., ResNet). The\nrecovered DNN specifications can be re-compiled into a new DNN executable\nexhibiting identical behavior to the input executable. We show that BTD can\nboost two representative attacks, adversarial example generation and knowledge\nstealing, against DNN executables. We also demonstrate cross-architecture\nlegacy code reuse using BTD, and envision BTD being used for other critical\ndownstream tasks like DNN security hardening and patching.",
          "arxiv_id": "2210.01075v2"
        },
        {
          "title": "DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips",
          "year": "2020-03",
          "abstract": "Security of machine learning is increasingly becoming a major concern due to\nthe ubiquitous deployment of deep learning in many security-sensitive domains.\nMany prior studies have shown external attacks such as adversarial examples\nthat tamper with the integrity of DNNs using maliciously crafted inputs.\nHowever, the security implication of internal threats (i.e., hardware\nvulnerability) to DNN models has not yet been well understood. In this paper,\nwe demonstrate the first hardware-based attack on quantized deep neural\nnetworks-DeepHammer-that deterministically induces bit flips in model weights\nto compromise DNN inference by exploiting the rowhammer vulnerability.\nDeepHammer performs aggressive bit search in the DNN model to identify the most\nvulnerable weight bits that are flippable under system constraints. To trigger\ndeterministic bit flips across multiple pages within reasonable amount of time,\nwe develop novel system-level techniques that enable fast deployment of victim\npages, memory-efficient rowhammering and precise flipping of targeted bits.\nDeepHammer can deliberately degrade the inference accuracy of the victim DNN\nsystem to a level that is only as good as random guess, thus completely\ndepleting the intelligence of targeted DNN systems. We systematically\ndemonstrate our attacks on real systems against 12 DNN architectures with 4\ndifferent datasets and different application domains. Our evaluation shows that\nDeepHammer is able to successfully tamper DNN inference behavior at run-time\nwithin a few minutes. We further discuss several mitigation techniques from\nboth algorithm and system levels to protect DNNs against such attacks. Our work\nhighlights the need to incorporate security mechanisms in future deep learning\nsystem to enhance the robustness of DNN against hardware-based deterministic\nfault injections.",
          "arxiv_id": "2003.13746v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T18:49:05Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}