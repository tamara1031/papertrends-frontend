{
  "topics": {
    "data": {
      "0": {
        "name": "0_ASR_speech_models_model",
        "keywords": [
          [
            "ASR",
            0.022171389135551688
          ],
          [
            "speech",
            0.019631832817726857
          ],
          [
            "models",
            0.015403634909939682
          ],
          [
            "model",
            0.014734730812337853
          ],
          [
            "language",
            0.013515241318880355
          ],
          [
            "recognition",
            0.012650635635403302
          ],
          [
            "training",
            0.012266895955329822
          ],
          [
            "Speech",
            0.011935046319806804
          ],
          [
            "speech recognition",
            0.011875795587809946
          ],
          [
            "data",
            0.01145125626937477
          ]
        ],
        "count": 2224
      },
      "1": {
        "name": "1_music_generation_musical_audio",
        "keywords": [
          [
            "music",
            0.040927752914282176
          ],
          [
            "generation",
            0.018803734800690225
          ],
          [
            "musical",
            0.01753011083237159
          ],
          [
            "audio",
            0.015868409593107133
          ],
          [
            "Music",
            0.01562334691424509
          ],
          [
            "model",
            0.011902264599798084
          ],
          [
            "models",
            0.010800096938262817
          ],
          [
            "music generation",
            0.008313981663223786
          ],
          [
            "text",
            0.007850139543877808
          ],
          [
            "symbolic",
            0.007778443010348625
          ]
        ],
        "count": 1413
      },
      "2": {
        "name": "2_speech_enhancement_speech enhancement_noise",
        "keywords": [
          [
            "speech",
            0.023744753117829264
          ],
          [
            "enhancement",
            0.02031265208867178
          ],
          [
            "speech enhancement",
            0.01698731457432658
          ],
          [
            "noise",
            0.013121323830679706
          ],
          [
            "separation",
            0.01178794786130921
          ],
          [
            "network",
            0.01146981895237553
          ],
          [
            "performance",
            0.011376049109508315
          ],
          [
            "Speech",
            0.011333933858084694
          ],
          [
            "model",
            0.011326122538236146
          ],
          [
            "channel",
            0.01031309846825299
          ]
        ],
        "count": 1190
      },
      "3": {
        "name": "3_attacks_audio_detection_adversarial",
        "keywords": [
          [
            "attacks",
            0.0218709764491267
          ],
          [
            "audio",
            0.018717242013413918
          ],
          [
            "detection",
            0.017524211223808612
          ],
          [
            "adversarial",
            0.016664517083104204
          ],
          [
            "deepfake",
            0.015347937747684764
          ],
          [
            "speech",
            0.014136788490956526
          ],
          [
            "spoofing",
            0.013516806757730638
          ],
          [
            "attack",
            0.013017482713048855
          ],
          [
            "privacy",
            0.011846227239552776
          ],
          [
            "speaker",
            0.011227571618504137
          ]
        ],
        "count": 883
      },
      "4": {
        "name": "4_sound_acoustic_room_method",
        "keywords": [
          [
            "sound",
            0.027360097648070018
          ],
          [
            "acoustic",
            0.018961242731323186
          ],
          [
            "room",
            0.01578770932022825
          ],
          [
            "method",
            0.014242820119777172
          ],
          [
            "source",
            0.013900261942271641
          ],
          [
            "microphone",
            0.012829414742105083
          ],
          [
            "field",
            0.012602731539550247
          ],
          [
            "estimation",
            0.011880831209095057
          ],
          [
            "spatial",
            0.011811173840532364
          ],
          [
            "localization",
            0.011334264445707787
          ]
        ],
        "count": 720
      },
      "5": {
        "name": "5_event_sound_audio_classification",
        "keywords": [
          [
            "event",
            0.020010059853746074
          ],
          [
            "sound",
            0.01934216919120176
          ],
          [
            "audio",
            0.018637599493385403
          ],
          [
            "classification",
            0.014903006267309133
          ],
          [
            "events",
            0.012740325989165942
          ],
          [
            "sound event",
            0.012224003290782564
          ],
          [
            "SED",
            0.012021130399684879
          ],
          [
            "data",
            0.011858496134307181
          ],
          [
            "Sound",
            0.011564491003470971
          ],
          [
            "learning",
            0.010628566984117122
          ]
        ],
        "count": 616
      },
      "6": {
        "name": "6_emotion_emotion recognition_Emotion_recognition",
        "keywords": [
          [
            "emotion",
            0.03911065181115486
          ],
          [
            "emotion recognition",
            0.025118129640445064
          ],
          [
            "Emotion",
            0.024442376316334474
          ],
          [
            "recognition",
            0.019170035230070763
          ],
          [
            "Recognition",
            0.01558330207004949
          ],
          [
            "emotional",
            0.013824387515033993
          ],
          [
            "emotions",
            0.013572956353035123
          ],
          [
            "speech emotion",
            0.013329884212486884
          ],
          [
            "speech",
            0.01299148021178026
          ],
          [
            "Speech",
            0.012624988629233868
          ]
        ],
        "count": 546
      },
      "7": {
        "name": "7_audio_visual_Audio_text",
        "keywords": [
          [
            "audio",
            0.04350973116538339
          ],
          [
            "visual",
            0.022175633385142668
          ],
          [
            "Audio",
            0.0218449823876212
          ],
          [
            "text",
            0.014567508606072838
          ],
          [
            "language",
            0.012831382207972619
          ],
          [
            "captioning",
            0.012593802906017769
          ],
          [
            "models",
            0.01138340696489301
          ],
          [
            "modal",
            0.011071747873793805
          ],
          [
            "video",
            0.010747970244473224
          ],
          [
            "sound",
            0.01037779138726458
          ]
        ],
        "count": 536
      },
      "8": {
        "name": "8_speech_EEG_features_dysarthric",
        "keywords": [
          [
            "speech",
            0.02734633386535092
          ],
          [
            "EEG",
            0.018505705374072486
          ],
          [
            "features",
            0.014332769784047232
          ],
          [
            "dysarthric",
            0.013465935437039502
          ],
          [
            "articulatory",
            0.012681302272804083
          ],
          [
            "data",
            0.01069075143835785
          ],
          [
            "Speech",
            0.010477154501219886
          ],
          [
            "model",
            0.009434503156283624
          ],
          [
            "signals",
            0.008870219556339962
          ],
          [
            "disease",
            0.00870251010875215
          ]
        ],
        "count": 501
      },
      "9": {
        "name": "9_TTS_speech_style_prosody",
        "keywords": [
          [
            "TTS",
            0.03665287981481092
          ],
          [
            "speech",
            0.028050513781835297
          ],
          [
            "style",
            0.02439074575357436
          ],
          [
            "prosody",
            0.01977537165929877
          ],
          [
            "text",
            0.018883086873340976
          ],
          [
            "speaker",
            0.0167338969072943
          ],
          [
            "model",
            0.014159184563547762
          ],
          [
            "synthesis",
            0.01412336732758124
          ],
          [
            "Speech",
            0.012127817896785804
          ],
          [
            "speech synthesis",
            0.011092030351589418
          ]
        ],
        "count": 488
      },
      "10": {
        "name": "10_speaker_diarization_Speaker_speaker diarization",
        "keywords": [
          [
            "speaker",
            0.04630496177176722
          ],
          [
            "diarization",
            0.035902818489676094
          ],
          [
            "Speaker",
            0.017487031499780877
          ],
          [
            "speaker diarization",
            0.017048730976753794
          ],
          [
            "speech",
            0.016137703660024316
          ],
          [
            "speakers",
            0.015676987606870783
          ],
          [
            "multi",
            0.014793133823918921
          ],
          [
            "end",
            0.012937471857582646
          ],
          [
            "target",
            0.01271556878304788
          ],
          [
            "EEND",
            0.011961389714711642
          ]
        ],
        "count": 448
      },
      "11": {
        "name": "11_speaker_verification_speaker verification_Speaker",
        "keywords": [
          [
            "speaker",
            0.04478791889172735
          ],
          [
            "verification",
            0.02935765067057568
          ],
          [
            "speaker verification",
            0.025897259782429335
          ],
          [
            "Speaker",
            0.024186991743766515
          ],
          [
            "Verification",
            0.01354640065700206
          ],
          [
            "speaker recognition",
            0.0133370260669154
          ],
          [
            "VoxCeleb",
            0.012502411757616152
          ],
          [
            "performance",
            0.011299262070584064
          ],
          [
            "EER",
            0.01033062051348976
          ],
          [
            "embeddings",
            0.010185775842724143
          ]
        ],
        "count": 409
      },
      "12": {
        "name": "12_speech_quality_audio_neural",
        "keywords": [
          [
            "speech",
            0.019886015743992725
          ],
          [
            "quality",
            0.016900683223319505
          ],
          [
            "audio",
            0.015544294200809725
          ],
          [
            "neural",
            0.01363904982805699
          ],
          [
            "high",
            0.013090060898011612
          ],
          [
            "model",
            0.01291632205136226
          ],
          [
            "models",
            0.012108774744027674
          ],
          [
            "generative",
            0.011839267671872093
          ],
          [
            "vocoder",
            0.011130154710307493
          ],
          [
            "codecs",
            0.010784705391168749
          ]
        ],
        "count": 408
      },
      "13": {
        "name": "13_COVID_respiratory_cough_heart",
        "keywords": [
          [
            "COVID",
            0.03941796853251749
          ],
          [
            "respiratory",
            0.028365006017249225
          ],
          [
            "cough",
            0.02552835570799453
          ],
          [
            "heart",
            0.01918332895795137
          ],
          [
            "sounds",
            0.01827884379563202
          ],
          [
            "sound",
            0.016519435812333432
          ],
          [
            "detection",
            0.01401197708572074
          ],
          [
            "classification",
            0.013432810463049049
          ],
          [
            "data",
            0.01244012926510764
          ],
          [
            "lung",
            0.011769123276468795
          ]
        ],
        "count": 329
      },
      "14": {
        "name": "14_VC_conversion_voice conversion_voice",
        "keywords": [
          [
            "VC",
            0.058548903862702
          ],
          [
            "conversion",
            0.05097772272375789
          ],
          [
            "voice conversion",
            0.03877812373227338
          ],
          [
            "voice",
            0.034971910374949164
          ],
          [
            "speaker",
            0.028281835516148088
          ],
          [
            "Conversion",
            0.025298550199223037
          ],
          [
            "Voice",
            0.024249231998981106
          ],
          [
            "content",
            0.019249486194657644
          ],
          [
            "speech",
            0.01891495773876431
          ],
          [
            "shot",
            0.01421865608462649
          ]
        ],
        "count": 231
      },
      "15": {
        "name": "15_audio_effects_sound_synthesis",
        "keywords": [
          [
            "audio",
            0.02523859629168775
          ],
          [
            "effects",
            0.017136068774359595
          ],
          [
            "sound",
            0.014506403639773069
          ],
          [
            "synthesis",
            0.01355138788244272
          ],
          [
            "neural",
            0.012189883223425242
          ],
          [
            "time",
            0.010995538491496207
          ],
          [
            "model",
            0.010812480394698594
          ],
          [
            "signal",
            0.01066065764251161
          ],
          [
            "audio effects",
            0.010527973549522179
          ],
          [
            "control",
            0.010188425870138778
          ]
        ],
        "count": 213
      },
      "16": {
        "name": "16_singing_singing voice_voice_Singing",
        "keywords": [
          [
            "singing",
            0.07976979165895665
          ],
          [
            "singing voice",
            0.0445150933427692
          ],
          [
            "voice",
            0.04035592453901736
          ],
          [
            "Singing",
            0.031736792033217553
          ],
          [
            "SVS",
            0.02466843165796646
          ],
          [
            "singer",
            0.020035328401983615
          ],
          [
            "Voice",
            0.017309834594850974
          ],
          [
            "voice synthesis",
            0.01730413243334476
          ],
          [
            "pitch",
            0.0172778433480648
          ],
          [
            "singing voice synthesis",
            0.0151014348283206
          ]
        ],
        "count": 205
      },
      "17": {
        "name": "17_species_bird_classification_data",
        "keywords": [
          [
            "species",
            0.030974480416296656
          ],
          [
            "bird",
            0.023017463681159287
          ],
          [
            "classification",
            0.01913863844225731
          ],
          [
            "data",
            0.018161199505658823
          ],
          [
            "learning",
            0.01779486380740021
          ],
          [
            "monitoring",
            0.017348609202406513
          ],
          [
            "bioacoustic",
            0.016439558289558863
          ],
          [
            "detection",
            0.013988030244986594
          ],
          [
            "acoustic",
            0.012739314596504311
          ],
          [
            "animal",
            0.012667775244951959
          ]
        ],
        "count": 196
      },
      "18": {
        "name": "18_KWS_keyword_spotting_Keyword",
        "keywords": [
          [
            "KWS",
            0.057284615830958445
          ],
          [
            "keyword",
            0.04499324205154979
          ],
          [
            "spotting",
            0.032653029385081675
          ],
          [
            "Keyword",
            0.03066642141344441
          ],
          [
            "keyword spotting",
            0.02739632363724138
          ],
          [
            "Spotting",
            0.023179350442308226
          ],
          [
            "keywords",
            0.018200935046170805
          ],
          [
            "model",
            0.01462877445618381
          ],
          [
            "false",
            0.012180139672257513
          ],
          [
            "devices",
            0.011628316751265053
          ]
        ],
        "count": 164
      },
      "19": {
        "name": "19_anomalous_detection_machine_ASD",
        "keywords": [
          [
            "anomalous",
            0.03263172895353492
          ],
          [
            "detection",
            0.027090315499722774
          ],
          [
            "machine",
            0.02684658652976225
          ],
          [
            "ASD",
            0.02169608876761309
          ],
          [
            "sound",
            0.02053305769857637
          ],
          [
            "anomaly",
            0.0181093226890758
          ],
          [
            "Anomalous",
            0.017905942320475685
          ],
          [
            "data",
            0.01756387854040863
          ],
          [
            "anomalous sound",
            0.017478056582494548
          ],
          [
            "sounds",
            0.017400168588019616
          ]
        ],
        "count": 144
      },
      "20": {
        "name": "20_face_talking_lip_facial",
        "keywords": [
          [
            "face",
            0.031660934085613185
          ],
          [
            "talking",
            0.029635626034892316
          ],
          [
            "lip",
            0.02921007802733249
          ],
          [
            "facial",
            0.02311635001382437
          ],
          [
            "speech",
            0.02270773074664242
          ],
          [
            "video",
            0.020962269849535826
          ],
          [
            "audio",
            0.017471756828405915
          ],
          [
            "visual",
            0.014715176627429197
          ],
          [
            "head",
            0.014649832845100411
          ],
          [
            "3D",
            0.014167298361435916
          ]
        ],
        "count": 142
      },
      "21": {
        "name": "21_separation_source separation_source_music",
        "keywords": [
          [
            "separation",
            0.056600138001772556
          ],
          [
            "source separation",
            0.04591251227459678
          ],
          [
            "source",
            0.04091815576188457
          ],
          [
            "music",
            0.023004814283662706
          ],
          [
            "Separation",
            0.020670941611771135
          ],
          [
            "Source",
            0.018941353510295852
          ],
          [
            "music source",
            0.017102148234171343
          ],
          [
            "music source separation",
            0.016424451501949888
          ],
          [
            "Music",
            0.014971612188779983
          ],
          [
            "model",
            0.013791174403595126
          ]
        ],
        "count": 136
      },
      "22": {
        "name": "22_visual_AVSR_audio_visual speech",
        "keywords": [
          [
            "visual",
            0.05672528050235785
          ],
          [
            "AVSR",
            0.02542278090982091
          ],
          [
            "audio",
            0.024734207002142922
          ],
          [
            "visual speech",
            0.024142291240388185
          ],
          [
            "Visual",
            0.022372468462106273
          ],
          [
            "speech",
            0.019521808875458877
          ],
          [
            "recognition",
            0.018698592424342093
          ],
          [
            "lip",
            0.01792509924125371
          ],
          [
            "speech recognition",
            0.01711718399737553
          ],
          [
            "Audio",
            0.015649396002109472
          ]
        ],
        "count": 132
      }
    },
    "correlations": [
      [
        1.0,
        -0.7100573222873876,
        -0.6502571481018613,
        -0.7170335014373864,
        -0.734182024377541,
        -0.7328201533429548,
        -0.716454256511647,
        -0.7019723494439023,
        -0.5566151466887982,
        -0.6268047437448738,
        -0.6924500706268419,
        -0.7185792523680798,
        -0.6316039024331692,
        -0.7476665416975533,
        -0.7296430728555974,
        -0.7347646013895555,
        -0.7474698651777384,
        -0.6834365405426575,
        -0.7480079516532719,
        -0.7431296555226552,
        -0.6598667310834966,
        -0.7252178861730838,
        -0.3636927198614225
      ],
      [
        -0.7100573222873876,
        1.0,
        -0.7534034765651902,
        -0.7003299201401103,
        -0.744536923444465,
        -0.7092702575988208,
        -0.7383729511387,
        -0.6491076492157648,
        -0.7483773284449728,
        -0.7342470050652361,
        -0.7606723218134337,
        -0.7617241069643657,
        -0.6617472435473837,
        -0.762448678751769,
        -0.7558645963359271,
        -0.6713787657917575,
        -0.7442093918188054,
        -0.7189180670027444,
        -0.7638816211233305,
        -0.7415482636834343,
        -0.7392073242911903,
        -0.6551768191744691,
        -0.69863508112609
      ],
      [
        -0.6502571481018613,
        -0.7534034765651902,
        1.0,
        -0.7479965524847096,
        -0.7366405391649822,
        -0.7528952536511213,
        -0.7487460562236494,
        -0.7422215211216732,
        -0.5452248855143903,
        -0.6375873824691362,
        -0.71746564189586,
        -0.729521968538076,
        -0.40635226138947345,
        -0.7586292856088765,
        -0.7302911145867614,
        -0.7467253616407814,
        -0.7560262057741222,
        -0.7341054574555539,
        -0.7591109372803166,
        -0.7562519370916552,
        -0.5833772400759574,
        -0.681590661846276,
        -0.7131563335241409
      ],
      [
        -0.7170335014373864,
        -0.7003299201401103,
        -0.7479965524847096,
        1.0,
        -0.7501361506538422,
        -0.6621893197161588,
        -0.7455247267357071,
        -0.6048538172040727,
        -0.7355099130231888,
        -0.7273770427834894,
        -0.7346366206372907,
        -0.6709007745043898,
        -0.6618259979596401,
        -0.748013610809614,
        -0.7229263161705429,
        -0.5906140856394906,
        -0.7407480233503196,
        -0.7260828749360114,
        -0.7552205396691668,
        -0.6048743445917519,
        -0.7178485456842834,
        -0.7520847537132586,
        -0.6629949985270733
      ],
      [
        -0.734182024377541,
        -0.744536923444465,
        -0.7366405391649822,
        -0.7501361506538422,
        1.0,
        -0.6498677358370507,
        -0.7586773412884292,
        -0.7276799249625894,
        -0.7463655669711388,
        -0.7520177371617467,
        -0.7521961754232065,
        -0.7550041868749542,
        -0.7356231512363061,
        -0.7446336719174567,
        -0.7567282881681933,
        -0.7313020146153173,
        -0.754195136682337,
        -0.7312728562884578,
        -0.7643964792100872,
        -0.6592332089575397,
        -0.750664597008977,
        -0.6878578264705,
        -0.7176058881426413
      ],
      [
        -0.7328201533429548,
        -0.7092702575988208,
        -0.7528952536511213,
        -0.6621893197161588,
        -0.6498677358370507,
        1.0,
        -0.7493514110843045,
        -0.5918507685345205,
        -0.7549256350076698,
        -0.7537883313897227,
        -0.7564897199058048,
        -0.7582002667567735,
        -0.677694385942955,
        -0.7378709879265963,
        -0.7599369552043715,
        -0.601875445172988,
        -0.761382727657719,
        -0.6841411355089648,
        -0.7577175707451456,
        -0.6508968149775135,
        -0.739450901420349,
        -0.7336021427788042,
        -0.6770502674082657
      ],
      [
        -0.716454256511647,
        -0.7383729511387,
        -0.7487460562236494,
        -0.7455247267357071,
        -0.7586773412884292,
        -0.7493514110843045,
        1.0,
        -0.7306457557022485,
        -0.7255100516119002,
        -0.7160818087430527,
        -0.7339481901347522,
        -0.7375368973150102,
        -0.7413171050866865,
        -0.7623135430101726,
        -0.7357528227000517,
        -0.7491529854241078,
        -0.7603186049419324,
        -0.746912844820193,
        -0.7599338072781371,
        -0.7545901453274635,
        -0.7298269300448699,
        -0.7565058551620478,
        -0.7122859889205964
      ],
      [
        -0.7019723494439023,
        -0.6491076492157648,
        -0.7422215211216732,
        -0.6048538172040727,
        -0.7276799249625894,
        -0.5918507685345205,
        -0.7306457557022485,
        1.0,
        -0.7393298112516931,
        -0.7197592649978761,
        -0.7398647795662526,
        -0.7445798680506059,
        -0.6151849433704505,
        -0.7555520900128215,
        -0.7503760037719466,
        -0.4989179064540842,
        -0.754006816462506,
        -0.7114776475785658,
        -0.7536395060385328,
        -0.7326297907322221,
        -0.6462904304584217,
        -0.7296790379360694,
        -0.3382157125096034
      ],
      [
        -0.5566151466887982,
        -0.7483773284449728,
        -0.5452248855143903,
        -0.7355099130231888,
        -0.7463655669711388,
        -0.7549256350076698,
        -0.7255100516119002,
        -0.7393298112516931,
        1.0,
        -0.570162756365388,
        -0.7006925812235199,
        -0.7213372093979652,
        -0.521323091890751,
        -0.7566624207578208,
        -0.7154251602980424,
        -0.7500965769113169,
        -0.7532343314362107,
        -0.7308097187494852,
        -0.7560706840327687,
        -0.7482592480089991,
        -0.45211538402170637,
        -0.7424457718393165,
        -0.6212261477759735
      ],
      [
        -0.6268047437448738,
        -0.7342470050652361,
        -0.6375873824691362,
        -0.7273770427834894,
        -0.7520177371617467,
        -0.7537883313897227,
        -0.7160818087430527,
        -0.7197592649978761,
        -0.570162756365388,
        1.0,
        -0.6497852954200899,
        -0.6802652491883686,
        -0.55902149520897,
        -0.7615281572017274,
        -0.6261421242740529,
        -0.7412648599013301,
        -0.730018003355785,
        -0.7214341588848588,
        -0.7585133815690444,
        -0.7552328286032539,
        -0.5858500385321127,
        -0.7438349819182623,
        -0.7138783466685953
      ],
      [
        -0.6924500706268419,
        -0.7606723218134337,
        -0.71746564189586,
        -0.7346366206372907,
        -0.7521961754232065,
        -0.7564897199058048,
        -0.7339481901347522,
        -0.7398647795662526,
        -0.7006925812235199,
        -0.6497852954200899,
        1.0,
        -0.23896352846265875,
        -0.7143943308501643,
        -0.7573563653676672,
        -0.5346705925010724,
        -0.752099963132359,
        -0.7286392686416538,
        -0.7380888083425613,
        -0.7600207203528078,
        -0.7524760561439183,
        -0.7091297421391416,
        -0.7026143104442057,
        -0.7065363352837157
      ],
      [
        -0.7185792523680798,
        -0.7617241069643657,
        -0.729521968538076,
        -0.6709007745043898,
        -0.7550041868749542,
        -0.7582002667567735,
        -0.7375368973150102,
        -0.7445798680506059,
        -0.7213372093979652,
        -0.6802652491883686,
        -0.23896352846265875,
        1.0,
        -0.7255188724715804,
        -0.7578160428550289,
        -0.6084577079912425,
        -0.7561902477610594,
        -0.7329421550899898,
        -0.7367633591265882,
        -0.7507302983593437,
        -0.7518423592679173,
        -0.7126641730645806,
        -0.7434844450230262,
        -0.7284898259998251
      ],
      [
        -0.6316039024331692,
        -0.6617472435473837,
        -0.40635226138947345,
        -0.6618259979596401,
        -0.7356231512363061,
        -0.677694385942955,
        -0.7413171050866865,
        -0.6151849433704505,
        -0.521323091890751,
        -0.55902149520897,
        -0.7143943308501643,
        -0.7255188724715804,
        1.0,
        -0.7554421584650718,
        -0.70596822567249,
        -0.6111807118982733,
        -0.7345905671079842,
        -0.719159325511511,
        -0.7581434603446575,
        -0.7419484261433923,
        -0.5362245822781945,
        -0.7258884080646604,
        -0.6544587249193867
      ],
      [
        -0.7476665416975533,
        -0.762448678751769,
        -0.7586292856088765,
        -0.748013610809614,
        -0.7446336719174567,
        -0.7378709879265963,
        -0.7623135430101726,
        -0.7555520900128215,
        -0.7566624207578208,
        -0.7615281572017274,
        -0.7573563653676672,
        -0.7578160428550289,
        -0.7554421584650718,
        1.0,
        -0.7582904212444976,
        -0.757319814512628,
        -0.759212404661417,
        -0.7348292072093595,
        -0.7571139240657839,
        -0.716614767906693,
        -0.7584923587724521,
        -0.7579836893496936,
        -0.7541123075270948
      ],
      [
        -0.7296430728555974,
        -0.7558645963359271,
        -0.7302911145867614,
        -0.7229263161705429,
        -0.7567282881681933,
        -0.7599369552043715,
        -0.7357528227000517,
        -0.7503760037719466,
        -0.7154251602980424,
        -0.6261421242740529,
        -0.5346705925010724,
        -0.6084577079912425,
        -0.70596822567249,
        -0.7582904212444976,
        1.0,
        -0.7526794580138423,
        -0.5459464340097933,
        -0.747599080863226,
        -0.7628507609445335,
        -0.7567179562826967,
        -0.7181258469388507,
        -0.741677235119957,
        -0.7385886544612983
      ],
      [
        -0.7347646013895555,
        -0.6713787657917575,
        -0.7467253616407814,
        -0.5906140856394906,
        -0.7313020146153173,
        -0.601875445172988,
        -0.7491529854241078,
        -0.4989179064540842,
        -0.7500965769113169,
        -0.7412648599013301,
        -0.752099963132359,
        -0.7561902477610594,
        -0.6111807118982733,
        -0.757319814512628,
        -0.7526794580138423,
        1.0,
        -0.7482931142490085,
        -0.7381239192268487,
        -0.7590924316224636,
        -0.7370222650050149,
        -0.7191510675083794,
        -0.7407754206062751,
        -0.6180211020990956
      ],
      [
        -0.7474698651777384,
        -0.7442093918188054,
        -0.7560262057741222,
        -0.7407480233503196,
        -0.754195136682337,
        -0.761382727657719,
        -0.7603186049419324,
        -0.754006816462506,
        -0.7532343314362107,
        -0.730018003355785,
        -0.7286392686416538,
        -0.7329421550899898,
        -0.7345905671079842,
        -0.759212404661417,
        -0.5459464340097933,
        -0.7482931142490085,
        1.0,
        -0.7517705285038664,
        -0.7628446328636527,
        -0.7546851295178079,
        -0.7386357917730251,
        -0.7279690849381347,
        -0.7504176352479359
      ],
      [
        -0.6834365405426575,
        -0.7189180670027444,
        -0.7341054574555539,
        -0.7260828749360114,
        -0.7312728562884578,
        -0.6841411355089648,
        -0.746912844820193,
        -0.7114776475785658,
        -0.7308097187494852,
        -0.7214341588848588,
        -0.7380888083425613,
        -0.7367633591265882,
        -0.719159325511511,
        -0.7348292072093595,
        -0.747599080863226,
        -0.7381239192268487,
        -0.7517705285038664,
        1.0,
        -0.7527240067649443,
        -0.6324645401352081,
        -0.7341990898007482,
        -0.7343932195471248,
        -0.7199894358206499
      ],
      [
        -0.7480079516532719,
        -0.7638816211233305,
        -0.7591109372803166,
        -0.7552205396691668,
        -0.7643964792100872,
        -0.7577175707451456,
        -0.7599338072781371,
        -0.7536395060385328,
        -0.7560706840327687,
        -0.7585133815690444,
        -0.7600207203528078,
        -0.7507302983593437,
        -0.7581434603446575,
        -0.7571139240657839,
        -0.7628507609445335,
        -0.7590924316224636,
        -0.7628446328636527,
        -0.7527240067649443,
        1.0,
        -0.7577247460495755,
        -0.7586162262462244,
        -0.7648089995310915,
        -0.7554344822321459
      ],
      [
        -0.7431296555226552,
        -0.7415482636834343,
        -0.7562519370916552,
        -0.6048743445917519,
        -0.6592332089575397,
        -0.6508968149775135,
        -0.7545901453274635,
        -0.7326297907322221,
        -0.7482592480089991,
        -0.7552328286032539,
        -0.7524760561439183,
        -0.7518423592679173,
        -0.7419484261433923,
        -0.716614767906693,
        -0.7567179562826967,
        -0.7370222650050149,
        -0.7546851295178079,
        -0.6324645401352081,
        -0.7577247460495755,
        1.0,
        -0.7454035548797571,
        -0.742528168997671,
        -0.7394922955112604
      ],
      [
        -0.6598667310834966,
        -0.7392073242911903,
        -0.5833772400759574,
        -0.7178485456842834,
        -0.750664597008977,
        -0.739450901420349,
        -0.7298269300448699,
        -0.6462904304584217,
        -0.45211538402170637,
        -0.5858500385321127,
        -0.7091297421391416,
        -0.7126641730645806,
        -0.5362245822781945,
        -0.7584923587724521,
        -0.7181258469388507,
        -0.7191510675083794,
        -0.7386357917730251,
        -0.7341990898007482,
        -0.7586162262462244,
        -0.7454035548797571,
        1.0,
        -0.7406076466699167,
        -0.5889640693413712
      ],
      [
        -0.7252178861730838,
        -0.6551768191744691,
        -0.681590661846276,
        -0.7520847537132586,
        -0.6878578264705,
        -0.7336021427788042,
        -0.7565058551620478,
        -0.7296790379360694,
        -0.7424457718393165,
        -0.7438349819182623,
        -0.7026143104442057,
        -0.7434844450230262,
        -0.7258884080646604,
        -0.7579836893496936,
        -0.741677235119957,
        -0.7407754206062751,
        -0.7279690849381347,
        -0.7343932195471248,
        -0.7648089995310915,
        -0.742528168997671,
        -0.7406076466699167,
        1.0,
        -0.7189027639185184
      ],
      [
        -0.3636927198614225,
        -0.69863508112609,
        -0.7131563335241409,
        -0.6629949985270733,
        -0.7176058881426413,
        -0.6770502674082657,
        -0.7122859889205964,
        -0.3382157125096034,
        -0.6212261477759735,
        -0.7138783466685953,
        -0.7065363352837157,
        -0.7284898259998251,
        -0.6544587249193867,
        -0.7541123075270948,
        -0.7385886544612983,
        -0.6180211020990956,
        -0.7504176352479359,
        -0.7199894358206499,
        -0.7554344822321459,
        -0.7394922955112604,
        -0.5889640693413712,
        -0.7189027639185184,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        39,
        4,
        5,
        3,
        2,
        4,
        4,
        3,
        2,
        2,
        3,
        6,
        0,
        2,
        2,
        0,
        1,
        3,
        3,
        0,
        0,
        3,
        3
      ],
      "2020-02": [
        31,
        6,
        11,
        3,
        2,
        9,
        8,
        5,
        4,
        4,
        6,
        12,
        0,
        0,
        6,
        1,
        3,
        5,
        1,
        2,
        3,
        13,
        5
      ],
      "2020-03": [
        38,
        2,
        5,
        2,
        3,
        3,
        6,
        1,
        1,
        4,
        1,
        6,
        2,
        1,
        1,
        0,
        2,
        1,
        0,
        1,
        1,
        8,
        8
      ],
      "2020-04": [
        41,
        11,
        3,
        1,
        2,
        2,
        0,
        2,
        2,
        5,
        9,
        13,
        2,
        5,
        3,
        0,
        4,
        0,
        1,
        1,
        1,
        7,
        7
      ],
      "2020-05": [
        87,
        4,
        10,
        2,
        5,
        4,
        5,
        8,
        5,
        14,
        13,
        7,
        4,
        6,
        10,
        0,
        5,
        7,
        7,
        5,
        2,
        9,
        11
      ],
      "2020-06": [
        44,
        12,
        12,
        2,
        5,
        4,
        4,
        2,
        4,
        6,
        12,
        3,
        1,
        4,
        2,
        2,
        6,
        7,
        1,
        6,
        2,
        7,
        5
      ],
      "2020-07": [
        47,
        12,
        8,
        3,
        8,
        15,
        3,
        9,
        2,
        3,
        2,
        18,
        5,
        0,
        3,
        0,
        2,
        5,
        2,
        2,
        0,
        8,
        9
      ],
      "2020-08": [
        41,
        24,
        10,
        1,
        4,
        1,
        10,
        5,
        4,
        15,
        14,
        22,
        3,
        2,
        8,
        1,
        8,
        2,
        3,
        1,
        3,
        11,
        6
      ],
      "2020-09": [
        23,
        4,
        3,
        0,
        1,
        2,
        6,
        2,
        1,
        3,
        2,
        5,
        0,
        2,
        7,
        1,
        2,
        4,
        4,
        2,
        0,
        9,
        1
      ],
      "2020-10": [
        97,
        22,
        12,
        5,
        7,
        10,
        10,
        10,
        5,
        15,
        10,
        26,
        3,
        6,
        19,
        5,
        3,
        5,
        3,
        1,
        4,
        19,
        7
      ],
      "2020-11": [
        54,
        11,
        14,
        1,
        9,
        4,
        6,
        2,
        2,
        15,
        11,
        14,
        1,
        5,
        7,
        0,
        3,
        2,
        3,
        1,
        1,
        15,
        6
      ],
      "2020-12": [
        31,
        7,
        5,
        1,
        8,
        1,
        1,
        4,
        2,
        8,
        6,
        10,
        2,
        8,
        1,
        1,
        4,
        2,
        1,
        4,
        0,
        7,
        5
      ],
      "2021-01": [
        24,
        9,
        4,
        2,
        3,
        3,
        2,
        3,
        1,
        3,
        5,
        2,
        0,
        5,
        4,
        0,
        1,
        2,
        1,
        1,
        0,
        8,
        4
      ],
      "2021-02": [
        57,
        10,
        9,
        2,
        2,
        12,
        6,
        2,
        3,
        5,
        8,
        7,
        0,
        5,
        4,
        3,
        7,
        6,
        3,
        7,
        2,
        11,
        7
      ],
      "2021-03": [
        53,
        9,
        3,
        7,
        8,
        6,
        6,
        2,
        5,
        7,
        5,
        4,
        5,
        5,
        4,
        1,
        2,
        5,
        3,
        2,
        0,
        10,
        6
      ],
      "2021-04": [
        77,
        11,
        15,
        5,
        4,
        5,
        7,
        4,
        6,
        14,
        11,
        12,
        2,
        4,
        12,
        1,
        2,
        6,
        4,
        2,
        3,
        4,
        13
      ],
      "2021-05": [
        52,
        11,
        8,
        0,
        2,
        9,
        7,
        3,
        3,
        5,
        9,
        4,
        3,
        3,
        5,
        2,
        4,
        6,
        1,
        2,
        1,
        9,
        6
      ],
      "2021-06": [
        75,
        6,
        9,
        2,
        6,
        11,
        9,
        6,
        6,
        25,
        14,
        6,
        3,
        10,
        8,
        1,
        4,
        7,
        5,
        3,
        1,
        14,
        9
      ],
      "2021-07": [
        55,
        14,
        8,
        4,
        6,
        14,
        7,
        6,
        2,
        11,
        12,
        10,
        1,
        9,
        9,
        1,
        4,
        14,
        2,
        3,
        4,
        10,
        5
      ],
      "2021-08": [
        50,
        15,
        3,
        3,
        1,
        2,
        1,
        7,
        5,
        5,
        2,
        6,
        2,
        5,
        2,
        0,
        3,
        7,
        4,
        4,
        1,
        8,
        6
      ],
      "2021-09": [
        42,
        9,
        8,
        7,
        9,
        2,
        5,
        4,
        4,
        7,
        9,
        19,
        6,
        5,
        4,
        0,
        1,
        6,
        2,
        0,
        1,
        7,
        6
      ],
      "2021-10": [
        100,
        12,
        16,
        4,
        6,
        15,
        10,
        13,
        4,
        26,
        11,
        15,
        1,
        8,
        15,
        1,
        16,
        14,
        6,
        1,
        1,
        24,
        3
      ],
      "2021-11": [
        50,
        14,
        15,
        2,
        5,
        8,
        7,
        4,
        3,
        12,
        6,
        8,
        2,
        5,
        10,
        1,
        4,
        0,
        3,
        5,
        1,
        13,
        6
      ],
      "2021-12": [
        30,
        7,
        10,
        4,
        5,
        1,
        10,
        7,
        4,
        1,
        4,
        7,
        3,
        3,
        4,
        2,
        1,
        4,
        2,
        3,
        1,
        10,
        4
      ],
      "2022-01": [
        42,
        5,
        6,
        2,
        2,
        2,
        11,
        5,
        3,
        8,
        2,
        8,
        2,
        11,
        6,
        0,
        3,
        6,
        4,
        5,
        0,
        6,
        6
      ],
      "2022-02": [
        66,
        15,
        17,
        9,
        8,
        11,
        6,
        4,
        4,
        14,
        12,
        11,
        3,
        3,
        5,
        1,
        4,
        4,
        1,
        7,
        0,
        6,
        14
      ],
      "2022-03": [
        92,
        10,
        18,
        9,
        9,
        8,
        22,
        6,
        4,
        18,
        18,
        26,
        1,
        5,
        13,
        2,
        11,
        9,
        3,
        6,
        6,
        15,
        10
      ],
      "2022-04": [
        109,
        7,
        9,
        5,
        5,
        6,
        10,
        14,
        4,
        21,
        14,
        18,
        2,
        5,
        10,
        0,
        7,
        7,
        4,
        3,
        6,
        12,
        9
      ],
      "2022-05": [
        56,
        8,
        10,
        3,
        2,
        2,
        2,
        10,
        3,
        11,
        3,
        4,
        1,
        3,
        5,
        0,
        4,
        8,
        3,
        2,
        1,
        5,
        8
      ],
      "2022-06": [
        73,
        6,
        13,
        5,
        2,
        9,
        10,
        8,
        1,
        17,
        9,
        6,
        4,
        6,
        8,
        0,
        4,
        7,
        4,
        4,
        2,
        11,
        14
      ],
      "2022-07": [
        48,
        9,
        7,
        4,
        8,
        5,
        14,
        6,
        1,
        15,
        7,
        10,
        4,
        2,
        5,
        3,
        4,
        5,
        5,
        1,
        2,
        11,
        14
      ],
      "2022-08": [
        38,
        15,
        3,
        6,
        6,
        2,
        6,
        6,
        2,
        6,
        5,
        6,
        1,
        5,
        3,
        2,
        3,
        1,
        4,
        8,
        3,
        5,
        6
      ],
      "2022-09": [
        44,
        17,
        4,
        2,
        4,
        4,
        3,
        6,
        2,
        8,
        2,
        15,
        1,
        3,
        6,
        0,
        3,
        5,
        4,
        2,
        2,
        6,
        5
      ],
      "2022-10": [
        130,
        15,
        13,
        8,
        8,
        9,
        15,
        13,
        11,
        27,
        18,
        20,
        9,
        6,
        14,
        1,
        9,
        7,
        6,
        5,
        1,
        11,
        19
      ],
      "2022-11": [
        124,
        18,
        21,
        7,
        6,
        4,
        17,
        11,
        3,
        28,
        15,
        13,
        6,
        6,
        8,
        4,
        7,
        5,
        6,
        2,
        1,
        15,
        9
      ],
      "2022-12": [
        46,
        10,
        7,
        5,
        2,
        1,
        4,
        3,
        2,
        8,
        1,
        2,
        3,
        3,
        5,
        0,
        3,
        5,
        1,
        1,
        2,
        10,
        11
      ],
      "2023-01": [
        21,
        19,
        5,
        3,
        6,
        2,
        3,
        5,
        1,
        9,
        2,
        3,
        3,
        2,
        2,
        1,
        2,
        1,
        0,
        2,
        1,
        4,
        6
      ],
      "2023-02": [
        51,
        8,
        18,
        0,
        4,
        6,
        9,
        3,
        8,
        10,
        6,
        11,
        2,
        0,
        7,
        0,
        1,
        5,
        1,
        0,
        2,
        11,
        11
      ],
      "2023-03": [
        69,
        11,
        12,
        4,
        5,
        9,
        8,
        10,
        6,
        17,
        13,
        11,
        9,
        3,
        2,
        0,
        4,
        1,
        3,
        4,
        5,
        10,
        15
      ],
      "2023-04": [
        35,
        15,
        3,
        3,
        5,
        1,
        5,
        10,
        6,
        8,
        1,
        3,
        0,
        2,
        2,
        0,
        4,
        3,
        4,
        3,
        2,
        5,
        3
      ],
      "2023-05": [
        108,
        16,
        11,
        5,
        8,
        8,
        15,
        20,
        11,
        39,
        9,
        15,
        4,
        4,
        11,
        1,
        6,
        5,
        8,
        9,
        3,
        15,
        17
      ],
      "2023-06": [
        106,
        24,
        13,
        4,
        14,
        8,
        15,
        9,
        3,
        18,
        9,
        6,
        3,
        3,
        11,
        4,
        4,
        6,
        2,
        2,
        4,
        12,
        16
      ],
      "2023-07": [
        58,
        27,
        9,
        6,
        7,
        1,
        7,
        6,
        2,
        12,
        3,
        5,
        1,
        3,
        5,
        3,
        0,
        6,
        2,
        2,
        2,
        7,
        14
      ],
      "2023-08": [
        54,
        18,
        6,
        7,
        7,
        5,
        11,
        8,
        7,
        16,
        6,
        10,
        4,
        3,
        8,
        3,
        7,
        10,
        4,
        3,
        2,
        6,
        10
      ],
      "2023-09": [
        141,
        24,
        26,
        12,
        8,
        16,
        14,
        32,
        9,
        37,
        20,
        13,
        9,
        9,
        21,
        4,
        10,
        12,
        6,
        4,
        4,
        10,
        15
      ],
      "2023-10": [
        80,
        17,
        13,
        6,
        7,
        4,
        11,
        4,
        0,
        17,
        14,
        8,
        3,
        3,
        7,
        2,
        7,
        3,
        1,
        5,
        4,
        8,
        14
      ],
      "2023-11": [
        34,
        16,
        2,
        1,
        8,
        2,
        2,
        7,
        3,
        11,
        5,
        3,
        4,
        5,
        7,
        2,
        4,
        3,
        2,
        3,
        2,
        4,
        9
      ],
      "2023-12": [
        57,
        17,
        8,
        4,
        6,
        7,
        17,
        7,
        2,
        13,
        16,
        7,
        4,
        3,
        5,
        2,
        5,
        7,
        4,
        7,
        3,
        7,
        13
      ],
      "2024-01": [
        84,
        14,
        9,
        3,
        10,
        9,
        11,
        12,
        9,
        11,
        9,
        5,
        7,
        3,
        9,
        1,
        9,
        2,
        3,
        3,
        3,
        10,
        16
      ],
      "2024-02": [
        54,
        21,
        9,
        4,
        11,
        5,
        10,
        13,
        6,
        12,
        9,
        6,
        4,
        2,
        2,
        1,
        6,
        7,
        0,
        5,
        0,
        5,
        6
      ],
      "2024-03": [
        42,
        9,
        5,
        4,
        12,
        10,
        13,
        8,
        3,
        13,
        4,
        3,
        6,
        1,
        3,
        0,
        5,
        6,
        4,
        3,
        1,
        5,
        17
      ],
      "2024-04": [
        42,
        23,
        4,
        5,
        2,
        3,
        8,
        6,
        1,
        11,
        5,
        7,
        2,
        2,
        6,
        1,
        4,
        4,
        2,
        0,
        0,
        6,
        8
      ],
      "2024-05": [
        54,
        21,
        7,
        6,
        5,
        1,
        9,
        14,
        2,
        11,
        5,
        5,
        2,
        7,
        6,
        1,
        5,
        4,
        2,
        2,
        1,
        3,
        7
      ],
      "2024-06": [
        135,
        21,
        11,
        14,
        11,
        13,
        19,
        17,
        11,
        46,
        12,
        17,
        9,
        7,
        9,
        3,
        14,
        16,
        9,
        4,
        3,
        8,
        22
      ],
      "2024-07": [
        69,
        34,
        8,
        6,
        7,
        4,
        15,
        13,
        6,
        22,
        10,
        8,
        6,
        6,
        4,
        5,
        4,
        14,
        3,
        1,
        1,
        8,
        15
      ],
      "2024-08": [
        60,
        20,
        6,
        12,
        11,
        4,
        7,
        14,
        3,
        19,
        5,
        8,
        4,
        5,
        12,
        1,
        5,
        5,
        6,
        6,
        5,
        11,
        16
      ],
      "2024-09": [
        132,
        40,
        21,
        12,
        12,
        11,
        20,
        33,
        5,
        49,
        31,
        16,
        8,
        4,
        18,
        3,
        16,
        10,
        4,
        16,
        5,
        16,
        19
      ],
      "2024-10": [
        83,
        39,
        10,
        9,
        7,
        9,
        12,
        19,
        5,
        32,
        7,
        9,
        7,
        3,
        9,
        1,
        7,
        9,
        1,
        7,
        1,
        7,
        11
      ],
      "2024-11": [
        55,
        20,
        1,
        9,
        9,
        3,
        7,
        8,
        8,
        10,
        4,
        3,
        0,
        1,
        5,
        2,
        3,
        2,
        2,
        5,
        4,
        7,
        8
      ],
      "2024-12": [
        55,
        20,
        7,
        6,
        5,
        1,
        11,
        23,
        9,
        22,
        10,
        8,
        7,
        4,
        5,
        2,
        4,
        9,
        3,
        2,
        3,
        0,
        18
      ],
      "2025-01": [
        76,
        21,
        8,
        8,
        6,
        3,
        12,
        14,
        11,
        16,
        7,
        7,
        6,
        4,
        12,
        1,
        8,
        4,
        2,
        4,
        3,
        7,
        15
      ],
      "2025-02": [
        55,
        24,
        12,
        7,
        5,
        7,
        8,
        17,
        4,
        17,
        2,
        4,
        2,
        2,
        7,
        3,
        2,
        5,
        1,
        2,
        4,
        5,
        10
      ],
      "2025-03": [
        56,
        21,
        6,
        2,
        5,
        6,
        9,
        15,
        2,
        11,
        4,
        4,
        2,
        1,
        2,
        2,
        2,
        5,
        0,
        5,
        6,
        7,
        16
      ],
      "2025-04": [
        43,
        15,
        2,
        5,
        9,
        4,
        3,
        9,
        2,
        10,
        3,
        1,
        5,
        4,
        8,
        0,
        4,
        8,
        2,
        1,
        2,
        7,
        8
      ],
      "2025-05": [
        126,
        32,
        14,
        18,
        9,
        6,
        20,
        36,
        17,
        35,
        16,
        8,
        8,
        7,
        19,
        2,
        9,
        9,
        8,
        6,
        7,
        17,
        25
      ],
      "2025-06": [
        133,
        48,
        18,
        7,
        10,
        6,
        14,
        20,
        6,
        37,
        11,
        12,
        2,
        3,
        14,
        1,
        7,
        12,
        8,
        4,
        4,
        10,
        18
      ],
      "2025-07": [
        111,
        35,
        14,
        10,
        9,
        13,
        9,
        14,
        1,
        25,
        8,
        6,
        8,
        3,
        9,
        2,
        3,
        4,
        4,
        6,
        2,
        9,
        9
      ],
      "2025-08": [
        92,
        31,
        11,
        11,
        9,
        4,
        21,
        20,
        3,
        27,
        10,
        8,
        7,
        5,
        4,
        1,
        5,
        6,
        5,
        4,
        5,
        8,
        15
      ],
      "2025-09": [
        49,
        18,
        3,
        12,
        5,
        2,
        6,
        13,
        3,
        12,
        2,
        3,
        2,
        4,
        2,
        0,
        1,
        4,
        1,
        3,
        2,
        6,
        4
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Jointly Trained Transformers models for Spoken Language Translation",
          "year": "2020-04",
          "abstract": "Conventional spoken language translation (SLT) systems are pipeline based\nsystems, where we have an Automatic Speech Recognition (ASR) system to convert\nthe modality of source from speech to text and a Machine Translation (MT)\nsystems to translate source text to text in target language. Recent progress in\nthe sequence-sequence architectures have reduced the performance gap between\nthe pipeline based SLT systems (cascaded ASR-MT) and End-to-End approaches.\nThough End-to-End and cascaded ASR-MT systems are reaching to the comparable\nlevels of performances, we can see a large performance gap using the ASR\nhypothesis and oracle text w.r.t MT models. This performance gap indicates that\nthe MT systems are prone to large performance degradation due to noisy ASR\nhypothesis as opposed to oracle text transcript. In this work this degradation\nin the performance is reduced by creating an end to-end differentiable pipeline\nbetween the ASR and MT systems. In this work, we train SLT systems with ASR\nobjective as an auxiliary loss and both the networks are connected through the\nneural hidden representations. This train ing would have an End-to-End\ndifferentiable path w.r.t to the final objective function as well as utilize\nthe ASR objective for better performance of the SLT systems. This architecture\nhas improved from BLEU from 36.8 to 44.5. Due to the Multi-task training the\nmodel also generates the ASR hypothesis which are used by a pre-trained MT\nmodel. Combining the proposed systems with the MT model has increased the BLEU\nscore by 1. All the experiments are reported on English-Portuguese speech\ntranslation task using How2 corpus. The final BLEU score is on-par with the\nbest speech translation system on How2 dataset with no additional training data\nand language model and much less parameters.",
          "arxiv_id": "2004.12111v1"
        },
        {
          "title": "MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition",
          "year": "2022-11",
          "abstract": "In this paper, we propose a novel multi-modal multi-task encoder-decoder\npre-training framework (MMSpeech) for Mandarin automatic speech recognition\n(ASR), which employs both unlabeled speech and text data. The main difficulty\nin speech-text joint pre-training comes from the significant difference between\nspeech and text modalities, especially for Mandarin speech and text. Unlike\nEnglish and other languages with an alphabetic writing system, Mandarin uses an\nideographic writing system where character and sound are not tightly mapped to\none another. Therefore, we propose to introduce the phoneme modality into\npre-training, which can help capture modality-invariant information between\nMandarin speech and text. Specifically, we employ a multi-task learning\nframework including five self-supervised and supervised tasks with speech and\ntext data. For end-to-end pre-training, we introduce self-supervised\nspeech-to-pseudo-codes (S2C) and phoneme-to-text (P2T) tasks utilizing\nunlabeled speech and text data, where speech-pseudo-codes pairs and\nphoneme-text pairs are a supplement to the supervised speech-text pairs. To\ntrain the encoder to learn better speech representation, we introduce\nself-supervised masked speech prediction (MSP) and supervised phoneme\nprediction (PP) tasks to learn to map speech into phonemes. Besides, we\ndirectly add the downstream supervised speech-to-text (S2T) task into the\npre-training process, which can further improve the pre-training performance\nand achieve better recognition results even without fine-tuning. Experiments on\nAISHELL-1 show that our proposed method achieves state-of-the-art performance,\nwith a more than 40% relative improvement compared with other pre-training\nmethods.",
          "arxiv_id": "2212.00500v1"
        },
        {
          "title": "Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation",
          "year": "2020-06",
          "abstract": "Transfer learning from high-resource languages is known to be an efficient\nway to improve end-to-end automatic speech recognition (ASR) for low-resource\nlanguages. Pre-trained or jointly trained encoder-decoder models, however, do\nnot share the language modeling (decoder) for the same language, which is\nlikely to be inefficient for distant target languages. We introduce\nspeech-to-text translation (ST) as an auxiliary task to incorporate additional\nknowledge of the target language and enable transferring from that target\nlanguage. Specifically, we first translate high-resource ASR transcripts into a\ntarget low-resource language, with which a ST model is trained. Both ST and\ntarget ASR share the same attention-based encoder-decoder architecture and\nvocabulary. The former task then provides a fully pre-trained model for the\nlatter, bringing up to 24.6% word error rate (WER) reduction to the baseline\n(direct transfer from high-resource ASR). We show that training ST with human\ntranslations is not necessary. ST trained with machine translation (MT)\npseudo-labels brings consistent gains. It can even outperform those using human\nlabels when transferred to target ASR by leveraging only 500K MT examples. Even\nwith pseudo-labels from low-resource MT (200K examples), ST-enhanced transfer\nbrings up to 8.9% WER reduction to direct transfer.",
          "arxiv_id": "2006.05474v2"
        }
      ],
      "1": [
        {
          "title": "MMT-BERT: Chord-aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT",
          "year": "2024-09",
          "abstract": "We propose a novel symbolic music representation and Generative Adversarial\nNetwork (GAN) framework specially designed for symbolic multitrack music\ngeneration. The main theme of symbolic music generation primarily encompasses\nthe preprocessing of music data and the implementation of a deep learning\nframework. Current techniques dedicated to symbolic music generation generally\nencounter two significant challenges: training data's lack of information about\nchords and scales and the requirement of specially designed model architecture\nadapted to the unique format of symbolic music representation. In this paper,\nwe solve the above problems by introducing new symbolic music representation\nwith MusicLang chord analysis model. We propose our MMT-BERT architecture\nadapting to the representation. To build a robust multitrack music generator,\nwe fine-tune a pre-trained MusicBERT model to serve as the discriminator, and\nincorporate relativistic standard loss. This approach, supported by the\nin-depth understanding of symbolic music encoded within MusicBERT, fortifies\nthe consonance and humanity of music generated by our method. Experimental\nresults demonstrate the effectiveness of our approach which strictly follows\nthe state-of-the-art methods.",
          "arxiv_id": "2409.00919v1"
        },
        {
          "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
          "year": "2025-01",
          "abstract": "In recent years, remarkable advancements in artificial intelligence-generated\ncontent (AIGC) have been achieved in the fields of image synthesis and text\ngeneration, generating content comparable to that produced by humans. However,\nthe quality of AI-generated music has not yet reached this standard, primarily\ndue to the challenge of effectively controlling musical emotions and ensuring\nhigh-quality outputs. This paper presents a generalized symbolic music\ngeneration framework, XMusic, which supports flexible prompts (i.e., images,\nvideos, texts, tags, and humming) to generate emotionally controllable and\nhigh-quality symbolic music. XMusic consists of two core components, XProjector\nand XComposer. XProjector parses the prompts of various modalities into\nsymbolic music elements (i.e., emotions, genres, rhythms and notes) within the\nprojection space to generate matching music. XComposer contains a Generator and\na Selector. The Generator generates emotionally controllable and melodious\nmusic based on our innovative symbolic music representation, whereas the\nSelector identifies high-quality symbolic music by constructing a multi-task\nlearning scheme involving quality assessment, emotion recognition, and genre\nrecognition tasks. In addition, we build XMIDI, a large-scale symbolic music\ndataset that contains 108,023 MIDI files annotated with precise emotion and\ngenre labels. Objective and subjective evaluations show that XMusic\nsignificantly outperforms the current state-of-the-art methods with impressive\nmusic quality. Our XMusic has been awarded as one of the nine Highlights of\nCollectibles at WAIC 2023. The project homepage of XMusic is\nhttps://xmusic-project.github.io.",
          "arxiv_id": "2501.08809v1"
        },
        {
          "title": "BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features",
          "year": "2024-07",
          "abstract": "Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.",
          "arxiv_id": "2407.10462v1"
        }
      ],
      "2": [
        {
          "title": "SRIB-LEAP submission to Far-field Multi-Channel Speech Enhancement Challenge for Video Conferencing",
          "year": "2021-06",
          "abstract": "This paper presents the details of the SRIB-LEAP submission to the\nConferencingSpeech challenge 2021. The challenge involved the task of\nmulti-channel speech enhancement to improve the quality of far field speech\nfrom microphone arrays in a video conferencing room. We propose a two stage\nmethod involving a beamformer followed by single channel enhancement. For the\nbeamformer, we incorporated self-attention mechanism as inter-channel\nprocessing layer in the filter-and-sum network (FaSNet), an end-to-end\ntime-domain beamforming system. The single channel speech enhancement is done\nin log spectral domain using convolution neural network (CNN)-long short term\nmemory (LSTM) based architecture. We achieved improvements in objective quality\nmetrics - perceptual evaluation of speech quality (PESQ) of 0.5 on the noisy\ndata. On subjective quality evaluation, the proposed approach improved the mean\nopinion score (MOS) by an absolute measure of 0.9 over the noisy audio.",
          "arxiv_id": "2106.12763v1"
        },
        {
          "title": "Phoneme-based Distribution Regularization for Speech Enhancement",
          "year": "2021-04",
          "abstract": "Existing speech enhancement methods mainly separate speech from noises at the\nsignal level or in the time-frequency domain. They seldom pay attention to the\nsemantic information of a corrupted signal. In this paper, we aim to bridge\nthis gap by extracting phoneme identities to help speech enhancement.\nSpecifically, we propose a phoneme-based distribution regularization (PbDr) for\nspeech enhancement, which incorporates frame-wise phoneme information into\nspeech enhancement network in a conditional manner. As different phonemes\nalways lead to different feature distributions in frequency, we propose to\nlearn a parameter pair, i.e. scale and bias, through a phoneme classification\nvector to modulate the speech enhancement network. The modulation parameter\npair includes not only frame-wise but also frequency-wise conditions, which\neffectively map features to phoneme-related distributions. In this way, we\nexplicitly regularize speech enhancement features by recognition vectors.\nExperiments on public datasets demonstrate that the proposed PbDr module can\nnot only boost the perceptual quality for speech enhancement but also the\nrecognition accuracy of an ASR system on the enhanced speech. This PbDr module\ncould be readily incorporated into other speech enhancement networks as well.",
          "arxiv_id": "2104.03759v1"
        },
        {
          "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
          "year": "2022-10",
          "abstract": "It has been shown that the intelligibility of noisy speech can be improved by\nspeech enhancement algorithms. However, speech enhancement has not been\nestablished as an effective frontend for robust automatic speech recognition\n(ASR) in noisy conditions compared to an ASR model trained on noisy speech\ndirectly. The divide between speech enhancement and ASR impedes the progress of\nrobust ASR systems especially as speech enhancement has made big strides in\nrecent years. In this work, we focus on eliminating this divide with an ARN\n(attentive recurrent network) based time-domain enhancement model. The proposed\nsystem fully decouples speech enhancement and an acoustic model trained only on\nclean speech. Results on the CHiME-2 corpus show that ARN enhanced speech\ntranslates to improved ASR results. The proposed system achieves $6.28\\%$\naverage word error rate, outperforming the previous best by $19.3\\%$\nrelatively.",
          "arxiv_id": "2210.13318v3"
        }
      ],
      "3": [
        {
          "title": "Voice Spoofing Countermeasures: Taxonomy, State-of-the-art, experimental analysis of generalizability, open challenges, and the way forward",
          "year": "2022-10",
          "abstract": "Malicious actors may seek to use different voice-spoofing attacks to fool ASV\nsystems and even use them for spreading misinformation. Various countermeasures\nhave been proposed to detect these spoofing attacks. Due to the extensive work\ndone on spoofing detection in automated speaker verification (ASV) systems in\nthe last 6-7 years, there is a need to classify the research and perform\nqualitative and quantitative comparisons on state-of-the-art countermeasures.\nAdditionally, no existing survey paper has reviewed integrated solutions to\nvoice spoofing evaluation and speaker verification, adversarial/antiforensics\nattacks on spoofing countermeasures, and ASV itself, or unified solutions to\ndetect multiple attacks using a single model. Further, no work has been done to\nprovide an apples-to-apples comparison of published countermeasures in order to\nassess their generalizability by evaluating them across corpora. In this work,\nwe conduct a review of the literature on spoofing detection using hand-crafted\nfeatures, deep learning, end-to-end, and universal spoofing countermeasure\nsolutions to detect speech synthesis (SS), voice conversion (VC), and replay\nattacks. Additionally, we also review integrated solutions to voice spoofing\nevaluation and speaker verification, adversarial and anti-forensics attacks on\nvoice countermeasures, and ASV. The limitations and challenges of the existing\nspoofing countermeasures are also presented. We report the performance of these\ncountermeasures on several datasets and evaluate them across corpora. For the\nexperiments, we employ the ASVspoof2019 and VSDC datasets along with GMM, SVM,\nCNN, and CNN-GRU classifiers. (For reproduceability of the results, the code of\nthe test bed can be found in our GitHub Repository.",
          "arxiv_id": "2210.00417v2"
        },
        {
          "title": "Defense against adversarial attacks on spoofing countermeasures of ASV",
          "year": "2020-03",
          "abstract": "Various forefront countermeasure methods for automatic speaker verification\n(ASV) with considerable performance in anti-spoofing are proposed in the\nASVspoof 2019 challenge. However, previous work has shown that countermeasure\nmodels are vulnerable to adversarial examples indistinguishable from natural\ndata. A good countermeasure model should not only be robust against spoofing\naudio, including synthetic, converted, and replayed audios; but counteract\ndeliberately generated examples by malicious adversaries. In this work, we\nintroduce a passive defense method, spatial smoothing, and a proactive defense\nmethod, adversarial training, to mitigate the vulnerability of ASV spoofing\ncountermeasure models against adversarial examples. This paper is among the\nfirst to use defense methods to improve the robustness of ASV spoofing\ncountermeasure models under adversarial attacks. The experimental results show\nthat these two defense methods positively help spoofing countermeasure models\ncounter adversarial examples.",
          "arxiv_id": "2003.03065v1"
        },
        {
          "title": "Toward Improving Synthetic Audio Spoofing Detection Robustness via Meta-Learning and Disentangled Training With Adversarial Examples",
          "year": "2024-08",
          "abstract": "Advances in automatic speaker verification (ASV) promote research into the\nformulation of spoofing detection systems for real-world applications. The\nperformance of ASV systems can be degraded severely by multiple types of\nspoofing attacks, namely, synthetic speech (SS), voice conversion (VC), replay,\ntwins and impersonation, especially in the case of unseen synthetic spoofing\nattacks. A reliable and robust spoofing detection system can act as a security\ngate to filter out spoofing attacks instead of having them reach the ASV\nsystem. A weighted additive angular margin loss is proposed to address the data\nimbalance issue, and different margins has been assigned to improve\ngeneralization to unseen spoofing attacks in this study. Meanwhile, we\nincorporate a meta-learning loss function to optimize differences between the\nembeddings of support versus query set in order to learn a\nspoofing-category-independent embedding space for utterances. Furthermore, we\ncraft adversarial examples by adding imperceptible perturbations to spoofing\nspeech as a data augmentation strategy, then we use an auxiliary batch\nnormalization (BN) to guarantee that corresponding normalization statistics are\nperformed exclusively on the adversarial examples. Additionally, A simple\nattention module is integrated into the residual block to refine the feature\nextraction process. Evaluation results on the Logical Access (LA) track of the\nASVspoof 2019 corpus provides confirmation of our proposed approaches'\neffectiveness in terms of a pooled EER of 0.87%, and a min t-DCF of 0.0277.\nThese advancements offer effective options to reduce the impact of spoofing\nattacks on voice recognition/authentication systems.",
          "arxiv_id": "2408.13341v1"
        }
      ],
      "4": [
        {
          "title": "In situ sound absorption estimation with the discrete complex image source method",
          "year": "2024-04",
          "abstract": "Estimating the sound absorption in situ relies on accurately describing the\nmeasured sound field. Evidence suggests that modeling the reflection of\nimpinging spherical waves is important, especially for compact measurement\nsystems. This article proposes a method for estimating the sound absorption\ncoefficient of a material sample by mapping the sound pressure, measured by a\nmicrophone array, to a distribution of monopoles along a line in the complex\nplane. The proposed method is compared to modeling the sound field as a\nsuperposition of two sources (a monopole and an image source). The obtained\ninverse problems are solved with Tikhonov regularization, with automatic choice\nof the regularization parameter by the L-curve criterion. The sound absorption\nmeasurement is tested with simulations of the sound field above infinite and\nfinite porous absorbers. The approaches are compared to the plane-wave\nabsorption coefficient and the one obtained by spherical wave incidence.\nExperimental analysis of two porous samples and one resonant absorber is also\ncarried out in situ. Four arrays were tested with an increasing aperture and\nnumber of sensors. It was demonstrated that measurements are feasible even with\nan array with only a few microphones. The discretization of the integral\nequation led to a more accurate reconstruction of the sound pressure and\nparticle velocity at the sample's surface. The resulting absorption coefficient\nagrees with the one obtained for spherical wave incidence, indicating that\nincluding more monopoles along the complex line is an essential feature of the\nsound field.",
          "arxiv_id": "2404.11399v1"
        },
        {
          "title": "Deep Sound Field Reconstruction in Real Rooms: Introducing the ISOBEL Sound Field Dataset",
          "year": "2021-02",
          "abstract": "Knowledge of loudspeaker responses are useful in a number of applications,\nwhere a sound system is located inside a room that alters the listening\nexperience depending on position within the room. Acquisition of sound fields\nfor sound sources located in reverberant rooms can be achieved through labor\nintensive measurements of impulse response functions covering the room, or\nalternatively by means of reconstruction methods which can potentially require\nsignificantly fewer measurements. This paper extends evaluations of sound field\nreconstruction at low frequencies by introducing a dataset with measurements\nfrom four real rooms. The ISOBEL Sound Field dataset is publicly available, and\naims to bridge the gap between synthetic and real-world sound fields in\nrectangular rooms. Moreover, the paper advances on a recent deep learning-based\nmethod for sound field reconstruction using a very low number of microphones,\nand proposes an approach for modeling both magnitude and phase response in a\nU-Net-like neural network architecture. The complex-valued sound field\nreconstruction demonstrates that the estimated room transfer functions are of\nhigh enough accuracy to allow for personalized sound zones with contrast ratios\ncomparable to ideal room transfer functions using 15 microphones below 150 Hz.",
          "arxiv_id": "2102.06455v1"
        },
        {
          "title": "Room geometry blind inference based on the localization of real sound source and first order reflections",
          "year": "2022-07",
          "abstract": "The conventional room geometry blind inference techniques with acoustic\nsignals are conducted based on the prior knowledge of the environment, such as\nthe room impulse response (RIR) or the sound source position, which will limit\nits application under unknown scenarios. To solve this problem, we have\nproposed a room geometry reconstruction method in this paper by using the\ngeometric relation between the direct signal and first-order reflections. In\naddition to the information of the compact microphone array itself, this method\ndoes not need any precognition of the environmental parameters. Besides, the\nlearning-based DNN models are designed and used to improve the accuracy and\nintegrity of the localization results of the direct source and first-order\nreflections. The direction of arrival (DOA) and time difference of arrival\n(TDOA) information of the direct and reflected signals are firstly estimated\nusing the proposed DCNN and TD-CNN models, which have higher sensitivity and\naccuracy than the conventional methods. Then the position of the sound source\nis inferred by integrating the DOA, TDOA and array height using the proposed\nDNN model. After that, the positions of image sources and corresponding\nboundaries are derived based on the geometric relation. Experimental results of\nboth simulations and real measurements verify the effectiveness and accuracy of\nthe proposed techniques compared with the conventional methods under different\nreverberant environments.",
          "arxiv_id": "2207.10478v2"
        }
      ],
      "5": [
        {
          "title": "Cross-Referencing Self-Training Network for Sound Event Detection in Audio Mixtures",
          "year": "2021-05",
          "abstract": "Sound event detection is an important facet of audio tagging that aims to\nidentify sounds of interest and define both the sound category and time\nboundaries for each sound event in a continuous recording. With advances in\ndeep neural networks, there has been tremendous improvement in the performance\nof sound event detection systems, although at the expense of costly data\ncollection and labeling efforts. In fact, current state-of-the-art methods\nemploy supervised training methods that leverage large amounts of data samples\nand corresponding labels in order to facilitate identification of sound\ncategory and time stamps of events. As an alternative, the current study\nproposes a semi-supervised method for generating pseudo-labels from\nunsupervised data using a student-teacher scheme that balances self-training\nand cross-training. Additionally, this paper explores post-processing which\nextracts sound intervals from network prediction, for further improvement in\nsound event detection performance. The proposed approach is evaluated on sound\nevent detection task for the DCASE2020 challenge. The results of these methods\non both \"validation\" and \"public evaluation\" sets of DESED database show\nsignificant improvement compared to the state-of-the art systems in\nsemi-supervised learning.",
          "arxiv_id": "2105.13392v1"
        },
        {
          "title": "Sound Event Detection Utilizing Graph Laplacian Regularization with Event Co-occurrence",
          "year": "2020-04",
          "abstract": "A limited number of types of sound event occur in an acoustic scene and some\nsound events tend to co-occur in the scene; for example, the sound events\n\"dishes\" and \"glass jingling\" are likely to co-occur in the acoustic scene\n\"cooking\". In this paper, we propose a method of sound event detection using\ngraph Laplacian regularization with sound event co-occurrence taken into\naccount. In the proposed method, the occurrences of sound events are expressed\nas a graph whose nodes indicate the frequencies of event occurrence and whose\nedges indicate the sound event co-occurrences. This graph representation is\nthen utilized for the model training of sound event detection, which is\noptimized under an objective function with a regularization term considering\nthe graph structure of sound event occurrence and co-occurrence. Evaluation\nexperiments using the TUT Sound Events 2016 and 2017 detasets, and the TUT\nAcoustic Scenes 2016 dataset show that the proposed method improves the\nperformance of sound event detection by 7.9 percentage points compared with the\nconventional CNN-BiGRU-based detection method in terms of the segment-based F1\nscore. In particular, the experimental results indicate that the proposed\nmethod enables the detection of co-occurring sound events more accurately than\nthe conventional method.",
          "arxiv_id": "2004.12046v1"
        },
        {
          "title": "Event-Independent Network for Polyphonic Sound Event Localization and Detection",
          "year": "2020-09",
          "abstract": "Polyphonic sound event localization and detection is not only detecting what\nsound events are happening but localizing corresponding sound sources. This\nseries of tasks was first introduced in DCASE 2019 Task 3. In 2020, the sound\nevent localization and detection task introduces additional challenges in\nmoving sound sources and overlapping-event cases, which include two events of\nthe same type with two different direction-of-arrival (DoA) angles. In this\npaper, a novel event-independent network for polyphonic sound event\nlocalization and detection is proposed. Unlike the two-stage method we proposed\nin DCASE 2019 Task 3, this new network is fully end-to-end. Inputs to the\nnetwork are first-order Ambisonics (FOA) time-domain signals, which are then\nfed into a 1-D convolutional layer to extract acoustic features. The network is\nthen split into two parallel branches. The first branch is for sound event\ndetection (SED), and the second branch is for DoA estimation. There are three\ntypes of predictions from the network, SED predictions, DoA predictions, and\nevent activity detection (EAD) predictions that are used to combine the SED and\nDoA features for on-set and off-set estimation. All of these predictions have\nthe format of two tracks indicating that there are at most two overlapping\nevents. Within each track, there could be at most one event happening. This\narchitecture introduces a problem of track permutation. To address this\nproblem, a frame-level permutation invariant training method is used.\nExperimental results show that the proposed method can detect polyphonic sound\nevents and their corresponding DoAs. Its performance on the Task 3 dataset is\ngreatly increased as compared with that of the baseline method.",
          "arxiv_id": "2010.00140v1"
        }
      ],
      "6": [
        {
          "title": "EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark",
          "year": "2024-06",
          "abstract": "Speech emotion recognition (SER) is an important part of human-computer\ninteraction, receiving extensive attention from both industry and academia.\nHowever, the current research field of SER has long suffered from the following\nproblems: 1) There are few reasonable and universal splits of the datasets,\nmaking comparing different models and methods difficult. 2) No commonly used\nbenchmark covers numerous corpus and languages for researchers to refer to,\nmaking reproduction a burden. In this paper, we propose EmoBox, an\nout-of-the-box multilingual multi-corpus speech emotion recognition toolkit,\nalong with a benchmark for both intra-corpus and cross-corpus settings. For\nintra-corpus settings, we carefully designed the data partitioning for\ndifferent datasets. For cross-corpus settings, we employ a foundation SER\nmodel, emotion2vec, to mitigate annotation errors and obtain a test set that is\nfully balanced in speakers and emotions distributions. Based on EmoBox, we\npresent the intra-corpus SER results of 10 pre-trained speech models on 32\nemotion datasets with 14 languages, and the cross-corpus SER results on 4\ndatasets with the fully balanced test sets. To the best of our knowledge, this\nis the largest SER benchmark, across language scopes and quantity scales. We\nhope that our toolkit and benchmark can facilitate the research of SER in the\ncommunity.",
          "arxiv_id": "2406.07162v1"
        },
        {
          "title": "MSAC: Multiple Speech Attribute Control Method for Reliable Speech Emotion Recognition",
          "year": "2023-08",
          "abstract": "Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.",
          "arxiv_id": "2308.04025v3"
        },
        {
          "title": "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition",
          "year": "2024-12",
          "abstract": "Speech emotion recognition (SER) remains a challenging yet crucial task due\nto the inherent complexity and diversity of human emotions. To address this\nproblem, researchers attempt to fuse information from other modalities via\nmultimodal learning. However, existing multimodal fusion techniques often\noverlook the intricacies of cross-modal interactions, resulting in suboptimal\nfeature representations. In this paper, we propose WavFusion, a multimodal\nspeech emotion recognition framework that addresses critical research problems\nin effective multimodal fusion, heterogeneity among modalities, and\ndiscriminative representation learning. By leveraging a gated cross-modal\nattention mechanism and multimodal homogeneous feature discrepancy learning,\nWavFusion demonstrates improved performance over existing state-of-the-art\nmethods on benchmark datasets. Our work highlights the importance of capturing\nnuanced cross-modal interactions and learning discriminative representations\nfor accurate multimodal SER. Experimental results on two benchmark datasets\n(IEMOCAP and MELD) demonstrate that WavFusion succeeds over the\nstate-of-the-art strategies on emotion recognition.",
          "arxiv_id": "2412.05558v1"
        }
      ],
      "7": [
        {
          "title": "Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues",
          "year": "2024-02",
          "abstract": "How to effectively interact audio with vision has garnered considerable\ninterest within the multi-modality research field. Recently, a novel\naudio-visual segmentation (AVS) task has been proposed, aiming to segment the\nsounding objects in video frames under the guidance of audio cues. However,\nmost existing AVS methods are hindered by a modality imbalance where the visual\nfeatures tend to dominate those of the audio modality, due to a unidirectional\nand insufficient integration of audio cues. This imbalance skews the feature\nrepresentation towards the visual aspect, impeding the learning of joint\naudio-visual representations and potentially causing segmentation inaccuracies.\nTo address this issue, we propose AVSAC. Our approach features a Bidirectional\nAudio-Visual Decoder (BAVD) with integrated bidirectional bridges, enhancing\naudio cues and fostering continuous interplay between audio and visual\nmodalities. This bidirectional interaction narrows the modality imbalance,\nfacilitating more effective learning of integrated audio-visual\nrepresentations. Additionally, we present a strategy for audio-visual\nframe-wise synchrony as fine-grained guidance of BAVD. This strategy enhances\nthe share of auditory components in visual features, contributing to a more\nbalanced audio-visual representation learning. Extensive experiments show that\nour method attains new benchmarks in AVS performance.",
          "arxiv_id": "2402.02327v2"
        },
        {
          "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
          "year": "2025-04",
          "abstract": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations.",
          "arxiv_id": "2504.02061v1"
        },
        {
          "title": "CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing",
          "year": "2024-01",
          "abstract": "There has been a long-standing quest for a unified audio-visual-text model to\nenable various multimodal understanding tasks, which mimics the listening,\nseeing and reading process of human beings. Humans tends to represent knowledge\nusing two separate systems: one for representing verbal (textual) information\nand one for representing non-verbal (visual and auditory) information. These\ntwo systems can operate independently but can also interact with each other.\nMotivated by this understanding of human cognition, in this paper, we introduce\nCoAVT -- a novel cognition-inspired Correlated Audio-Visual-Text pre-training\nmodel to connect the three modalities. It contains a joint audio-visual encoder\nthat learns to encode audio-visual synchronization information together with\nthe audio and visual content for non-verbal information, and a text encoder to\nhandle textual input for verbal information. To bridge the gap between\nmodalities, CoAVT employs a query encoder, which contains a set of learnable\nquery embeddings, and extracts the most informative audiovisual features of the\ncorresponding text. Additionally, to leverage the correspondences between audio\nand vision with language respectively, we also establish the audio-text and\nvisual-text bi-modal alignments upon the foundational audiovisual-text\ntri-modal alignment to enhance the multimodal representation learning. Finally,\nwe jointly optimize CoAVT model with three multimodal objectives: contrastive\nloss, matching loss and language modeling loss. Extensive experiments show that\nCoAVT can learn strong multimodal correlations and be generalized to various\ndownstream tasks. CoAVT establishes new state-of-the-art performance on\ntext-video retrieval task on AudioCaps for both zero-shot and fine-tuning\nsettings, audio-visual event classification and audio-visual retrieval tasks on\nAudioSet and VGGSound.",
          "arxiv_id": "2401.12264v2"
        }
      ],
      "8": [
        {
          "title": "Frequency-Based Alignment of EEG and Audio Signals Using Contrastive Learning and SincNet for Auditory Attention Detection",
          "year": "2025-03",
          "abstract": "Humans exhibit a remarkable ability to focus auditory attention in complex\nacoustic environments, such as cocktail parties. Auditory attention detection\n(AAD) aims to identify the attended speaker by analyzing brain signals, such as\nelectroencephalography (EEG) data. Existing AAD algorithms often leverage deep\nlearning's powerful nonlinear modeling capabilities, few consider the neural\nmechanisms underlying auditory processing in the brain. In this paper, we\npropose SincAlignNet, a novel network based on an improved SincNet and\ncontrastive learning, designed to align audio and EEG features for auditory\nattention detection. The SincNet component simulates the brain's processing of\naudio during auditory attention, while contrastive learning guides the model to\nlearn the relationship between EEG signals and attended speech. During\ninference, we calculate the cosine similarity between EEG and audio features\nand also explore direct inference of the attended speaker using EEG data.\nCross-trial evaluations results demonstrate that SincAlignNet outperforms\nstate-of-the-art AAD methods on two publicly available datasets, KUL and DTU,\nachieving average accuracies of 78.3% and 92.2%, respectively, with a 1-second\ndecision window. The model exhibits strong interpretability, revealing that the\nleft and right temporal lobes are more active during both male and female\nspeaker scenarios. Furthermore, we found that using data from only six\nelectrodes near the temporal lobes maintains similar or even better performance\ncompared to using 64 electrodes. These findings indicate that efficient\nlow-density EEG online decoding is achievable, marking an important step toward\nthe practical implementation of neuro-guided hearing aids in real-world\napplications. Code is available at: https://github.com/LiaoEuan/SincAlignNet.",
          "arxiv_id": "2503.04156v1"
        },
        {
          "title": "Towards Ultrasound Tongue Image prediction from EEG during speech production",
          "year": "2023-05",
          "abstract": "Previous initial research has already been carried out to propose\nspeech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG /\nECoG), but there is a lack of combined methods that investigate non-invasive\nbrain, articulation, and speech signals together and analyze the cognitive\nprocesses in the brain, the kinematics of the articulatory movement and the\nresulting speech signal. In this paper, we describe our multimodal\n(electroencephalography, ultrasound tongue imaging, and speech) analysis and\nsynthesis experiments, as a feasibility study. We extend the analysis of brain\nsignals recorded during speech production with ultrasound-based articulation\ndata. From the brain signal measured with EEG, we predict ultrasound images of\nthe tongue with a fully connected deep neural network. The results show that\nthere is a weak but noticeable relationship between EEG and ultrasound tongue\nimages, i.e. the network can differentiate articulated speech and neutral\ntongue position.",
          "arxiv_id": "2306.05374v2"
        },
        {
          "title": "Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation",
          "year": "2025-01",
          "abstract": "Decoding speech from non-invasive brain signals, such as\nelectroencephalography (EEG), has the potential to advance brain-computer\ninterfaces (BCIs), with applications in silent communication and assistive\ntechnologies for individuals with speech impairments. However, EEG-based speech\ndecoding faces major challenges, such as noisy data, limited datasets, and poor\nperformance on complex tasks like speech perception. This study attempts to\naddress these challenges by employing variational autoencoders (VAEs) for EEG\ndata augmentation to improve data quality and applying a state-of-the-art\n(SOTA) sequence-to-sequence deep learning architecture, originally successful\nin electromyography (EMG) tasks, to EEG-based speech decoding. Additionally, we\nadapt this architecture for word classification tasks. Using the Brennan\ndataset, which contains EEG recordings of subjects listening to narrated\nspeech, we preprocess the data and evaluate both classification and\nsequence-to-sequence models for EEG-to-words/sentences tasks. Our experiments\nshow that VAEs have the potential to reconstruct artificial EEG data for\naugmentation. Meanwhile, our sequence-to-sequence model achieves more promising\nperformance in generating sentences compared to our classification model,\nthough both remain challenging tasks. These findings lay the groundwork for\nfuture research on EEG speech perception decoding, with possible extensions to\nspeech production tasks such as silent or imagined speech.",
          "arxiv_id": "2501.04359v1"
        }
      ],
      "9": [
        {
          "title": "Style Mixture of Experts for Expressive Text-To-Speech Synthesis",
          "year": "2024-06",
          "abstract": "Recent advances in style transfer text-to-speech (TTS) have improved the\nexpressiveness of synthesized speech. However, encoding stylistic information\n(e.g., timbre, emotion, and prosody) from diverse and unseen reference speech\nremains a challenge. This paper introduces StyleMoE, an approach that addresses\nthe issue of learning averaged style representations in the style encoder by\ncreating style experts that learn from subsets of data. The proposed method\nreplaces the style encoder in a TTS framework with a Mixture of Experts (MoE)\nlayer. The style experts specialize by learning from subsets of reference\nspeech routed to them by the gating network, enabling them to handle different\naspects of the style space. As a result, StyleMoE improves the style coverage\nof the style encoder for style transfer TTS. Our experiments, both objective\nand subjective, demonstrate improved style transfer for diverse and unseen\nreference speech. The proposed method enhances the performance of existing\nstate-of-the-art style transfer TTS models and represents the first study of\nstyle MoE in TTS.",
          "arxiv_id": "2406.03637v2"
        },
        {
          "title": "MM-TTS: Multi-modal Prompt based Style Transfer for Expressive Text-to-Speech Synthesis",
          "year": "2023-12",
          "abstract": "The style transfer task in Text-to-Speech refers to the process of\ntransferring style information into text content to generate corresponding\nspeech with a specific style. However, most existing style transfer approaches\nare either based on fixed emotional labels or reference speech clips, which\ncannot achieve flexible style transfer. Recently, some methods have adopted\ntext descriptions to guide style transfer. In this paper, we propose a more\nflexible multi-modal and style controllable TTS framework named MM-TTS. It can\nutilize any modality as the prompt in unified multi-modal prompt space,\nincluding reference speech, emotional facial images, and text descriptions, to\ncontrol the style of the generated speech in a system. The challenges of\nmodeling such a multi-modal style controllable TTS mainly lie in two\naspects:1)aligning the multi-modal information into a unified style space to\nenable the input of arbitrary modality as the style prompt in a single system,\nand 2)efficiently transferring the unified style representation into the given\ntext content, thereby empowering the ability to generate prompt style-related\nvoice. To address these problems, we propose an aligned multi-modal prompt\nencoder that embeds different modalities into a unified style space, supporting\nstyle transfer for different modalities. Additionally, we present a new\nadaptive style transfer method named Style Adaptive Convolutions to achieve a\nbetter style representation. Furthermore, we design a Rectified Flow based\nRefiner to solve the problem of over-smoothing Mel-spectrogram and generate\naudio of higher fidelity. Since there is no public dataset for multi-modal TTS,\nwe construct a dataset named MEAD-TTS, which is related to the field of\nexpressive talking head. Our experiments on the MEAD-TTS dataset and\nout-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results\nbased on multi-modal prompts.",
          "arxiv_id": "2312.10687v4"
        },
        {
          "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
          "year": "2023-09",
          "abstract": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's\nvoice without adaptation parameters. By quantizing speech waveform into\ndiscrete acoustic tokens and modeling these tokens with the language model,\nrecent language model-based TTS models show zero-shot speaker adaptation\ncapabilities with only a 3-second acoustic prompt of an unseen speaker.\nHowever, they are limited by the length of the acoustic prompt, which makes it\ndifficult to clone personal speaking style. In this paper, we propose a novel\nzero-shot TTS model with the multi-scale acoustic prompts based on a neural\ncodec language model VALL-E. A speaker-aware text encoder is proposed to learn\nthe personal speaking style at the phoneme-level from the style prompt\nconsisting of multiple sentences. Following that, a VALL-E based acoustic\ndecoder is utilized to model the timbre from the timbre prompt at the\nframe-level and generate speech. The experimental results show that our\nproposed method outperforms baselines in terms of naturalness and speaker\nsimilarity, and can achieve better performance by scaling out to a longer style\nprompt.",
          "arxiv_id": "2309.11977v3"
        }
      ],
      "10": [
        {
          "title": "End-to-End Neural Diarization: Reformulating Speaker Diarization as Simple Multi-label Classification",
          "year": "2020-02",
          "abstract": "The most common approach to speaker diarization is clustering of speaker\nembeddings. However, the clustering-based approach has a number of problems;\ni.e., (i) it is not optimized to minimize diarization errors directly, (ii) it\ncannot handle speaker overlaps correctly, and (iii) it has trouble adapting\ntheir speaker embedding models to real audio recordings with speaker overlaps.\nTo solve these problems, we propose the End-to-End Neural Diarization (EEND),\nin which a neural network directly outputs speaker diarization results given a\nmulti-speaker recording. To realize such an end-to-end model, we formulate the\nspeaker diarization problem as a multi-label classification problem and\nintroduce a permutation-free objective function to directly minimize\ndiarization errors. Besides its end-to-end simplicity, the EEND method can\nexplicitly handle speaker overlaps during training and inference. Just by\nfeeding multi-speaker recordings with corresponding speaker segment labels, our\nmodel can be easily adapted to real conversations. We evaluated our method on\nsimulated speech mixtures and real conversation datasets. The results showed\nthat the EEND method outperformed the state-of-the-art x-vector\nclustering-based method, while it correctly handled speaker overlaps. We\nexplored the neural network architecture for the EEND method, and found that\nthe self-attention-based neural network was the key to achieving excellent\nperformance. In contrast to conditioning the network only on its previous and\nnext hidden states, as is done using bidirectional long short-term memory\n(BLSTM), self-attention is directly conditioned on all the frames. By\nvisualizing the attention weights, we show that self-attention captures global\nspeaker characteristics in addition to local speech activity dynamics, making\nit especially suitable for dealing with the speaker diarization problem.",
          "arxiv_id": "2003.02966v1"
        },
        {
          "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
          "year": "2023-10",
          "abstract": "This paper proposes an online target speaker voice activity detection system\nfor speaker diarization tasks, which does not require a priori knowledge from\nthe clustering-based diarization system to obtain the target speaker\nembeddings. By adapting the conventional target speaker voice activity\ndetection for real-time operation, this framework can identify speaker\nactivities using self-generated embeddings, resulting in consistent performance\nwithout permutation inconsistencies in the inference phase. During the\ninference process, we employ a front-end model to extract the frame-level\nspeaker embeddings for each coming block of a signal. Next, we predict the\ndetection state of each speaker based on these frame-level speaker embeddings\nand the previously estimated target speaker embedding. Then, the target speaker\nembeddings are updated by aggregating these frame-level speaker embeddings\naccording to the predictions in the current block. Our model predicts the\nresults for each block and updates the target speakers' embeddings until\nreaching the end of the signal. Experimental results show that the proposed\nmethod outperforms the offline clustering-based diarization system on the\nDIHARD III and AliMeeting datasets. The proposed method is further extended to\nmulti-channel data, which achieves similar performance with the\nstate-of-the-art offline diarization systems.",
          "arxiv_id": "2310.08696v1"
        },
        {
          "title": "Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR",
          "year": "2021-10",
          "abstract": "This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.",
          "arxiv_id": "2110.03151v2"
        }
      ],
      "11": [
        {
          "title": "The DKU-DukeECE System for the Self-Supervision Speaker Verification Task of the 2021 VoxCeleb Speaker Recognition Challenge",
          "year": "2021-09",
          "abstract": "This report describes the submission of the DKU-DukeECE team to the\nself-supervision speaker verification task of the 2021 VoxCeleb Speaker\nRecognition Challenge (VoxSRC). Our method employs an iterative labeling\nframework to learn self-supervised speaker representation based on a deep\nneural network (DNN). The framework starts with training a self-supervision\nspeaker embedding network by maximizing agreement between different segments\nwithin an utterance via a contrastive loss. Taking advantage of DNN's ability\nto learn from data with label noise, we propose to cluster the speaker\nembedding obtained from the previous speaker network and use the subsequent\nclass assignments as pseudo labels to train a new DNN. Moreover, we iteratively\ntrain the speaker network with pseudo labels generated from the previous step\nto bootstrap the discriminative power of a DNN. Also, visual modal data is\nincorporated in this self-labeling framework. The visual pseudo label and the\naudio pseudo label are fused with a cluster ensemble algorithm to generate a\nrobust supervisory signal for representation learning. Our submission achieves\nan equal error rate (EER) of 5.58% and 5.59% on the challenge development and\ntest set, respectively.",
          "arxiv_id": "2109.02853v1"
        },
        {
          "title": "Neural PLDA Modeling for End-to-End Speaker Verification",
          "year": "2020-08",
          "abstract": "While deep learning models have made significant advances in supervised\nclassification problems, the application of these models for out-of-set\nverification tasks like speaker recognition has been limited to deriving\nfeature embeddings. The state-of-the-art x-vector PLDA based speaker\nverification systems use a generative model based on probabilistic linear\ndiscriminant analysis (PLDA) for computing the verification score. Recently, we\nhad proposed a neural network approach for backend modeling in speaker\nverification called the neural PLDA (NPLDA) where the likelihood ratio score of\nthe generative PLDA model is posed as a discriminative similarity function and\nthe learnable parameters of the score function are optimized using a\nverification cost. In this paper, we extend this work to achieve joint\noptimization of the embedding neural network (x-vector network) with the NPLDA\nnetwork in an end-to-end (E2E) fashion. This proposed end-to-end model is\noptimized directly from the acoustic features with a verification cost function\nand during testing, the model directly outputs the likelihood ratio score. With\nvarious experiments using the NIST speaker recognition evaluation (SRE) 2018\nand 2019 datasets, we show that the proposed E2E model improves significantly\nover the x-vector PLDA baseline speaker verification system.",
          "arxiv_id": "2008.04527v1"
        },
        {
          "title": "Margin-Mixup: A Method for Robust Speaker Verification in Multi-Speaker Audio",
          "year": "2023-04",
          "abstract": "This paper is concerned with the task of speaker verification on audio with\nmultiple overlapping speakers. Most speaker verification systems are designed\nwith the assumption of a single speaker being present in a given audio segment.\nHowever, in a real-world setting this assumption does not always hold. In this\npaper, we demonstrate that current speaker verification systems are not robust\nagainst audio with noticeable speaker overlap. To alleviate this issue, we\npropose margin-mixup, a simple training strategy that can easily be adopted by\nexisting speaker verification pipelines to make the resulting speaker\nembeddings robust against multi-speaker audio. In contrast to other methods,\nmargin-mixup requires no alterations to regular speaker verification\narchitectures, while attaining better results. On our multi-speaker test set\nbased on VoxCeleb1, the proposed margin-mixup strategy improves the EER on\naverage with 44.4% relative to our state-of-the-art speaker verification\nbaseline systems.",
          "arxiv_id": "2304.03515v1"
        }
      ],
      "12": [
        {
          "title": "Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion",
          "year": "2023-06",
          "abstract": "Denoising Diffusion Probabilistic Models have shown extraordinary ability on\nvarious generative tasks. However, their slow inference speed renders them\nimpractical in speech synthesis. This paper proposes a linear diffusion model\n(LinDiff) based on an ordinary differential equation to simultaneously reach\nfast inference and high sample quality. Firstly, we employ linear interpolation\nbetween the target and noise to design a diffusion sequence for training, while\npreviously the diffusion path that links the noise and target is a curved\nsegment. When decreasing the number of sampling steps (i.e., the number of line\nsegments used to fit the path), the ease of fitting straight lines compared to\ncurves allows us to generate higher quality samples from a random noise with\nfewer iterations. Secondly, to reduce computational complexity and achieve\neffective global modeling of noisy speech, LinDiff employs a patch-based\nprocessing approach that partitions the input signal into small patches. The\npatch-wise token leverages Transformer architecture for effective modeling of\nglobal information. Adversarial training is used to further improve the sample\nquality with decreased sampling steps. We test proposed method with speech\nsynthesis conditioned on acoustic feature (Mel-spectrograms). Experimental\nresults verify that our model can synthesize high-quality speech even with only\none diffusion step. Both subjective and objective evaluations demonstrate that\nour model can synthesize speech of a quality comparable to that of\nautoregressive models with faster synthesis speed (3 diffusion steps).",
          "arxiv_id": "2306.05708v2"
        },
        {
          "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
          "year": "2022-07",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hinder their applications to text-to-speech deployment.\nThrough the preliminary study on diffusion model parameterization, we find that\nprevious gradient-based TTS models require hundreds or thousands of iterations\nto guarantee high sample quality, which poses a challenge for accelerating\nsampling. In this work, we propose ProDiff, on progressive fast diffusion model\nfor high-quality text-to-speech. Unlike previous work estimating the gradient\nfor data density, ProDiff parameterizes the denoising model by directly\npredicting clean data to avoid distinct quality degradation in accelerating\nsampling. To tackle the model convergence challenge with decreased diffusion\niterations, ProDiff reduces the data variance in the target site via knowledge\ndistillation. Specifically, the denoising model uses the generated\nmel-spectrogram from an N-step DDIM teacher as the training target and distills\nthe behavior into a new model with N/2 steps. As such, it allows the TTS model\nto make sharp predictions and further reduces the sampling time by orders of\nmagnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to\nsynthesize high-fidelity mel-spectrograms, while it maintains sample quality\nand diversity competitive with state-of-the-art models using hundreds of steps.\nProDiff enables a sampling speed of 24x faster than real-time on a single\nNVIDIA 2080Ti GPU, making diffusion models practically applicable to\ntext-to-speech synthesis deployment for the first time. Our extensive ablation\nstudies demonstrate that each design in ProDiff is effective, and we further\nshow that ProDiff can be easily extended to the multi-speaker setting. Audio\nsamples are available at \\url{https://ProDiff.github.io/.}",
          "arxiv_id": "2207.06389v1"
        },
        {
          "title": "FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis",
          "year": "2022-04",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hindered their applications to speech synthesis. This\npaper proposes FastDiff, a fast conditional diffusion model for high-quality\nspeech synthesis. FastDiff employs a stack of time-aware location-variable\nconvolutions of diverse receptive field patterns to efficiently model long-term\ntime dependencies with adaptive conditions. A noise schedule predictor is also\nadopted to reduce the sampling steps without sacrificing the generation\nquality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer,\nFastDiff-TTS, which generates high-fidelity speech waveforms without any\nintermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff\ndemonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech\nsamples. Also, FastDiff enables a sampling speed of 58x faster than real-time\non a V100 GPU, making diffusion models practically applicable to speech\nsynthesis deployment for the first time. We further show that FastDiff\ngeneralized well to the mel-spectrogram inversion of unseen speakers, and\nFastDiff-TTS outperformed other competing methods in end-to-end text-to-speech\nsynthesis. Audio samples are available at \\url{https://FastDiff.github.io/}.",
          "arxiv_id": "2204.09934v1"
        }
      ],
      "13": [
        {
          "title": "QUCoughScope: An Artificially Intelligent Mobile Application to Detect Asymptomatic COVID-19 Patients using Cough and Breathing Sounds",
          "year": "2021-03",
          "abstract": "In the break of COVID-19 pandemic, mass testing has become essential to\nreduce the spread of the virus. Several recent studies suggest that a\nsignificant number of COVID-19 patients display no physical symptoms\nwhatsoever. Therefore, it is unlikely that these patients will undergo COVID-19\ntest, which increases their chances of unintentionally spreading the virus.\nCurrently, the primary diagnostic tool to detect COVID-19 is RT-PCR test on\ncollected respiratory specimens from the suspected case. This requires patients\nto travel to a laboratory facility to be tested, thereby potentially infecting\nothers along the way.It is evident from recent researches that asymptomatic\nCOVID-19 patients cough and breath in a different way than the healthy people.\nSeveral research groups have created mobile and web-platform for crowdsourcing\nthe symptoms, cough and breathing sounds from healthy, COVID-19 and Non-COVID\npatients. Some of these data repositories were made public. We have received\nsuch a repository from Cambridge University team under data-sharing agreement,\nwhere we have cough and breathing sound samples for 582 and 141 healthy and\nCOVID-19 patients, respectively. 87 COVID-19 patients were asymptomatic, while\nrest of them have cough. We have developed an Android application to\nautomatically screen COVID-19 from the comfort of people homes. Test subjects\ncan simply download a mobile application, enter their symptoms, record an audio\nclip of their cough and breath, and upload the data anonymously to our servers.\nOur backend server converts the audio clip to spectrogram and then apply our\nstate-of-the-art machine learning model to classify between cough sounds\nproduced by COVID-19 patients, as opposed to healthy subjects or those with\nother respiratory conditions. The system can detect asymptomatic COVID-19\npatients with a sensitivity more than 91%.",
          "arxiv_id": "2103.12063v1"
        },
        {
          "title": "Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data",
          "year": "2020-06",
          "abstract": "Audio signals generated by the human body (e.g., sighs, breathing, heart,\ndigestion, vibration sounds) have routinely been used by clinicians as\nindicators to diagnose disease or assess disease progression. Until recently,\nsuch signals were usually collected through manual auscultation at scheduled\nvisits. Research has now started to use digital technology to gather bodily\nsounds (e.g., from digital stethoscopes) for cardiovascular or respiratory\nexamination, which could then be used for automatic analysis. Some initial work\nshows promise in detecting diagnostic signals of COVID-19 from voice and\ncoughs. In this paper we describe our data analysis over a large-scale\ncrowdsourced dataset of respiratory sounds collected to aid diagnosis of\nCOVID-19. We use coughs and breathing to understand how discernible COVID-19\nsounds are from those in asthma or healthy controls. Our results show that even\na simple binary machine learning classifier is able to classify correctly\nhealthy and COVID-19 sounds. We also show how we distinguish a user who tested\npositive for COVID-19 and has a cough from a healthy user with a cough, and\nusers who tested positive for COVID-19 and have a cough from users with asthma\nand a cough. Our models achieve an AUC of above 80% across all tasks. These\nresults are preliminary and only scratch the surface of the potential of this\ntype of data and audio-based machine learning. This work opens the door to\nfurther investigation of how automatically analysed respiratory patterns could\nbe used as pre-screening signals to aid COVID-19 diagnosis.",
          "arxiv_id": "2006.05919v3"
        },
        {
          "title": "AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough Samples via an App",
          "year": "2020-04",
          "abstract": "Background: The inability to test at scale has become humanity's Achille's\nheel in the ongoing war against the COVID-19 pandemic. A scalable screening\ntool would be a game changer. Building on the prior work on cough-based\ndiagnosis of respiratory diseases, we propose, develop and test an Artificial\nIntelligence (AI)-powered screening solution for COVID-19 infection that is\ndeployable via a smartphone app. The app, named AI4COVID-19 records and sends\nthree 3-second cough sounds to an AI engine running in the cloud, and returns a\nresult within two minutes. Methods: Cough is a symptom of over thirty\nnon-COVID-19 related medical conditions. This makes the diagnosis of a COVID-19\ninfection by cough alone an extremely challenging multidisciplinary problem. We\naddress this problem by investigating the distinctness of pathomorphological\nalterations in the respiratory system induced by COVID-19 infection when\ncompared to other respiratory infections. To overcome the COVID-19 cough\ntraining data shortage we exploit transfer learning. To reduce the misdiagnosis\nrisk stemming from the complex dimensionality of the problem, we leverage a\nmulti-pronged mediator centered risk-averse AI architecture. Results: Results\nshow AI4COVID-19 can distinguish among COVID-19 coughs and several types of\nnon-COVID-19 coughs. The accuracy is promising enough to encourage a\nlarge-scale collection of labeled cough data to gauge the generalization\ncapability of AI4COVID-19. AI4COVID-19 is not a clinical grade testing tool.\nInstead, it offers a screening tool deployable anytime, anywhere, by anyone. It\ncan also be a clinical decision assistance tool used to channel\nclinical-testing and treatment to those who need it the most, thereby saving\nmore lives.",
          "arxiv_id": "2004.01275v6"
        }
      ],
      "14": [
        {
          "title": "StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion",
          "year": "2021-07",
          "abstract": "We present an unsupervised non-parallel many-to-many voice conversion (VC)\nmethod using a generative adversarial network (GAN) called StarGAN v2. Using a\ncombination of adversarial source classifier loss and perceptual loss, our\nmodel significantly outperforms previous VC models. Although our model is\ntrained only with 20 English speakers, it generalizes to a variety of voice\nconversion tasks, such as any-to-many, cross-lingual, and singing conversion.\nUsing a style encoder, our framework can also convert plain reading speech into\nstylistic speech, such as emotional and falsetto speech. Subjective and\nobjective evaluation experiments on a non-parallel many-to-many voice\nconversion task revealed that our model produces natural sounding voices, close\nto the sound quality of state-of-the-art text-to-speech (TTS) based voice\nconversion methods without the need for text labels. Moreover, our model is\ncompletely convolutional and with a faster-than-real-time vocoder such as\nParallel WaveGAN can perform real-time voice conversion.",
          "arxiv_id": "2107.10394v2"
        },
        {
          "title": "Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion",
          "year": "2022-07",
          "abstract": "Voice conversion is to generate a new speech with the source content and a\ntarget voice style. In this paper, we focus on one general setting, i.e.,\nnon-parallel many-to-many voice conversion, which is close to the real-world\nscenario. As the name implies, non-parallel many-to-many voice conversion does\nnot require the paired source and reference speeches and can be applied to\narbitrary voice transfer. In recent years, Generative Adversarial Networks\n(GANs) and other techniques such as Conditional Variational Autoencoders\n(CVAEs) have made considerable progress in this field. However, due to the\nsophistication of voice conversion, the style similarity of the converted\nspeech is still unsatisfactory. Inspired by the inherent structure of\nmel-spectrogram, we propose a new voice conversion framework, i.e.,\nSubband-based Generative Adversarial Network for Voice Conversion (SGAN-VC).\nSGAN-VC converts each subband content of the source speech separately by\nexplicitly utilizing the spatial characteristics between different subbands.\nSGAN-VC contains one style encoder, one content encoder, and one decoder. In\nparticular, the style encoder network is designed to learn style codes for\ndifferent subbands of the target speaker. The content encoder network can\ncapture the content information on the source speech. Finally, the decoder\ngenerates particular subband content. In addition, we propose a pitch-shift\nmodule to fine-tune the pitch of the source speaker, making the converted tone\nmore accurate and explainable. Extensive experiments demonstrate that the\nproposed approach achieves state-of-the-art performance on VCTK Corpus and\nAISHELL3 datasets both qualitatively and quantitatively, whether on seen or\nunseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data\neven exceeds that of StarGANv2-VC with ASR network assistance.",
          "arxiv_id": "2207.06057v2"
        },
        {
          "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
          "year": "2023-10",
          "abstract": "Voice conversion aims to convert source speech into a target voice using\nrecordings of the target speaker as a reference. Newer models are producing\nincreasingly realistic output. But what happens when models are fed with\nnon-standard data, such as speech from a user with a speech impairment? We\ninvestigate how a recent voice conversion model performs on non-standard\ndownstream voice conversion tasks. We use a simple but robust approach called\nk-nearest neighbors voice conversion (kNN-VC). We look at four non-standard\napplications: stuttered voice conversion, cross-lingual voice conversion,\nmusical instrument conversion, and text-to-voice conversion. The latter\ninvolves converting to a target voice specified through a text description,\ne.g. \"a young man with a high-pitched voice\". Compared to an established\nbaseline, we find that kNN-VC retains high performance in stuttered and\ncross-lingual voice conversion. Results are more mixed for the musical\ninstrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some\ninstruments like drums but not on others. Nevertheless, this shows that voice\nconversion models - and kNN-VC in particular - are increasingly applicable in a\nrange of non-standard downstream tasks. But there are still limitations when\nsamples are very far from the training distribution. Code, samples, trained\nmodels: https://rf5.github.io/sacair2023-knnvc-demo/.",
          "arxiv_id": "2310.08104v1"
        }
      ],
      "15": [
        {
          "title": "Differentiable Grey-box Modelling of Phaser Effects using Frame-based Spectral Processing",
          "year": "2023-06",
          "abstract": "Machine learning approaches to modelling analog audio effects have seen\nintensive investigation in recent years, particularly in the context of\nnon-linear time-invariant effects such as guitar amplifiers. For modulation\neffects such as phasers, however, new challenges emerge due to the presence of\nthe low-frequency oscillator which controls the slowly time-varying nature of\nthe effect. Existing approaches have either required foreknowledge of this\ncontrol signal, or have been non-causal in implementation. This work presents a\ndifferentiable digital signal processing approach to modelling phaser effects\nin which the underlying control signal and time-varying spectral response of\nthe effect are jointly learned. The proposed model processes audio in short\nframes to implement a time-varying filter in the frequency domain, with a\ntransfer function based on typical analog phaser circuit topology. We show that\nthe model can be trained to emulate an analog reference device, while retaining\ninterpretable and adjustable parameters. The frame duration is an important\nhyper-parameter of the proposed model, so an investigation was carried out into\nits effect on model accuracy. The optimal frame length depends on both the rate\nand transient decay-time of the target effect, but the frame length can be\naltered at inference time without a significant change in accuracy.",
          "arxiv_id": "2306.01332v1"
        },
        {
          "title": "Steerable discovery of neural audio effects",
          "year": "2021-12",
          "abstract": "Applications of deep learning for audio effects often focus on modeling\nanalog effects or learning to control effects to emulate a trained audio\nengineer. However, deep learning approaches also have the potential to expand\ncreativity through neural audio effects that enable new sound transformations.\nWhile recent work demonstrated that neural networks with random weights produce\ncompelling audio effects, control of these effects is limited and unintuitive.\nTo address this, we introduce a method for the steerable discovery of neural\naudio effects. This method enables the design of effects using example\nrecordings provided by the user. We demonstrate how this method produces an\neffect similar to the target effect, along with interesting inaccuracies, while\nalso providing perceptually relevant controls.",
          "arxiv_id": "2112.02926v1"
        },
        {
          "title": "Style Transfer of Audio Effects with Differentiable Signal Processing",
          "year": "2022-07",
          "abstract": "We present a framework that can impose the audio effects and production style\nfrom one recording to another by example with the goal of simplifying the audio\nproduction process. We train a deep neural network to analyze an input\nrecording and a style reference recording, and predict the control parameters\nof audio effects used to render the output. In contrast to past work, we\nintegrate audio effects as differentiable operators in our framework, perform\nbackpropagation through audio effects, and optimize end-to-end using an\naudio-domain loss. We use a self-supervised training strategy enabling\nautomatic control of audio effects without the use of any labeled or paired\ntraining data. We survey a range of existing and new approaches for\ndifferentiable signal processing, showing how each can be integrated into our\nframework while discussing their trade-offs. We evaluate our approach on both\nspeech and music tasks, demonstrating that our approach generalizes both to\nunseen recordings and even to sample rates different than those seen during\ntraining. Our approach produces convincing production style transfer results\nwith the ability to transform input recordings to produced recordings, yielding\naudio effect control parameters that enable interpretability and user\ninteraction.",
          "arxiv_id": "2207.08759v1"
        }
      ],
      "16": [
        {
          "title": "JVS-MuSiC: Japanese multispeaker singing-voice corpus",
          "year": "2020-01",
          "abstract": "Thanks to developments in machine learning techniques, it has become possible\nto synthesize high-quality singing voices of a single singer. An open\nmultispeaker singing-voice corpus would further accelerate the research in\nsinging-voice synthesis. However, conventional singing-voice corpora only\nconsist of the singing voices of a single singer. We designed a Japanese\nmultispeaker singing-voice corpus called \"JVS-MuSiC\" with the aim to analyze\nand synthesize a variety of voices. The corpus consists of 100 singers'\nrecordings of the same song, Katatsumuri, which is a Japanese children's song.\nIt also includes another song that is different for each singer. In this paper,\nwe describe the design of the corpus and experimental analyses using JVS-MuSiC.\nWe investigated the relationship between 1) the similarity of singing voices\nand perceptual oneness of unison singing voices and between 2) the similarity\nof singing voices and that of speech. The results suggest that 1) there is a\npositive and moderate correlation between singing-voice similarity and the\noneness of unison and that 2) the correlation between singing-voice similarity\nand speech similarity is weak. This corpus is freely available online.",
          "arxiv_id": "2001.07044v1"
        },
        {
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System",
          "year": "2021-08",
          "abstract": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "arxiv_id": "2108.02776v1"
        },
        {
          "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis",
          "year": "2023-12",
          "abstract": "Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://aaronz345.github.io/StyleSingerDemo/.",
          "arxiv_id": "2312.10741v5"
        }
      ],
      "17": [
        {
          "title": "Learning to detect an animal sound from five examples",
          "year": "2023-05",
          "abstract": "Automatic detection and classification of animal sounds has many applications\nin biodiversity monitoring and animal behaviour. In the past twenty years, the\nvolume of digitised wildlife sound available has massively increased, and\nautomatic classification through deep learning now shows strong results.\nHowever, bioacoustics is not a single task but a vast range of small-scale\ntasks (such as individual ID, call type, emotional indication) with wide\nvariety in data characteristics, and most bioacoustic tasks do not come with\nstrongly-labelled training data. The standard paradigm of supervised learning,\nfocussed on a single large-scale dataset and/or a generic pre-trained\nalgorithm, is insufficient. In this work we recast bioacoustic sound event\ndetection within the AI framework of few-shot learning. We adapt this framework\nto sound event detection, such that a system can be given the annotated\nstart/end times of as few as 5 events, and can then detect events in\nlong-duration audio -- even when the sound category was not known at the time\nof algorithm training. We introduce a collection of open datasets designed to\nstrongly test a system's ability to perform few-shot sound event detections,\nand we present the results of a public contest to address the task. We show\nthat prototypical networks are a strong-performing method, when enhanced with\nadaptations for general characteristics of animal sounds. We demonstrate that\nwidely-varying sound event durations are an important factor in performance, as\nwell as non-stationarity, i.e. gradual changes in conditions throughout the\nduration of a recording. For fine-grained bioacoustic recognition tasks without\nmassive annotated training data, our results demonstrate that few-shot sound\nevent detection is a powerful new method, strongly outperforming traditional\nsignal-processing detection methods in the fully automated scenario.",
          "arxiv_id": "2305.13210v1"
        },
        {
          "title": "A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doana",
          "year": "2025-03",
          "abstract": "Passive Acoustic Monitoring is a key tool for biodiversity conservation, but\nthe large volumes of unsupervised audio it generates present major challenges\nfor extracting meaningful information. Deep Learning offers promising\nsolutions. BirdNET, a widely used bird identification model, has shown success\nin many study systems but is limited at local scale due to biases in its\ntraining data, which focus on specific locations and target sounds rather than\nentire soundscapes. A key challenge in bird species identification is that many\nrecordings either lack target species or contain overlapping vocalizations,\ncomplicating automatic identification. To address these problems, we developed\na multi-stage pipeline for automatic bird vocalization identification in\nDo\\~nana National Park (SW Spain), a wetland of high conservation concern. We\ndeployed AudioMoth recorders in three main habitats across nine locations and\nmanually annotated 461 minutes of audio, resulting in 3749 labeled segments\nspanning 34 classes. We first applied a Bird Song Detector to isolate bird\nvocalizations using spectrogram-based image processing. Then, species were\nclassified using custom models trained at the local scale. Applying the Bird\nSong Detector before classification improved species identification, as all\nmodels performed better when analyzing only the segments where birds were\ndetected. Specifically, the combination of detector and fine-tuned BirdNET\noutperformed the baseline without detection. This approach demonstrates the\neffectiveness of integrating a Bird Song Detector with local classification\nmodels. These findings highlight the need to adapt general-purpose tools to\nspecific ecological challenges. Automatically detecting bird species helps\ntrack the health of this threatened ecosystem, given birds sensitivity to\nenvironmental change, and supports conservation planning to reduce biodiversity\nloss.",
          "arxiv_id": "2503.15576v2"
        },
        {
          "title": "Global birdsong embeddings enable superior transfer learning for bioacoustic classification",
          "year": "2023-07",
          "abstract": "Automated bioacoustic analysis aids understanding and protection of both\nmarine and terrestrial animals and their habitats across extensive\nspatiotemporal scales, and typically involves analyzing vast collections of\nacoustic data. With the advent of deep learning models, classification of\nimportant signals from these datasets has markedly improved. These models power\ncritical data analyses for research and decision-making in biodiversity\nmonitoring, animal behaviour studies, and natural resource management. However,\ndeep learning models are often data-hungry and require a significant amount of\nlabeled training data to perform well. While sufficient training data is\navailable for certain taxonomic groups (e.g., common bird species), many\nclasses (such as rare and endangered species, many non-bird taxa, and\ncall-type) lack enough data to train a robust model from scratch. This study\ninvestigates the utility of feature embeddings extracted from audio\nclassification models to identify bioacoustic classes other than the ones these\nmodels were originally trained on. We evaluate models on diverse datasets,\nincluding different bird calls and dialect types, bat calls, marine mammals\ncalls, and amphibians calls. The embeddings extracted from the models trained\non bird vocalization data consistently allowed higher quality classification\nthan the embeddings trained on general audio datasets. The results of this\nstudy indicate that high-quality feature embeddings from large-scale acoustic\nbird classifiers can be harnessed for few-shot transfer learning, enabling the\nlearning of new classes from a limited quantity of training data. Our findings\nreveal the potential for efficient analyses of novel bioacoustic tasks, even in\nscenarios where available training data is limited to a few samples.",
          "arxiv_id": "2307.06292v2"
        }
      ],
      "18": [
        {
          "title": "Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting",
          "year": "2022-11",
          "abstract": "In this paper, we present a novel approach to adapt a sequence-to-sequence\nTransformer-Transducer ASR system to the keyword spotting (KWS) task. We\nachieve this by replacing the keyword in the text transcription with a special\ntoken <kw> and training the system to detect the <kw> token in an audio stream.\nAt inference time, we create a decision function inspired by conventional KWS\napproaches, to make our approach more suitable for the KWS task. Furthermore,\nwe introduce a specific keyword spotting loss by adapting the\nsequence-discriminative Minimum Bayes-Risk training technique. We find that our\napproach significantly outperforms ASR based KWS systems. When compared with a\nconventional keyword spotting system, our proposal has similar performance\nwhile bringing the advantages and flexibility of sequence-to-sequence training.\nAdditionally, when combined with the conventional KWS system, our approach can\nimprove the performance at any operation point.",
          "arxiv_id": "2211.06478v1"
        },
        {
          "title": "Progressive Continual Learning for Spoken Keyword Spotting",
          "year": "2022-01",
          "abstract": "Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. To tackle such challenges, we propose a\nprogressive continual learning strategy for small-footprint spoken keyword\nspotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a\nnetwork instantiator to generate the task-specific sub-networks for remembering\npreviously learned keywords. As a result, the PCL-KWS approach incrementally\nlearns new keywords without forgetting prior knowledge. Besides, the\nkeyword-aware network scaling mechanism of PCL-KWS constrains the growth of\nmodel parameters while achieving high performance. Experimental results show\nthat after learning five new tasks sequentially, our proposed PCL-KWS approach\narchives the new state-of-the-art performance of 92.8% average accuracy for all\nthe tasks on Google Speech Command dataset compared with other baselines.",
          "arxiv_id": "2201.12546v2"
        },
        {
          "title": "Personalized Keyword Spotting through Multi-task Learning",
          "year": "2022-06",
          "abstract": "Keyword spotting (KWS) plays an essential role in enabling speech-based user\ninteraction on smart devices, and conventional KWS (C-KWS) approaches have\nconcentrated on detecting user-agnostic pre-defined keywords. However, in\npractice, most user interactions come from target users enrolled in the device\nwhich motivates to construct personalized keyword spotting. We design two\npersonalized KWS tasks; (1) Target user Biased KWS (TB-KWS) and (2) Target user\nOnly KWS (TO-KWS). To solve the tasks, we propose personalized keyword spotting\nthrough multi-task learning (PK-MTL) that consists of multi-task learning and\ntask-adaptation. First, we introduce applying multi-task learning on keyword\nspotting and speaker verification to leverage user information to the keyword\nspotting system. Next, we design task-specific scoring functions to adapt to\nthe personalized KWS tasks thoroughly. We evaluate our framework on\nconventional and personalized scenarios, and the results show that PK-MTL can\ndramatically reduce the false alarm rate, especially in various practical\nscenarios.",
          "arxiv_id": "2206.13708v1"
        }
      ],
      "19": [
        {
          "title": "Distributed collaborative anomalous sound detection by embedding sharing",
          "year": "2024-03",
          "abstract": "To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.",
          "arxiv_id": "2403.16610v1"
        },
        {
          "title": "Transformer-based Autoencoder with ID Constraint for Unsupervised Anomalous Sound Detection",
          "year": "2023-10",
          "abstract": "Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous\nsounds of devices when only normal sound data is available. The autoencoder\n(AE) and self-supervised learning based methods are two mainstream methods.\nHowever, the AE-based methods could be limited as the feature learned from\nnormal sounds can also fit with anomalous sounds, reducing the ability of the\nmodel in detecting anomalies from sound. The self-supervised methods are not\nalways stable and perform differently, even for machines of the same type. In\naddition, the anomalous sound may be short-lived, making it even harder to\ndistinguish from normal sound. This paper proposes an ID constrained\nTransformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly\nscore computation for unsupervised ASD. Machine ID is employed to constrain the\nlatent space of the Transformer-based autoencoder (TransAE) by introducing a\nsimple ID classifier to learn the difference in the distribution for the same\nmachine type and enhance the ability of the model in distinguishing anomalous\nsound. Moreover, weighted anomaly score computation is introduced to highlight\nthe anomaly scores of anomalous events that only appear for a short time.\nExperiments performed on DCASE 2020 Challenge Task2 development dataset\ndemonstrate the effectiveness and superiority of our proposed method.",
          "arxiv_id": "2310.08950v1"
        },
        {
          "title": "SSDPT: Self-Supervised Dual-Path Transformer for Anomalous Sound Detection in Machine Condition Monitoring",
          "year": "2022-08",
          "abstract": "Anomalous sound detection for machine condition monitoring has great\npotential in the development of Industry 4.0. However, these anomalous sounds\nof machines are usually unavailable in normal conditions. Therefore, the models\nemployed have to learn acoustic representations with normal sounds for\ntraining, and detect anomalous sounds while testing. In this article, we\npropose a self-supervised dual-path Transformer (SSDPT) network to detect\nanomalous sounds in machine monitoring. The SSDPT network splits the acoustic\nfeatures into segments and employs several DPT blocks for time and frequency\nmodeling. DPT blocks use attention modules to alternately model the interactive\ninformation about the frequency and temporal components of the segmented\nacoustic features. To address the problem of lack of anomalous sound, we adopt\na self-supervised learning approach to train the network with normal sound.\nSpecifically, this approach randomly masks and reconstructs the acoustic\nfeatures, and jointly classifies machine identity information to improve the\nperformance of anomalous sound detection. We evaluated our method on the\nDCASE2021 task2 dataset. The experimental results show that the SSDPT network\nachieves a significant increase in the harmonic mean AUC score, in comparison\nto present state-of-the-art methods of anomalous sound detection.",
          "arxiv_id": "2208.03421v1"
        }
      ],
      "20": [
        {
          "title": "StableFace: Analyzing and Improving Motion Stability for Talking Face Generation",
          "year": "2022-08",
          "abstract": "While previous speech-driven talking face generation methods have made\nsignificant progress in improving the visual quality and lip-sync quality of\nthe synthesized videos, they pay less attention to lip motion jitters which\ngreatly undermine the realness of talking face videos. What causes motion\njitters, and how to mitigate the problem? In this paper, we conduct systematic\nanalyses on the motion jittering problem based on a state-of-the-art pipeline\nthat uses 3D face representations to bridge the input audio and output video,\nand improve the motion stability with a series of effective designs. We find\nthat several issues can lead to jitters in synthesized talking face video: 1)\njitters from the input 3D face representations; 2) training-inference mismatch;\n3) lack of dependency modeling among video frames. Accordingly, we propose\nthree effective solutions to address this issue: 1) we propose a gaussian-based\nadaptive smoothing module to smooth the 3D face representations to eliminate\njitters in the input; 2) we add augmented erosions on the input data of the\nneural renderer in training to simulate the distortion in inference to reduce\nmismatch; 3) we develop an audio-fused transformer generator to model\ndependency among video frames. Besides, considering there is no off-the-shelf\nmetric for measuring motion jitters in talking face video, we devise an\nobjective metric (Motion Stability Index, MSI), to quantitatively measure the\nmotion jitters by calculating the reciprocal of variance acceleration.\nExtensive experimental results show the superiority of our method on\nmotion-stable face video generation, with better quality than previous systems.",
          "arxiv_id": "2208.13717v1"
        },
        {
          "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation",
          "year": "2023-03",
          "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk",
          "arxiv_id": "2303.11089v2"
        },
        {
          "title": "AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person",
          "year": "2021-08",
          "abstract": "Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.",
          "arxiv_id": "2108.04325v2"
        }
      ],
      "21": [
        {
          "title": "A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems",
          "year": "2024-06",
          "abstract": "Despite significant recent progress across multiple subtasks of audio source\nseparation, few music source separation systems support separation beyond the\nfour-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current\nsystems that support source separation beyond this setup, most continue to rely\non an inflexible decoder setup that can only support a fixed pre-defined set of\nstems. Increasing stem support in these inflexible systems correspondingly\nrequires increasing computational complexity, rendering extensions of these\nsystems computationally infeasible for long-tail instruments. In this work, we\npropose Banquet, a system that allows source separation of multiple stems using\njust one decoder. A bandsplit source separation model is extended to work in a\nquery-based setup in tandem with a music instrument recognition PaSST model. On\nthe MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached\nthe performance level of the significantly more complex 6-stem Hybrid\nTransformer Demucs on VDBO stems and outperformed it on guitar and piano. The\nquery-based setup allows for the separation of narrow instrument classes such\nas clean acoustic guitars, and can be successfully applied to the extraction of\nless common stems such as reeds and organs. Implementation is available at\nhttps://github.com/kwatcharasupat/query-bandit.",
          "arxiv_id": "2406.18747v2"
        },
        {
          "title": "Separate This, and All of these Things Around It: Music Source Separation via Hyperellipsoidal Queries",
          "year": "2025-01",
          "abstract": "Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.",
          "arxiv_id": "2501.16171v1"
        },
        {
          "title": "Task-Aware Unified Source Separation",
          "year": "2024-10",
          "abstract": "Several attempts have been made to handle multiple source separation tasks\nsuch as speech enhancement, speech separation, sound event separation, music\nsource separation (MSS), or cinematic audio source separation (CASS) with a\nsingle model. These models are trained on large-scale data including speech,\ninstruments, or sound events and can often successfully separate a wide range\nof sources. However, it is still challenging for such models to cover all\nseparation tasks because some of them are contradictory (e.g., musical\ninstruments are separated in MSS while they have to be grouped in CASS). To\novercome this issue and support all the major separation tasks, we propose a\ntask-aware unified source separation (TUSS) model. The model uses a variable\nnumber of learnable prompts to specify which source to separate, and changes\nits behavior depending on the given prompts, enabling it to handle all the\nmajor separation tasks including contradictory ones. Experimental results\ndemonstrate that the proposed TUSS model successfully handles the five major\nseparation tasks mentioned earlier. We also provide some audio examples,\nincluding both synthetic mixtures and real recordings, to demonstrate how\nflexibly the TUSS model changes its behavior at inference depending on the\nprompts.",
          "arxiv_id": "2410.23987v1"
        }
      ],
      "22": [
        {
          "title": "Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer",
          "year": "2024-03",
          "abstract": "Humans are adept at leveraging visual cues from lip movements for recognizing\nspeech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR)\nmodels follow similar approach to achieve robust speech recognition in noisy\nconditions. In this work, we present a multilingual AVSR model incorporating\nseveral enhancements to improve performance and audio noise robustness.\nNotably, we adapt the recently proposed Fast Conformer model to process both\naudio and visual modalities using a novel hybrid CTC/RNN-T architecture. We\nincrease the amount of audio-visual training data for six distinct languages,\ngenerating automatic transcriptions of unlabelled multilingual datasets\n(VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art\nperformance on the LRS3 dataset, reaching WER of 0.8%. On the recently\nintroduced MuAViC benchmark, our model yields an absolute average-WER reduction\nof 11.9% in comparison to the original baseline. Finally, we demonstrate the\nability of the proposed model to perform audio-only, visual-only, and\naudio-visual speech recognition at test time.",
          "arxiv_id": "2405.12983v1"
        },
        {
          "title": "Learning Contextually Fused Audio-visual Representations for Audio-visual Speech Recognition",
          "year": "2022-02",
          "abstract": "With the advance in self-supervised learning for audio and visual modalities,\nit has become possible to learn a robust audio-visual speech representation.\nThis would be beneficial for improving the audio-visual speech recognition\n(AVSR) performance, as the multi-modal inputs contain more fruitful information\nin principle. In this paper, based on existing self-supervised representation\nlearning methods for audio modality, we therefore propose an audio-visual\nrepresentation learning approach. The proposed approach explores both the\ncomplementarity of audio-visual modalities and long-term context dependency\nusing a transformer-based fusion module and a flexible masking strategy. After\npre-training, the model is able to extract fused representations required by\nAVSR. Without loss of generality, it can be applied to single-modal tasks, e.g.\naudio/visual speech recognition by simply masking out one modality in the\nfusion module. The proposed pre-trained model is evaluated on speech\nrecognition and lipreading tasks using one or two modalities, where the\nsuperiority is revealed.",
          "arxiv_id": "2202.07428v2"
        },
        {
          "title": "AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition",
          "year": "2023-09",
          "abstract": "Audio-visual speech contains synchronized audio and visual information that\nprovides cross-modal supervision to learn representations for both automatic\nspeech recognition (ASR) and visual speech recognition (VSR). We introduce\ncontinuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a\nsemi-supervised method to train an audio-visual speech recognition (AVSR) model\non a combination of labeled and unlabeled videos with continuously regenerated\npseudo-labels. Our models are trained for speech recognition from audio-visual\ninputs and can perform speech recognition using both audio and visual\nmodalities, or only one modality. Our method uses the same audio-visual model\nfor both supervised training and pseudo-label generation, mitigating the need\nfor external speech recognition models to generate pseudo-labels. AV-CPL\nobtains significant improvements in VSR performance on the LRS3 dataset while\nmaintaining practical ASR and AVSR performance. Finally, using visual-only\nspeech data, our method is able to leverage unlabeled visual speech to improve\nVSR.",
          "arxiv_id": "2309.17395v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T19:43:26Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}