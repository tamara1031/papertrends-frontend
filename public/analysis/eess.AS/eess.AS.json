{
  "topics": {
    "data": {
      "0": {
        "name": "0_music_generation_musical_Music",
        "keywords": [
          [
            "music",
            0.03902134310041004
          ],
          [
            "generation",
            0.018468222067368277
          ],
          [
            "musical",
            0.016734768554910018
          ],
          [
            "Music",
            0.014917565925465788
          ],
          [
            "audio",
            0.014717358660918611
          ],
          [
            "model",
            0.01075327240549517
          ],
          [
            "models",
            0.009436130748126917
          ],
          [
            "music generation",
            0.008494206981242135
          ],
          [
            "text",
            0.007910621659443244
          ],
          [
            "melody",
            0.007710981504938271
          ]
        ],
        "count": 1325
      },
      "1": {
        "name": "1_enhancement_speech_speech enhancement_separation",
        "keywords": [
          [
            "enhancement",
            0.023441545688715183
          ],
          [
            "speech",
            0.02118170253222282
          ],
          [
            "speech enhancement",
            0.020123995548721366
          ],
          [
            "separation",
            0.012664372626030714
          ],
          [
            "noise",
            0.012523352562442502
          ],
          [
            "channel",
            0.01234087682395824
          ],
          [
            "network",
            0.012038842326164987
          ],
          [
            "SE",
            0.010890854649828651
          ],
          [
            "Speech",
            0.010841438496492538
          ],
          [
            "performance",
            0.010698272515506894
          ]
        ],
        "count": 998
      },
      "2": {
        "name": "2_attacks_detection_audio_adversarial",
        "keywords": [
          [
            "attacks",
            0.021120110576905195
          ],
          [
            "detection",
            0.015910112254692774
          ],
          [
            "audio",
            0.015549811146888175
          ],
          [
            "adversarial",
            0.01538514988702752
          ],
          [
            "spoofing",
            0.014651809334306725
          ],
          [
            "deepfake",
            0.01317394777000835
          ],
          [
            "privacy",
            0.012238472785016122
          ],
          [
            "attack",
            0.012089511476268617
          ],
          [
            "speech",
            0.011794160166736532
          ],
          [
            "speaker",
            0.011508705450476556
          ]
        ],
        "count": 979
      },
      "3": {
        "name": "3_sound_acoustic_room_method",
        "keywords": [
          [
            "sound",
            0.024481093348780642
          ],
          [
            "acoustic",
            0.01658073835200344
          ],
          [
            "room",
            0.015310595991527296
          ],
          [
            "method",
            0.013260236499945762
          ],
          [
            "source",
            0.013155785079510705
          ],
          [
            "microphone",
            0.012723766356671794
          ],
          [
            "spatial",
            0.012641205452096524
          ],
          [
            "field",
            0.012553844152869809
          ],
          [
            "estimation",
            0.010803396358227782
          ],
          [
            "localization",
            0.010615640622252601
          ]
        ],
        "count": 832
      },
      "4": {
        "name": "4_ASR_CTC_streaming_model",
        "keywords": [
          [
            "ASR",
            0.023344917879424214
          ],
          [
            "CTC",
            0.01890422673181976
          ],
          [
            "streaming",
            0.016530220599382642
          ],
          [
            "model",
            0.016054226691900538
          ],
          [
            "recognition",
            0.015548140840175237
          ],
          [
            "speech recognition",
            0.014915409710062057
          ],
          [
            "models",
            0.01471812878739657
          ],
          [
            "RNN",
            0.013708727182401756
          ],
          [
            "attention",
            0.013459356866171432
          ],
          [
            "end",
            0.013049737747610694
          ]
        ],
        "count": 695
      },
      "5": {
        "name": "5_event_sound_audio_classification",
        "keywords": [
          [
            "event",
            0.019039553492654564
          ],
          [
            "sound",
            0.01826122273602468
          ],
          [
            "audio",
            0.0175112282104796
          ],
          [
            "classification",
            0.013551525269706054
          ],
          [
            "SED",
            0.011869692755677841
          ],
          [
            "sound event",
            0.011778313845306292
          ],
          [
            "events",
            0.011772931672287889
          ],
          [
            "Sound",
            0.011246005482012092
          ],
          [
            "data",
            0.01051687617702904
          ],
          [
            "Event",
            0.01027440741941173
          ]
        ],
        "count": 677
      },
      "6": {
        "name": "6_emotion_emotion recognition_Emotion_recognition",
        "keywords": [
          [
            "emotion",
            0.03204604332219626
          ],
          [
            "emotion recognition",
            0.020837226973544235
          ],
          [
            "Emotion",
            0.02026016132115956
          ],
          [
            "recognition",
            0.015463785079178685
          ],
          [
            "Recognition",
            0.012522924158129565
          ],
          [
            "emotional",
            0.011694625990619588
          ],
          [
            "emotions",
            0.011255635364162056
          ],
          [
            "features",
            0.011238911005415807
          ],
          [
            "speech",
            0.01122173963391392
          ],
          [
            "speech emotion",
            0.011161881819429089
          ]
        ],
        "count": 671
      },
      "7": {
        "name": "7_audio_Audio_visual_text",
        "keywords": [
          [
            "audio",
            0.04056339028503609
          ],
          [
            "Audio",
            0.021163857184148933
          ],
          [
            "visual",
            0.02076684527403033
          ],
          [
            "text",
            0.01405490122689769
          ],
          [
            "captioning",
            0.012338157504481329
          ],
          [
            "language",
            0.01229644285371896
          ],
          [
            "video",
            0.0113668062270633
          ],
          [
            "modal",
            0.010882590992583749
          ],
          [
            "models",
            0.010231455634740831
          ],
          [
            "tasks",
            0.009907261755019322
          ]
        ],
        "count": 551
      },
      "8": {
        "name": "8_TTS_speech_style_prosody",
        "keywords": [
          [
            "TTS",
            0.035608484686620256
          ],
          [
            "speech",
            0.02285817240762644
          ],
          [
            "style",
            0.020786664912257707
          ],
          [
            "prosody",
            0.019574853468213162
          ],
          [
            "speaker",
            0.016770119672035885
          ],
          [
            "text",
            0.01573757609026058
          ],
          [
            "synthesis",
            0.01496538125988089
          ],
          [
            "model",
            0.012502200632901785
          ],
          [
            "shot",
            0.011443863294184673
          ],
          [
            "speech synthesis",
            0.011422041910986966
          ]
        ],
        "count": 549
      },
      "9": {
        "name": "9_speaker_verification_speaker verification_Speaker",
        "keywords": [
          [
            "speaker",
            0.04073873261115035
          ],
          [
            "verification",
            0.02665497368676021
          ],
          [
            "speaker verification",
            0.023693241549538707
          ],
          [
            "Speaker",
            0.021072241826908172
          ],
          [
            "speaker recognition",
            0.014700424936265154
          ],
          [
            "Verification",
            0.012753836786436913
          ],
          [
            "embeddings",
            0.010595040350133027
          ],
          [
            "performance",
            0.010092343643573552
          ],
          [
            "recognition",
            0.009841312083628855
          ],
          [
            "TDNN",
            0.00949050284445405
          ]
        ],
        "count": 475
      },
      "10": {
        "name": "10_speaker_diarization_speaker diarization_Speaker",
        "keywords": [
          [
            "speaker",
            0.04210496439438611
          ],
          [
            "diarization",
            0.04109550055340792
          ],
          [
            "speaker diarization",
            0.01901141915125861
          ],
          [
            "Speaker",
            0.016741332502724266
          ],
          [
            "speakers",
            0.014305894201461639
          ],
          [
            "multi",
            0.014164994513531943
          ],
          [
            "clustering",
            0.01414596395883325
          ],
          [
            "Diarization",
            0.013865517550557264
          ],
          [
            "EEND",
            0.013064620491793006
          ],
          [
            "speech",
            0.01292079860821814
          ]
        ],
        "count": 436
      },
      "11": {
        "name": "11_speech_dysarthric_articulatory_features",
        "keywords": [
          [
            "speech",
            0.023913793635383616
          ],
          [
            "dysarthric",
            0.017990163214816193
          ],
          [
            "articulatory",
            0.014845742211328245
          ],
          [
            "features",
            0.012781508533976539
          ],
          [
            "dysarthric speech",
            0.011666050605244135
          ],
          [
            "Speech",
            0.011500489898293869
          ],
          [
            "disease",
            0.011151401051846295
          ],
          [
            "AD",
            0.010687643136496042
          ],
          [
            "data",
            0.010363963340322779
          ],
          [
            "detection",
            0.009545037779141606
          ]
        ],
        "count": 432
      },
      "12": {
        "name": "12_diffusion_speech_quality_neural",
        "keywords": [
          [
            "diffusion",
            0.019784636609112668
          ],
          [
            "speech",
            0.016785807494962356
          ],
          [
            "quality",
            0.01645809541451127
          ],
          [
            "neural",
            0.013682859802001082
          ],
          [
            "vocoder",
            0.01364236285197771
          ],
          [
            "high",
            0.012358681478977094
          ],
          [
            "GAN",
            0.012202317467870052
          ],
          [
            "audio",
            0.01210378996297479
          ],
          [
            "codecs",
            0.011110511468222365
          ],
          [
            "model",
            0.011018150081467388
          ]
        ],
        "count": 356
      },
      "13": {
        "name": "13_COVID_respiratory_cough_heart",
        "keywords": [
          [
            "COVID",
            0.03750102183246956
          ],
          [
            "respiratory",
            0.026682078215095033
          ],
          [
            "cough",
            0.025015492975491205
          ],
          [
            "heart",
            0.01941877083101155
          ],
          [
            "sounds",
            0.019055165079884133
          ],
          [
            "sound",
            0.01577622316172666
          ],
          [
            "detection",
            0.013113039673432084
          ],
          [
            "classification",
            0.013019624354195174
          ],
          [
            "lung",
            0.011662039190149543
          ],
          [
            "data",
            0.010882374697558192
          ]
        ],
        "count": 335
      },
      "14": {
        "name": "14_supervised_training_pre_speech",
        "keywords": [
          [
            "supervised",
            0.021225943111440215
          ],
          [
            "training",
            0.016820681213989195
          ],
          [
            "pre",
            0.016531881014506603
          ],
          [
            "speech",
            0.01648252483306156
          ],
          [
            "Self",
            0.015391082061380056
          ],
          [
            "data",
            0.014931555019560377
          ],
          [
            "self",
            0.014288943687597122
          ],
          [
            "models",
            0.014135832594681468
          ],
          [
            "domain",
            0.013723767764154767
          ],
          [
            "ASR",
            0.013706157000297543
          ]
        ],
        "count": 311
      },
      "15": {
        "name": "15_SLU_spoken_speech_dialogue",
        "keywords": [
          [
            "SLU",
            0.027323424045573516
          ],
          [
            "spoken",
            0.02121848970168056
          ],
          [
            "speech",
            0.018766014349420868
          ],
          [
            "dialogue",
            0.01778057447583615
          ],
          [
            "Spoken",
            0.017320506335040484
          ],
          [
            "language",
            0.017130120229867995
          ],
          [
            "understanding",
            0.014096053965783253
          ],
          [
            "models",
            0.014049839647415612
          ],
          [
            "end",
            0.013475567040150908
          ],
          [
            "text",
            0.01341311143333346
          ]
        ],
        "count": 301
      },
      "16": {
        "name": "16_VC_conversion_voice conversion_voice",
        "keywords": [
          [
            "VC",
            0.05246225874619898
          ],
          [
            "conversion",
            0.04807921969808792
          ],
          [
            "voice conversion",
            0.03672225777868178
          ],
          [
            "voice",
            0.033358742223585224
          ],
          [
            "speaker",
            0.027010945238842843
          ],
          [
            "Conversion",
            0.023820580164435987
          ],
          [
            "Voice",
            0.022523290701271206
          ],
          [
            "content",
            0.017202364726004955
          ],
          [
            "speech",
            0.015892799410625085
          ],
          [
            "shot",
            0.01357148672516393
          ]
        ],
        "count": 260
      },
      "17": {
        "name": "17_singing_singing voice_voice_Singing",
        "keywords": [
          [
            "singing",
            0.07344412360381454
          ],
          [
            "singing voice",
            0.04072722888867085
          ],
          [
            "voice",
            0.036744973769076726
          ],
          [
            "Singing",
            0.029874166573703852
          ],
          [
            "SVS",
            0.02306285708046374
          ],
          [
            "singer",
            0.020113478466785403
          ],
          [
            "pitch",
            0.016725339669173007
          ],
          [
            "Voice",
            0.01652599753981196
          ],
          [
            "voice synthesis",
            0.01561207963429011
          ],
          [
            "singing voice synthesis",
            0.0136322785710406
          ]
        ],
        "count": 222
      },
      "18": {
        "name": "18_species_bird_classification_data",
        "keywords": [
          [
            "species",
            0.02679437440412425
          ],
          [
            "bird",
            0.02046121808147547
          ],
          [
            "classification",
            0.0174435575282543
          ],
          [
            "data",
            0.0161043355998999
          ],
          [
            "learning",
            0.016003271736028064
          ],
          [
            "monitoring",
            0.015220028540947856
          ],
          [
            "bioacoustic",
            0.01478556700137157
          ],
          [
            "underwater",
            0.014414940647095554
          ],
          [
            "acoustic",
            0.014072234070683002
          ],
          [
            "detection",
            0.011820752937916087
          ]
        ],
        "count": 213
      },
      "19": {
        "name": "19_KWS_keyword_spotting_Keyword",
        "keywords": [
          [
            "KWS",
            0.048934324586935085
          ],
          [
            "keyword",
            0.042821497426339276
          ],
          [
            "spotting",
            0.029557197498476274
          ],
          [
            "Keyword",
            0.028052247104934157
          ],
          [
            "keyword spotting",
            0.025406103211500983
          ],
          [
            "Spotting",
            0.021447109211215436
          ],
          [
            "keywords",
            0.01621913811964716
          ],
          [
            "model",
            0.012846921220438002
          ],
          [
            "devices",
            0.009969089234418138
          ],
          [
            "learning",
            0.009867508397429206
          ]
        ],
        "count": 204
      },
      "20": {
        "name": "20_audio_effects_synthesis_sound",
        "keywords": [
          [
            "audio",
            0.023009924815818168
          ],
          [
            "effects",
            0.017935918634853283
          ],
          [
            "synthesis",
            0.013514609752912547
          ],
          [
            "sound",
            0.01266595365545096
          ],
          [
            "neural",
            0.01174178302980657
          ],
          [
            "time",
            0.011150351902327807
          ],
          [
            "control",
            0.010824870881589531
          ],
          [
            "audio effects",
            0.010749553275236502
          ],
          [
            "model",
            0.010554297480398359
          ],
          [
            "differentiable",
            0.010304292199585752
          ]
        ],
        "count": 189
      },
      "21": {
        "name": "21_translation_speech translation_Translation_speech",
        "keywords": [
          [
            "translation",
            0.06833895685156847
          ],
          [
            "speech translation",
            0.03368827654288691
          ],
          [
            "Translation",
            0.03022392305522324
          ],
          [
            "speech",
            0.02777472002764134
          ],
          [
            "text",
            0.022127162871280973
          ],
          [
            "end",
            0.019073164956610652
          ],
          [
            "Speech",
            0.01889295917322518
          ],
          [
            "BLEU",
            0.016848465284560816
          ],
          [
            "language",
            0.015728734210221994
          ],
          [
            "MT",
            0.014083520691790296
          ]
        ],
        "count": 187
      },
      "22": {
        "name": "22_visual_audio_lip_visual speech",
        "keywords": [
          [
            "visual",
            0.05182931286799482
          ],
          [
            "audio",
            0.022730995048804332
          ],
          [
            "lip",
            0.020611777269854716
          ],
          [
            "visual speech",
            0.02000082892304145
          ],
          [
            "Visual",
            0.019973348971799303
          ],
          [
            "AVSR",
            0.01994443946940359
          ],
          [
            "speech",
            0.016588567965221627
          ],
          [
            "AV",
            0.015552757544926453
          ],
          [
            "recognition",
            0.01442814764948469
          ],
          [
            "Speech",
            0.013965431236148832
          ]
        ],
        "count": 180
      },
      "23": {
        "name": "23_face_lip_speech_facial",
        "keywords": [
          [
            "face",
            0.024577446727230876
          ],
          [
            "lip",
            0.020229093466218085
          ],
          [
            "speech",
            0.019583886265253096
          ],
          [
            "facial",
            0.019423637410681722
          ],
          [
            "gestures",
            0.016623147162366483
          ],
          [
            "video",
            0.016457326394912854
          ],
          [
            "gesture",
            0.015348139089500562
          ],
          [
            "audio",
            0.01484787705294819
          ],
          [
            "motion",
            0.014555521887851542
          ],
          [
            "3D",
            0.01380700199459873
          ]
        ],
        "count": 180
      },
      "24": {
        "name": "24_ASR_languages_speech_Speech",
        "keywords": [
          [
            "ASR",
            0.028802320159349824
          ],
          [
            "languages",
            0.024371532908250983
          ],
          [
            "speech",
            0.01941979342911988
          ],
          [
            "Speech",
            0.01719188723807801
          ],
          [
            "language",
            0.016586914394047164
          ],
          [
            "recognition",
            0.016141697449166294
          ],
          [
            "speech recognition",
            0.015692279001295818
          ],
          [
            "corpus",
            0.014864983706312746
          ],
          [
            "data",
            0.014500012028158014
          ],
          [
            "Recognition",
            0.013575797344406912
          ]
        ],
        "count": 170
      },
      "25": {
        "name": "25_MOS_quality_speech_speech quality",
        "keywords": [
          [
            "MOS",
            0.041846713998305356
          ],
          [
            "quality",
            0.0286077393332936
          ],
          [
            "speech",
            0.022075036815648125
          ],
          [
            "speech quality",
            0.021145942670969295
          ],
          [
            "prediction",
            0.019954739579249134
          ],
          [
            "assessment",
            0.015978890725727543
          ],
          [
            "evaluation",
            0.014896323001625867
          ],
          [
            "Speech",
            0.013539558196498418
          ],
          [
            "intrusive",
            0.01318969179968805
          ],
          [
            "Quality",
            0.012458649679664561
          ]
        ],
        "count": 161
      },
      "26": {
        "name": "26_separation_source separation_source_music",
        "keywords": [
          [
            "separation",
            0.05122233367985409
          ],
          [
            "source separation",
            0.04190223699351796
          ],
          [
            "source",
            0.03740903129928507
          ],
          [
            "music",
            0.022389283480247733
          ],
          [
            "Separation",
            0.019244060901718336
          ],
          [
            "music source",
            0.017777951149888286
          ],
          [
            "music source separation",
            0.01720940199533414
          ],
          [
            "Source",
            0.01695040087366579
          ],
          [
            "Music",
            0.014035131744061895
          ],
          [
            "model",
            0.012150031421671802
          ]
        ],
        "count": 148
      },
      "27": {
        "name": "27_EEG_brain_decoding_auditory",
        "keywords": [
          [
            "EEG",
            0.07577851731149395
          ],
          [
            "brain",
            0.03755190245245889
          ],
          [
            "decoding",
            0.0246283719466917
          ],
          [
            "auditory",
            0.023122184594880364
          ],
          [
            "speech",
            0.02104474124545522
          ],
          [
            "signals",
            0.01972589100489085
          ],
          [
            "AAD",
            0.017085689919186044
          ],
          [
            "attention",
            0.016476268101930265
          ],
          [
            "neural",
            0.014734185179875892
          ],
          [
            "auditory attention",
            0.01329602878511859
          ]
        ],
        "count": 141
      },
      "28": {
        "name": "28_anomalous_detection_machine_anomaly",
        "keywords": [
          [
            "anomalous",
            0.03282834618446103
          ],
          [
            "detection",
            0.027868940204486364
          ],
          [
            "machine",
            0.025904138318345257
          ],
          [
            "anomaly",
            0.021921410284720475
          ],
          [
            "ASD",
            0.020989577360716844
          ],
          [
            "sound",
            0.019402096335081435
          ],
          [
            "anomalous sound",
            0.017407996560863146
          ],
          [
            "sounds",
            0.017017387209630204
          ],
          [
            "Anomalous",
            0.016784934164230284
          ],
          [
            "sound detection",
            0.016652209865418724
          ]
        ],
        "count": 139
      }
    },
    "correlations": [
      [
        1.0,
        -0.7519140467239332,
        -0.6979525343094948,
        -0.7466723248485507,
        -0.7189219933244847,
        -0.7028165155723993,
        -0.7414230617149866,
        -0.6374915649873321,
        -0.7369105570396766,
        -0.7604741829996701,
        -0.7602464712268019,
        -0.7573643601074682,
        -0.7005946503709305,
        -0.758753236908088,
        -0.7131311245900028,
        -0.7321697335872479,
        -0.7552132804455529,
        -0.7418016293614005,
        -0.7255263723080515,
        -0.7634990651604345,
        -0.6581776013191293,
        -0.7334514802205568,
        -0.6846649915885408,
        -0.7458048968994212,
        -0.7550229481689352,
        -0.7337902127584472,
        -0.6473251940027661,
        -0.7544845331558381,
        -0.7473055278311883
      ],
      [
        -0.7519140467239332,
        1.0,
        -0.7501833663385096,
        -0.7231327215862303,
        -0.690186587261953,
        -0.750608146940269,
        -0.7501709337529537,
        -0.7413659098592427,
        -0.6806029071099808,
        -0.7277672551243188,
        -0.725225210442633,
        -0.6254569015598619,
        -0.5944167115619134,
        -0.7587530478292541,
        -0.6626404697809917,
        -0.6816697535814524,
        -0.7364109916626853,
        -0.7581286056792912,
        -0.7270395888280482,
        -0.7562144342552954,
        -0.7437985921343475,
        -0.6655556238645226,
        -0.7274602534489245,
        -0.6332139579752543,
        -0.6742732526113588,
        -0.5786484015395416,
        -0.6693311150406975,
        -0.7554201413557088,
        -0.7575179151044057
      ],
      [
        -0.6979525343094948,
        -0.7501833663385096,
        1.0,
        -0.7537650234710009,
        -0.7275538017320149,
        -0.661757872155396,
        -0.7466832939784787,
        -0.5954763783599719,
        -0.7243828538718462,
        -0.6558322372530982,
        -0.7421444484904542,
        -0.7387187827249597,
        -0.7345518646219589,
        -0.7465854511731103,
        -0.7159651641457851,
        -0.7441675653199951,
        -0.7165389494562227,
        -0.746488786004526,
        -0.7298979211176989,
        -0.7544370228429196,
        -0.5895259598196816,
        -0.7384085574808983,
        -0.6548099986721776,
        -0.7362169824852345,
        -0.7288907817173251,
        -0.7452330553526991,
        -0.7534221022519195,
        -0.7598303152872239,
        -0.6066793014356714
      ],
      [
        -0.7466723248485507,
        -0.7231327215862303,
        -0.7537650234710009,
        1.0,
        -0.7356449503509472,
        -0.6443154249665833,
        -0.7595750725888486,
        -0.7293309778486622,
        -0.7549814143192286,
        -0.7541303078041337,
        -0.7524335568604774,
        -0.7565285324649673,
        -0.7423540320886162,
        -0.7389665418643272,
        -0.7363778731189641,
        -0.7541991066731837,
        -0.7557613480164713,
        -0.7569593966678378,
        -0.7246973017529926,
        -0.7630127480053444,
        -0.7405359675197326,
        -0.7454166624081582,
        -0.7186437641821244,
        -0.7562138845194697,
        -0.7536258736652284,
        -0.7505111346732265,
        -0.7045561250513606,
        -0.7601360372501409,
        -0.7388958959402523
      ],
      [
        -0.7189219933244847,
        -0.690186587261953,
        -0.7275538017320149,
        -0.7356449503509472,
        1.0,
        -0.7370169962284707,
        -0.7138650450593551,
        -0.7118152218751843,
        -0.6834642929236289,
        -0.7214916390131103,
        -0.7069448909559504,
        -0.6462010799375864,
        -0.6974445918211142,
        -0.7486855017479837,
        -0.43318502570256945,
        -0.66260634170586,
        -0.7286390795383442,
        -0.7487487385281101,
        -0.7011578171656396,
        -0.7473783261560223,
        -0.738457073168059,
        -0.6089690269130013,
        -0.6565248020254715,
        -0.7253515004019253,
        0.018265359658506473,
        -0.7069301658920345,
        -0.7262836362843847,
        -0.7430959915381963,
        -0.7521053441653609
      ],
      [
        -0.7028165155723993,
        -0.750608146940269,
        -0.661757872155396,
        -0.6443154249665833,
        -0.7370169962284707,
        1.0,
        -0.7533830518946766,
        -0.5829882252198781,
        -0.7508491386973603,
        -0.758444758185117,
        -0.7586725081815109,
        -0.7583639181414339,
        -0.7456106964812949,
        -0.7319871887949736,
        -0.7148443656941743,
        -0.752761495202942,
        -0.7602515140576687,
        -0.7611693739638511,
        -0.6882689364479962,
        -0.7572699527977378,
        -0.5958801142643031,
        -0.7454182858887637,
        -0.6571625249571049,
        -0.7500572464003217,
        -0.7531840988476994,
        -0.7477252958571299,
        -0.7374549331427386,
        -0.7597097799120156,
        -0.7140349635370049
      ],
      [
        -0.7414230617149866,
        -0.7501709337529537,
        -0.7466832939784787,
        -0.7595750725888486,
        -0.7138650450593551,
        -0.7533830518946766,
        1.0,
        -0.7314802011629106,
        -0.7208771187849128,
        -0.73903774725337,
        -0.7448987873627404,
        -0.7375406423240929,
        -0.7456738301803092,
        -0.7620358212827336,
        -0.7064558047402691,
        -0.732693477411666,
        -0.733286872926926,
        -0.7585020289519726,
        -0.7470247569702227,
        -0.7612466791219745,
        -0.748455402039832,
        -0.7338701694578387,
        -0.7242722533429062,
        -0.7393009284498803,
        -0.7268660542868022,
        -0.7424927495047025,
        -0.7575333027631433,
        -0.7594095112988284,
        -0.7543075301594593
      ],
      [
        -0.6374915649873321,
        -0.7413659098592427,
        -0.5954763783599719,
        -0.7293309778486622,
        -0.7118152218751843,
        -0.5829882252198781,
        -0.7314802011629106,
        1.0,
        -0.7186921333883312,
        -0.7471614798244635,
        -0.7421708630922683,
        -0.7421986905697677,
        -0.7186978540661834,
        -0.7495346716323285,
        -0.6958008740275587,
        -0.7098105209314559,
        -0.7504724262605361,
        -0.755883912149542,
        -0.7229981300222574,
        -0.7530028366857677,
        -0.4563865269899272,
        -0.7123214188061255,
        -0.23090911134505088,
        -0.7068200037303543,
        -0.7276207512239086,
        -0.7353620324797193,
        -0.7339046548308317,
        -0.7506404480924724,
        -0.7417281122550627
      ],
      [
        -0.7369105570396766,
        -0.6806029071099808,
        -0.7243828538718462,
        -0.7549814143192286,
        -0.6834642929236289,
        -0.7508491386973603,
        -0.7208771187849128,
        -0.7186921333883312,
        1.0,
        -0.6766749226751637,
        -0.6549291607806038,
        -0.6232632862672309,
        -0.5607880637223228,
        -0.7612070209644539,
        -0.6423460407756082,
        -0.6229932583160438,
        -0.6153778292605564,
        -0.7334027326226675,
        -0.726733772529374,
        -0.7576261513980884,
        -0.7373418236674698,
        -0.5811305611131525,
        -0.7332375200499148,
        -0.6112028623270993,
        -0.6482536971635582,
        -0.5711570243283829,
        -0.7462311504187376,
        -0.7546411089445467,
        -0.757520594715916
      ],
      [
        -0.7604741829996701,
        -0.7277672551243188,
        -0.6558322372530982,
        -0.7541303078041337,
        -0.7214916390131103,
        -0.758444758185117,
        -0.73903774725337,
        -0.7471614798244635,
        -0.6766749226751637,
        1.0,
        -0.35681571354812486,
        -0.7300011415960576,
        -0.7305176018400237,
        -0.7558205035784094,
        -0.6871747854838375,
        -0.7384527394782183,
        -0.5967484775986971,
        -0.7332767317473148,
        -0.7339036922677324,
        -0.7525699525468755,
        -0.7556727483112731,
        -0.7365520243593775,
        -0.734502174590707,
        -0.7268366076479402,
        -0.7303671178789668,
        -0.7325912096894953,
        -0.7440267207589728,
        -0.7585541630191694,
        -0.7541496720295604
      ],
      [
        -0.7602464712268019,
        -0.725225210442633,
        -0.7421444484904542,
        -0.7524335568604774,
        -0.7069448909559504,
        -0.7586725081815109,
        -0.7448987873627404,
        -0.7421708630922683,
        -0.6549291607806038,
        -0.35681571354812486,
        1.0,
        -0.7304036746691147,
        -0.7281586194108405,
        -0.7563718667245143,
        -0.7052966744789064,
        -0.7392307192324866,
        -0.5320842438312626,
        -0.7314097437469766,
        -0.7417535766538421,
        -0.7621562128707,
        -0.7535270984592515,
        -0.7244453326901223,
        -0.7256810674072737,
        -0.7236972269022961,
        -0.7025317987038011,
        -0.7317219382265628,
        -0.7142587668803031,
        -0.753341013595151,
        -0.7574584781595894
      ],
      [
        -0.7573643601074682,
        -0.6254569015598619,
        -0.7387187827249597,
        -0.7565285324649673,
        -0.6462010799375864,
        -0.7583639181414339,
        -0.7375406423240929,
        -0.7421986905697677,
        -0.6232632862672309,
        -0.7300011415960576,
        -0.7304036746691147,
        1.0,
        -0.5802576952837927,
        -0.7591660371205102,
        -0.6050303873482674,
        -0.5972431597652819,
        -0.7168149634262164,
        -0.7529771107240749,
        -0.7407555903813432,
        -0.7566554538526291,
        -0.7491998690509047,
        -0.6021915215728125,
        -0.6827853970725202,
        -0.5035911927825825,
        -0.5626918332039936,
        -0.5969695237349695,
        -0.7480604712037161,
        -0.7506351490619547,
        -0.7423818671232569
      ],
      [
        -0.7005946503709305,
        -0.5944167115619134,
        -0.7345518646219589,
        -0.7423540320886162,
        -0.6974445918211142,
        -0.7456106964812949,
        -0.7456738301803092,
        -0.7186978540661834,
        -0.5607880637223228,
        -0.7305176018400237,
        -0.7281586194108405,
        -0.5802576952837927,
        1.0,
        -0.7550285736423151,
        -0.663810561482124,
        -0.6342757527607511,
        -0.7065652803815953,
        -0.7318768358496048,
        -0.7374363809996329,
        -0.7595358433145818,
        -0.731393929027097,
        -0.6319764840948051,
        -0.7287189781424128,
        -0.5619133228626139,
        -0.674348968970059,
        -0.33926202305194686,
        -0.7351538161687415,
        -0.7536266729555194,
        -0.7537875806127899
      ],
      [
        -0.758753236908088,
        -0.7587530478292541,
        -0.7465854511731103,
        -0.7389665418643272,
        -0.7486855017479837,
        -0.7319871887949736,
        -0.7620358212827336,
        -0.7495346716323285,
        -0.7612070209644539,
        -0.7558205035784094,
        -0.7563718667245143,
        -0.7591660371205102,
        -0.7550285736423151,
        1.0,
        -0.7480132588053432,
        -0.7563402616150521,
        -0.7577438548694347,
        -0.7600908779182993,
        -0.7254198599573103,
        -0.7581001981974267,
        -0.7539026331374081,
        -0.757742523719511,
        -0.7547565013607532,
        -0.7616376658802184,
        -0.7551823550554426,
        -0.7583744406912469,
        -0.7522423039846402,
        -0.7598013398376322,
        -0.7142392778543352
      ],
      [
        -0.7131311245900028,
        -0.6626404697809917,
        -0.7159651641457851,
        -0.7363778731189641,
        -0.43318502570256945,
        -0.7148443656941743,
        -0.7064558047402691,
        -0.6958008740275587,
        -0.6423460407756082,
        -0.6871747854838375,
        -0.7052966744789064,
        -0.6050303873482674,
        -0.663810561482124,
        -0.7480132588053432,
        1.0,
        -0.63582979126949,
        -0.7115623299085783,
        -0.7395761769530214,
        -0.6730834998979578,
        -0.7438628610329157,
        -0.7380720660176596,
        -0.6276466998924632,
        -0.6727763681486193,
        -0.6682137623730796,
        -0.40665567605680525,
        -0.6494561102859403,
        -0.7192961824708544,
        -0.7488436671783494,
        -0.7352705270249039
      ],
      [
        -0.7321697335872479,
        -0.6816697535814524,
        -0.7441675653199951,
        -0.7541991066731837,
        -0.66260634170586,
        -0.752761495202942,
        -0.732693477411666,
        -0.7098105209314559,
        -0.6229932583160438,
        -0.7384527394782183,
        -0.7392307192324866,
        -0.5972431597652819,
        -0.6342757527607511,
        -0.7563402616150521,
        -0.63582979126949,
        1.0,
        -0.7402600791584311,
        -0.7564052710177572,
        -0.736499046052988,
        -0.7553227104370124,
        -0.7512417265701989,
        -0.6009419231246738,
        -0.7294540404798007,
        -0.6091465262233196,
        -0.5993576954312567,
        -0.6563644365652326,
        -0.7502847604837174,
        -0.7525591457479515,
        -0.7558921310112469
      ],
      [
        -0.7552132804455529,
        -0.7364109916626853,
        -0.7165389494562227,
        -0.7557613480164713,
        -0.7286390795383442,
        -0.7602515140576687,
        -0.733286872926926,
        -0.7504724262605361,
        -0.6153778292605564,
        -0.5967484775986971,
        -0.5320842438312626,
        -0.7168149634262164,
        -0.7065652803815953,
        -0.7577438548694347,
        -0.7115623299085783,
        -0.7402600791584311,
        1.0,
        -0.5306135255330022,
        -0.7493253214885203,
        -0.7630152157060661,
        -0.7504421586187058,
        -0.7259277686470633,
        -0.740899674552272,
        -0.7202004247401349,
        -0.7317393035914724,
        -0.6955955653218009,
        -0.7394284099354842,
        -0.7560883152591815,
        -0.7574572433829831
      ],
      [
        -0.7418016293614005,
        -0.7581286056792912,
        -0.746488786004526,
        -0.7569593966678378,
        -0.7487487385281101,
        -0.7611693739638511,
        -0.7585020289519726,
        -0.755883912149542,
        -0.7334027326226675,
        -0.7332767317473148,
        -0.7314097437469766,
        -0.7529771107240749,
        -0.7318768358496048,
        -0.7600908779182993,
        -0.7395761769530214,
        -0.7564052710177572,
        -0.5306135255330022,
        1.0,
        -0.7552076375795019,
        -0.7632359249855973,
        -0.7494709889288105,
        -0.7512905507674192,
        -0.7521219379296509,
        -0.7440221206230431,
        -0.7537684896131864,
        -0.7330475182833538,
        -0.7272760047408677,
        -0.7637471563569724,
        -0.7599749330773982
      ],
      [
        -0.7255263723080515,
        -0.7270395888280482,
        -0.7298979211176989,
        -0.7246973017529926,
        -0.7011578171656396,
        -0.6882689364479962,
        -0.7470247569702227,
        -0.7229981300222574,
        -0.726733772529374,
        -0.7339036922677324,
        -0.7417535766538421,
        -0.7407555903813432,
        -0.7374363809996329,
        -0.7254198599573103,
        -0.6730834998979578,
        -0.736499046052988,
        -0.7493253214885203,
        -0.7552076375795019,
        1.0,
        -0.752631781419969,
        -0.7385521755574578,
        -0.7234183652201778,
        -0.7277482250761004,
        -0.7415435959365302,
        -0.7066600643889831,
        -0.7305943927080427,
        -0.7320415469097044,
        -0.752695763234386,
        -0.7043171041405446
      ],
      [
        -0.7634990651604345,
        -0.7562144342552954,
        -0.7544370228429196,
        -0.7630127480053444,
        -0.7473783261560223,
        -0.7572699527977378,
        -0.7612466791219745,
        -0.7530028366857677,
        -0.7576261513980884,
        -0.7525699525468755,
        -0.7621562128707,
        -0.7566554538526291,
        -0.7595358433145818,
        -0.7581001981974267,
        -0.7438628610329157,
        -0.7553227104370124,
        -0.7630152157060661,
        -0.7632359249855973,
        -0.752631781419969,
        1.0,
        -0.758742827899676,
        -0.7562220361722116,
        -0.7571704842816755,
        -0.7579182107126823,
        -0.7486430112266298,
        -0.7607608932426837,
        -0.7642878921130177,
        -0.7635916682878543,
        -0.7585093111706789
      ],
      [
        -0.6581776013191293,
        -0.7437985921343475,
        -0.5895259598196816,
        -0.7405359675197326,
        -0.738457073168059,
        -0.5958801142643031,
        -0.748455402039832,
        -0.4563865269899272,
        -0.7373418236674698,
        -0.7556727483112731,
        -0.7535270984592515,
        -0.7491998690509047,
        -0.731393929027097,
        -0.7539026331374081,
        -0.7380720660176596,
        -0.7512417265701989,
        -0.7504421586187058,
        -0.7494709889288105,
        -0.7385521755574578,
        -0.758742827899676,
        1.0,
        -0.7379512255599923,
        -0.5768702591414374,
        -0.7273913143344695,
        -0.7493230671214361,
        -0.740792290664694,
        -0.7437265240682869,
        -0.760576098897048,
        -0.7482801722992583
      ],
      [
        -0.7334514802205568,
        -0.6655556238645226,
        -0.7384085574808983,
        -0.7454166624081582,
        -0.6089690269130013,
        -0.7454182858887637,
        -0.7338701694578387,
        -0.7123214188061255,
        -0.5811305611131525,
        -0.7365520243593775,
        -0.7244453326901223,
        -0.6021915215728125,
        -0.6319764840948051,
        -0.757742523719511,
        -0.6276466998924632,
        -0.6009419231246738,
        -0.7259277686470633,
        -0.7512905507674192,
        -0.7234183652201778,
        -0.7562220361722116,
        -0.7379512255599923,
        1.0,
        -0.7125599571244663,
        -0.6186288482462545,
        -0.5855254405421133,
        -0.6472306075807497,
        -0.7310476798653849,
        -0.752221532981497,
        -0.7529445772984464
      ],
      [
        -0.6846649915885408,
        -0.7274602534489245,
        -0.6548099986721776,
        -0.7186437641821244,
        -0.6565248020254715,
        -0.6571625249571049,
        -0.7242722533429062,
        -0.23090911134505088,
        -0.7332375200499148,
        -0.734502174590707,
        -0.7256810674072737,
        -0.6827853970725202,
        -0.7287189781424128,
        -0.7547565013607532,
        -0.6727763681486193,
        -0.7294540404798007,
        -0.740899674552272,
        -0.7521219379296509,
        -0.7277482250761004,
        -0.7571704842816755,
        -0.5768702591414374,
        -0.7125599571244663,
        1.0,
        -0.6580986474351638,
        -0.6503211468427559,
        -0.7348706371562347,
        -0.7250755500430703,
        -0.7402134987645479,
        -0.749451358716588
      ],
      [
        -0.7458048968994212,
        -0.6332139579752543,
        -0.7362169824852345,
        -0.7562138845194697,
        -0.7253515004019253,
        -0.7500572464003217,
        -0.7393009284498803,
        -0.7068200037303543,
        -0.6112028623270993,
        -0.7268366076479402,
        -0.7236972269022961,
        -0.5035911927825825,
        -0.5619133228626139,
        -0.7616376658802184,
        -0.6682137623730796,
        -0.6091465262233196,
        -0.7202004247401349,
        -0.7440221206230431,
        -0.7415435959365302,
        -0.7579182107126823,
        -0.7273913143344695,
        -0.6186288482462545,
        -0.6580986474351638,
        1.0,
        -0.6569088173470454,
        -0.6099176178680743,
        -0.7498975220397532,
        -0.7502937732221974,
        -0.7520267944917156
      ],
      [
        -0.7550229481689352,
        -0.6742732526113588,
        -0.7288907817173251,
        -0.7536258736652284,
        0.018265359658506473,
        -0.7531840988476994,
        -0.7268660542868022,
        -0.7276207512239086,
        -0.6482536971635582,
        -0.7303671178789668,
        -0.7025317987038011,
        -0.5626918332039936,
        -0.674348968970059,
        -0.7551823550554426,
        -0.40665567605680525,
        -0.5993576954312567,
        -0.7317393035914724,
        -0.7537684896131864,
        -0.7066600643889831,
        -0.7486430112266298,
        -0.7493230671214361,
        -0.5855254405421133,
        -0.6503211468427559,
        -0.6569088173470454,
        1.0,
        -0.6749130634940057,
        -0.7422395912688795,
        -0.7505524727708253,
        -0.7567162310340071
      ],
      [
        -0.7337902127584472,
        -0.5786484015395416,
        -0.7452330553526991,
        -0.7505111346732265,
        -0.7069301658920345,
        -0.7477252958571299,
        -0.7424927495047025,
        -0.7353620324797193,
        -0.5711570243283829,
        -0.7325912096894953,
        -0.7317219382265628,
        -0.5969695237349695,
        -0.33926202305194686,
        -0.7583744406912469,
        -0.6494561102859403,
        -0.6563644365652326,
        -0.6955955653218009,
        -0.7330475182833538,
        -0.7305943927080427,
        -0.7607608932426837,
        -0.740792290664694,
        -0.6472306075807497,
        -0.7348706371562347,
        -0.6099176178680743,
        -0.6749130634940057,
        1.0,
        -0.7407419838766617,
        -0.7561369957878618,
        -0.748279550259898
      ],
      [
        -0.6473251940027661,
        -0.6693311150406975,
        -0.7534221022519195,
        -0.7045561250513606,
        -0.7262836362843847,
        -0.7374549331427386,
        -0.7575333027631433,
        -0.7339046548308317,
        -0.7462311504187376,
        -0.7440267207589728,
        -0.7142587668803031,
        -0.7480604712037161,
        -0.7351538161687415,
        -0.7522423039846402,
        -0.7192961824708544,
        -0.7502847604837174,
        -0.7394284099354842,
        -0.7272760047408677,
        -0.7320415469097044,
        -0.7642878921130177,
        -0.7437265240682869,
        -0.7310476798653849,
        -0.7250755500430703,
        -0.7498975220397532,
        -0.7422395912688795,
        -0.7407419838766617,
        1.0,
        -0.7610225221968556,
        -0.7491264045142672
      ],
      [
        -0.7544845331558381,
        -0.7554201413557088,
        -0.7598303152872239,
        -0.7601360372501409,
        -0.7430959915381963,
        -0.7597097799120156,
        -0.7594095112988284,
        -0.7506404480924724,
        -0.7546411089445467,
        -0.7585541630191694,
        -0.753341013595151,
        -0.7506351490619547,
        -0.7536266729555194,
        -0.7598013398376322,
        -0.7488436671783494,
        -0.7525591457479515,
        -0.7560883152591815,
        -0.7637471563569724,
        -0.752695763234386,
        -0.7635916682878543,
        -0.760576098897048,
        -0.752221532981497,
        -0.7402134987645479,
        -0.7502937732221974,
        -0.7505524727708253,
        -0.7561369957878618,
        -0.7610225221968556,
        1.0,
        -0.7565068132493996
      ],
      [
        -0.7473055278311883,
        -0.7575179151044057,
        -0.6066793014356714,
        -0.7388958959402523,
        -0.7521053441653609,
        -0.7140349635370049,
        -0.7543075301594593,
        -0.7417281122550627,
        -0.757520594715916,
        -0.7541496720295604,
        -0.7574584781595894,
        -0.7423818671232569,
        -0.7537875806127899,
        -0.7142392778543352,
        -0.7352705270249039,
        -0.7558921310112469,
        -0.7574572433829831,
        -0.7599749330773982,
        -0.7043171041405446,
        -0.7585093111706789,
        -0.7482801722992583,
        -0.7529445772984464,
        -0.749451358716588,
        -0.7520267944917156,
        -0.7567162310340071,
        -0.748279550259898,
        -0.7491264045142672,
        -0.7565068132493996,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        18,
        8,
        3,
        2,
        15,
        5,
        3,
        2,
        3,
        10,
        1,
        1,
        0,
        3,
        4,
        3,
        1,
        2,
        5,
        3,
        0,
        1,
        4,
        1,
        7,
        1,
        3,
        0,
        0
      ],
      "2020-02": [
        13,
        13,
        3,
        2,
        14,
        11,
        10,
        6,
        5,
        14,
        6,
        2,
        0,
        1,
        2,
        4,
        6,
        3,
        5,
        0,
        1,
        4,
        7,
        1,
        8,
        1,
        13,
        6,
        3
      ],
      "2020-03": [
        18,
        8,
        4,
        3,
        7,
        4,
        6,
        1,
        3,
        7,
        3,
        1,
        0,
        1,
        1,
        2,
        3,
        5,
        0,
        0,
        0,
        3,
        6,
        1,
        11,
        4,
        5,
        1,
        2
      ],
      "2020-04": [
        26,
        4,
        4,
        2,
        6,
        2,
        2,
        2,
        3,
        15,
        6,
        2,
        1,
        6,
        2,
        4,
        5,
        4,
        0,
        1,
        0,
        10,
        7,
        1,
        7,
        3,
        8,
        2,
        1
      ],
      "2020-05": [
        34,
        13,
        2,
        3,
        37,
        4,
        5,
        7,
        13,
        14,
        6,
        6,
        1,
        6,
        13,
        4,
        12,
        6,
        4,
        9,
        0,
        12,
        11,
        2,
        22,
        10,
        9,
        5,
        5
      ],
      "2020-06": [
        28,
        14,
        3,
        4,
        16,
        5,
        4,
        2,
        7,
        7,
        7,
        4,
        0,
        4,
        7,
        0,
        3,
        6,
        4,
        1,
        2,
        9,
        5,
        2,
        12,
        2,
        6,
        1,
        5
      ],
      "2020-07": [
        24,
        8,
        3,
        6,
        12,
        15,
        4,
        9,
        3,
        20,
        2,
        2,
        0,
        1,
        9,
        4,
        2,
        4,
        3,
        2,
        0,
        5,
        9,
        1,
        9,
        8,
        8,
        0,
        3
      ],
      "2020-08": [
        41,
        13,
        2,
        3,
        15,
        1,
        10,
        6,
        17,
        34,
        10,
        2,
        0,
        3,
        8,
        9,
        10,
        8,
        1,
        2,
        1,
        5,
        8,
        4,
        10,
        8,
        16,
        2,
        1
      ],
      "2020-09": [
        17,
        3,
        0,
        1,
        8,
        4,
        6,
        2,
        5,
        9,
        1,
        3,
        0,
        2,
        3,
        2,
        7,
        3,
        1,
        4,
        0,
        3,
        2,
        0,
        6,
        3,
        7,
        0,
        2
      ],
      "2020-10": [
        65,
        16,
        6,
        6,
        27,
        7,
        9,
        9,
        14,
        28,
        13,
        3,
        1,
        6,
        8,
        6,
        21,
        3,
        6,
        3,
        5,
        8,
        5,
        3,
        18,
        9,
        19,
        3,
        1
      ],
      "2020-11": [
        26,
        20,
        1,
        5,
        26,
        8,
        4,
        2,
        10,
        18,
        10,
        3,
        1,
        7,
        7,
        7,
        8,
        5,
        2,
        3,
        0,
        6,
        7,
        0,
        9,
        6,
        17,
        1,
        1
      ],
      "2020-12": [
        25,
        5,
        1,
        6,
        9,
        1,
        1,
        2,
        8,
        14,
        3,
        3,
        2,
        9,
        6,
        0,
        2,
        3,
        1,
        3,
        1,
        5,
        6,
        0,
        5,
        4,
        5,
        2,
        4
      ],
      "2021-01": [
        16,
        6,
        2,
        2,
        8,
        4,
        1,
        3,
        3,
        2,
        5,
        0,
        0,
        5,
        6,
        0,
        3,
        2,
        3,
        3,
        0,
        0,
        3,
        1,
        5,
        1,
        9,
        0,
        1
      ],
      "2021-02": [
        24,
        11,
        4,
        3,
        16,
        11,
        6,
        1,
        7,
        7,
        4,
        2,
        0,
        5,
        12,
        6,
        4,
        9,
        5,
        3,
        3,
        2,
        8,
        2,
        17,
        7,
        12,
        6,
        5
      ],
      "2021-03": [
        26,
        4,
        6,
        10,
        20,
        5,
        4,
        0,
        4,
        7,
        4,
        1,
        1,
        6,
        9,
        2,
        4,
        2,
        5,
        1,
        1,
        2,
        5,
        0,
        11,
        6,
        10,
        4,
        1
      ],
      "2021-04": [
        31,
        12,
        5,
        7,
        32,
        5,
        5,
        4,
        14,
        14,
        14,
        7,
        3,
        5,
        13,
        7,
        11,
        2,
        7,
        7,
        1,
        9,
        12,
        1,
        21,
        11,
        3,
        0,
        0
      ],
      "2021-05": [
        25,
        12,
        1,
        2,
        20,
        9,
        5,
        2,
        5,
        5,
        5,
        1,
        1,
        3,
        5,
        6,
        7,
        4,
        5,
        0,
        1,
        5,
        7,
        1,
        6,
        7,
        7,
        3,
        2
      ],
      "2021-06": [
        26,
        13,
        2,
        7,
        28,
        13,
        9,
        7,
        24,
        9,
        10,
        6,
        5,
        9,
        11,
        12,
        7,
        4,
        2,
        5,
        1,
        10,
        9,
        0,
        16,
        7,
        13,
        3,
        4
      ],
      "2021-07": [
        35,
        11,
        7,
        4,
        16,
        16,
        5,
        3,
        6,
        10,
        8,
        1,
        0,
        9,
        16,
        3,
        10,
        4,
        9,
        2,
        1,
        9,
        4,
        2,
        15,
        6,
        8,
        2,
        5
      ],
      "2021-08": [
        31,
        6,
        1,
        1,
        18,
        0,
        2,
        5,
        7,
        7,
        2,
        6,
        1,
        3,
        8,
        4,
        1,
        3,
        2,
        4,
        0,
        2,
        6,
        2,
        12,
        5,
        7,
        0,
        3
      ],
      "2021-09": [
        22,
        7,
        8,
        9,
        13,
        2,
        6,
        3,
        5,
        18,
        7,
        1,
        3,
        5,
        8,
        4,
        3,
        1,
        4,
        4,
        0,
        8,
        7,
        1,
        7,
        7,
        7,
        1,
        1
      ],
      "2021-10": [
        45,
        17,
        5,
        7,
        31,
        16,
        10,
        8,
        20,
        21,
        6,
        5,
        1,
        7,
        38,
        5,
        15,
        15,
        11,
        8,
        1,
        10,
        6,
        4,
        21,
        10,
        20,
        4,
        1
      ],
      "2021-11": [
        27,
        16,
        2,
        5,
        19,
        7,
        8,
        2,
        6,
        10,
        3,
        3,
        0,
        6,
        8,
        4,
        8,
        4,
        0,
        3,
        1,
        5,
        6,
        1,
        11,
        8,
        13,
        1,
        5
      ],
      "2021-12": [
        21,
        10,
        4,
        2,
        10,
        2,
        10,
        5,
        0,
        10,
        2,
        3,
        2,
        3,
        8,
        0,
        3,
        1,
        4,
        2,
        3,
        5,
        5,
        0,
        9,
        2,
        11,
        2,
        4
      ],
      "2022-01": [
        16,
        7,
        4,
        2,
        15,
        2,
        9,
        5,
        8,
        6,
        1,
        3,
        2,
        11,
        4,
        1,
        6,
        3,
        5,
        5,
        0,
        4,
        7,
        0,
        10,
        2,
        6,
        2,
        4
      ],
      "2022-02": [
        38,
        18,
        9,
        6,
        15,
        10,
        4,
        2,
        12,
        14,
        11,
        4,
        2,
        3,
        10,
        1,
        6,
        4,
        6,
        1,
        0,
        7,
        17,
        0,
        19,
        8,
        7,
        1,
        7
      ],
      "2022-03": [
        37,
        19,
        8,
        10,
        22,
        10,
        19,
        7,
        17,
        34,
        15,
        8,
        5,
        6,
        26,
        5,
        15,
        10,
        3,
        4,
        2,
        10,
        11,
        5,
        32,
        9,
        16,
        2,
        7
      ],
      "2022-04": [
        40,
        12,
        5,
        4,
        30,
        5,
        9,
        13,
        16,
        22,
        10,
        2,
        1,
        6,
        29,
        11,
        9,
        8,
        3,
        5,
        1,
        15,
        11,
        1,
        17,
        13,
        11,
        2,
        4
      ],
      "2022-05": [
        21,
        10,
        4,
        3,
        16,
        3,
        3,
        5,
        11,
        6,
        4,
        5,
        1,
        3,
        13,
        4,
        4,
        4,
        5,
        5,
        0,
        11,
        8,
        1,
        13,
        5,
        7,
        1,
        3
      ],
      "2022-06": [
        31,
        17,
        4,
        4,
        22,
        9,
        9,
        8,
        15,
        12,
        8,
        4,
        3,
        6,
        18,
        5,
        8,
        4,
        7,
        8,
        0,
        8,
        16,
        2,
        18,
        9,
        9,
        1,
        7
      ],
      "2022-07": [
        21,
        6,
        4,
        7,
        17,
        5,
        13,
        3,
        15,
        12,
        4,
        0,
        3,
        2,
        13,
        4,
        6,
        4,
        4,
        5,
        3,
        2,
        13,
        1,
        20,
        8,
        12,
        5,
        1
      ],
      "2022-08": [
        34,
        3,
        5,
        4,
        10,
        2,
        6,
        6,
        5,
        8,
        4,
        2,
        0,
        7,
        9,
        2,
        3,
        4,
        1,
        3,
        2,
        1,
        5,
        3,
        6,
        2,
        5,
        2,
        7
      ],
      "2022-09": [
        42,
        4,
        3,
        6,
        14,
        4,
        4,
        6,
        7,
        18,
        3,
        4,
        0,
        5,
        12,
        1,
        6,
        2,
        2,
        4,
        0,
        5,
        5,
        2,
        12,
        5,
        8,
        1,
        1
      ],
      "2022-10": [
        35,
        13,
        6,
        7,
        37,
        9,
        13,
        11,
        19,
        18,
        13,
        10,
        3,
        7,
        39,
        10,
        12,
        9,
        6,
        8,
        0,
        22,
        20,
        2,
        37,
        6,
        12,
        3,
        4
      ],
      "2022-11": [
        51,
        24,
        5,
        2,
        31,
        4,
        17,
        9,
        25,
        14,
        9,
        3,
        5,
        7,
        38,
        9,
        9,
        8,
        5,
        8,
        4,
        11,
        11,
        1,
        17,
        10,
        13,
        0,
        2
      ],
      "2022-12": [
        28,
        10,
        5,
        3,
        11,
        1,
        4,
        2,
        11,
        3,
        2,
        0,
        1,
        3,
        17,
        7,
        5,
        3,
        3,
        1,
        0,
        11,
        9,
        2,
        8,
        4,
        6,
        1,
        1
      ],
      "2023-01": [
        32,
        5,
        2,
        5,
        3,
        2,
        3,
        4,
        10,
        4,
        3,
        1,
        3,
        3,
        4,
        2,
        2,
        2,
        2,
        1,
        1,
        2,
        7,
        2,
        4,
        2,
        3,
        1,
        2
      ],
      "2023-02": [
        18,
        20,
        0,
        1,
        20,
        6,
        9,
        3,
        10,
        12,
        3,
        4,
        1,
        0,
        16,
        0,
        7,
        1,
        5,
        1,
        0,
        5,
        12,
        0,
        12,
        1,
        7,
        2,
        1
      ],
      "2023-03": [
        41,
        9,
        2,
        8,
        22,
        11,
        7,
        9,
        17,
        11,
        11,
        5,
        5,
        5,
        21,
        5,
        3,
        6,
        1,
        4,
        0,
        6,
        19,
        3,
        15,
        6,
        11,
        4,
        3
      ],
      "2023-04": [
        24,
        4,
        2,
        9,
        12,
        2,
        7,
        12,
        6,
        3,
        1,
        6,
        1,
        6,
        7,
        5,
        3,
        4,
        3,
        5,
        0,
        6,
        3,
        1,
        8,
        2,
        7,
        1,
        4
      ],
      "2023-05": [
        43,
        12,
        7,
        8,
        26,
        8,
        13,
        17,
        33,
        19,
        7,
        13,
        4,
        7,
        39,
        21,
        12,
        6,
        7,
        10,
        1,
        25,
        20,
        4,
        32,
        9,
        12,
        5,
        5
      ],
      "2023-06": [
        49,
        14,
        4,
        12,
        30,
        12,
        15,
        7,
        18,
        11,
        9,
        5,
        5,
        3,
        39,
        10,
        13,
        7,
        5,
        1,
        3,
        13,
        20,
        4,
        38,
        6,
        14,
        5,
        1
      ],
      "2023-07": [
        48,
        9,
        3,
        9,
        17,
        2,
        5,
        6,
        12,
        11,
        3,
        5,
        2,
        4,
        16,
        7,
        7,
        0,
        7,
        2,
        1,
        9,
        15,
        2,
        21,
        5,
        5,
        6,
        2
      ],
      "2023-08": [
        34,
        7,
        7,
        9,
        13,
        5,
        9,
        7,
        13,
        9,
        7,
        4,
        1,
        3,
        15,
        2,
        8,
        6,
        8,
        5,
        3,
        7,
        12,
        2,
        14,
        8,
        5,
        3,
        3
      ],
      "2023-09": [
        58,
        27,
        13,
        10,
        40,
        16,
        16,
        26,
        25,
        16,
        15,
        8,
        8,
        9,
        31,
        12,
        18,
        12,
        7,
        6,
        4,
        22,
        18,
        4,
        27,
        10,
        8,
        1,
        4
      ],
      "2023-10": [
        43,
        21,
        6,
        8,
        20,
        6,
        11,
        6,
        13,
        10,
        15,
        3,
        3,
        4,
        29,
        11,
        5,
        8,
        3,
        1,
        2,
        13,
        17,
        3,
        18,
        8,
        8,
        4,
        5
      ],
      "2023-11": [
        33,
        6,
        2,
        12,
        14,
        4,
        5,
        9,
        10,
        4,
        6,
        2,
        3,
        6,
        11,
        6,
        7,
        3,
        1,
        2,
        3,
        6,
        9,
        3,
        6,
        5,
        4,
        4,
        4
      ],
      "2023-12": [
        38,
        12,
        5,
        9,
        18,
        6,
        16,
        6,
        8,
        10,
        12,
        2,
        1,
        3,
        22,
        3,
        6,
        3,
        4,
        4,
        1,
        9,
        13,
        3,
        8,
        4,
        7,
        4,
        4
      ],
      "2024-01": [
        38,
        11,
        4,
        15,
        24,
        8,
        11,
        11,
        12,
        4,
        8,
        5,
        6,
        2,
        15,
        7,
        9,
        10,
        2,
        4,
        1,
        5,
        18,
        3,
        16,
        8,
        10,
        5,
        3
      ],
      "2024-02": [
        43,
        7,
        3,
        13,
        13,
        2,
        10,
        13,
        15,
        8,
        7,
        9,
        3,
        5,
        18,
        6,
        2,
        6,
        5,
        0,
        0,
        8,
        8,
        0,
        9,
        3,
        8,
        3,
        3
      ],
      "2024-03": [
        35,
        10,
        3,
        15,
        12,
        11,
        16,
        5,
        13,
        7,
        3,
        2,
        3,
        1,
        9,
        6,
        2,
        5,
        4,
        4,
        1,
        10,
        18,
        1,
        13,
        3,
        5,
        2,
        5
      ],
      "2024-04": [
        38,
        4,
        6,
        2,
        9,
        4,
        8,
        5,
        8,
        7,
        2,
        1,
        0,
        3,
        12,
        1,
        8,
        6,
        4,
        2,
        3,
        6,
        12,
        1,
        11,
        4,
        4,
        1,
        0
      ],
      "2024-05": [
        36,
        9,
        5,
        6,
        10,
        1,
        10,
        11,
        7,
        8,
        3,
        2,
        5,
        7,
        11,
        2,
        5,
        7,
        4,
        3,
        0,
        8,
        8,
        2,
        17,
        2,
        3,
        4,
        3
      ],
      "2024-06": [
        71,
        24,
        14,
        11,
        37,
        15,
        17,
        17,
        51,
        17,
        14,
        9,
        6,
        7,
        38,
        22,
        14,
        15,
        10,
        11,
        2,
        25,
        22,
        0,
        38,
        7,
        11,
        5,
        7
      ],
      "2024-07": [
        78,
        8,
        8,
        6,
        17,
        6,
        13,
        12,
        23,
        18,
        10,
        5,
        4,
        7,
        19,
        9,
        7,
        5,
        14,
        6,
        3,
        10,
        17,
        0,
        20,
        1,
        10,
        5,
        1
      ],
      "2024-08": [
        48,
        12,
        11,
        14,
        13,
        6,
        7,
        11,
        22,
        12,
        5,
        5,
        3,
        3,
        25,
        8,
        13,
        5,
        4,
        6,
        2,
        2,
        16,
        5,
        12,
        5,
        10,
        2,
        5
      ],
      "2024-09": [
        76,
        21,
        14,
        15,
        21,
        11,
        21,
        24,
        45,
        27,
        20,
        9,
        9,
        4,
        34,
        14,
        21,
        17,
        7,
        6,
        2,
        9,
        19,
        5,
        32,
        17,
        13,
        6,
        15
      ],
      "2024-10": [
        78,
        8,
        8,
        5,
        15,
        6,
        15,
        18,
        29,
        11,
        7,
        3,
        4,
        5,
        15,
        9,
        10,
        7,
        7,
        3,
        2,
        16,
        14,
        3,
        27,
        10,
        5,
        7,
        9
      ],
      "2024-11": [
        44,
        4,
        7,
        9,
        12,
        5,
        7,
        11,
        11,
        5,
        6,
        1,
        0,
        4,
        9,
        9,
        4,
        4,
        5,
        3,
        2,
        4,
        9,
        5,
        11,
        7,
        9,
        6,
        4
      ],
      "2024-12": [
        54,
        11,
        8,
        7,
        10,
        1,
        10,
        19,
        19,
        12,
        5,
        10,
        8,
        5,
        13,
        9,
        2,
        4,
        7,
        4,
        1,
        9,
        21,
        2,
        17,
        1,
        2,
        4,
        1
      ],
      "2025-01": [
        52,
        8,
        6,
        7,
        14,
        4,
        13,
        10,
        14,
        6,
        4,
        9,
        3,
        4,
        16,
        12,
        11,
        10,
        2,
        2,
        1,
        7,
        17,
        0,
        20,
        6,
        9,
        11,
        5
      ],
      "2025-02": [
        35,
        14,
        8,
        4,
        8,
        6,
        7,
        15,
        12,
        3,
        2,
        1,
        1,
        2,
        9,
        11,
        7,
        1,
        5,
        1,
        2,
        9,
        9,
        3,
        16,
        7,
        6,
        2,
        2
      ],
      "2025-03": [
        45,
        12,
        2,
        3,
        7,
        5,
        7,
        12,
        7,
        4,
        3,
        2,
        1,
        1,
        17,
        9,
        3,
        2,
        6,
        0,
        3,
        8,
        17,
        4,
        8,
        3,
        9,
        4,
        4
      ],
      "2025-04": [
        31,
        3,
        3,
        5,
        6,
        4,
        4,
        7,
        8,
        4,
        3,
        2,
        2,
        2,
        7,
        5,
        6,
        3,
        7,
        1,
        1,
        9,
        9,
        0,
        6,
        3,
        6,
        2,
        0
      ],
      "2025-05": [
        90,
        21,
        18,
        8,
        24,
        6,
        21,
        25,
        32,
        13,
        8,
        10,
        6,
        6,
        22,
        19,
        21,
        9,
        11,
        8,
        2,
        14,
        26,
        8,
        37,
        15,
        15,
        13,
        5
      ],
      "2025-06": [
        105,
        17,
        6,
        12,
        27,
        5,
        16,
        16,
        30,
        11,
        11,
        4,
        2,
        3,
        37,
        21,
        15,
        6,
        6,
        9,
        0,
        14,
        16,
        3,
        33,
        23,
        10,
        3,
        3
      ],
      "2025-07": [
        82,
        20,
        7,
        12,
        14,
        14,
        7,
        17,
        23,
        6,
        11,
        4,
        3,
        4,
        25,
        18,
        10,
        4,
        6,
        4,
        1,
        7,
        10,
        3,
        24,
        13,
        13,
        2,
        7
      ],
      "2025-08": [
        66,
        15,
        7,
        9,
        15,
        4,
        19,
        11,
        22,
        11,
        4,
        4,
        4,
        4,
        19,
        12,
        7,
        3,
        8,
        8,
        1,
        13,
        18,
        6,
        21,
        8,
        12,
        5,
        3
      ],
      "2025-09": [
        27,
        4,
        8,
        7,
        14,
        4,
        7,
        6,
        8,
        5,
        2,
        0,
        1,
        4,
        5,
        4,
        1,
        2,
        4,
        0,
        0,
        4,
        4,
        1,
        7,
        2,
        3,
        0,
        2
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Do Music Generation Models Encode Music Theory?",
          "year": "2024-10",
          "abstract": "Music foundation models possess impressive music generation capabilities.\nWhen people compose music, they may infuse their understanding of music into\ntheir work, by using notes and intervals to craft melodies, chords to build\nprogressions, and tempo to create a rhythmic feel. To what extent is this true\nof music generation models? More specifically, are fundamental Western music\ntheory concepts observable within the \"inner workings\" of these models? Recent\nwork proposed leveraging latent audio representations from music generation\nmodels towards music information retrieval tasks (e.g. genre classification,\nemotion recognition), which suggests that high-level musical characteristics\nare encoded within these models. However, probing individual music theory\nconcepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus,\nwe introduce SynTheory, a synthetic MIDI and audio music theory dataset,\nconsisting of tempos, time signatures, notes, intervals, scales, chords, and\nchord progressions concepts. We then propose a framework to probe for these\nmusic theory concepts in music foundation models (Jukebox and MusicGen) and\nassess how strongly they encode these concepts within their internal\nrepresentations. Our findings suggest that music theory concepts are\ndiscernible within foundation models and that the degree to which they are\ndetectable varies by model size and layer.",
          "arxiv_id": "2410.00872v1"
        },
        {
          "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
          "year": "2025-01",
          "abstract": "In recent years, remarkable advancements in artificial intelligence-generated\ncontent (AIGC) have been achieved in the fields of image synthesis and text\ngeneration, generating content comparable to that produced by humans. However,\nthe quality of AI-generated music has not yet reached this standard, primarily\ndue to the challenge of effectively controlling musical emotions and ensuring\nhigh-quality outputs. This paper presents a generalized symbolic music\ngeneration framework, XMusic, which supports flexible prompts (i.e., images,\nvideos, texts, tags, and humming) to generate emotionally controllable and\nhigh-quality symbolic music. XMusic consists of two core components, XProjector\nand XComposer. XProjector parses the prompts of various modalities into\nsymbolic music elements (i.e., emotions, genres, rhythms and notes) within the\nprojection space to generate matching music. XComposer contains a Generator and\na Selector. The Generator generates emotionally controllable and melodious\nmusic based on our innovative symbolic music representation, whereas the\nSelector identifies high-quality symbolic music by constructing a multi-task\nlearning scheme involving quality assessment, emotion recognition, and genre\nrecognition tasks. In addition, we build XMIDI, a large-scale symbolic music\ndataset that contains 108,023 MIDI files annotated with precise emotion and\ngenre labels. Objective and subjective evaluations show that XMusic\nsignificantly outperforms the current state-of-the-art methods with impressive\nmusic quality. Our XMusic has been awarded as one of the nine Highlights of\nCollectibles at WAIC 2023. The project homepage of XMusic is\nhttps://xmusic-project.github.io.",
          "arxiv_id": "2501.08809v1"
        },
        {
          "title": "What is missing in deep music generation? A study of repetition and structure in popular music",
          "year": "2022-09",
          "abstract": "Structure is one of the most essential aspects of music, and music structure\nis commonly indicated through repetition. However, the nature of repetition and\nstructure in music is still not well understood, especially in the context of\nmusic generation, and much remains to be explored with Music Information\nRetrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and\nAmerican) illustrate important music construction principles: (1) structure\nexists at multiple hierarchical levels, (2) songs use repetition and limited\nvocabulary so that individual songs do not follow general statistics of song\ncollections, (3) structure interacts with rhythm, melody, harmony, and\npredictability, and (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy. These and other\nfindings offer challenges as well as opportunities for deep-learning music\ngeneration and suggest new formal music criteria and evaluation methods. Music\nfrom recent music generation systems is analyzed and compared to human-composed\nmusic in our datasets, often revealing striking differences from a structural\nperspective.",
          "arxiv_id": "2209.00182v1"
        }
      ],
      "1": [
        {
          "title": "SRIB-LEAP submission to Far-field Multi-Channel Speech Enhancement Challenge for Video Conferencing",
          "year": "2021-06",
          "abstract": "This paper presents the details of the SRIB-LEAP submission to the\nConferencingSpeech challenge 2021. The challenge involved the task of\nmulti-channel speech enhancement to improve the quality of far field speech\nfrom microphone arrays in a video conferencing room. We propose a two stage\nmethod involving a beamformer followed by single channel enhancement. For the\nbeamformer, we incorporated self-attention mechanism as inter-channel\nprocessing layer in the filter-and-sum network (FaSNet), an end-to-end\ntime-domain beamforming system. The single channel speech enhancement is done\nin log spectral domain using convolution neural network (CNN)-long short term\nmemory (LSTM) based architecture. We achieved improvements in objective quality\nmetrics - perceptual evaluation of speech quality (PESQ) of 0.5 on the noisy\ndata. On subjective quality evaluation, the proposed approach improved the mean\nopinion score (MOS) by an absolute measure of 0.9 over the noisy audio.",
          "arxiv_id": "2106.12763v1"
        },
        {
          "title": "Self-attending RNN for Speech Enhancement to Improve Cross-corpus Generalization",
          "year": "2021-05",
          "abstract": "Deep neural networks (DNNs) represent the mainstream methodology for\nsupervised speech enhancement, primarily due to their capability to model\ncomplex functions using hierarchical representations. However, a recent study\nrevealed that DNNs trained on a single corpus fail to generalize to untrained\ncorpora, especially in low signal-to-noise ratio (SNR) conditions. Developing a\nnoise, speaker, and corpus independent speech enhancement algorithm is\nessential for real-world applications. In this study, we propose a\nself-attending recurrent neural network, or attentive recurrent network (ARN),\nfor time-domain speech enhancement to improve cross-corpus generalization. ARN\ncomprises of recurrent neural networks (RNNs) augmented with self-attention\nblocks and feedforward blocks. We evaluate ARN on different corpora with\nnonstationary noises in low SNR conditions. Experimental results demonstrate\nthat ARN substantially outperforms competitive approaches to time-domain speech\nenhancement, such as RNNs and dual-path ARNs. Additionally, we report an\nimportant finding that the two popular approaches to speech enhancement:\ncomplex spectral mapping and time-domain enhancement, obtain similar results\nfor RNN and ARN with large-scale training. We also provide a challenging subset\nof the test set used in this study for evaluating future algorithms and\nfacilitating direct comparisons.",
          "arxiv_id": "2105.12831v3"
        },
        {
          "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
          "year": "2022-10",
          "abstract": "It has been shown that the intelligibility of noisy speech can be improved by\nspeech enhancement algorithms. However, speech enhancement has not been\nestablished as an effective frontend for robust automatic speech recognition\n(ASR) in noisy conditions compared to an ASR model trained on noisy speech\ndirectly. The divide between speech enhancement and ASR impedes the progress of\nrobust ASR systems especially as speech enhancement has made big strides in\nrecent years. In this work, we focus on eliminating this divide with an ARN\n(attentive recurrent network) based time-domain enhancement model. The proposed\nsystem fully decouples speech enhancement and an acoustic model trained only on\nclean speech. Results on the CHiME-2 corpus show that ARN enhanced speech\ntranslates to improved ASR results. The proposed system achieves $6.28\\%$\naverage word error rate, outperforming the previous best by $19.3\\%$\nrelatively.",
          "arxiv_id": "2210.13318v3"
        }
      ],
      "2": [
        {
          "title": "Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning",
          "year": "2021-06",
          "abstract": "Previous works have shown that automatic speaker verification (ASV) is\nseriously vulnerable to malicious spoofing attacks, such as replay, synthetic\nspeech, and recently emerged adversarial attacks. Great efforts have been\ndedicated to defending ASV against replay and synthetic speech; however, only a\nfew approaches have been explored to deal with adversarial attacks. All the\nexisting approaches to tackle adversarial attacks for ASV require the knowledge\nfor adversarial samples generation, but it is impractical for defenders to know\nthe exact attack algorithms that are applied by the in-the-wild attackers. This\nwork is among the first to perform adversarial defense for ASV without knowing\nthe specific attack algorithms. Inspired by self-supervised learning models\n(SSLMs) that possess the merits of alleviating the superficial noise in the\ninputs and reconstructing clean samples from the interrupted ones, this work\nregards adversarial perturbations as one kind of noise and conducts adversarial\ndefense for ASV by SSLMs. Specifically, we propose to perform adversarial\ndefense from two perspectives: 1) adversarial perturbation purification and 2)\nadversarial perturbation detection. Experimental results show that our\ndetection module effectively shields the ASV by detecting adversarial samples\nwith an accuracy of around 80%. Moreover, since there is no common metric for\nevaluating the adversarial defense performance for ASV, this work also\nformalizes evaluation metrics for adversarial defense considering both\npurification and detection based approaches into account. We sincerely\nencourage future works to benchmark their approaches based on the proposed\nevaluation framework.",
          "arxiv_id": "2106.00273v4"
        },
        {
          "title": "Toward Improving Synthetic Audio Spoofing Detection Robustness via Meta-Learning and Disentangled Training With Adversarial Examples",
          "year": "2024-08",
          "abstract": "Advances in automatic speaker verification (ASV) promote research into the\nformulation of spoofing detection systems for real-world applications. The\nperformance of ASV systems can be degraded severely by multiple types of\nspoofing attacks, namely, synthetic speech (SS), voice conversion (VC), replay,\ntwins and impersonation, especially in the case of unseen synthetic spoofing\nattacks. A reliable and robust spoofing detection system can act as a security\ngate to filter out spoofing attacks instead of having them reach the ASV\nsystem. A weighted additive angular margin loss is proposed to address the data\nimbalance issue, and different margins has been assigned to improve\ngeneralization to unseen spoofing attacks in this study. Meanwhile, we\nincorporate a meta-learning loss function to optimize differences between the\nembeddings of support versus query set in order to learn a\nspoofing-category-independent embedding space for utterances. Furthermore, we\ncraft adversarial examples by adding imperceptible perturbations to spoofing\nspeech as a data augmentation strategy, then we use an auxiliary batch\nnormalization (BN) to guarantee that corresponding normalization statistics are\nperformed exclusively on the adversarial examples. Additionally, A simple\nattention module is integrated into the residual block to refine the feature\nextraction process. Evaluation results on the Logical Access (LA) track of the\nASVspoof 2019 corpus provides confirmation of our proposed approaches'\neffectiveness in terms of a pooled EER of 0.87%, and a min t-DCF of 0.0277.\nThese advancements offer effective options to reduce the impact of spoofing\nattacks on voice recognition/authentication systems.",
          "arxiv_id": "2408.13341v1"
        },
        {
          "title": "Adversarial Transformation of Spoofing Attacks for Voice Biometrics",
          "year": "2022-01",
          "abstract": "Voice biometric systems based on automatic speaker verification (ASV) are\nexposed to \\textit{spoofing} attacks which may compromise their security. To\nincrease the robustness against such attacks, anti-spoofing or presentation\nattack detection (PAD) systems have been proposed for the detection of replay,\nsynthesis and voice conversion based attacks. Recently, the scientific\ncommunity has shown that PAD systems are also vulnerable to adversarial\nattacks. However, to the best of our knowledge, no previous work have studied\nthe robustness of full voice biometrics systems (ASV + PAD) to these new types\nof adversarial \\textit{spoofing} attacks. In this work, we develop a new\nadversarial biometrics transformation network (ABTN) which jointly processes\nthe loss of the PAD and ASV systems in order to generate white-box and\nblack-box adversarial \\textit{spoofing} attacks. The core idea of this system\nis to generate adversarial \\textit{spoofing} attacks which are able to fool the\nPAD system without being detected by the ASV system. The experiments were\ncarried out on the ASVspoof 2019 corpus, including both logical access (LA) and\nphysical access (PA) scenarios. The experimental results show that the proposed\nABTN clearly outperforms some well-known adversarial techniques in both\nwhite-box and black-box attack scenarios.",
          "arxiv_id": "2201.01226v1"
        }
      ],
      "3": [
        {
          "title": "Blind Identification of Binaural Room Impulse Responses from Smart Glasses",
          "year": "2024-03",
          "abstract": "Smart glasses are increasingly recognized as a key medium for augmented\nreality, offering a hands-free platform with integrated microphones and\nnon-ear-occluding loudspeakers to seamlessly mix virtual sound sources into the\nreal-world acoustic scene. To convincingly integrate virtual sound sources, the\nroom acoustic rendering of the virtual sources must match the real-world\nacoustics. Information about a user's acoustic environment however is typically\nnot available. This work uses a microphone array in a pair of smart glasses to\nblindly identify binaural room impulse responses (BRIRs) from a few seconds of\nspeech in the real-world environment. The proposed method uses dereverberation\nand beamforming to generate a pseudo reference signal that is used by a\nmultichannel Wiener filter to estimate room impulse responses which are then\nconverted to BRIRs. The multichannel room impulse responses can be used to\nestimate room acoustic parameters which is shown to outperform baseline\nalgorithms in the estimation of reverberation time and direct-to-reverberant\nenergy ratio. Results from a listening experiment further indicate that the\nestimated BRIRs often reproduce the real-world room acoustics perceptually more\nconvincingly than measured BRIRs from other rooms of similar size.",
          "arxiv_id": "2403.19217v2"
        },
        {
          "title": "Room geometry blind inference based on the localization of real sound source and first order reflections",
          "year": "2022-07",
          "abstract": "The conventional room geometry blind inference techniques with acoustic\nsignals are conducted based on the prior knowledge of the environment, such as\nthe room impulse response (RIR) or the sound source position, which will limit\nits application under unknown scenarios. To solve this problem, we have\nproposed a room geometry reconstruction method in this paper by using the\ngeometric relation between the direct signal and first-order reflections. In\naddition to the information of the compact microphone array itself, this method\ndoes not need any precognition of the environmental parameters. Besides, the\nlearning-based DNN models are designed and used to improve the accuracy and\nintegrity of the localization results of the direct source and first-order\nreflections. The direction of arrival (DOA) and time difference of arrival\n(TDOA) information of the direct and reflected signals are firstly estimated\nusing the proposed DCNN and TD-CNN models, which have higher sensitivity and\naccuracy than the conventional methods. Then the position of the sound source\nis inferred by integrating the DOA, TDOA and array height using the proposed\nDNN model. After that, the positions of image sources and corresponding\nboundaries are derived based on the geometric relation. Experimental results of\nboth simulations and real measurements verify the effectiveness and accuracy of\nthe proposed techniques compared with the conventional methods under different\nreverberant environments.",
          "arxiv_id": "2207.10478v2"
        },
        {
          "title": "Room impulse response reconstruction with physics-informed deep learning",
          "year": "2024-01",
          "abstract": "A method is presented for estimating and reconstructing the sound field\nwithin a room using physics-informed neural networks. By incorporating a\nlimited set of experimental room impulse responses as training data, this\napproach combines neural network processing capabilities with the underlying\nphysics of sound propagation, as articulated by the wave equation. The\nnetwork's ability to estimate particle velocity and intensity, in addition to\nsound pressure, demonstrates its capacity to represent the flow of acoustic\nenergy and completely characterise the sound field with only a few\nmeasurements. Additionally, an investigation into the potential of this network\nas a tool for improving acoustic simulations is conducted. This is due to its\nprofficiency in offering grid-free sound field mappings with minimal inference\ntime. Furthermore, a study is carried out which encompasses comparative\nanalyses against current approaches for sound field reconstruction.\nSpecifically, the proposed approach is evaluated against both data-driven\ntechniques and elementary wave-based regression methods. The results\ndemonstrate that the physics-informed neural network stands out when\nreconstructing the early part of the room impulse response, while\nsimultaneously allowing for complete sound field characterisation in the time\ndomain.",
          "arxiv_id": "2401.01206v1"
        }
      ],
      "4": [
        {
          "title": "Joint Beam Search Integrating CTC, Attention, and Transducer Decoders",
          "year": "2024-06",
          "abstract": "End-to-end automatic speech recognition (E2E-ASR) can be classified by its\ndecoder architectures, such as connectionist temporal classification (CTC),\nrecurrent neural network transducer (RNN-T), attention-based encoder-decoder,\nand Mask-CTC models. Each decoder architecture has advantages and\ndisadvantages, leading practitioners to switch between these different models\ndepending on application requirements. Instead of building separate models, we\npropose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and\nMask-CTC) share the same encoder -- we refer to this as 4D modeling. The 4D\nmodel is trained jointly, which will bring model regularization and maximize\nthe model robustness thanks to their complementary properties. To efficiently\ntrain the 4D model, we introduce a two-stage training strategy that stabilizes\nthe joint training. In addition, we propose three novel joint beam search\nalgorithms by combining three decoders (CTC, RNN-T, and attention) to further\nimprove performance. These three beam search algorithms differ in which decoder\nis used as the primary decoder. We carefully evaluate the performance and\ncomputational tradeoffs associated with each algorithm. Experimental results\ndemonstrate that the jointly trained 4D model outperforms the E2E-ASR models\ntrained with only one individual decoder. Furthermore, we demonstrate that the\nproposed joint beam search algorithm outperforms the previously proposed\nCTC/attention decoding.",
          "arxiv_id": "2406.02950v2"
        },
        {
          "title": "One In A Hundred: Select The Best Predicted Sequence from Numerous Candidates for Streaming Speech Recognition",
          "year": "2020-10",
          "abstract": "The RNN-Transducers and improved attention-based encoder-decoder models are\nwidely applied to streaming speech recognition. Compared with these two\nend-to-end models, the CTC model is more efficient in training and inference.\nHowever, it cannot capture the linguistic dependencies between the output\ntokens. Inspired by the success of two-pass end-to-end models, we introduce a\ntransformer decoder and the two-stage inference method into the streaming CTC\nmodel. During inference, the CTC decoder first generates many candidates in a\nstreaming fashion. Then the transformer decoder selects the best candidate\nbased on the corresponding acoustic encoded states. The second-stage\ntransformer decoder can be regarded as a conditional language model. We assume\nthat a large enough number and enough diversity of candidates generated in the\nfirst stage can compensate the CTC model for the lack of language modeling\nability. All the experiments are conducted on a Chinese Mandarin dataset\nAISHELL-1. The results show that our proposed model can implement streaming\ndecoding in a fast and straightforward way. Our model can achieve up to a 20%\nreduction in the character error rate than the baseline CTC model. In addition,\nour model can also perform non-streaming inference with only a little\nperformance degradation.",
          "arxiv_id": "2010.14791v3"
        },
        {
          "title": "Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition",
          "year": "2020-12",
          "abstract": "In this paper, we present a novel two-pass approach to unify streaming and\nnon-streaming end-to-end (E2E) speech recognition in a single model. Our model\nadopts the hybrid CTC/attention architecture, in which the conformer layers in\nthe encoder are modified. We propose a dynamic chunk-based attention strategy\nto allow arbitrary right context length. At inference time, the CTC decoder\ngenerates n-best hypotheses in a streaming way. The inference latency could be\neasily controlled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. This efficient\nrescoring process causes very little sentence-level latency. Our experiments on\nthe open 170-hour AISHELL-1 dataset show that, the proposed method can unify\nthe streaming and non-streaming model simply and efficiently. On the AISHELL-1\ntest set, our unified model achieves 5.60% relative character error rate (CER)\nreduction in non-streaming ASR compared to a standard non-streaming\ntransformer. The same model achieves 5.42% CER with 640ms latency in a\nstreaming ASR system.",
          "arxiv_id": "2012.05481v2"
        }
      ],
      "5": [
        {
          "title": "Improving Sound Event Detection In Domestic Environments Using Sound Separation",
          "year": "2020-07",
          "abstract": "Performing sound event detection on real-world recordings often implies\ndealing with overlapping target sound events and non-target sounds, also\nreferred to as interference or noise. Until now these problems were mainly\ntackled at the classifier level. We propose to use sound separation as a\npre-processing for sound event detection. In this paper we start from a sound\nseparation model trained on the Free Universal Sound Separation dataset and the\nDCASE 2020 task 4 sound event detection baseline. We explore different methods\nto combine separated sound sources and the original mixture within the sound\nevent detection. Furthermore, we investigate the impact of adapting the sound\nseparation model to the sound event detection data on both the sound separation\nand the sound event detection.",
          "arxiv_id": "2007.03932v1"
        },
        {
          "title": "Sound Event Detection Using Duration Robust Loss Function",
          "year": "2020-06",
          "abstract": "Many methods of sound event detection (SED) based on machine learning regard\na segmented time frame as one data sample to model training. However, the sound\ndurations of sound events vary greatly depending on the sound event class,\ne.g., the sound event ``fan'' has a long time duration, while the sound event\n``mouse clicking'' is instantaneous. The difference in the time duration\nbetween sound event classes thus causes a serious data imbalance problem in\nSED. In this paper, we propose a method for SED using a duration robust loss\nfunction, which can focus model training on sound events of short duration. In\nthe proposed method, we focus on a relationship between the duration of the\nsound event and the ease/difficulty of model training. In particular, many\nsound events of long duration (e.g., sound event ``fan'') are stationary\nsounds, which have less variation in their acoustic features and their model\ntraining is easy. Meanwhile, some sound events of short duration (e.g., sound\nevent ``object impact'') have more than one audio pattern, such as attack,\ndecay, and release parts. We thus apply a class-wise reweighting to the\nbinary-cross entropy loss function depending on the ease/difficulty of model\ntraining. Evaluation experiments conducted using TUT Sound Events 2016/2017 and\nTUT Acoustic Scenes 2016 datasets show that the proposed method respectively\nimproves the detection performance of sound events by 3.15 and 4.37 percentage\npoints in macro- and micro-Fscores compared with a conventional method using\nthe binary-cross entropy loss function.",
          "arxiv_id": "2006.15253v1"
        },
        {
          "title": "Event-Independent Network for Polyphonic Sound Event Localization and Detection",
          "year": "2020-09",
          "abstract": "Polyphonic sound event localization and detection is not only detecting what\nsound events are happening but localizing corresponding sound sources. This\nseries of tasks was first introduced in DCASE 2019 Task 3. In 2020, the sound\nevent localization and detection task introduces additional challenges in\nmoving sound sources and overlapping-event cases, which include two events of\nthe same type with two different direction-of-arrival (DoA) angles. In this\npaper, a novel event-independent network for polyphonic sound event\nlocalization and detection is proposed. Unlike the two-stage method we proposed\nin DCASE 2019 Task 3, this new network is fully end-to-end. Inputs to the\nnetwork are first-order Ambisonics (FOA) time-domain signals, which are then\nfed into a 1-D convolutional layer to extract acoustic features. The network is\nthen split into two parallel branches. The first branch is for sound event\ndetection (SED), and the second branch is for DoA estimation. There are three\ntypes of predictions from the network, SED predictions, DoA predictions, and\nevent activity detection (EAD) predictions that are used to combine the SED and\nDoA features for on-set and off-set estimation. All of these predictions have\nthe format of two tracks indicating that there are at most two overlapping\nevents. Within each track, there could be at most one event happening. This\narchitecture introduces a problem of track permutation. To address this\nproblem, a frame-level permutation invariant training method is used.\nExperimental results show that the proposed method can detect polyphonic sound\nevents and their corresponding DoAs. Its performance on the Task 3 dataset is\ngreatly increased as compared with that of the baseline method.",
          "arxiv_id": "2010.00140v1"
        }
      ],
      "6": [
        {
          "title": "Information Fusion in Attention Networks Using Adaptive and Multi-level Factorized Bilinear Pooling for Audio-visual Emotion Recognition",
          "year": "2021-11",
          "abstract": "Multimodal emotion recognition is a challenging task in emotion computing as\nit is quite difficult to extract discriminative features to identify the subtle\ndifferences in human emotions with abstract concept and multiple expressions.\nMoreover, how to fully utilize both audio and visual information is still an\nopen problem. In this paper, we propose a novel multimodal fusion attention\nnetwork for audio-visual emotion recognition based on adaptive and multi-level\nfactorized bilinear pooling (FBP). First, for the audio stream, a fully\nconvolutional network (FCN) equipped with 1-D attention mechanism and local\nresponse normalization is designed for speech emotion recognition. Next, a\nglobal FBP (G-FBP) approach is presented to perform audio-visual information\nfusion by integrating selfattention based video stream with the proposed audio\nstream. To improve G-FBP, an adaptive strategy (AG-FBP) to dynamically\ncalculate the fusion weight of two modalities is devised based on the\nemotion-related representation vectors from the attention mechanism of\nrespective modalities. Finally, to fully utilize the local emotion information,\nadaptive and multi-level FBP (AMFBP) is introduced by combining both\nglobal-trunk and intratrunk data in one recording on top of AG-FBP. Tested on\nthe IEMOCAP corpus for speech emotion recognition with only audio stream, the\nnew FCN method outperforms the state-ofthe-art results with an accuracy of\n71.40%. Moreover, validated on the AFEW database of EmotiW2019 sub-challenge\nand the IEMOCAP corpus for audio-visual emotion recognition, the proposed\nAM-FBP approach achieves the best accuracy of 63.09% and 75.49% respectively on\nthe test set.",
          "arxiv_id": "2111.08910v1"
        },
        {
          "title": "EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark",
          "year": "2024-06",
          "abstract": "Speech emotion recognition (SER) is an important part of human-computer\ninteraction, receiving extensive attention from both industry and academia.\nHowever, the current research field of SER has long suffered from the following\nproblems: 1) There are few reasonable and universal splits of the datasets,\nmaking comparing different models and methods difficult. 2) No commonly used\nbenchmark covers numerous corpus and languages for researchers to refer to,\nmaking reproduction a burden. In this paper, we propose EmoBox, an\nout-of-the-box multilingual multi-corpus speech emotion recognition toolkit,\nalong with a benchmark for both intra-corpus and cross-corpus settings. For\nintra-corpus settings, we carefully designed the data partitioning for\ndifferent datasets. For cross-corpus settings, we employ a foundation SER\nmodel, emotion2vec, to mitigate annotation errors and obtain a test set that is\nfully balanced in speakers and emotions distributions. Based on EmoBox, we\npresent the intra-corpus SER results of 10 pre-trained speech models on 32\nemotion datasets with 14 languages, and the cross-corpus SER results on 4\ndatasets with the fully balanced test sets. To the best of our knowledge, this\nis the largest SER benchmark, across language scopes and quantity scales. We\nhope that our toolkit and benchmark can facilitate the research of SER in the\ncommunity.",
          "arxiv_id": "2406.07162v1"
        },
        {
          "title": "Multimodal Speech Emotion Recognition Using Modality-specific Self-Supervised Frameworks",
          "year": "2023-12",
          "abstract": "Emotion recognition is a topic of significant interest in assistive robotics\ndue to the need to equip robots with the ability to comprehend human behavior,\nfacilitating their effective interaction in our society. Consequently,\nefficient and dependable emotion recognition systems supporting optimal\nhuman-machine communication are required. Multi-modality (including speech,\naudio, text, images, and videos) is typically exploited in emotion recognition\ntasks. Much relevant research is based on merging multiple data modalities and\ntraining deep learning models utilizing low-level data representations.\nHowever, most existing emotion databases are not large (or complex) enough to\nallow machine learning approaches to learn detailed representations. This paper\nexplores modalityspecific pre-trained transformer frameworks for\nself-supervised learning of speech and text representations for data-efficient\nemotion recognition while achieving state-of-the-art performance in recognizing\nemotions. This model applies feature-level fusion using nonverbal cue data\npoints from motion capture to provide multimodal speech emotion recognition.\nThe model was trained using the publicly available IEMOCAP dataset, achieving\nan overall accuracy of 77.58% for four emotions, outperforming state-of-the-art\napproaches",
          "arxiv_id": "2312.01568v1"
        }
      ],
      "7": [
        {
          "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
          "year": "2025-04",
          "abstract": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations.",
          "arxiv_id": "2504.02061v1"
        },
        {
          "title": "CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing",
          "year": "2024-01",
          "abstract": "There has been a long-standing quest for a unified audio-visual-text model to\nenable various multimodal understanding tasks, which mimics the listening,\nseeing and reading process of human beings. Humans tends to represent knowledge\nusing two separate systems: one for representing verbal (textual) information\nand one for representing non-verbal (visual and auditory) information. These\ntwo systems can operate independently but can also interact with each other.\nMotivated by this understanding of human cognition, in this paper, we introduce\nCoAVT -- a novel cognition-inspired Correlated Audio-Visual-Text pre-training\nmodel to connect the three modalities. It contains a joint audio-visual encoder\nthat learns to encode audio-visual synchronization information together with\nthe audio and visual content for non-verbal information, and a text encoder to\nhandle textual input for verbal information. To bridge the gap between\nmodalities, CoAVT employs a query encoder, which contains a set of learnable\nquery embeddings, and extracts the most informative audiovisual features of the\ncorresponding text. Additionally, to leverage the correspondences between audio\nand vision with language respectively, we also establish the audio-text and\nvisual-text bi-modal alignments upon the foundational audiovisual-text\ntri-modal alignment to enhance the multimodal representation learning. Finally,\nwe jointly optimize CoAVT model with three multimodal objectives: contrastive\nloss, matching loss and language modeling loss. Extensive experiments show that\nCoAVT can learn strong multimodal correlations and be generalized to various\ndownstream tasks. CoAVT establishes new state-of-the-art performance on\ntext-video retrieval task on AudioCaps for both zero-shot and fine-tuning\nsettings, audio-visual event classification and audio-visual retrieval tasks on\nAudioSet and VGGSound.",
          "arxiv_id": "2401.12264v2"
        },
        {
          "title": "AudioSetCaps: An Enriched Audio-Caption Dataset using Automated Generation Pipeline with Large Audio and Language Models",
          "year": "2024-11",
          "abstract": "With the emergence of audio-language models, constructing large-scale paired\naudio-language datasets has become essential yet challenging for model\ndevelopment, primarily due to the time-intensive and labour-heavy demands\ninvolved. While large language models (LLMs) have improved the efficiency of\nsynthetic audio caption generation, current approaches struggle to effectively\nextract and incorporate detailed audio information. In this paper, we propose\nan automated pipeline that integrates audio-language models for fine-grained\ncontent extraction, LLMs for synthetic caption generation, and a contrastive\nlanguage-audio pretraining (CLAP) model-based refinement process to improve the\nquality of captions. Specifically, we employ prompt chaining techniques in the\ncontent extraction stage to obtain accurate and fine-grained audio information,\nwhile we use the refinement process to mitigate potential hallucinations in the\ngenerated captions. Leveraging the AudioSet dataset and the proposed approach,\nwe create AudioSetCaps, a dataset comprising 1.9 million audio-caption pairs,\nthe largest audio-caption dataset at the time of writing. The models trained\nwith AudioSetCaps achieve state-of-the-art performance on audio-text retrieval\nwith R@1 scores of 46.3% for text-to-audio and 59.7% for audio-to-text\nretrieval and automated audio captioning with the CIDEr score of 84.8. As our\napproach has shown promising results with AudioSetCaps, we create another\ndataset containing 4.1 million synthetic audio-language pairs based on the\nYoutube-8M and VGGSound datasets. To facilitate research in audio-language\nlearning, we have made our pipeline, datasets with 6 million audio-language\npairs, and pre-trained models publicly available at\nhttps://github.com/JishengBai/AudioSetCaps.",
          "arxiv_id": "2411.18953v1"
        }
      ],
      "8": [
        {
          "title": "Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis",
          "year": "2021-07",
          "abstract": "Cross-speaker style transfer is crucial to the applications of multi-style\nand expressive speech synthesis at scale. It does not require the target\nspeakers to be experts in expressing all styles and to collect corresponding\nrecordings for model training. However, the performances of existing style\ntransfer methods are still far behind real application needs. The root causes\nare mainly twofold. Firstly, the style embedding extracted from single\nreference speech can hardly provide fine-grained and appropriate prosody\ninformation for arbitrary text to synthesize. Secondly, in these models the\ncontent/text, prosody, and speaker timbre are usually highly entangled, it's\ntherefore not realistic to expect a satisfied result when freely combining\nthese components, such as to transfer speaking style between speakers. In this\npaper, we propose a cross-speaker style transfer text-to-speech (TTS) model\nwith explicit prosody bottleneck. The prosody bottleneck builds up the kernels\naccounting for speaking style robustly, and disentangles the prosody from\ncontent and speaker timbre, therefore guarantees high quality cross-speaker\nstyle transfer. Evaluation result shows the proposed method even achieves\non-par performance with source speaker's speaker-dependent (SD) model in\nobjective measurement of prosody, and significantly outperforms the cycle\nconsistency and GMVAE-based baselines in objective and subjective evaluations.",
          "arxiv_id": "2107.12562v1"
        },
        {
          "title": "MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech",
          "year": "2024-10",
          "abstract": "Text-to-speech (TTS) systems that scale up the amount of training data have\nachieved significant improvements in zero-shot speech synthesis. However, these\nsystems have certain limitations: they require a large amount of training data,\nwhich increases costs, and often overlook prosody similarity. To address these\nissues, we propose MultiVerse, a zero-shot multi-task TTS system that is able\nto perform TTS or speech style transfer in zero-shot and cross-lingual\nconditions. MultiVerse requires much less training data than traditional\ndata-driven approaches. To ensure zero-shot performance even with limited data,\nwe leverage source-filter theory-based disentanglement, utilizing the prompt\nfor modeling filter-related and source-related representations. Additionally,\nto further enhance prosody similarity, we adopt a prosody modeling approach\ncombining prompt-based autoregressive and non-autoregressive methods.\nEvaluations demonstrate the remarkable zero-shot multi-task TTS performance of\nMultiVerse and show that MultiVerse not only achieves zero-shot TTS performance\ncomparable to data-driven TTS systems with much less data, but also\nsignificantly outperforms other zero-shot TTS systems trained with the same\nsmall amount of data. In particular, our novel prosody modeling technique\nsignificantly contributes to MultiVerse's ability to generate speech with high\nprosody similarity to the given prompts. Our samples are available at\nhttps://nc-ai.github.io/speech/publications/multiverse/index.html",
          "arxiv_id": "2410.03192v1"
        },
        {
          "title": "Expressive Text-to-Speech using Style Tag",
          "year": "2021-04",
          "abstract": "As recent text-to-speech (TTS) systems have been rapidly improved in speech\nquality and generation speed, many researchers now focus on a more challenging\nissue: expressive TTS. To control speaking styles, existing expressive TTS\nmodels use categorical style index or reference speech as style input. In this\nwork, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that\nutilizes a style tag written in natural language. Using a style-tagged TTS\ndataset and a pre-trained language model, we modeled the relationship between\nlinguistic embedding and speaking style domain, which enables our model to work\neven with style tags unseen during training. As style tag is written in natural\nlanguage, it can control speaking style in a more intuitive, interpretable, and\nscalable way compared with style index or reference speech. In addition, in\nterms of model architecture, we propose an efficient non-autoregressive (NAR)\nTTS architecture with single-stage training. The experimental result shows that\nST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech\nquality and expressiveness.",
          "arxiv_id": "2104.00436v2"
        }
      ],
      "9": [
        {
          "title": "The IDLAB VoxSRC-20 Submission: Large Margin Fine-Tuning and Quality-Aware Score Calibration in DNN Based Speaker Verification",
          "year": "2020-10",
          "abstract": "In this paper we propose and analyse a large margin fine-tuning strategy and\na quality-aware score calibration in text-independent speaker verification.\nLarge margin fine-tuning is a secondary training stage for DNN based speaker\nverification systems trained with margin-based loss functions. It enables the\nnetwork to create more robust speaker embeddings by enabling the use of longer\ntraining utterances in combination with a more aggressive margin penalty. Score\ncalibration is a common practice in speaker verification systems to map output\nscores to well-calibrated log-likelihood-ratios, which can be converted to\ninterpretable probabilities. By including quality features in the calibration\nsystem, the decision thresholds of the evaluation metrics become\nquality-dependent and more consistent across varying trial conditions. Applying\nboth enhancements on the ECAPA-TDNN architecture leads to state-of-the-art\nresults on all publicly available VoxCeleb1 test sets and contributed to our\nwinning submissions in the supervised verification tracks of the VoxCeleb\nSpeaker Recognition Challenge 2020.",
          "arxiv_id": "2010.11255v2"
        },
        {
          "title": "Self-Distillation Prototypes Network: Learning Robust Speaker Representations without Supervision",
          "year": "2023-08",
          "abstract": "Training speaker-discriminative and robust speaker verification systems\nwithout explicit speaker labels remains a persistent challenge. In this paper,\nwe propose a novel self-supervised speaker verification approach,\nSelf-Distillation Prototypes Network (SDPN), which effectively facilitates\nself-supervised speaker representation learning. SDPN assigns the\nrepresentation of the augmented views of an utterance to the same prototypes as\nthe representation of the original view, thereby enabling effective knowledge\ntransfer between the augmented and original views. Due to lack of negative\npairs in the SDPN training process, the network tends to align positive pairs\nquite closely in the embedding space, a phenomenon known as model collapse. To\nmitigate this problem, we introduce a diversity regularization term to\nembeddings in SDPN. Comprehensive experiments on the VoxCeleb datasets\ndemonstrate the superiority of SDPN among self-supervised speaker verification\napproaches. SDPN sets a new state-of-the-art on the VoxCeleb1 speaker\nverification evaluation benchmark, achieving Equal Error Rate 1.80%, 1.99%, and\n3.62% for trial VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1-H, without using any\nspeaker labels in training. Ablation studies show that both proposed learnable\nprototypes in self-distillation network and diversity regularization contribute\nto the verification performance.",
          "arxiv_id": "2308.02774v6"
        },
        {
          "title": "Neural PLDA Modeling for End-to-End Speaker Verification",
          "year": "2020-08",
          "abstract": "While deep learning models have made significant advances in supervised\nclassification problems, the application of these models for out-of-set\nverification tasks like speaker recognition has been limited to deriving\nfeature embeddings. The state-of-the-art x-vector PLDA based speaker\nverification systems use a generative model based on probabilistic linear\ndiscriminant analysis (PLDA) for computing the verification score. Recently, we\nhad proposed a neural network approach for backend modeling in speaker\nverification called the neural PLDA (NPLDA) where the likelihood ratio score of\nthe generative PLDA model is posed as a discriminative similarity function and\nthe learnable parameters of the score function are optimized using a\nverification cost. In this paper, we extend this work to achieve joint\noptimization of the embedding neural network (x-vector network) with the NPLDA\nnetwork in an end-to-end (E2E) fashion. This proposed end-to-end model is\noptimized directly from the acoustic features with a verification cost function\nand during testing, the model directly outputs the likelihood ratio score. With\nvarious experiments using the NIST speaker recognition evaluation (SRE) 2018\nand 2019 datasets, we show that the proposed E2E model improves significantly\nover the x-vector PLDA baseline speaker verification system.",
          "arxiv_id": "2008.04527v1"
        }
      ],
      "10": [
        {
          "title": "Attention-based Encoder-Decoder End-to-End Neural Diarization with Embedding Enhancer",
          "year": "2023-09",
          "abstract": "Deep neural network-based systems have significantly improved the performance\nof speaker diarization tasks. However, end-to-end neural diarization (EEND)\nsystems often struggle to generalize to scenarios with an unseen number of\nspeakers, while target speaker voice activity detection (TS-VAD) systems tend\nto be overly complex. In this paper, we propose a simple attention-based\nencoder-decoder network for end-to-end neural diarization (AED-EEND). In our\ntraining process, we introduce a teacher-forcing strategy to address the\nspeaker permutation problem, leading to faster model convergence. For\nevaluation, we propose an iterative decoding method that outputs diarization\nresults for each speaker sequentially. Additionally, we propose an Enhancer\nmodule to enhance the frame-level speaker embeddings, enabling the model to\nhandle scenarios with an unseen number of speakers. We also explore replacing\nthe transformer encoder with a Conformer architecture, which better models\nlocal information. Furthermore, we discovered that commonly used simulation\ndatasets for speaker diarization have a much higher overlap ratio compared to\nreal data. We found that using simulated training data that is more consistent\nwith real data can achieve an improvement in consistency. Extensive\nexperimental validation demonstrates the effectiveness of our proposed\nmethodologies. Our best system achieved a new state-of-the-art diarization\nerror rate (DER) performance on all the CALLHOME (10.08%), DIHARD II (24.64%),\nand AMI (13.00%) evaluation benchmarks, when no oracle voice activity detection\n(VAD) is used. Beyond speaker diarization, our AED-EEND system also shows\nremarkable competitiveness as a speech type detection model.",
          "arxiv_id": "2309.06672v1"
        },
        {
          "title": "End-to-End Neural Diarization: Reformulating Speaker Diarization as Simple Multi-label Classification",
          "year": "2020-02",
          "abstract": "The most common approach to speaker diarization is clustering of speaker\nembeddings. However, the clustering-based approach has a number of problems;\ni.e., (i) it is not optimized to minimize diarization errors directly, (ii) it\ncannot handle speaker overlaps correctly, and (iii) it has trouble adapting\ntheir speaker embedding models to real audio recordings with speaker overlaps.\nTo solve these problems, we propose the End-to-End Neural Diarization (EEND),\nin which a neural network directly outputs speaker diarization results given a\nmulti-speaker recording. To realize such an end-to-end model, we formulate the\nspeaker diarization problem as a multi-label classification problem and\nintroduce a permutation-free objective function to directly minimize\ndiarization errors. Besides its end-to-end simplicity, the EEND method can\nexplicitly handle speaker overlaps during training and inference. Just by\nfeeding multi-speaker recordings with corresponding speaker segment labels, our\nmodel can be easily adapted to real conversations. We evaluated our method on\nsimulated speech mixtures and real conversation datasets. The results showed\nthat the EEND method outperformed the state-of-the-art x-vector\nclustering-based method, while it correctly handled speaker overlaps. We\nexplored the neural network architecture for the EEND method, and found that\nthe self-attention-based neural network was the key to achieving excellent\nperformance. In contrast to conditioning the network only on its previous and\nnext hidden states, as is done using bidirectional long short-term memory\n(BLSTM), self-attention is directly conditioned on all the frames. By\nvisualizing the attention weights, we show that self-attention captures global\nspeaker characteristics in addition to local speech activity dynamics, making\nit especially suitable for dealing with the speaker diarization problem.",
          "arxiv_id": "2003.02966v1"
        },
        {
          "title": "Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR",
          "year": "2021-10",
          "abstract": "This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.",
          "arxiv_id": "2110.03151v2"
        }
      ],
      "11": [
        {
          "title": "Recent Progress in the CUHK Dysarthric Speech Recognition System",
          "year": "2022-01",
          "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\nin the past few decades, recognition of disordered speech remains a highly\nchallenging task to date. Disordered speech presents a wide spectrum of\nchallenges to current data intensive deep neural networks (DNNs) based ASR\ntechnologies that predominantly target normal speech. This paper presents\nrecent research efforts at the Chinese University of Hong Kong (CUHK) to\nimprove the performance of disordered speech recognition systems on the largest\npublicly available UASpeech dysarthric speech corpus. A set of novel modelling\ntechniques including neural architectural search, data augmentation using\nspectra-temporal perturbation, model based speaker adaptation and cross-domain\ngeneration of visual features within an audio-visual speech recognition (AVSR)\nsystem framework were employed to address the above challenges. The combination\nof these techniques produced the lowest published word error rate (WER) of\n25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER\nreduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric\nspeech recognition system featuring a 6-way DNN system combination and cross\nadaptation of out-of-domain normal speech data trained systems. Bayesian model\nadaptation further allows rapid adaptation to individual dysarthric speakers to\nbe performed using as little as 3.06 seconds of speech. The efficacy of these\ntechniques were further demonstrated on a CUDYS Cantonese dysarthric speech\nrecognition task.",
          "arxiv_id": "2201.05845v2"
        },
        {
          "title": "Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition",
          "year": "2022-02",
          "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\ntargeting normal speech in recent decades, accurate recognition of dysarthric\nand elderly speech remains highly challenging tasks to date. Sources of\nheterogeneity commonly found in normal speech including accent or gender, when\nfurther compounded with the variability over age and speech pathology severity\nlevel, create large diversity among speakers. To this end, speaker adaptation\ntechniques play a key role in personalization of ASR systems for such users.\nMotivated by the spectro-temporal level differences between dysarthric, elderly\nand normal speech that systematically manifest in articulatory imprecision,\ndecreased volume and clarity, slower speaking rates and increased dysfluencies,\nnovel spectrotemporal subspace basis deep embedding features derived using SVD\nspeech spectrum decomposition are proposed in this paper to facilitate\nauxiliary feature based speaker adaptation of state-of-the-art hybrid DNN/TDNN\nand end-to-end Conformer speech recognition systems. Experiments were conducted\non four tasks: the English UASpeech and TORGO dysarthric speech corpora; the\nEnglish DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets.\nThe proposed spectro-temporal deep feature adapted systems outperformed\nbaseline i-Vector and xVector adaptation by up to 2.63% absolute (8.63%\nrelative) reduction in word error rate (WER). Consistent performance\nimprovements were retained after model based speaker adaptation using learning\nhidden unit contributions (LHUC) was further applied. The best speaker adapted\nsystem using the proposed spectral basis embedding features produced the lowest\npublished WER of 25.05% on the UASpeech test set of 16 dysarthric speakers.",
          "arxiv_id": "2202.10290v3"
        },
        {
          "title": "Accurate synthesis of Dysarthric Speech for ASR data augmentation",
          "year": "2023-08",
          "abstract": "Dysarthria is a motor speech disorder often characterized by reduced speech\nintelligibility through slow, uncoordinated control of speech production\nmuscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers\ncommunicate more effectively. However, robust dysarthria-specific ASR requires\na significant amount of training speech, which is not readily available for\ndysarthric talkers. This paper presents a new dysarthric speech synthesis\nmethod for the purpose of ASR training data augmentation. Differences in\nprosodic and acoustic characteristics of dysarthric spontaneous speech at\nvarying severity levels are important components for dysarthric speech\nmodeling, synthesis, and augmentation. For dysarthric speech synthesis, a\nmodified neural multi-talker TTS is implemented by adding a dysarthria severity\nlevel coefficient and a pause insertion model to synthesize dysarthric speech\nfor varying severity levels. To evaluate the effectiveness for synthesis of\ntraining data for ASR, dysarthria-specific speech recognition was used. Results\nshow that a DNN-HMM model trained on additional synthetic dysarthric speech\nachieves WER improvement of 12.2% compared to the baseline, and that the\naddition of the severity level and pause insertion controls decrease WER by\n6.5%, showing the effectiveness of adding these parameters. Overall results on\nthe TORGO database demonstrate that using dysarthric synthetic speech to\nincrease the amount of dysarthric-patterned speech for training has significant\nimpact on the dysarthric ASR systems. In addition, we have conducted a\nsubjective evaluation to evaluate the dysarthric-ness and similarity of\nsynthesized speech. Our subjective evaluation shows that the perceived\ndysartrhic-ness of synthesized speech is similar to that of true dysarthric\nspeech, especially for higher levels of dysarthria",
          "arxiv_id": "2308.08438v1"
        }
      ],
      "12": [
        {
          "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
          "year": "2022-07",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hinder their applications to text-to-speech deployment.\nThrough the preliminary study on diffusion model parameterization, we find that\nprevious gradient-based TTS models require hundreds or thousands of iterations\nto guarantee high sample quality, which poses a challenge for accelerating\nsampling. In this work, we propose ProDiff, on progressive fast diffusion model\nfor high-quality text-to-speech. Unlike previous work estimating the gradient\nfor data density, ProDiff parameterizes the denoising model by directly\npredicting clean data to avoid distinct quality degradation in accelerating\nsampling. To tackle the model convergence challenge with decreased diffusion\niterations, ProDiff reduces the data variance in the target site via knowledge\ndistillation. Specifically, the denoising model uses the generated\nmel-spectrogram from an N-step DDIM teacher as the training target and distills\nthe behavior into a new model with N/2 steps. As such, it allows the TTS model\nto make sharp predictions and further reduces the sampling time by orders of\nmagnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to\nsynthesize high-fidelity mel-spectrograms, while it maintains sample quality\nand diversity competitive with state-of-the-art models using hundreds of steps.\nProDiff enables a sampling speed of 24x faster than real-time on a single\nNVIDIA 2080Ti GPU, making diffusion models practically applicable to\ntext-to-speech synthesis deployment for the first time. Our extensive ablation\nstudies demonstrate that each design in ProDiff is effective, and we further\nshow that ProDiff can be easily extended to the multi-speaker setting. Audio\nsamples are available at \\url{https://ProDiff.github.io/.}",
          "arxiv_id": "2207.06389v1"
        },
        {
          "title": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model",
          "year": "2023-05",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have shown promising\nperformance for speech synthesis. However, a large number of iterative steps\nare required to achieve high sample quality, which restricts the inference\nspeed. Maintaining sample quality while increasing sampling speed has become a\nchallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\n\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a\nsingle diffusion sampling step while achieving high audio quality. The\nconsistency constraint is applied to distill a consistency model from a\nwell-designed diffusion-based teacher model, which ultimately yields superior\nperformances in the distilled CoMoSpeech. Our experiments show that by\ngenerating audio recordings by a single sampling step, the CoMoSpeech achieves\nan inference speed more than 150 times faster than real-time on a single NVIDIA\nA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based\nspeech synthesis truly practical. Meanwhile, objective and subjective\nevaluations on text-to-speech and singing voice synthesis show that the\nproposed teacher models yield the best audio quality, and the one-step sampling\nbased CoMoSpeech achieves the best inference speed with better or comparable\naudio quality to other conventional multi-step diffusion model baselines. Audio\nsamples are available at https://comospeech.github.io/.",
          "arxiv_id": "2305.06908v4"
        },
        {
          "title": "DSPGAN: a GAN-based universal vocoder for high-fidelity TTS by time-frequency domain supervision from DSP",
          "year": "2022-11",
          "abstract": "Recent development of neural vocoders based on the generative adversarial\nneural network (GAN) has shown obvious advantages of generating raw waveform\nconditioned on mel-spectrogram with fast inference speed and lightweight\nnetworks. Whereas, it is still challenging to train a universal neural vocoder\nthat can synthesize high-fidelity speech from various scenarios with unseen\nspeakers, languages, and speaking styles. In this paper, we propose DSPGAN, a\nGAN-based universal vocoder for high-fidelity speech synthesis by applying the\ntime-frequency domain supervision from digital signal processing (DSP). To\neliminate the mismatch problem caused by the ground-truth spectrograms in the\ntraining phase and the predicted spectrograms in the inference phase, we\nleverage the mel-spectrogram extracted from the waveform generated by a DSP\nmodule, rather than the predicted mel-spectrogram from the Text-to-Speech (TTS)\nacoustic model, as the time-frequency domain supervision to the GAN-based\nvocoder. We also utilize sine excitation as the time-domain supervision to\nimprove the harmonic modeling and eliminate various artifacts of the GAN-based\nvocoder. Experiments show that DSPGAN significantly outperforms the compared\napproaches and it can generate high-fidelity speech for various TTS models\ntrained using diverse data.",
          "arxiv_id": "2211.01087v3"
        }
      ],
      "13": [
        {
          "title": "COVID-19 Cough Classification using Machine Learning and Global Smartphone Recordings",
          "year": "2020-12",
          "abstract": "We present a machine learning based COVID-19 cough classifier which can\ndiscriminate COVID-19 positive coughs from both COVID-19 negative and healthy\ncoughs recorded on a smartphone. This type of screening is non-contact, easy to\napply, and can reduce the workload in testing centres as well as limit\ntransmission by recommending early self-isolation to those who have a cough\nsuggestive of COVID-19. The datasets used in this study include subjects from\nall six continents and contain both forced and natural coughs, indicating that\nthe approach is widely applicable. The publicly available Coswara dataset\ncontains 92 COVID-19 positive and 1079 healthy subjects, while the second\nsmaller dataset was collected mostly in South Africa and contains 18 COVID-19\npositive and 26 COVID-19 negative subjects who have undergone a SARS-CoV\nlaboratory test. Both datasets indicate that COVID-19 positive coughs are\n15\\%-20\\% shorter than non-COVID coughs. Dataset skew was addressed by applying\nthe synthetic minority oversampling technique (SMOTE). A leave-$p$-out\ncross-validation scheme was used to train and evaluate seven machine learning\nclassifiers: LR, KNN, SVM, MLP, CNN, LSTM and Resnet50. Our results show that\nalthough all classifiers were able to identify COVID-19 coughs, the best\nperformance was exhibited by the Resnet50 classifier, which was best able to\ndiscriminate between the COVID-19 positive and the healthy coughs with an area\nunder the ROC curve (AUC) of 0.98. An LSTM classifier was best able to\ndiscriminate between the COVID-19 positive and COVID-19 negative coughs, with\nan AUC of 0.94 after selecting the best 13 features from a sequential forward\nselection (SFS). Since this type of cough audio classification is\ncost-effective and easy to deploy, it is potentially a useful and viable means\nof non-contact COVID-19 screening.",
          "arxiv_id": "2012.01926v2"
        },
        {
          "title": "QUCoughScope: An Artificially Intelligent Mobile Application to Detect Asymptomatic COVID-19 Patients using Cough and Breathing Sounds",
          "year": "2021-03",
          "abstract": "In the break of COVID-19 pandemic, mass testing has become essential to\nreduce the spread of the virus. Several recent studies suggest that a\nsignificant number of COVID-19 patients display no physical symptoms\nwhatsoever. Therefore, it is unlikely that these patients will undergo COVID-19\ntest, which increases their chances of unintentionally spreading the virus.\nCurrently, the primary diagnostic tool to detect COVID-19 is RT-PCR test on\ncollected respiratory specimens from the suspected case. This requires patients\nto travel to a laboratory facility to be tested, thereby potentially infecting\nothers along the way.It is evident from recent researches that asymptomatic\nCOVID-19 patients cough and breath in a different way than the healthy people.\nSeveral research groups have created mobile and web-platform for crowdsourcing\nthe symptoms, cough and breathing sounds from healthy, COVID-19 and Non-COVID\npatients. Some of these data repositories were made public. We have received\nsuch a repository from Cambridge University team under data-sharing agreement,\nwhere we have cough and breathing sound samples for 582 and 141 healthy and\nCOVID-19 patients, respectively. 87 COVID-19 patients were asymptomatic, while\nrest of them have cough. We have developed an Android application to\nautomatically screen COVID-19 from the comfort of people homes. Test subjects\ncan simply download a mobile application, enter their symptoms, record an audio\nclip of their cough and breath, and upload the data anonymously to our servers.\nOur backend server converts the audio clip to spectrogram and then apply our\nstate-of-the-art machine learning model to classify between cough sounds\nproduced by COVID-19 patients, as opposed to healthy subjects or those with\nother respiratory conditions. The system can detect asymptomatic COVID-19\npatients with a sensitivity more than 91%.",
          "arxiv_id": "2103.12063v1"
        },
        {
          "title": "Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data",
          "year": "2020-06",
          "abstract": "Audio signals generated by the human body (e.g., sighs, breathing, heart,\ndigestion, vibration sounds) have routinely been used by clinicians as\nindicators to diagnose disease or assess disease progression. Until recently,\nsuch signals were usually collected through manual auscultation at scheduled\nvisits. Research has now started to use digital technology to gather bodily\nsounds (e.g., from digital stethoscopes) for cardiovascular or respiratory\nexamination, which could then be used for automatic analysis. Some initial work\nshows promise in detecting diagnostic signals of COVID-19 from voice and\ncoughs. In this paper we describe our data analysis over a large-scale\ncrowdsourced dataset of respiratory sounds collected to aid diagnosis of\nCOVID-19. We use coughs and breathing to understand how discernible COVID-19\nsounds are from those in asthma or healthy controls. Our results show that even\na simple binary machine learning classifier is able to classify correctly\nhealthy and COVID-19 sounds. We also show how we distinguish a user who tested\npositive for COVID-19 and has a cough from a healthy user with a cough, and\nusers who tested positive for COVID-19 and have a cough from users with asthma\nand a cough. Our models achieve an AUC of above 80% across all tasks. These\nresults are preliminary and only scratch the surface of the potential of this\ntype of data and audio-based machine learning. This work opens the door to\nfurther investigation of how automatically analysed respiratory patterns could\nbe used as pre-screening signals to aid COVID-19 diagnosis.",
          "arxiv_id": "2006.05919v3"
        }
      ],
      "14": [
        {
          "title": "Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks",
          "year": "2021-10",
          "abstract": "Representation learning from unlabeled data has been of major interest in\nartificial intelligence research. While self-supervised speech representation\nlearning has been popular in the speech research community, very few works have\ncomprehensively analyzed audio representation learning for non-speech audio\ntasks. In this paper, we propose a self-supervised audio representation\nlearning method and apply it to a variety of downstream non-speech audio tasks.\nWe combine the well-known wav2vec 2.0 framework, which has shown success in\nself-supervised learning for speech tasks, with parameter-efficient conformer\narchitectures. Our self-supervised pre-training can reduce the need for labeled\ndata by two-thirds. On the AudioSet benchmark, we achieve a mean average\nprecision (mAP) score of 0.415, which is a new state-of-the-art on this dataset\nthrough audio-only self-supervised learning. Our fine-tuned conformers also\nsurpass or match the performance of previous systems pre-trained in a\nsupervised way on several downstream tasks. We further discuss the important\ndesign considerations for both pre-training and fine-tuning.",
          "arxiv_id": "2110.07313v3"
        },
        {
          "title": "Deploying self-supervised learning in the wild for hybrid automatic speech recognition",
          "year": "2022-05",
          "abstract": "Self-supervised learning (SSL) methods have proven to be very successful in\nautomatic speech recognition (ASR). These great improvements have been reported\nmostly based on highly curated datasets such as LibriSpeech for non-streaming\nEnd-to-End ASR models. However, the pivotal characteristics of SSL is to be\nutilized for any untranscribed audio data. In this paper, we provide a full\nexploration on how to utilize uncurated audio data in SSL from data\npre-processing to deploying an streaming hybrid ASR model. More specifically,\nwe present (1) the effect of Audio Event Detection (AED) model in data\npre-processing pipeline (2) analysis on choosing optimizer and learning rate\nscheduling (3) comparison of recently developed contrastive losses, (4)\ncomparison of various pre-training strategies such as utilization of in-domain\nversus out-domain pre-training data, monolingual versus multilingual\npre-training data, multi-head multilingual SSL versus single-head multilingual\nSSL and supervised pre-training versus SSL. The experimental results show that\nSSL pre-training with in-domain uncurated data can achieve better performance\nin comparison to all the alternative out-domain pre-training strategies.",
          "arxiv_id": "2205.08598v1"
        },
        {
          "title": "Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition",
          "year": "2022-03",
          "abstract": "Self-supervised learning (SSL) to learn high-level speech representations has\nbeen a popular approach to building Automatic Speech Recognition (ASR) systems\nin low-resource settings. However, the common assumption made in literature is\nthat a considerable amount of unlabeled data is available for the same domain\nor language that can be leveraged for SSL pre-training, which we acknowledge is\nnot feasible in a real-world setting. In this paper, as part of the Interspeech\nGram Vaani ASR challenge, we try to study the effect of domain, language,\ndataset size, and other aspects of our upstream pre-training SSL data on the\nfinal performance low-resource downstream ASR task. We also build on the\ncontinued pre-training paradigm to study the effect of prior knowledge\npossessed by models trained using SSL. Extensive experiments and studies reveal\nthat the performance of ASR systems is susceptible to the data used for SSL\npre-training. Their performance improves with an increase in similarity and\nvolume of pre-training data. We believe our work will be helpful to the speech\ncommunity in building better ASR systems in low-resource settings and steer\nresearch towards improving generalization in SSL-based pre-training for speech\nsystems.",
          "arxiv_id": "2203.16973v4"
        }
      ],
      "15": [
        {
          "title": "Do as I mean, not as I say: Sequence Loss Training for Spoken Language Understanding",
          "year": "2021-02",
          "abstract": "Spoken language understanding (SLU) systems extract transcriptions, as well\nas semantics of intent or named entities from speech, and are essential\ncomponents of voice activated systems. SLU models, which either directly\nextract semantics from audio or are composed of pipelined automatic speech\nrecognition (ASR) and natural language understanding (NLU) models, are\ntypically trained via differentiable cross-entropy losses, even when the\nrelevant performance metrics of interest are word or semantic error rates. In\nthis work, we propose non-differentiable sequence losses based on SLU metrics\nas a proxy for semantic error and use the REINFORCE trick to train ASR and SLU\nmodels with this loss. We show that custom sequence loss training is the\nstate-of-the-art on open SLU datasets and leads to 6% relative improvement in\nboth ASR and NLU performance metrics on large proprietary datasets. We also\ndemonstrate how the semantic sequence loss training paradigm can be used to\nupdate ASR and SLU models without transcripts, using semantic feedback alone.",
          "arxiv_id": "2102.06750v1"
        },
        {
          "title": "Speech-language Pre-training for End-to-end Spoken Language Understanding",
          "year": "2021-02",
          "abstract": "End-to-end (E2E) spoken language understanding (SLU) can infer semantics\ndirectly from speech signal without cascading an automatic speech recognizer\n(ASR) with a natural language understanding (NLU) module. However, paired\nutterance recordings and corresponding semantics may not always be available or\nsufficient to train an E2E SLU model in a real production environment. In this\npaper, we propose to unify a well-optimized E2E ASR encoder (speech) and a\npre-trained language model encoder (language) into a transformer decoder. The\nunified speech-language pre-trained model (SLP) is continually enhanced on\nlimited labeled data from a target domain by using a conditional masked\nlanguage model (MLM) objective, and thus can effectively generate a sequence of\nintent, slot type, and slot value for given input speech in the inference. The\nexperimental results on two public corpora show that our approach to E2E SLU is\nsuperior to the conventional cascaded method. It also outperforms the present\nstate-of-the-art approaches to E2E SLU with much less paired data.",
          "arxiv_id": "2102.06283v1"
        },
        {
          "title": "Ensemble Chinese End-to-End Spoken Language Understanding for Abnormal Event Detection from audio stream",
          "year": "2020-10",
          "abstract": "Conventional spoken language understanding (SLU) consist of two stages, the\nfirst stage maps speech to text by automatic speech recognition (ASR), and the\nsecond stage maps text to intent by natural language understanding (NLU).\nEnd-to-end SLU maps speech directly to intent through a single deep learning\nmodel. Previous end-to-end SLU models are primarily used for English\nenvironment due to lacking large scale SLU dataset in Chines, and use only one\nASR model to extract features from speech. With the help of Kuaishou\ntechnology, a large scale SLU dataset in Chinese is collected to detect\nabnormal event in their live audio stream. Based on this dataset, this paper\nproposed a ensemble end-to-end SLU model used for Chinese environment. This\nensemble SLU models extracted hierarchies features using multiple pre-trained\nASR models, leading to better representation of phoneme level and word level\ninformation. This proposed approached achieve 9.7% increase of accuracy\ncompared to previous end-to-end SLU model.",
          "arxiv_id": "2010.09235v2"
        }
      ],
      "16": [
        {
          "title": "Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion",
          "year": "2022-07",
          "abstract": "Voice conversion is to generate a new speech with the source content and a\ntarget voice style. In this paper, we focus on one general setting, i.e.,\nnon-parallel many-to-many voice conversion, which is close to the real-world\nscenario. As the name implies, non-parallel many-to-many voice conversion does\nnot require the paired source and reference speeches and can be applied to\narbitrary voice transfer. In recent years, Generative Adversarial Networks\n(GANs) and other techniques such as Conditional Variational Autoencoders\n(CVAEs) have made considerable progress in this field. However, due to the\nsophistication of voice conversion, the style similarity of the converted\nspeech is still unsatisfactory. Inspired by the inherent structure of\nmel-spectrogram, we propose a new voice conversion framework, i.e.,\nSubband-based Generative Adversarial Network for Voice Conversion (SGAN-VC).\nSGAN-VC converts each subband content of the source speech separately by\nexplicitly utilizing the spatial characteristics between different subbands.\nSGAN-VC contains one style encoder, one content encoder, and one decoder. In\nparticular, the style encoder network is designed to learn style codes for\ndifferent subbands of the target speaker. The content encoder network can\ncapture the content information on the source speech. Finally, the decoder\ngenerates particular subband content. In addition, we propose a pitch-shift\nmodule to fine-tune the pitch of the source speaker, making the converted tone\nmore accurate and explainable. Extensive experiments demonstrate that the\nproposed approach achieves state-of-the-art performance on VCTK Corpus and\nAISHELL3 datasets both qualitatively and quantitatively, whether on seen or\nunseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data\neven exceeds that of StarGANv2-VC with ASR network assistance.",
          "arxiv_id": "2207.06057v2"
        },
        {
          "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
          "year": "2023-12",
          "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to\narbitrary unseen target speaker timbre, while keeping the linguistic content\nunchanged. Although the voice of generated speech can be controlled by\nproviding the speaker embedding of the target speaker, the speaker similarity\nstill lags behind the ground truth recordings. In this paper, we propose\nSEF-VC, a speaker embedding free voice conversion model, which is designed to\nlearn and incorporate speaker timbre from reference speech via a powerful\nposition-agnostic cross-attention mechanism, and then reconstruct waveform from\nHuBERT semantic tokens in a non-autoregressive manner. The concise design of\nSEF-VC enhances its training stability and voice conversion performance.\nObjective and subjective evaluations demonstrate the superiority of SEF-VC to\ngenerate high-quality speech with better similarity to target reference than\nstrong zero-shot VC baselines, even for very short reference speeches.",
          "arxiv_id": "2312.08676v2"
        },
        {
          "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
          "year": "2023-10",
          "abstract": "Voice conversion aims to convert source speech into a target voice using\nrecordings of the target speaker as a reference. Newer models are producing\nincreasingly realistic output. But what happens when models are fed with\nnon-standard data, such as speech from a user with a speech impairment? We\ninvestigate how a recent voice conversion model performs on non-standard\ndownstream voice conversion tasks. We use a simple but robust approach called\nk-nearest neighbors voice conversion (kNN-VC). We look at four non-standard\napplications: stuttered voice conversion, cross-lingual voice conversion,\nmusical instrument conversion, and text-to-voice conversion. The latter\ninvolves converting to a target voice specified through a text description,\ne.g. \"a young man with a high-pitched voice\". Compared to an established\nbaseline, we find that kNN-VC retains high performance in stuttered and\ncross-lingual voice conversion. Results are more mixed for the musical\ninstrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some\ninstruments like drums but not on others. Nevertheless, this shows that voice\nconversion models - and kNN-VC in particular - are increasingly applicable in a\nrange of non-standard downstream tasks. But there are still limitations when\nsamples are very far from the training distribution. Code, samples, trained\nmodels: https://rf5.github.io/sacair2023-knnvc-demo/.",
          "arxiv_id": "2310.08104v1"
        }
      ],
      "17": [
        {
          "title": "JVS-MuSiC: Japanese multispeaker singing-voice corpus",
          "year": "2020-01",
          "abstract": "Thanks to developments in machine learning techniques, it has become possible\nto synthesize high-quality singing voices of a single singer. An open\nmultispeaker singing-voice corpus would further accelerate the research in\nsinging-voice synthesis. However, conventional singing-voice corpora only\nconsist of the singing voices of a single singer. We designed a Japanese\nmultispeaker singing-voice corpus called \"JVS-MuSiC\" with the aim to analyze\nand synthesize a variety of voices. The corpus consists of 100 singers'\nrecordings of the same song, Katatsumuri, which is a Japanese children's song.\nIt also includes another song that is different for each singer. In this paper,\nwe describe the design of the corpus and experimental analyses using JVS-MuSiC.\nWe investigated the relationship between 1) the similarity of singing voices\nand perceptual oneness of unison singing voices and between 2) the similarity\nof singing voices and that of speech. The results suggest that 1) there is a\npositive and moderate correlation between singing-voice similarity and the\noneness of unison and that 2) the correlation between singing-voice similarity\nand speech similarity is weak. This corpus is freely available online.",
          "arxiv_id": "2001.07044v1"
        },
        {
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System",
          "year": "2021-08",
          "abstract": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "arxiv_id": "2108.02776v1"
        },
        {
          "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis",
          "year": "2023-12",
          "abstract": "Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://aaronz345.github.io/StyleSingerDemo/.",
          "arxiv_id": "2312.10741v5"
        }
      ],
      "18": [
        {
          "title": "Unsupervised classification to improve the quality of a bird song recording dataset",
          "year": "2023-02",
          "abstract": "Open audio databases such as Xeno-Canto are widely used to build datasets to\nexplore bird song repertoire or to train models for automatic bird sound\nclassification by deep learning algorithms. However, such databases suffer from\nthe fact that bird sounds are weakly labelled: a species name is attributed to\neach audio recording without timestamps that provide the temporal localization\nof the bird song of interest. Manual annotations can solve this issue, but they\nare time consuming, expert-dependent, and cannot run on large datasets. Another\nsolution consists in using a labelling function that automatically segments\naudio recordings before assigning a label to each segmented audio sample.\nAlthough labelling functions were introduced to expedite strong label\nassignment, their classification performance remains mostly unknown. To address\nthis issue and reduce label noise (wrong label assignment) in large bird song\ndatasets, we introduce a data-centric novel labelling function composed of\nthree successive steps: 1) time-frequency sound unit segmentation, 2) feature\ncomputation for each sound unit, and 3) classification of each sound unit as\nbird song or noise with either an unsupervised DBSCAN algorithm or the\nsupervised BirdNET neural network. The labelling function was optimized,\nvalidated, and tested on the songs of 44 West-Palearctic common bird species.\nWe first showed that the segmentation of bird songs alone aggregated from 10%\nto 83% of label noise depending on the species. We also demonstrated that our\nlabelling function was able to significantly reduce the initial label noise\npresent in the dataset by up to a factor of three. Finally, we discuss\ndifferent opportunities to design suitable labelling functions to build\nhigh-quality animal vocalizations with minimum expert annotation effort.",
          "arxiv_id": "2302.07560v1"
        },
        {
          "title": "Learning to detect an animal sound from five examples",
          "year": "2023-05",
          "abstract": "Automatic detection and classification of animal sounds has many applications\nin biodiversity monitoring and animal behaviour. In the past twenty years, the\nvolume of digitised wildlife sound available has massively increased, and\nautomatic classification through deep learning now shows strong results.\nHowever, bioacoustics is not a single task but a vast range of small-scale\ntasks (such as individual ID, call type, emotional indication) with wide\nvariety in data characteristics, and most bioacoustic tasks do not come with\nstrongly-labelled training data. The standard paradigm of supervised learning,\nfocussed on a single large-scale dataset and/or a generic pre-trained\nalgorithm, is insufficient. In this work we recast bioacoustic sound event\ndetection within the AI framework of few-shot learning. We adapt this framework\nto sound event detection, such that a system can be given the annotated\nstart/end times of as few as 5 events, and can then detect events in\nlong-duration audio -- even when the sound category was not known at the time\nof algorithm training. We introduce a collection of open datasets designed to\nstrongly test a system's ability to perform few-shot sound event detections,\nand we present the results of a public contest to address the task. We show\nthat prototypical networks are a strong-performing method, when enhanced with\nadaptations for general characteristics of animal sounds. We demonstrate that\nwidely-varying sound event durations are an important factor in performance, as\nwell as non-stationarity, i.e. gradual changes in conditions throughout the\nduration of a recording. For fine-grained bioacoustic recognition tasks without\nmassive annotated training data, our results demonstrate that few-shot sound\nevent detection is a powerful new method, strongly outperforming traditional\nsignal-processing detection methods in the fully automated scenario.",
          "arxiv_id": "2305.13210v1"
        },
        {
          "title": "Unsupervised outlier detection to improve bird audio dataset labels",
          "year": "2025-04",
          "abstract": "The Xeno-Canto bird audio repository is an invaluable resource for those\ninterested in vocalizations and other sounds made by birds around the world.\nThis is particularly the case for machine learning researchers attempting to\nimprove on the bird species recognition accuracy of classification models.\nHowever, the task of extracting labeled datasets from the recordings found in\nthis crowd-sourced repository faces several challenges. One challenge of\nparticular significance to machine learning practitioners is that one bird\nspecies label is applied to each audio recording, but frequently other sounds\nare also captured including other bird species, other animal sounds,\nanthropogenic and other ambient sounds. These non-target bird species sounds\ncan result in dataset labeling discrepancies referred to as label noise. In\nthis work we present a cleaning process consisting of audio preprocessing\nfollowed by dimensionality reduction and unsupervised outlier detection (UOD)\nto reduce the label noise in a dataset derived from Xeno-Canto recordings. We\ninvestigate three neural network dimensionality reduction techniques: two\nflavors of convolutional autoencoders and variational deep embedding (VaDE\n(Jiang, 2017)). While both methods show some degree of effectiveness at\ndetecting outliers for most bird species datasets, we found significant\nvariation in the performance of the methods from one species to the next. We\nbelieve that the results of this investigation demonstrate that the application\nof our cleaning process can meaningfully reduce the label noise of bird species\ndatasets derived from Xeno-Canto audio repository but results vary across\nspecies.",
          "arxiv_id": "2504.18650v1"
        }
      ],
      "19": [
        {
          "title": "Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting",
          "year": "2022-11",
          "abstract": "In this paper, we present a novel approach to adapt a sequence-to-sequence\nTransformer-Transducer ASR system to the keyword spotting (KWS) task. We\nachieve this by replacing the keyword in the text transcription with a special\ntoken <kw> and training the system to detect the <kw> token in an audio stream.\nAt inference time, we create a decision function inspired by conventional KWS\napproaches, to make our approach more suitable for the KWS task. Furthermore,\nwe introduce a specific keyword spotting loss by adapting the\nsequence-discriminative Minimum Bayes-Risk training technique. We find that our\napproach significantly outperforms ASR based KWS systems. When compared with a\nconventional keyword spotting system, our proposal has similar performance\nwhile bringing the advantages and flexibility of sequence-to-sequence training.\nAdditionally, when combined with the conventional KWS system, our approach can\nimprove the performance at any operation point.",
          "arxiv_id": "2211.06478v1"
        },
        {
          "title": "Progressive Continual Learning for Spoken Keyword Spotting",
          "year": "2022-01",
          "abstract": "Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. To tackle such challenges, we propose a\nprogressive continual learning strategy for small-footprint spoken keyword\nspotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a\nnetwork instantiator to generate the task-specific sub-networks for remembering\npreviously learned keywords. As a result, the PCL-KWS approach incrementally\nlearns new keywords without forgetting prior knowledge. Besides, the\nkeyword-aware network scaling mechanism of PCL-KWS constrains the growth of\nmodel parameters while achieving high performance. Experimental results show\nthat after learning five new tasks sequentially, our proposed PCL-KWS approach\narchives the new state-of-the-art performance of 92.8% average accuracy for all\nthe tasks on Google Speech Command dataset compared with other baselines.",
          "arxiv_id": "2201.12546v2"
        },
        {
          "title": "Personalized Keyword Spotting through Multi-task Learning",
          "year": "2022-06",
          "abstract": "Keyword spotting (KWS) plays an essential role in enabling speech-based user\ninteraction on smart devices, and conventional KWS (C-KWS) approaches have\nconcentrated on detecting user-agnostic pre-defined keywords. However, in\npractice, most user interactions come from target users enrolled in the device\nwhich motivates to construct personalized keyword spotting. We design two\npersonalized KWS tasks; (1) Target user Biased KWS (TB-KWS) and (2) Target user\nOnly KWS (TO-KWS). To solve the tasks, we propose personalized keyword spotting\nthrough multi-task learning (PK-MTL) that consists of multi-task learning and\ntask-adaptation. First, we introduce applying multi-task learning on keyword\nspotting and speaker verification to leverage user information to the keyword\nspotting system. Next, we design task-specific scoring functions to adapt to\nthe personalized KWS tasks thoroughly. We evaluate our framework on\nconventional and personalized scenarios, and the results show that PK-MTL can\ndramatically reduce the false alarm rate, especially in various practical\nscenarios.",
          "arxiv_id": "2206.13708v1"
        }
      ],
      "20": [
        {
          "title": "Differentiable Grey-box Modelling of Phaser Effects using Frame-based Spectral Processing",
          "year": "2023-06",
          "abstract": "Machine learning approaches to modelling analog audio effects have seen\nintensive investigation in recent years, particularly in the context of\nnon-linear time-invariant effects such as guitar amplifiers. For modulation\neffects such as phasers, however, new challenges emerge due to the presence of\nthe low-frequency oscillator which controls the slowly time-varying nature of\nthe effect. Existing approaches have either required foreknowledge of this\ncontrol signal, or have been non-causal in implementation. This work presents a\ndifferentiable digital signal processing approach to modelling phaser effects\nin which the underlying control signal and time-varying spectral response of\nthe effect are jointly learned. The proposed model processes audio in short\nframes to implement a time-varying filter in the frequency domain, with a\ntransfer function based on typical analog phaser circuit topology. We show that\nthe model can be trained to emulate an analog reference device, while retaining\ninterpretable and adjustable parameters. The frame duration is an important\nhyper-parameter of the proposed model, so an investigation was carried out into\nits effect on model accuracy. The optimal frame length depends on both the rate\nand transient decay-time of the target effect, but the frame length can be\naltered at inference time without a significant change in accuracy.",
          "arxiv_id": "2306.01332v1"
        },
        {
          "title": "Steerable discovery of neural audio effects",
          "year": "2021-12",
          "abstract": "Applications of deep learning for audio effects often focus on modeling\nanalog effects or learning to control effects to emulate a trained audio\nengineer. However, deep learning approaches also have the potential to expand\ncreativity through neural audio effects that enable new sound transformations.\nWhile recent work demonstrated that neural networks with random weights produce\ncompelling audio effects, control of these effects is limited and unintuitive.\nTo address this, we introduce a method for the steerable discovery of neural\naudio effects. This method enables the design of effects using example\nrecordings provided by the user. We demonstrate how this method produces an\neffect similar to the target effect, along with interesting inaccuracies, while\nalso providing perceptually relevant controls.",
          "arxiv_id": "2112.02926v1"
        },
        {
          "title": "Style Transfer of Audio Effects with Differentiable Signal Processing",
          "year": "2022-07",
          "abstract": "We present a framework that can impose the audio effects and production style\nfrom one recording to another by example with the goal of simplifying the audio\nproduction process. We train a deep neural network to analyze an input\nrecording and a style reference recording, and predict the control parameters\nof audio effects used to render the output. In contrast to past work, we\nintegrate audio effects as differentiable operators in our framework, perform\nbackpropagation through audio effects, and optimize end-to-end using an\naudio-domain loss. We use a self-supervised training strategy enabling\nautomatic control of audio effects without the use of any labeled or paired\ntraining data. We survey a range of existing and new approaches for\ndifferentiable signal processing, showing how each can be integrated into our\nframework while discussing their trade-offs. We evaluate our approach on both\nspeech and music tasks, demonstrating that our approach generalizes both to\nunseen recordings and even to sample rates different than those seen during\ntraining. Our approach produces convincing production style transfer results\nwith the ability to transform input recordings to produced recordings, yielding\naudio effect control parameters that enable interpretability and user\ninteraction.",
          "arxiv_id": "2207.08759v1"
        }
      ],
      "21": [
        {
          "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
          "year": "2025-02",
          "abstract": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.",
          "arxiv_id": "2502.03382v2"
        },
        {
          "title": "End-to-End Simultaneous Speech Translation with Differentiable Segmentation",
          "year": "2023-05",
          "abstract": "End-to-end simultaneous speech translation (SimulST) outputs translation\nwhile receiving the streaming speech inputs (a.k.a. streaming speech\ntranslation), and hence needs to segment the speech inputs and then translate\nbased on the current received speech. However, segmenting the speech inputs at\nunfavorable moments can disrupt the acoustic integrity and adversely affect the\nperformance of the translation model. Therefore, learning to segment the speech\ninputs at those moments that are beneficial for the translation model to\nproduce high-quality translation is the key to SimulST. Existing SimulST\nmethods, either using the fixed-length segmentation or external segmentation\nmodel, always separate segmentation from the underlying translation model,\nwhere the gap results in segmentation outcomes that are not necessarily\nbeneficial for the translation process. In this paper, we propose\nDifferentiable Segmentation (DiSeg) for SimulST to directly learn segmentation\nfrom the underlying translation model. DiSeg turns hard segmentation into\ndifferentiable through the proposed expectation training, enabling it to be\njointly trained with the translation model and thereby learn\ntranslation-beneficial segmentation. Experimental results demonstrate that\nDiSeg achieves state-of-the-art performance and exhibits superior segmentation\ncapability.",
          "arxiv_id": "2305.16093v2"
        },
        {
          "title": "StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning",
          "year": "2024-06",
          "abstract": "Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech\ntranslation) outputs target speech while receiving streaming speech inputs,\nwhich is critical for real-time communication. Beyond accomplishing translation\nbetween speech, Simul-S2ST requires a policy to control the model to generate\ncorresponding target speech at the opportune moment within speech inputs,\nthereby posing a double challenge of translation and policy. In this paper, we\npropose StreamSpeech, a direct Simul-S2ST model that jointly learns translation\nand simultaneous policy in a unified framework of multi-task learning. Adhering\nto a multi-task learning approach, StreamSpeech can perform offline and\nsimultaneous speech recognition, speech translation and speech synthesis via an\n\"All-in-One\" seamless model. Experiments on CVSS benchmark demonstrate that\nStreamSpeech achieves state-of-the-art performance in both offline S2ST and\nSimul-S2ST tasks. Besides, StreamSpeech is able to present high-quality\nintermediate results (i.e., ASR or translation results) during simultaneous\ntranslation process, offering a more comprehensive real-time communication\nexperience.",
          "arxiv_id": "2406.03049v1"
        }
      ],
      "22": [
        {
          "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction",
          "year": "2022-01",
          "abstract": "Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert",
          "arxiv_id": "2201.02184v2"
        },
        {
          "title": "Learning Contextually Fused Audio-visual Representations for Audio-visual Speech Recognition",
          "year": "2022-02",
          "abstract": "With the advance in self-supervised learning for audio and visual modalities,\nit has become possible to learn a robust audio-visual speech representation.\nThis would be beneficial for improving the audio-visual speech recognition\n(AVSR) performance, as the multi-modal inputs contain more fruitful information\nin principle. In this paper, based on existing self-supervised representation\nlearning methods for audio modality, we therefore propose an audio-visual\nrepresentation learning approach. The proposed approach explores both the\ncomplementarity of audio-visual modalities and long-term context dependency\nusing a transformer-based fusion module and a flexible masking strategy. After\npre-training, the model is able to extract fused representations required by\nAVSR. Without loss of generality, it can be applied to single-modal tasks, e.g.\naudio/visual speech recognition by simply masking out one modality in the\nfusion module. The proposed pre-trained model is evaluated on speech\nrecognition and lipreading tasks using one or two modalities, where the\nsuperiority is revealed.",
          "arxiv_id": "2202.07428v2"
        },
        {
          "title": "AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition",
          "year": "2023-09",
          "abstract": "Audio-visual speech contains synchronized audio and visual information that\nprovides cross-modal supervision to learn representations for both automatic\nspeech recognition (ASR) and visual speech recognition (VSR). We introduce\ncontinuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a\nsemi-supervised method to train an audio-visual speech recognition (AVSR) model\non a combination of labeled and unlabeled videos with continuously regenerated\npseudo-labels. Our models are trained for speech recognition from audio-visual\ninputs and can perform speech recognition using both audio and visual\nmodalities, or only one modality. Our method uses the same audio-visual model\nfor both supervised training and pseudo-label generation, mitigating the need\nfor external speech recognition models to generate pseudo-labels. AV-CPL\nobtains significant improvements in VSR performance on the LRS3 dataset while\nmaintaining practical ASR and AVSR performance. Finally, using visual-only\nspeech data, our method is able to leverage unlabeled visual speech to improve\nVSR.",
          "arxiv_id": "2309.17395v1"
        }
      ],
      "23": [
        {
          "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation",
          "year": "2023-03",
          "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk",
          "arxiv_id": "2303.11089v2"
        },
        {
          "title": "StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation",
          "year": "2022-08",
          "abstract": "We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.",
          "arxiv_id": "2208.10922v2"
        },
        {
          "title": "AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person",
          "year": "2021-08",
          "abstract": "Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.",
          "arxiv_id": "2108.04325v2"
        }
      ],
      "24": [
        {
          "title": "Enabling ASR for Low-Resource Languages: A Comprehensive Dataset Creation Approach",
          "year": "2024-06",
          "abstract": "In recent years, automatic speech recognition (ASR) systems have\nsignificantly improved, especially in languages with a vast amount of\ntranscribed speech data. However, ASR systems tend to perform poorly for\nlow-resource languages with fewer resources, such as minority and regional\nlanguages. This study introduces a novel pipeline designed to generate ASR\ntraining datasets from audiobooks, which typically feature a single transcript\nassociated with hours-long audios. The common structure of these audiobooks\nposes a unique challenge due to the extensive length of audio segments, whereas\noptimal ASR training requires segments ranging from 4 to 15 seconds. To address\nthis, we propose a method for effectively aligning audio with its corresponding\ntext and segmenting it into lengths suitable for ASR training. Our approach\nsimplifies data preparation for ASR systems in low-resource languages and\ndemonstrates its application through a case study involving the Armenian\nlanguage. Our method, which is \"portable\" to many low-resource languages, not\nonly mitigates the issue of data scarcity but also enhances the performance of\nASR models for underrepresented languages.",
          "arxiv_id": "2406.01446v1"
        },
        {
          "title": "An Automatic Speech Recognition System for Bengali Language based on Wav2Vec2 and Transfer Learning",
          "year": "2022-09",
          "abstract": "An independent, automated method of decoding and transcribing oral speech is\nknown as automatic speech recognition (ASR). A typical ASR system extracts\nfeature from audio recordings or streams and run one or more algorithms to map\nthe features to corresponding texts. Numerous of research has been done in the\nfield of speech signal processing in recent years. When given adequate\nresources, both conventional ASR and emerging end-to-end (E2E) speech\nrecognition have produced promising results. However, for low-resource\nlanguages like Bengali, the current state of ASR lags behind, although the low\nresource state does not reflect upon the fact that this language is spoken by\nover 500 million people all over the world. Despite its popularity, there\naren't many diverse open-source datasets available, which makes it difficult to\nconduct research on Bengali speech recognition systems. This paper is a part of\nthe competition named `BUET CSE Fest DL Sprint'. The purpose of this paper is\nto improve the speech recognition performance of the Bengali language by\nadopting speech recognition technology on the E2E structure based on the\ntransfer learning framework. The proposed method effectively models the Bengali\nlanguage and achieves 3.819 score in `Levenshtein Mean Distance' on the test\ndataset of 7747 samples, when only 1000 samples of train dataset were used to\ntrain.",
          "arxiv_id": "2209.08119v2"
        },
        {
          "title": "Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages",
          "year": "2024-11",
          "abstract": "This paper presents a novel multistage fine-tuning strategy designed to\nenhance automatic speech recognition (ASR) performance in low-resource\nlanguages using OpenAI's Whisper model. In this approach we aim to build ASR\nmodel for languages with limited digital resources by sequentially adapting the\nmodel across linguistically similar languages. We experimented this on the\nMalasar language, a Dravidian language spoken by approximately ten thousand\npeople in the Western Ghats of South India. Malasar language faces critical\nchallenges for technological intervention due to its lack of a native script\nand absence of digital or spoken data resources. Working in collaboration with\nWycliffe India and Malasar community members, we created a spoken Malasar\ncorpus paired with transcription in Tamil script, a closely related major\nlanguage. In our approach to build ASR model for Malasar, we first build an\nintermediate Tamil ASR, leveraging higher data availability for Tamil annotated\nspeech. This intermediate model is subsequently fine-tuned on Malasar data,\nallowing for more effective ASR adaptation despite limited resources. The\nmultistage fine-tuning strategy demonstrated significant improvements over\ndirect fine-tuning on Malasar data alone, achieving a word error rate (WER) of\n51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning\nmethod. Further a WER reduction to 47.3% was achieved through punctuation\nremoval in post-processing, which addresses formatting inconsistencies that\nimpact evaluation. Our results underscore the effectiveness of sequential\nmultistage fine-tuning combined with targeted post-processing as a scalable\nstrategy for ASR system development in low-resource languages, especially where\nlinguistic similarities can be leveraged to bridge gaps in training data.",
          "arxiv_id": "2411.04573v1"
        }
      ],
      "25": [
        {
          "title": "SAMOS: A Neural MOS Prediction Model Leveraging Semantic Representations and Acoustic Features",
          "year": "2024-11",
          "abstract": "Assessing the naturalness of speech using mean opinion score (MOS) prediction\nmodels has positive implications for the automatic evaluation of speech\nsynthesis systems. Early MOS prediction models took the raw waveform or\namplitude spectrum of speech as input, whereas more advanced methods employed\nself-supervised-learning (SSL) based models to extract semantic representations\nfrom speech for MOS prediction. These methods utilized limited aspects of\nspeech information for MOS prediction, resulting in restricted prediction\naccuracy. Therefore, in this paper, we propose SAMOS, a MOS prediction model\nthat leverages both Semantic and Acoustic information of speech to be assessed.\nSpecifically, the proposed SAMOS leverages a pretrained wav2vec2 to extract\nsemantic representations and uses the feature extractor of a pretrained\nBiVocoder to extract acoustic features. These two types of features are then\nfed into the prediction network, which includes multi-task heads and an\naggregation layer, to obtain the final MOS score. Experimental results\ndemonstrate that the proposed SAMOS outperforms current state-of-the-art MOS\nprediction models on the BVCC dataset and performs comparable performance on\nthe BC2019 dataset, according to the results of system-level evaluation\nmetrics.",
          "arxiv_id": "2411.11232v1"
        },
        {
          "title": "MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment",
          "year": "2021-04",
          "abstract": "The objective speech quality assessment is usually conducted by comparing\nreceived speech signal with its clean reference, while human beings are capable\nof evaluating the speech quality without any reference, such as in the mean\nopinion score (MOS) tests. Non-intrusive speech quality assessment has\nattracted much attention recently due to the lack of access to clean reference\nsignals for objective evaluations in real scenarios. In this paper, we propose\na novel non-intrusive speech quality measurement model, MetricNet, which\nleverages label distribution learning and joint speech reconstruction learning\nto achieve significantly improved performance compared to the existing\nnon-intrusive speech quality measurement models. We demonstrate that the\nproposed approach yields promisingly high correlation to the intrusive\nobjective evaluation of speech quality on clean, noisy and processed speech\ndata.",
          "arxiv_id": "2104.01227v1"
        },
        {
          "title": "Selecting N-lowest scores for training MOS prediction models",
          "year": "2025-06",
          "abstract": "The automatic speech quality assessment (SQA) has been extensively studied to\npredict the speech quality without time-consuming questionnaires. Recently,\nneural-based SQA models have been actively developed for speech samples\nproduced by text-to-speech or voice conversion, with a primary focus on\ntraining mean opinion score (MOS) prediction models. The quality of each speech\nsample may not be consistent across the entire duration, and it remains unclear\nwhich segments of the speech receive the primary focus from humans when\nassigning subjective evaluation for MOS calculation. We hypothesize that when\nhumans rate speech, they tend to assign more weight to low-quality speech\nsegments, and the variance in ratings for each sample is mainly due to\naccidental assignment of higher scores when overlooking the poor quality speech\nsegments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC\ndatasets. Based on the hypothesis, we propose the more reliable representative\nvalue N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments\nshow that LCC and SRCC improve compared to regular MOS when employing N_low-MOS\nto MOSNet training. This result suggests that N_low-MOS is a more intrinsic\nrepresentative value of subjective speech quality and makes MOSNet a better\ncomparator of VC models.",
          "arxiv_id": "2506.18326v1"
        }
      ],
      "26": [
        {
          "title": "A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems",
          "year": "2024-06",
          "abstract": "Despite significant recent progress across multiple subtasks of audio source\nseparation, few music source separation systems support separation beyond the\nfour-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current\nsystems that support source separation beyond this setup, most continue to rely\non an inflexible decoder setup that can only support a fixed pre-defined set of\nstems. Increasing stem support in these inflexible systems correspondingly\nrequires increasing computational complexity, rendering extensions of these\nsystems computationally infeasible for long-tail instruments. In this work, we\npropose Banquet, a system that allows source separation of multiple stems using\njust one decoder. A bandsplit source separation model is extended to work in a\nquery-based setup in tandem with a music instrument recognition PaSST model. On\nthe MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached\nthe performance level of the significantly more complex 6-stem Hybrid\nTransformer Demucs on VDBO stems and outperformed it on guitar and piano. The\nquery-based setup allows for the separation of narrow instrument classes such\nas clean acoustic guitars, and can be successfully applied to the extraction of\nless common stems such as reeds and organs. Implementation is available at\nhttps://github.com/kwatcharasupat/query-bandit.",
          "arxiv_id": "2406.18747v2"
        },
        {
          "title": "Separate This, and All of these Things Around It: Music Source Separation via Hyperellipsoidal Queries",
          "year": "2025-01",
          "abstract": "Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.",
          "arxiv_id": "2501.16171v1"
        },
        {
          "title": "Task-Aware Unified Source Separation",
          "year": "2024-10",
          "abstract": "Several attempts have been made to handle multiple source separation tasks\nsuch as speech enhancement, speech separation, sound event separation, music\nsource separation (MSS), or cinematic audio source separation (CASS) with a\nsingle model. These models are trained on large-scale data including speech,\ninstruments, or sound events and can often successfully separate a wide range\nof sources. However, it is still challenging for such models to cover all\nseparation tasks because some of them are contradictory (e.g., musical\ninstruments are separated in MSS while they have to be grouped in CASS). To\novercome this issue and support all the major separation tasks, we propose a\ntask-aware unified source separation (TUSS) model. The model uses a variable\nnumber of learnable prompts to specify which source to separate, and changes\nits behavior depending on the given prompts, enabling it to handle all the\nmajor separation tasks including contradictory ones. Experimental results\ndemonstrate that the proposed TUSS model successfully handles the five major\nseparation tasks mentioned earlier. We also provide some audio examples,\nincluding both synthetic mixtures and real recordings, to demonstrate how\nflexibly the TUSS model changes its behavior at inference depending on the\nprompts.",
          "arxiv_id": "2410.23987v1"
        }
      ],
      "27": [
        {
          "title": "Comparison of linear and nonlinear methods for decoding selective attention to speech from ear-EEG recordings",
          "year": "2024-01",
          "abstract": "Many people with hearing loss struggle to comprehend speech in crowded\nauditory scenes, even when they are using hearing aids. It has recently been\ndemonstrated that the focus of a listener's selective attention to speech can\nbe decoded from their electroencephalography (EEG) recordings, raising the\nprospect of smart EEG-steered hearing aids which restore speech comprehension\nin adverse acoustic environments (such as the cocktail party). To this end, we\nhere assess the feasibility of using a novel, ultra-wearable ear-EEG device to\nclassify the selective attention of normal-hearing listeners who participated\nin a two-talker competing-speakers experiment.\n  Eighteen participants took part in a diotic listening task, whereby they were\nasked to attend to one narrator whilst ignoring the other. Encoding models were\nestimated from the recorded signals, and these confirmed that the device has\nthe ability to capture auditory responses that are consistent with those\nreported in high-density EEG studies. Several state-of-the-art auditory\nattention decoding algorithms were next compared, including\nstimulus-reconstruction algorithms based on linear regression as well as\nnon-linear deep neural networks, and canonical correlation analysis (CCA).\n  Meaningful markers of selective auditory attention could be extracted from\nthe ear-EEG signals of all 18 participants, even when those markers were\nderived from relatively short EEG segments of just five seconds in duration.\nAlgorithms which related the EEG signals to the rising edges of the speech\ntemporal envelope (onset envelope) were more successful than those which made\nuse of the temporal envelope itself. The CCA algorithm achieved the highest\nmean attention decoding accuracy, although differences between the performances\nof the three algorithms were both small and not statistically significant when\nEEG segments of short durations were employed.",
          "arxiv_id": "2401.05187v2"
        },
        {
          "title": "Frequency-Based Alignment of EEG and Audio Signals Using Contrastive Learning and SincNet for Auditory Attention Detection",
          "year": "2025-03",
          "abstract": "Humans exhibit a remarkable ability to focus auditory attention in complex\nacoustic environments, such as cocktail parties. Auditory attention detection\n(AAD) aims to identify the attended speaker by analyzing brain signals, such as\nelectroencephalography (EEG) data. Existing AAD algorithms often leverage deep\nlearning's powerful nonlinear modeling capabilities, few consider the neural\nmechanisms underlying auditory processing in the brain. In this paper, we\npropose SincAlignNet, a novel network based on an improved SincNet and\ncontrastive learning, designed to align audio and EEG features for auditory\nattention detection. The SincNet component simulates the brain's processing of\naudio during auditory attention, while contrastive learning guides the model to\nlearn the relationship between EEG signals and attended speech. During\ninference, we calculate the cosine similarity between EEG and audio features\nand also explore direct inference of the attended speaker using EEG data.\nCross-trial evaluations results demonstrate that SincAlignNet outperforms\nstate-of-the-art AAD methods on two publicly available datasets, KUL and DTU,\nachieving average accuracies of 78.3% and 92.2%, respectively, with a 1-second\ndecision window. The model exhibits strong interpretability, revealing that the\nleft and right temporal lobes are more active during both male and female\nspeaker scenarios. Furthermore, we found that using data from only six\nelectrodes near the temporal lobes maintains similar or even better performance\ncompared to using 64 electrodes. These findings indicate that efficient\nlow-density EEG online decoding is achievable, marking an important step toward\nthe practical implementation of neuro-guided hearing aids in real-world\napplications. Code is available at: https://github.com/LiaoEuan/SincAlignNet.",
          "arxiv_id": "2503.04156v1"
        },
        {
          "title": "Using Ear-EEG to Decode Auditory Attention in Multiple-speaker Environment",
          "year": "2024-09",
          "abstract": "Auditory Attention Decoding (AAD) can help to determine the identity of the\nattended speaker during an auditory selective attention task, by analyzing and\nprocessing measurements of electroencephalography (EEG) data. Most studies on\nAAD are based on scalp-EEG signals in two-speaker scenarios, which are far from\nreal application. Ear-EEG has recently gained significant attention due to its\nmotion tolerance and invisibility during data acquisition, making it easy to\nincorporate with other devices for applications. In this work, participants\nselectively attended to one of the four spatially separated speakers' speech in\nan anechoic room. The EEG data were concurrently collected from a scalp-EEG\nsystem and an ear-EEG system (cEEGrids). Temporal response functions (TRFs) and\nstimulus reconstruction (SR) were utilized using ear-EEG data. Results showed\nthat the attended speech TRFs were stronger than each unattended speech and\ndecoding accuracy was 41.3\\% in the 60s (chance level of 25\\%). To further\ninvestigate the impact of electrode placement and quantity, SR was utilized in\nboth scalp-EEG and ear-EEG, revealing that while the number of electrodes had a\nminor effect, their positioning had a significant influence on the decoding\naccuracy. One kind of auditory spatial attention detection (ASAD) method,\nSTAnet, was testified with this ear-EEG database, resulting in 93.1% in\n1-second decoding window. The implementation code and database for our work are\navailable on GitHub: https://github.com/zhl486/Ear_EEG_code.git and Zenodo:\nhttps://zenodo.org/records/10803261.",
          "arxiv_id": "2409.08710v1"
        }
      ],
      "28": [
        {
          "title": "First-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies Estimated by Metadata-Assisted Audio Generation",
          "year": "2023-10",
          "abstract": "First-shot (FS) unsupervised anomalous sound detection (ASD) is a brand-new\ntask introduced in DCASE 2023 Challenge Task 2, where the anomalous sounds for\nthe target machine types are unseen in training. Existing methods often rely on\nthe availability of normal and abnormal sound data from the target machines.\nHowever, due to the lack of anomalous sound data for the target machine types,\nit becomes challenging when adapting the existing ASD methods to the first-shot\ntask. In this paper, we propose a new framework for the first-shot unsupervised\nASD, where metadata-assisted audio generation is used to estimate unknown\nanomalies, by utilising the available machine information (i.e., metadata and\nsound data) to fine-tune a text-to-audio generation model for generating the\nanomalous sounds that contain unique acoustic characteristics accounting for\neach different machine type. We then use the method of Time-Weighted Frequency\ndomain audio Representation with Gaussian Mixture Model (TWFR-GMM) as the\nbackbone to achieve the first-shot unsupervised ASD. Our proposed FS-TWFR-GMM\nmethod achieves competitive performance amongst top systems in DCASE 2023\nChallenge Task 2, while requiring only 1% model parameters for detection, as\nvalidated in our experiments.",
          "arxiv_id": "2310.14173v2"
        },
        {
          "title": "SSDPT: Self-Supervised Dual-Path Transformer for Anomalous Sound Detection in Machine Condition Monitoring",
          "year": "2022-08",
          "abstract": "Anomalous sound detection for machine condition monitoring has great\npotential in the development of Industry 4.0. However, these anomalous sounds\nof machines are usually unavailable in normal conditions. Therefore, the models\nemployed have to learn acoustic representations with normal sounds for\ntraining, and detect anomalous sounds while testing. In this article, we\npropose a self-supervised dual-path Transformer (SSDPT) network to detect\nanomalous sounds in machine monitoring. The SSDPT network splits the acoustic\nfeatures into segments and employs several DPT blocks for time and frequency\nmodeling. DPT blocks use attention modules to alternately model the interactive\ninformation about the frequency and temporal components of the segmented\nacoustic features. To address the problem of lack of anomalous sound, we adopt\na self-supervised learning approach to train the network with normal sound.\nSpecifically, this approach randomly masks and reconstructs the acoustic\nfeatures, and jointly classifies machine identity information to improve the\nperformance of anomalous sound detection. We evaluated our method on the\nDCASE2021 task2 dataset. The experimental results show that the SSDPT network\nachieves a significant increase in the harmonic mean AUC score, in comparison\nto present state-of-the-art methods of anomalous sound detection.",
          "arxiv_id": "2208.03421v1"
        },
        {
          "title": "Transformer-based Autoencoder with ID Constraint for Unsupervised Anomalous Sound Detection",
          "year": "2023-10",
          "abstract": "Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous\nsounds of devices when only normal sound data is available. The autoencoder\n(AE) and self-supervised learning based methods are two mainstream methods.\nHowever, the AE-based methods could be limited as the feature learned from\nnormal sounds can also fit with anomalous sounds, reducing the ability of the\nmodel in detecting anomalies from sound. The self-supervised methods are not\nalways stable and perform differently, even for machines of the same type. In\naddition, the anomalous sound may be short-lived, making it even harder to\ndistinguish from normal sound. This paper proposes an ID constrained\nTransformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly\nscore computation for unsupervised ASD. Machine ID is employed to constrain the\nlatent space of the Transformer-based autoencoder (TransAE) by introducing a\nsimple ID classifier to learn the difference in the distribution for the same\nmachine type and enhance the ability of the model in distinguishing anomalous\nsound. Moreover, weighted anomaly score computation is introduced to highlight\nthe anomaly scores of anomalous events that only appear for a short time.\nExperiments performed on DCASE 2020 Challenge Task2 development dataset\ndemonstrate the effectiveness and superiority of our proposed method.",
          "arxiv_id": "2310.08950v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T19:45:42Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}