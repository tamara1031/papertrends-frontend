{
  "topics": {
    "data": {
      "0": {
        "name": "0_speech_ASR_Speech_recognition",
        "keywords": [
          [
            "speech",
            0.04062060918105922
          ],
          [
            "ASR",
            0.024316195344428172
          ],
          [
            "Speech",
            0.018842491550151292
          ],
          [
            "recognition",
            0.014365543904429979
          ],
          [
            "audio",
            0.013238557781016102
          ],
          [
            "speech recognition",
            0.012632413382648608
          ],
          [
            "speaker",
            0.012259217941309214
          ],
          [
            "end",
            0.010354529887422876
          ],
          [
            "model",
            0.010217441838759807
          ],
          [
            "TTS",
            0.008404921086892817
          ]
        ],
        "count": 5127
      },
      "1": {
        "name": "1_medical_clinical_health_patient",
        "keywords": [
          [
            "medical",
            0.024399062991927123
          ],
          [
            "clinical",
            0.021742038943279977
          ],
          [
            "health",
            0.011967445905241833
          ],
          [
            "patient",
            0.010265866094264596
          ],
          [
            "biomedical",
            0.009939724581763704
          ],
          [
            "healthcare",
            0.008091621019129799
          ],
          [
            "Medical",
            0.0078026707729610895
          ],
          [
            "LLMs",
            0.007675503127243838
          ],
          [
            "data",
            0.007594242949652793
          ],
          [
            "models",
            0.007085759410590755
          ]
        ],
        "count": 4181
      },
      "2": {
        "name": "2_image_visual_video_vision",
        "keywords": [
          [
            "image",
            0.024125423089751952
          ],
          [
            "visual",
            0.0213214111017995
          ],
          [
            "video",
            0.014084738428824774
          ],
          [
            "vision",
            0.01230918116170721
          ],
          [
            "images",
            0.012245589117955518
          ],
          [
            "multimodal",
            0.012232760292376601
          ],
          [
            "text",
            0.010615663895982334
          ],
          [
            "modal",
            0.009291927154727781
          ],
          [
            "Vision",
            0.008907995661815988
          ],
          [
            "vision language",
            0.008775470702098
          ]
        ],
        "count": 3775
      },
      "3": {
        "name": "3_attention_memory_training_model",
        "keywords": [
          [
            "attention",
            0.01374869022444147
          ],
          [
            "memory",
            0.011853813446510373
          ],
          [
            "training",
            0.010479019536634635
          ],
          [
            "model",
            0.009776852635424913
          ],
          [
            "quantization",
            0.009620209932520829
          ],
          [
            "performance",
            0.009586122334164827
          ],
          [
            "inference",
            0.009434261717533369
          ],
          [
            "KV",
            0.009125106444655064
          ],
          [
            "parameters",
            0.008612315782205702
          ],
          [
            "tuning",
            0.00859812439444571
          ]
        ],
        "count": 3171
      },
      "4": {
        "name": "4_attacks_safety_adversarial_attack",
        "keywords": [
          [
            "attacks",
            0.02056966475874318
          ],
          [
            "safety",
            0.019003431250658724
          ],
          [
            "adversarial",
            0.017748398733292804
          ],
          [
            "attack",
            0.016683197877489404
          ],
          [
            "privacy",
            0.015387731401899384
          ],
          [
            "LLMs",
            0.012066289493727119
          ],
          [
            "LLM",
            0.008586726847093327
          ],
          [
            "data",
            0.00849079320845948
          ],
          [
            "models",
            0.008368493196106943
          ],
          [
            "unlearning",
            0.008298768686707127
          ]
        ],
        "count": 2809
      },
      "5": {
        "name": "5_reasoning_Reasoning_mathematical_LLMs",
        "keywords": [
          [
            "reasoning",
            0.04018175169458317
          ],
          [
            "Reasoning",
            0.01299214792032191
          ],
          [
            "mathematical",
            0.012198918159976839
          ],
          [
            "LLMs",
            0.012082881662821107
          ],
          [
            "problems",
            0.009597011824855891
          ],
          [
            "models",
            0.008419793236129137
          ],
          [
            "tasks",
            0.008027552092701146
          ],
          [
            "performance",
            0.00799403374463182
          ],
          [
            "step",
            0.007965176359125185
          ],
          [
            "reasoning tasks",
            0.007612609663100965
          ]
        ],
        "count": 2761
      },
      "6": {
        "name": "6_dialogue_Dialogue_dialog_response",
        "keywords": [
          [
            "dialogue",
            0.03729850400286145
          ],
          [
            "Dialogue",
            0.01588801215937108
          ],
          [
            "dialog",
            0.013427041442501522
          ],
          [
            "response",
            0.010984258570804152
          ],
          [
            "conversational",
            0.010524252192275605
          ],
          [
            "user",
            0.01038972209104008
          ],
          [
            "systems",
            0.01028074889850503
          ],
          [
            "responses",
            0.009734482579576952
          ],
          [
            "task",
            0.009459112086060885
          ],
          [
            "conversation",
            0.009398968859710493
          ]
        ],
        "count": 2422
      },
      "7": {
        "name": "7_translation_Translation_machine translation_NMT",
        "keywords": [
          [
            "translation",
            0.05226210290672244
          ],
          [
            "Translation",
            0.026509878776886427
          ],
          [
            "machine translation",
            0.023528237106951337
          ],
          [
            "NMT",
            0.020476558686863003
          ],
          [
            "Machine",
            0.020086040614908283
          ],
          [
            "machine",
            0.019558832327331667
          ],
          [
            "MT",
            0.018704183461367523
          ],
          [
            "languages",
            0.013169607272870242
          ],
          [
            "English",
            0.012913213999830595
          ],
          [
            "translations",
            0.012271255359363409
          ]
        ],
        "count": 1725
      },
      "8": {
        "name": "8_bias_biases_gender_LLMs",
        "keywords": [
          [
            "bias",
            0.03395495347174839
          ],
          [
            "biases",
            0.025005378841736158
          ],
          [
            "gender",
            0.023520630823765695
          ],
          [
            "LLMs",
            0.014225838243309452
          ],
          [
            "AI",
            0.011994414856916717
          ],
          [
            "Bias",
            0.01179034118209307
          ],
          [
            "social",
            0.011457873535994123
          ],
          [
            "moral",
            0.009989683605619352
          ],
          [
            "human",
            0.009768979972031418
          ],
          [
            "gender bias",
            0.009095006683072018
          ]
        ],
        "count": 1502
      },
      "9": {
        "name": "9_parsing_syntactic_semantic_language",
        "keywords": [
          [
            "parsing",
            0.02256449042564684
          ],
          [
            "syntactic",
            0.019604923646720146
          ],
          [
            "semantic",
            0.012715707399825154
          ],
          [
            "language",
            0.011989862455722422
          ],
          [
            "dependency",
            0.01176934911968636
          ],
          [
            "linguistic",
            0.010386652637425204
          ],
          [
            "Parsing",
            0.009884791294369918
          ],
          [
            "neural",
            0.009195183025629374
          ],
          [
            "word",
            0.009101916751000366
          ],
          [
            "models",
            0.009095510061062403
          ]
        ],
        "count": 1266
      },
      "10": {
        "name": "10_summarization_summaries_Summarization_summary",
        "keywords": [
          [
            "summarization",
            0.05356986316051991
          ],
          [
            "summaries",
            0.030684069560102164
          ],
          [
            "Summarization",
            0.02618411824822562
          ],
          [
            "summary",
            0.023390323379535803
          ],
          [
            "abstractive",
            0.01676171482486188
          ],
          [
            "document",
            0.01515515002252263
          ],
          [
            "text summarization",
            0.010259208354982686
          ],
          [
            "Abstractive",
            0.009780181535308378
          ],
          [
            "text",
            0.009356482645290033
          ],
          [
            "abstractive summarization",
            0.008901791691926553
          ]
        ],
        "count": 1211
      },
      "11": {
        "name": "11_embeddings_word_semantic_sentence",
        "keywords": [
          [
            "embeddings",
            0.028890032058456524
          ],
          [
            "word",
            0.028688831254679706
          ],
          [
            "semantic",
            0.017790864793739838
          ],
          [
            "sentence",
            0.016751944908657463
          ],
          [
            "similarity",
            0.015389288573105018
          ],
          [
            "words",
            0.013544249639244964
          ],
          [
            "Word",
            0.011879558418097944
          ],
          [
            "sense",
            0.011876837207549316
          ],
          [
            "word embeddings",
            0.011532365139663953
          ],
          [
            "Embeddings",
            0.009758752390242795
          ]
        ],
        "count": 1144
      },
      "12": {
        "name": "12_hate_hate speech_speech_detection",
        "keywords": [
          [
            "hate",
            0.045932117390905636
          ],
          [
            "hate speech",
            0.03837402385985109
          ],
          [
            "speech",
            0.02587171812319407
          ],
          [
            "detection",
            0.021329972857796035
          ],
          [
            "Hate",
            0.019058789803497005
          ],
          [
            "social",
            0.017649234305592787
          ],
          [
            "offensive",
            0.016457419069105274
          ],
          [
            "content",
            0.016399334848688274
          ],
          [
            "speech detection",
            0.01540410271430517
          ],
          [
            "media",
            0.015249863802664914
          ]
        ],
        "count": 1063
      },
      "13": {
        "name": "13_code_Code_code generation_programming",
        "keywords": [
          [
            "code",
            0.0605759100683276
          ],
          [
            "Code",
            0.0215659422956126
          ],
          [
            "code generation",
            0.02116962612940355
          ],
          [
            "programming",
            0.015343989959637391
          ],
          [
            "generation",
            0.014705181902029447
          ],
          [
            "software",
            0.012746661638733645
          ],
          [
            "source",
            0.008558089835561576
          ],
          [
            "program",
            0.00796877206018669
          ],
          [
            "models",
            0.00761690790001427
          ],
          [
            "language",
            0.0075358154254116586
          ]
        ],
        "count": 1037
      },
      "14": {
        "name": "14_prompt_prompts_context_shot",
        "keywords": [
          [
            "prompt",
            0.03091508327077483
          ],
          [
            "prompts",
            0.019163908468660073
          ],
          [
            "context",
            0.01598426218552289
          ],
          [
            "shot",
            0.015860465411970115
          ],
          [
            "Prompt",
            0.014325872703636586
          ],
          [
            "learning",
            0.013555644998414232
          ],
          [
            "context learning",
            0.013548162936276715
          ],
          [
            "tasks",
            0.012957525519054913
          ],
          [
            "examples",
            0.011612388285590766
          ],
          [
            "demonstrations",
            0.011499116796334655
          ]
        ],
        "count": 957
      },
      "15": {
        "name": "15_sentiment_Sentiment_sentiment analysis_aspect",
        "keywords": [
          [
            "sentiment",
            0.053780831211263806
          ],
          [
            "Sentiment",
            0.031057222052610026
          ],
          [
            "sentiment analysis",
            0.027290250144617025
          ],
          [
            "aspect",
            0.023031456892575766
          ],
          [
            "analysis",
            0.021741172512761895
          ],
          [
            "Aspect",
            0.019351303352202476
          ],
          [
            "Analysis",
            0.01648581421261606
          ],
          [
            "reviews",
            0.014112160962662122
          ],
          [
            "opinion",
            0.011694156306037306
          ],
          [
            "classification",
            0.009799251912704695
          ]
        ],
        "count": 924
      },
      "16": {
        "name": "16_scientific_papers_citation_research",
        "keywords": [
          [
            "scientific",
            0.030464748984054688
          ],
          [
            "papers",
            0.023654357072357676
          ],
          [
            "citation",
            0.018394344518674458
          ],
          [
            "research",
            0.017152239406861173
          ],
          [
            "academic",
            0.011275217845514513
          ],
          [
            "review",
            0.01103391967297355
          ],
          [
            "literature",
            0.010976188218353328
          ],
          [
            "Scientific",
            0.010885646926964155
          ],
          [
            "paper",
            0.00987760049484962
          ],
          [
            "citations",
            0.00907226195685782
          ]
        ],
        "count": 900
      },
      "17": {
        "name": "17_languages_lingual_multilingual_cross",
        "keywords": [
          [
            "languages",
            0.03204683721734877
          ],
          [
            "lingual",
            0.0247158750745237
          ],
          [
            "multilingual",
            0.02068392606853692
          ],
          [
            "cross",
            0.017822521349245397
          ],
          [
            "language",
            0.01731337530898522
          ],
          [
            "transfer",
            0.015115691226114098
          ],
          [
            "resource",
            0.013628476379862961
          ],
          [
            "lingual transfer",
            0.011346675201026881
          ],
          [
            "models",
            0.011018602810489102
          ],
          [
            "English",
            0.010935444835242553
          ]
        ],
        "count": 876
      },
      "18": {
        "name": "18_agents_agent_LLM_Agent",
        "keywords": [
          [
            "agents",
            0.03749705331023628
          ],
          [
            "agent",
            0.03325725607923333
          ],
          [
            "LLM",
            0.014509362148291273
          ],
          [
            "Agent",
            0.013170513969382075
          ],
          [
            "tasks",
            0.012037377525467598
          ],
          [
            "Agents",
            0.01182782482629305
          ],
          [
            "planning",
            0.011494469790950984
          ],
          [
            "environments",
            0.010522609904056809
          ],
          [
            "web",
            0.009767635815100989
          ],
          [
            "multi agent",
            0.008870024946829695
          ]
        ],
        "count": 869
      },
      "19": {
        "name": "19_generation_text generation_text_Generation",
        "keywords": [
          [
            "generation",
            0.030446043676435282
          ],
          [
            "text generation",
            0.022818648154322763
          ],
          [
            "text",
            0.022736496288233782
          ],
          [
            "Generation",
            0.014681296277845427
          ],
          [
            "NLG",
            0.013279732967400293
          ],
          [
            "evaluation",
            0.012740505351002053
          ],
          [
            "metrics",
            0.011261684803022842
          ],
          [
            "Text",
            0.011094717333084885
          ],
          [
            "human",
            0.009772394724955738
          ],
          [
            "language generation",
            0.008992536110649893
          ]
        ],
        "count": 790
      },
      "20": {
        "name": "20_preference_reward_alignment_RLHF",
        "keywords": [
          [
            "preference",
            0.03582478970600612
          ],
          [
            "reward",
            0.032653052812303844
          ],
          [
            "alignment",
            0.02490670368374415
          ],
          [
            "RLHF",
            0.02480129171058794
          ],
          [
            "preferences",
            0.024084034554992122
          ],
          [
            "DPO",
            0.023517579316384582
          ],
          [
            "Preference",
            0.019949373284622717
          ],
          [
            "human",
            0.017841481636454472
          ],
          [
            "human preferences",
            0.015810868826445234
          ],
          [
            "Optimization",
            0.014401116878353324
          ]
        ],
        "count": 757
      },
      "21": {
        "name": "21_media_social_political_tweets",
        "keywords": [
          [
            "media",
            0.025666354315380415
          ],
          [
            "social",
            0.023453532989848817
          ],
          [
            "political",
            0.023023564447501497
          ],
          [
            "tweets",
            0.018140871119435145
          ],
          [
            "Twitter",
            0.017915753577567733
          ],
          [
            "pandemic",
            0.017307126494194407
          ],
          [
            "public",
            0.016823192751840334
          ],
          [
            "social media",
            0.015792201558998246
          ],
          [
            "news",
            0.014418985418366463
          ],
          [
            "analysis",
            0.013950635518030758
          ]
        ],
        "count": 696
      },
      "22": {
        "name": "22_legal_Legal_case_law",
        "keywords": [
          [
            "legal",
            0.09558993505476308
          ],
          [
            "Legal",
            0.036957995614136305
          ],
          [
            "case",
            0.017382581667983923
          ],
          [
            "law",
            0.01421020414246352
          ],
          [
            "legal domain",
            0.012358322577394828
          ],
          [
            "domain",
            0.012080296679140129
          ],
          [
            "cases",
            0.010363580189200626
          ],
          [
            "documents",
            0.009583010278279075
          ],
          [
            "retrieval",
            0.008467984373649151
          ],
          [
            "dataset",
            0.007875595066957316
          ]
        ],
        "count": 626
      },
      "23": {
        "name": "23_SQL_table_tables_Text",
        "keywords": [
          [
            "SQL",
            0.07277739490670192
          ],
          [
            "table",
            0.03411295480302679
          ],
          [
            "tables",
            0.021366502103270547
          ],
          [
            "Text",
            0.016998049087301625
          ],
          [
            "schema",
            0.014351257325605727
          ],
          [
            "Table",
            0.014308852977587971
          ],
          [
            "database",
            0.014216433700972204
          ],
          [
            "queries",
            0.01407853069147392
          ],
          [
            "tabular",
            0.012174203314563318
          ],
          [
            "data",
            0.010865724934735485
          ]
        ],
        "count": 615
      },
      "24": {
        "name": "24_NER_Named_entity_Entity",
        "keywords": [
          [
            "NER",
            0.07209690407379606
          ],
          [
            "Named",
            0.03706958633829009
          ],
          [
            "entity",
            0.03280921897273306
          ],
          [
            "Entity",
            0.03257539160719315
          ],
          [
            "Recognition",
            0.02661299156684222
          ],
          [
            "entities",
            0.018307561974826816
          ],
          [
            "entity recognition",
            0.017901284931094837
          ],
          [
            "recognition",
            0.014004790181955593
          ],
          [
            "data",
            0.00841912695114407
          ],
          [
            "domain",
            0.008318408370347271
          ]
        ],
        "count": 603
      },
      "25": {
        "name": "25_emotion_Emotion_emotions_emotional",
        "keywords": [
          [
            "emotion",
            0.05915717165848629
          ],
          [
            "Emotion",
            0.03171442704605332
          ],
          [
            "emotions",
            0.026612532083580148
          ],
          [
            "emotional",
            0.016422489320820514
          ],
          [
            "emotion recognition",
            0.014698613255427186
          ],
          [
            "multimodal",
            0.014234949061830518
          ],
          [
            "ERC",
            0.012905841695436444
          ],
          [
            "sentiment",
            0.012114811427251963
          ],
          [
            "recognition",
            0.011561836339219646
          ],
          [
            "modalities",
            0.010382483748773923
          ]
        ],
        "count": 599
      },
      "26": {
        "name": "26_RAG_Retrieval_retrieval_Augmented",
        "keywords": [
          [
            "RAG",
            0.0621687132706968
          ],
          [
            "Retrieval",
            0.02877865344298528
          ],
          [
            "retrieval",
            0.026190708214509605
          ],
          [
            "Augmented",
            0.02374598015814164
          ],
          [
            "Generation",
            0.01850549745355694
          ],
          [
            "knowledge",
            0.013631405047705807
          ],
          [
            "generation",
            0.012853192264714895
          ],
          [
            "augmented",
            0.012506264973252777
          ],
          [
            "LLMs",
            0.011543623277304755
          ],
          [
            "context",
            0.011528106387278384
          ]
        ],
        "count": 597
      },
      "27": {
        "name": "27_relation_extraction_Relation_RE",
        "keywords": [
          [
            "relation",
            0.04668337313386253
          ],
          [
            "extraction",
            0.035753194684129644
          ],
          [
            "Relation",
            0.03145276842433441
          ],
          [
            "RE",
            0.03067151593113271
          ],
          [
            "relation extraction",
            0.029574983072802576
          ],
          [
            "Extraction",
            0.029150363144281804
          ],
          [
            "relations",
            0.0211728748501977
          ],
          [
            "entity",
            0.017047416593057238
          ],
          [
            "IE",
            0.011882870987822695
          ],
          [
            "entities",
            0.0117006158237321
          ]
        ],
        "count": 591
      },
      "28": {
        "name": "28_recommendation_user_product_commerce",
        "keywords": [
          [
            "recommendation",
            0.03643821837926111
          ],
          [
            "user",
            0.025120543706174576
          ],
          [
            "product",
            0.024211444161818817
          ],
          [
            "commerce",
            0.02181072262246178
          ],
          [
            "item",
            0.01678366138612243
          ],
          [
            "recommender",
            0.014005272988500732
          ],
          [
            "Recommendation",
            0.013937021114886258
          ],
          [
            "recommendations",
            0.012215761080440514
          ],
          [
            "items",
            0.011659570616315703
          ],
          [
            "users",
            0.010126350399354746
          ]
        ],
        "count": 574
      },
      "29": {
        "name": "29_financial_Financial_stock_market",
        "keywords": [
          [
            "financial",
            0.054740737327940756
          ],
          [
            "Financial",
            0.019150942228314247
          ],
          [
            "stock",
            0.017178215248315566
          ],
          [
            "market",
            0.016193481113957856
          ],
          [
            "sentiment",
            0.013781484634686716
          ],
          [
            "analysis",
            0.011328366755581954
          ],
          [
            "finance",
            0.01100217284914821
          ],
          [
            "data",
            0.010302534269943231
          ],
          [
            "news",
            0.01026042311810889
          ],
          [
            "forecasting",
            0.009931513886721918
          ]
        ],
        "count": 556
      },
      "30": {
        "name": "30_hallucination_LLMs_uncertainty_Hallucination",
        "keywords": [
          [
            "hallucination",
            0.03332534933853742
          ],
          [
            "LLMs",
            0.017094021394449873
          ],
          [
            "uncertainty",
            0.015599292650327733
          ],
          [
            "Hallucination",
            0.015534101550933361
          ],
          [
            "hallucination detection",
            0.014953458925190366
          ],
          [
            "LLM",
            0.012635317747906123
          ],
          [
            "detection",
            0.011507644686591373
          ],
          [
            "Large",
            0.010387636462681265
          ],
          [
            "confidence",
            0.010335087078557535
          ],
          [
            "factual",
            0.009401968229372245
          ]
        ],
        "count": 544
      },
      "31": {
        "name": "31_explanations_explanation_rationales_methods",
        "keywords": [
          [
            "explanations",
            0.047360016700525996
          ],
          [
            "explanation",
            0.023121406781653593
          ],
          [
            "rationales",
            0.013877921637445827
          ],
          [
            "methods",
            0.012154093606200416
          ],
          [
            "model",
            0.011856680354317263
          ],
          [
            "interpretability",
            0.011402434049512137
          ],
          [
            "Explanations",
            0.011007793038039617
          ],
          [
            "faithfulness",
            0.01085039012763735
          ],
          [
            "NLP",
            0.010714671034245627
          ],
          [
            "human",
            0.010388756125405444
          ]
        ],
        "count": 516
      },
      "32": {
        "name": "32_students_student_educational_questions",
        "keywords": [
          [
            "students",
            0.025399042718504752
          ],
          [
            "student",
            0.02265266917874696
          ],
          [
            "educational",
            0.022268297496596395
          ],
          [
            "questions",
            0.01767035839229345
          ],
          [
            "education",
            0.013776301082333312
          ],
          [
            "feedback",
            0.011989948927397109
          ],
          [
            "learning",
            0.011958600269934307
          ],
          [
            "AI",
            0.011392987658767091
          ],
          [
            "grading",
            0.01062595899913463
          ],
          [
            "tutoring",
            0.010515527337468397
          ]
        ],
        "count": 511
      },
      "33": {
        "name": "33_graph_Knowledge_knowledge_KG",
        "keywords": [
          [
            "graph",
            0.033856939052714995
          ],
          [
            "Knowledge",
            0.031019408414707957
          ],
          [
            "knowledge",
            0.02901236587022613
          ],
          [
            "KG",
            0.027146239587050546
          ],
          [
            "KGs",
            0.02388973770617142
          ],
          [
            "Graph",
            0.022735092291389338
          ],
          [
            "knowledge graph",
            0.022147272723810375
          ],
          [
            "entities",
            0.02048624760705301
          ],
          [
            "graphs",
            0.019935440141844937
          ],
          [
            "entity",
            0.018346642998117297
          ]
        ],
        "count": 489
      },
      "34": {
        "name": "34_Arabic_morphological_languages_NLP",
        "keywords": [
          [
            "Arabic",
            0.045105378491400075
          ],
          [
            "morphological",
            0.030119970488386095
          ],
          [
            "languages",
            0.017563311347066003
          ],
          [
            "NLP",
            0.013905543696952578
          ],
          [
            "language",
            0.012710529035062706
          ],
          [
            "dialects",
            0.012416992184589245
          ],
          [
            "word",
            0.010185985340707861
          ],
          [
            "segmentation",
            0.009572573902318216
          ],
          [
            "dialect",
            0.00950248674085163
          ],
          [
            "tokenization",
            0.009338269924899681
          ]
        ],
        "count": 480
      },
      "35": {
        "name": "35_QA_question_answer_Question",
        "keywords": [
          [
            "QA",
            0.03600162333786649
          ],
          [
            "question",
            0.029817941994233387
          ],
          [
            "answer",
            0.027802537572199883
          ],
          [
            "Question",
            0.02469565194130429
          ],
          [
            "questions",
            0.020027775140104266
          ],
          [
            "Answering",
            0.019079496870548295
          ],
          [
            "Question Answering",
            0.018644927210938415
          ],
          [
            "answering",
            0.01706092765009257
          ],
          [
            "answers",
            0.01700782616007881
          ],
          [
            "question answering",
            0.014281776850660026
          ]
        ],
        "count": 473
      },
      "36": {
        "name": "36_event_temporal_Event_events",
        "keywords": [
          [
            "event",
            0.0671003349136599
          ],
          [
            "temporal",
            0.04034096453668841
          ],
          [
            "Event",
            0.03110956396741309
          ],
          [
            "events",
            0.022437231104327084
          ],
          [
            "extraction",
            0.02064843192988464
          ],
          [
            "Temporal",
            0.017053452173941996
          ],
          [
            "event extraction",
            0.015019386156645336
          ],
          [
            "Extraction",
            0.012446081666889033
          ],
          [
            "argument",
            0.011401337723970341
          ],
          [
            "temporal reasoning",
            0.011143889251653185
          ]
        ],
        "count": 458
      },
      "37": {
        "name": "37_news_fake_fake news_Fake",
        "keywords": [
          [
            "news",
            0.08282021930391899
          ],
          [
            "fake",
            0.06793404459036813
          ],
          [
            "fake news",
            0.06509283178997992
          ],
          [
            "Fake",
            0.03020071002506822
          ],
          [
            "news detection",
            0.028249753660959716
          ],
          [
            "detection",
            0.02780612913614154
          ],
          [
            "fake news detection",
            0.025940586641061185
          ],
          [
            "News",
            0.02506452066576684
          ],
          [
            "Detection",
            0.017536878326923095
          ],
          [
            "misinformation",
            0.017482845867300718
          ]
        ],
        "count": 458
      },
      "38": {
        "name": "38_fact_checking_fact checking_claim",
        "keywords": [
          [
            "fact",
            0.05454013413311928
          ],
          [
            "checking",
            0.04572007192116565
          ],
          [
            "fact checking",
            0.043308197319586815
          ],
          [
            "claim",
            0.04303825379823236
          ],
          [
            "claims",
            0.03984682963227618
          ],
          [
            "evidence",
            0.029931532157290556
          ],
          [
            "Fact",
            0.02564440185219234
          ],
          [
            "verification",
            0.024332043898442395
          ],
          [
            "Claim",
            0.013467357091482519
          ],
          [
            "veracity",
            0.012084550505274863
          ]
        ],
        "count": 451
      },
      "39": {
        "name": "39_retrieval_ranking_query_IR",
        "keywords": [
          [
            "retrieval",
            0.04019812191838269
          ],
          [
            "ranking",
            0.021907456243431656
          ],
          [
            "query",
            0.018994960683595317
          ],
          [
            "IR",
            0.017792472300943683
          ],
          [
            "dense",
            0.016873763714842623
          ],
          [
            "Retrieval",
            0.015869117505109012
          ],
          [
            "document",
            0.015211163047506747
          ],
          [
            "documents",
            0.014388209641997924
          ],
          [
            "queries",
            0.01223692377347074
          ],
          [
            "relevance",
            0.011765805059163912
          ]
        ],
        "count": 435
      },
      "40": {
        "name": "40_story_stories_narrative_story generation",
        "keywords": [
          [
            "story",
            0.04069661259524994
          ],
          [
            "stories",
            0.029486841519760682
          ],
          [
            "narrative",
            0.026069536024780307
          ],
          [
            "story generation",
            0.01395621933480092
          ],
          [
            "generation",
            0.013069222925869447
          ],
          [
            "Story",
            0.012916998870725994
          ],
          [
            "human",
            0.011683803799824517
          ],
          [
            "narratives",
            0.011436957179270263
          ],
          [
            "character",
            0.010460512671773049
          ],
          [
            "characters",
            0.010023584582897533
          ]
        ],
        "count": 411
      },
      "41": {
        "name": "41_protein_materials_molecular_chemical",
        "keywords": [
          [
            "protein",
            0.027456400805468763
          ],
          [
            "materials",
            0.024181915747159662
          ],
          [
            "molecular",
            0.023292211445822445
          ],
          [
            "chemical",
            0.016063320810825486
          ],
          [
            "molecule",
            0.01517975961391813
          ],
          [
            "chemistry",
            0.011789652724310455
          ],
          [
            "discovery",
            0.011579694870201
          ],
          [
            "materials science",
            0.011522049632651394
          ],
          [
            "scientific",
            0.011485726147103689
          ],
          [
            "science",
            0.010736962914236701
          ]
        ],
        "count": 383
      },
      "42": {
        "name": "42_simplification_paraphrase_sentence_Simplification",
        "keywords": [
          [
            "simplification",
            0.03497039625461535
          ],
          [
            "paraphrase",
            0.028550890360652895
          ],
          [
            "sentence",
            0.018100309220270443
          ],
          [
            "Simplification",
            0.017331014621070652
          ],
          [
            "text simplification",
            0.01508964520864427
          ],
          [
            "Paraphrase",
            0.014834879931225578
          ],
          [
            "text",
            0.014663803702949486
          ],
          [
            "paraphrases",
            0.014058546964771802
          ],
          [
            "paraphrasing",
            0.011893315301493857
          ],
          [
            "sentences",
            0.011727446905898132
          ]
        ],
        "count": 374
      },
      "43": {
        "name": "43_text_AI_detection_detectors",
        "keywords": [
          [
            "text",
            0.02979150336835385
          ],
          [
            "AI",
            0.029042313668178242
          ],
          [
            "detection",
            0.028885364180048164
          ],
          [
            "detectors",
            0.02668484700801065
          ],
          [
            "texts",
            0.021375072475335596
          ],
          [
            "Generated",
            0.01946195871806287
          ],
          [
            "Generated Text",
            0.017403057984866497
          ],
          [
            "human",
            0.01670744988897693
          ],
          [
            "text detection",
            0.01438483807865685
          ],
          [
            "Text",
            0.01413012000224883
          ]
        ],
        "count": 345
      },
      "44": {
        "name": "44_classification_text classification_label_data",
        "keywords": [
          [
            "classification",
            0.024041322604099396
          ],
          [
            "text classification",
            0.019465162424781305
          ],
          [
            "label",
            0.017386579988970616
          ],
          [
            "data",
            0.016132966698223877
          ],
          [
            "labels",
            0.015890154102499568
          ],
          [
            "text",
            0.01464527034312751
          ],
          [
            "augmentation",
            0.01383024364456358
          ],
          [
            "Classification",
            0.011975426794332113
          ],
          [
            "learning",
            0.011401102349091434
          ],
          [
            "training",
            0.011315128016989853
          ]
        ],
        "count": 331
      },
      "45": {
        "name": "45_instruction_Instruction_tuning_instructions",
        "keywords": [
          [
            "instruction",
            0.0562536391345117
          ],
          [
            "Instruction",
            0.02734833772795747
          ],
          [
            "tuning",
            0.023193067333539853
          ],
          [
            "instructions",
            0.022592925383329434
          ],
          [
            "instruction tuning",
            0.02112930772496881
          ],
          [
            "data",
            0.01703468537614423
          ],
          [
            "following",
            0.015629093622653495
          ],
          [
            "LLMs",
            0.013011114953407142
          ],
          [
            "quality",
            0.011293132369929414
          ],
          [
            "Tuning",
            0.011260319881733207
          ]
        ],
        "count": 320
      },
      "46": {
        "name": "46_navigation_instructions_robot_Navigation",
        "keywords": [
          [
            "navigation",
            0.033532371891655116
          ],
          [
            "instructions",
            0.02434407399597927
          ],
          [
            "robot",
            0.023213037438796387
          ],
          [
            "Navigation",
            0.023149373373369725
          ],
          [
            "environments",
            0.01981020374723886
          ],
          [
            "Vision",
            0.018039213431788964
          ],
          [
            "agent",
            0.016860296232119064
          ],
          [
            "embodied",
            0.015433561885262449
          ],
          [
            "environment",
            0.014757366868756512
          ],
          [
            "language",
            0.014461276441108548
          ]
        ],
        "count": 316
      },
      "47": {
        "name": "47_quantum_Quantum_classical_automata",
        "keywords": [
          [
            "quantum",
            0.0763917202091036
          ],
          [
            "Quantum",
            0.025937601331102907
          ],
          [
            "classical",
            0.017808955456930755
          ],
          [
            "automata",
            0.016515140713138657
          ],
          [
            "finite",
            0.01554277764844578
          ],
          [
            "theory",
            0.013365738466946837
          ],
          [
            "circuits",
            0.012622294760053308
          ],
          [
            "computing",
            0.012590571937720207
          ],
          [
            "calculus",
            0.012572611347418395
          ],
          [
            "logic",
            0.012026541640631537
          ]
        ],
        "count": 313
      },
      "48": {
        "name": "48_evaluation_LLM_LLMs_benchmarks",
        "keywords": [
          [
            "evaluation",
            0.02799586528681683
          ],
          [
            "LLM",
            0.02037620761669765
          ],
          [
            "LLMs",
            0.01584817041625087
          ],
          [
            "benchmarks",
            0.015219681985953942
          ],
          [
            "evaluators",
            0.011223146118577318
          ],
          [
            "judges",
            0.01118225607511149
          ],
          [
            "human",
            0.01083080268252282
          ],
          [
            "benchmark",
            0.010526715806594093
          ],
          [
            "evaluations",
            0.009486958588954698
          ],
          [
            "Large",
            0.009425738059522078
          ]
        ],
        "count": 295
      },
      "49": {
        "name": "49_editing_knowledge_Editing_Knowledge",
        "keywords": [
          [
            "editing",
            0.06304876643552668
          ],
          [
            "knowledge",
            0.052368138154144456
          ],
          [
            "Editing",
            0.024734483530398626
          ],
          [
            "Knowledge",
            0.02045449007098327
          ],
          [
            "knowledge editing",
            0.01931827466338416
          ],
          [
            "model editing",
            0.018543894103911027
          ],
          [
            "facts",
            0.013967055395509137
          ],
          [
            "LLMs",
            0.013402434391572519
          ],
          [
            "editing methods",
            0.013315173959346943
          ],
          [
            "factual",
            0.013253586259122669
          ]
        ],
        "count": 283
      }
    },
    "correlations": [
      [
        1.0,
        -0.7461649495179129,
        -0.7401019201456545,
        -0.7264976746497,
        -0.7402938771497978,
        -0.7513936999220994,
        -0.7400393166598102,
        -0.686408199533068,
        -0.7319270042350827,
        -0.7424766007628683,
        -0.7567451229172626,
        -0.7045141835929092,
        -0.5206318802619799,
        -0.7257346506162252,
        -0.7360634940536614,
        -0.7507931214390179,
        -0.7486446273388538,
        -0.6635469475665994,
        -0.7553404071623129,
        -0.7273555789785392,
        -0.7426442270258455,
        -0.7429920694786252,
        -0.7627373527776786,
        -0.7638030170467296,
        -0.7373613090508633,
        -0.7063195153293261,
        -0.7603894790704809,
        -0.7525895662255053,
        -0.7517967232276719,
        -0.7622667218151229,
        -0.7441556169393619,
        -0.760985767503713,
        -0.7443522513625997,
        -0.7478956585715058,
        -0.7160446595742793,
        -0.7512306327556177,
        -0.7348280167760419,
        -0.7532167700918874,
        -0.7500689204049696,
        -0.7532504239357223,
        -0.7595067848535157,
        -0.7516371414677991,
        -0.7552966053495354,
        -0.7133536262963088,
        -0.7166276422090869,
        -0.7336686804534891,
        -0.7543063188788711,
        -0.7654311541113826,
        -0.7323276295171324,
        -0.7353975684793493
      ],
      [
        -0.7461649495179129,
        1.0,
        -0.7209390720543758,
        -0.5683821924286226,
        -0.5809785408698451,
        -0.5860154939215395,
        -0.7320989198316519,
        -0.7490112017268636,
        -0.5903450827947241,
        -0.7453046476369765,
        -0.7154168083586261,
        -0.7375416215279571,
        -0.7347321468341925,
        -0.7061162077318546,
        -0.6186682573698357,
        -0.7458387883857586,
        -0.7254456458292992,
        -0.7338511638968866,
        -0.7270307575630532,
        -0.7186913173316676,
        -0.619434772222808,
        -0.7085075608252152,
        -0.756558160308378,
        -0.7581538942733315,
        -0.7190808045224273,
        -0.7526878158976413,
        -0.7176401111361814,
        -0.7101815475050688,
        -0.74366077165654,
        -0.7502141786329782,
        -0.5655544579161254,
        -0.7369576787110699,
        -0.5937059023921456,
        -0.7217934842584415,
        -0.7533156960720299,
        -0.6951080849286841,
        -0.7340870440903018,
        -0.7537486645379302,
        -0.7336676493627079,
        -0.7289976053555783,
        -0.750258794070841,
        -0.5653555334629528,
        -0.7526288407019648,
        -0.7071690384771544,
        -0.710197167746761,
        -0.6122964038197702,
        -0.7424265885963013,
        -0.7652212655280105,
        -0.5463774336986128,
        -0.7118047613135826
      ],
      [
        -0.7401019201456545,
        -0.7209390720543758,
        1.0,
        -0.7154202802013414,
        -0.7213462187445001,
        -0.6885113620563503,
        -0.7491186097537268,
        -0.7397120443446163,
        -0.7181941893787461,
        -0.7403388848800627,
        -0.7502541206822251,
        -0.7207406067124105,
        -0.7437865918929834,
        -0.685185523961592,
        -0.6975228515964075,
        -0.7426528299534272,
        -0.7423024367059217,
        -0.7208809451960335,
        -0.7360543312234937,
        -0.6804190400163173,
        -0.7216217118660945,
        -0.7454034408559794,
        -0.7639566008163778,
        -0.7573103791713995,
        -0.7548521129632291,
        -0.7548967507514975,
        -0.7462356510909984,
        -0.733702737411273,
        -0.7439270387538989,
        -0.759629606348571,
        -0.7075594522624571,
        -0.7432460977072948,
        -0.7406115033699969,
        -0.7332483806561685,
        -0.7573873320893039,
        -0.6599201369720704,
        -0.7259431642226923,
        -0.7504959980768939,
        -0.747183311956837,
        -0.7038764820352292,
        -0.7429846205256299,
        -0.7408168131300612,
        -0.7564416780349486,
        -0.6905575487489184,
        -0.7264293024691689,
        -0.6959942275142132,
        -0.5380753404699451,
        -0.7650317209557191,
        -0.7012026172205877,
        -0.7091370560313035
      ],
      [
        -0.7264976746497,
        -0.5683821924286226,
        -0.7154202802013414,
        1.0,
        -0.18540966173728207,
        -0.26368337179394286,
        -0.7264358444982173,
        -0.7200123434638159,
        -0.2411263582340494,
        -0.6937599125665448,
        -0.7224140832635638,
        -0.7183044919773995,
        -0.7332802291980133,
        -0.6307817471016539,
        -0.502911535983386,
        -0.7268997047972516,
        -0.7060726656360496,
        -0.6722967336319561,
        -0.6922540038819213,
        -0.673359994825909,
        -0.5414139673060921,
        -0.7315122164476338,
        -0.7434893738222108,
        -0.7429888506134006,
        -0.7454266366268298,
        -0.7473832924393625,
        -0.6986204284104447,
        -0.7294956771201087,
        -0.7223932827583455,
        -0.7368016924488426,
        -0.12062941681093053,
        -0.7293830689247688,
        -0.5369193450397907,
        -0.7052144463195564,
        -0.7426611562924368,
        -0.6669344096758146,
        -0.7345299069229987,
        -0.7398682140114091,
        -0.7156668087997122,
        -0.6988320899998648,
        -0.7387832549082647,
        -0.3142340448220653,
        -0.7471821018397506,
        -0.672774968459591,
        -0.6962406381060574,
        -0.3201674984886522,
        -0.7202320105970121,
        -0.761627721076654,
        0.020028687972609857,
        -0.6807088283177578
      ],
      [
        -0.7402938771497978,
        -0.5809785408698451,
        -0.7213462187445001,
        -0.18540966173728207,
        1.0,
        -0.220671139394356,
        -0.7350839516813015,
        -0.7323753711234314,
        -0.1560461327925546,
        -0.7326169672992153,
        -0.7337497256864491,
        -0.7230397990845037,
        -0.7310588372690521,
        -0.6423414960810805,
        -0.5303030623982804,
        -0.7274835871032808,
        -0.7160454557219464,
        -0.7084366207704779,
        -0.6805942075791245,
        -0.67657161029422,
        -0.5514738379517781,
        -0.7353114517469337,
        -0.7415527278708776,
        -0.746678561492139,
        -0.7491977464202184,
        -0.7511183909910053,
        -0.6962460695681251,
        -0.733588572465229,
        -0.7255536805233072,
        -0.737936269280822,
        0.015039247705488548,
        -0.7337777410088553,
        -0.5587230929949021,
        -0.7109461460640116,
        -0.7465472428607421,
        -0.6772210066130282,
        -0.7403995828086816,
        -0.7404574256898496,
        -0.7188586902044859,
        -0.7109063254078489,
        -0.7446238621178823,
        -0.27308000826988577,
        -0.7461477504517109,
        -0.6546900809236857,
        -0.6979816825380314,
        -0.5259511801657168,
        -0.7170728432068956,
        -0.7628441311336589,
        -0.06413493878565524,
        -0.6878832862102301
      ],
      [
        -0.7513936999220994,
        -0.5860154939215395,
        -0.6885113620563503,
        -0.26368337179394286,
        -0.220671139394356,
        1.0,
        -0.7372420727781923,
        -0.7391805844006935,
        -0.26549774953163435,
        -0.7241609294972884,
        -0.7374216496941881,
        -0.737154117938589,
        -0.7471089285361354,
        -0.6263091739781852,
        -0.5437822087946467,
        -0.7390853270914554,
        -0.7118505495456549,
        -0.7054227282340638,
        -0.666247781077873,
        -0.688046259968422,
        -0.5367949247346943,
        -0.7412468279436921,
        -0.7422680637695998,
        -0.7342278685617324,
        -0.7539590326026436,
        -0.7527802415744753,
        -0.6975216982718324,
        -0.7295934411344038,
        -0.7335000751710572,
        -0.7375741316676423,
        -0.12674040515296991,
        -0.7106762999214469,
        -0.5499360116872951,
        -0.6828739966902326,
        -0.7495021550708287,
        -0.615877935206605,
        -0.7291576280399723,
        -0.7500601417482884,
        -0.7034042230489574,
        -0.7070838902776366,
        -0.7398742840496761,
        -0.3513717843110711,
        -0.7521328999018333,
        -0.6825090733517432,
        -0.7235689031674483,
        -0.5329213906751016,
        -0.7019038269983802,
        -0.7615440600376031,
        -0.1517847810641803,
        -0.6732277731127874
      ],
      [
        -0.7400393166598102,
        -0.7320989198316519,
        -0.7491186097537268,
        -0.7264358444982173,
        -0.7350839516813015,
        -0.7372420727781923,
        1.0,
        -0.754551750982331,
        -0.7356137082871729,
        -0.7448969408462844,
        -0.7324004651365842,
        -0.747488425879383,
        -0.7540465179310842,
        -0.7362542185746511,
        -0.7245834200430514,
        -0.7525324576899749,
        -0.7528546414354382,
        -0.7413494790990052,
        -0.6902452357535864,
        -0.7087340609905635,
        -0.7309018433839698,
        -0.7546483201287781,
        -0.7638564251434405,
        -0.7600907347141037,
        -0.7579568788973516,
        -0.7227179175570208,
        -0.7540646891184585,
        -0.7477443850730094,
        -0.7080972031127851,
        -0.7606997057035587,
        -0.7291256912671444,
        -0.7569058199779384,
        -0.7357144029876124,
        -0.7329648154034489,
        -0.761052636474992,
        -0.736273004417428,
        -0.7530650547270701,
        -0.7594063382587986,
        -0.750595854994716,
        -0.7325803452508064,
        -0.7547168612911492,
        -0.7418359508566865,
        -0.7601572134438117,
        -0.7327427534108789,
        -0.7377516223568217,
        -0.7354745334449244,
        -0.740430865789163,
        -0.7647688035235667,
        -0.7119965772314758,
        -0.7199203823961577
      ],
      [
        -0.686408199533068,
        -0.7490112017268636,
        -0.7397120443446163,
        -0.7200123434638159,
        -0.7323753711234314,
        -0.7391805844006935,
        -0.754551750982331,
        1.0,
        -0.7166916196157211,
        -0.7309881510922593,
        -0.7375049958889272,
        -0.7204469899412654,
        -0.7414104075367247,
        -0.7150740505322075,
        -0.7305387175148382,
        -0.7431229416979819,
        -0.7468824353715297,
        -0.5659437165068086,
        -0.7573724922762829,
        -0.7128566623428128,
        -0.7353224039191895,
        -0.7505667636560649,
        -0.7593536073586374,
        -0.7571247199923389,
        -0.7491614353070206,
        -0.7598525297238095,
        -0.7563288955767802,
        -0.7534616780587237,
        -0.7548793543217625,
        -0.7585551788027877,
        -0.7252273523798566,
        -0.756917671916712,
        -0.7401972035863784,
        -0.7436994709739828,
        -0.6907829420111504,
        -0.7411204393817886,
        -0.75739454932285,
        -0.7465305227416602,
        -0.7476641333176655,
        -0.7433328872016847,
        -0.7593867167992729,
        -0.7412337577754394,
        -0.7329756701792192,
        -0.7260956825381395,
        -0.7294462338173803,
        -0.7294815048822865,
        -0.7571973233873197,
        -0.7608168933364763,
        -0.7120369454016013,
        -0.7226793668279445
      ],
      [
        -0.7319270042350827,
        -0.5903450827947241,
        -0.7181941893787461,
        -0.2411263582340494,
        -0.1560461327925546,
        -0.26549774953163435,
        -0.7356137082871729,
        -0.7166916196157211,
        1.0,
        -0.7181405530278901,
        -0.7318996980687212,
        -0.7046616912154451,
        -0.718154858754404,
        -0.6575366626308734,
        -0.5367392538093139,
        -0.7207437428813486,
        -0.7103460804957432,
        -0.6893158362231282,
        -0.6925034786235136,
        -0.6806933963093637,
        -0.5557809890303796,
        -0.7098247460328047,
        -0.7412092599974935,
        -0.7478353003110817,
        -0.7487332219171563,
        -0.7442443447248603,
        -0.7037716516761545,
        -0.7337920247655667,
        -0.7252441490610146,
        -0.7370303112517278,
        -0.022213837246173748,
        -0.7292310808834208,
        -0.5675979861845475,
        -0.7124809243850653,
        -0.7365363022431155,
        -0.6731811359156901,
        -0.7355156223005679,
        -0.7295090035994909,
        -0.7082618783172216,
        -0.7123844340769889,
        -0.735854229932809,
        -0.32137617112042793,
        -0.7514763022862556,
        -0.6617969002692159,
        -0.7054712452705667,
        -0.5462901197331977,
        -0.7216784002132426,
        -0.7632525388782392,
        -0.0848790171923358,
        -0.6893642882708869
      ],
      [
        -0.7424766007628683,
        -0.7453046476369765,
        -0.7403388848800627,
        -0.6937599125665448,
        -0.7326169672992153,
        -0.7241609294972884,
        -0.7448969408462844,
        -0.7309881510922593,
        -0.7181405530278901,
        1.0,
        -0.7521364754377544,
        -0.695014123353227,
        -0.749120044372673,
        -0.7149061056450555,
        -0.7138570800427331,
        -0.7343515187363525,
        -0.7419223343964823,
        -0.49664298978335364,
        -0.7528101277857364,
        -0.7164526973943144,
        -0.7353109050157755,
        -0.7528393886677431,
        -0.7595133602403636,
        -0.7278719838885044,
        -0.7391509969503854,
        -0.7594695708132444,
        -0.7534098709900986,
        -0.7256287791691038,
        -0.7508801376392964,
        -0.7568014304687216,
        -0.72216382107979,
        -0.7468243518275473,
        -0.7435135581020884,
        -0.7181122437058576,
        -0.7261451125456057,
        -0.7154555656402161,
        -0.7462113371698568,
        -0.7535149697614363,
        -0.7360276952482252,
        -0.7403080481404577,
        -0.7532018781295444,
        -0.7438830992962427,
        -0.7323102211233636,
        -0.7243273732298805,
        -0.7154199866328474,
        -0.7274485868194889,
        -0.7484220102778032,
        -0.7612336666876938,
        -0.699433471373925,
        -0.728189690014665
      ],
      [
        -0.7567451229172626,
        -0.7154168083586261,
        -0.7502541206822251,
        -0.7224140832635638,
        -0.7337497256864491,
        -0.7374216496941881,
        -0.7324004651365842,
        -0.7375049958889272,
        -0.7318996980687212,
        -0.7521364754377544,
        1.0,
        -0.7456111773829,
        -0.7587546207389245,
        -0.7282822832555569,
        -0.7359852314953197,
        -0.7440732186750174,
        -0.7117154417591549,
        -0.7393335179903104,
        -0.7541589451894017,
        -0.6969767961464572,
        -0.733905753604206,
        -0.7510257848587307,
        -0.7357369315333853,
        -0.7563892912527272,
        -0.7523832195772606,
        -0.763142184300984,
        -0.746144778357372,
        -0.7452213754825472,
        -0.7448104459912394,
        -0.7572489454973996,
        -0.712235005742262,
        -0.7530521393088911,
        -0.7436208713373258,
        -0.7423089462371425,
        -0.7575642591564252,
        -0.7233474482490152,
        -0.7470046347406553,
        -0.7102807200529719,
        -0.7427949578362879,
        -0.7319938215146806,
        -0.7421568164179762,
        -0.7384837617509715,
        -0.7413195283964331,
        -0.723717468832684,
        -0.7495278306085075,
        -0.736256251452946,
        -0.7559672962600212,
        -0.7650134128731014,
        -0.7142061570344411,
        -0.7442071404198203
      ],
      [
        -0.7045141835929092,
        -0.7375416215279571,
        -0.7207406067124105,
        -0.7183044919773995,
        -0.7230397990845037,
        -0.737154117938589,
        -0.747488425879383,
        -0.7204469899412654,
        -0.7046616912154451,
        -0.695014123353227,
        -0.7456111773829,
        1.0,
        -0.7259846513941736,
        -0.7205482353809924,
        -0.7159015251357866,
        -0.715705481727918,
        -0.7347777459623578,
        -0.669270003419897,
        -0.7588009061503109,
        -0.7175239659804371,
        -0.7382837818538481,
        -0.7349429226665991,
        -0.7580423588254339,
        -0.7574616901550443,
        -0.7341000219802319,
        -0.7514146326476673,
        -0.7478472351835226,
        -0.7185942757178303,
        -0.7314503300086541,
        -0.7478587155858181,
        -0.730120142738039,
        -0.7506337337301977,
        -0.7398454909402219,
        -0.7040200994779795,
        -0.721290029372649,
        -0.7287916201712498,
        -0.7401874590273595,
        -0.7417681136533025,
        -0.7331036291714832,
        -0.7096817058823865,
        -0.7523045443153837,
        -0.7418193703229353,
        -0.6664378059393304,
        -0.706987852315194,
        -0.6899440959655517,
        -0.734498251925048,
        -0.7526141272608939,
        -0.7584648676295771,
        -0.7154638385797336,
        -0.7331101916648528
      ],
      [
        -0.5206318802619799,
        -0.7347321468341925,
        -0.7437865918929834,
        -0.7332802291980133,
        -0.7310588372690521,
        -0.7471089285361354,
        -0.7540465179310842,
        -0.7414104075367247,
        -0.718154858754404,
        -0.749120044372673,
        -0.7587546207389245,
        -0.7259846513941736,
        1.0,
        -0.7349518889064623,
        -0.7371978592672008,
        -0.701996053426422,
        -0.742612033510684,
        -0.7056100447406113,
        -0.7569731398329543,
        -0.7343135733787807,
        -0.7487866223748626,
        -0.3150333712347136,
        -0.7569055226589035,
        -0.7645407033231302,
        -0.7470059313994675,
        -0.7237850532119541,
        -0.7605210564576608,
        -0.7505751705978155,
        -0.7436535495476212,
        -0.750598517947582,
        -0.7392607489105286,
        -0.7441265921438913,
        -0.7512414362131761,
        -0.7457817515845155,
        -0.7202955184851314,
        -0.7542059824758474,
        -0.733618983166294,
        -0.658926699627628,
        -0.7349538394720078,
        -0.7560536253814949,
        -0.7536129088488704,
        -0.7484404337317079,
        -0.7581750800424549,
        -0.6501358621917248,
        -0.716765824838608,
        -0.7462445760231616,
        -0.7590042139062585,
        -0.7654515567539087,
        -0.7301181301839204,
        -0.7435232305078431
      ],
      [
        -0.7257346506162252,
        -0.7061162077318546,
        -0.685185523961592,
        -0.6307817471016539,
        -0.6423414960810805,
        -0.6263091739781852,
        -0.7362542185746511,
        -0.7150740505322075,
        -0.6575366626308734,
        -0.7149061056450555,
        -0.7282822832555569,
        -0.7205482353809924,
        -0.7349518889064623,
        1.0,
        -0.6770910322722914,
        -0.7286699326152632,
        -0.7182912534349876,
        -0.6871139363919193,
        -0.7098965879164298,
        -0.6849383529065295,
        -0.6928019791247482,
        -0.7354800735570486,
        -0.7519161161030963,
        -0.7415322898589433,
        -0.7398898596399754,
        -0.7496667518287621,
        -0.7316397738812253,
        -0.7276479048815356,
        -0.7346715668978334,
        -0.7517039061771034,
        -0.6294357113859514,
        -0.7430934783935976,
        -0.705965811095147,
        -0.717756196562084,
        -0.7322377691381681,
        -0.6996866848144707,
        -0.735930169681654,
        -0.7479096853225262,
        -0.730545155471934,
        -0.7061350830366205,
        -0.7535845086114318,
        -0.6763210022792858,
        -0.7493754804404821,
        -0.697290655683771,
        -0.7033221181735998,
        -0.6653681804890523,
        -0.7219863676139868,
        -0.7613766213894251,
        -0.6088232605445518,
        -0.7069115906331904
      ],
      [
        -0.7360634940536614,
        -0.6186682573698357,
        -0.6975228515964075,
        -0.502911535983386,
        -0.5303030623982804,
        -0.5437822087946467,
        -0.7245834200430514,
        -0.7305387175148382,
        -0.5367392538093139,
        -0.7138570800427331,
        -0.7359852314953197,
        -0.7159015251357866,
        -0.7371978592672008,
        -0.6770910322722914,
        1.0,
        -0.7290756129051368,
        -0.7262507456062809,
        -0.6871129521142483,
        -0.7165626503637599,
        -0.6943789425480064,
        -0.6064415853723288,
        -0.7378293846173674,
        -0.7517174902485595,
        -0.7473616752284085,
        -0.7412187293018287,
        -0.7494998869260294,
        -0.7201016103335869,
        -0.7207652927901078,
        -0.7294539964517228,
        -0.748646268222249,
        -0.5117814449955173,
        -0.7375412794608717,
        -0.5932251309770336,
        -0.721358254145708,
        -0.747280735142434,
        -0.6850039009849589,
        -0.7375520343376363,
        -0.7448251509624966,
        -0.723999752786928,
        -0.7075969918441776,
        -0.7485752183406571,
        -0.5508950643477473,
        -0.7467722876822352,
        -0.6891551817853645,
        -0.6844272502832462,
        -0.5536822072472123,
        -0.7117802750564527,
        -0.763211639389344,
        -0.481002657336014,
        -0.70227303539689
      ],
      [
        -0.7507931214390179,
        -0.7458387883857586,
        -0.7426528299534272,
        -0.7268997047972516,
        -0.7274835871032808,
        -0.7390853270914554,
        -0.7525324576899749,
        -0.7431229416979819,
        -0.7207437428813486,
        -0.7343515187363525,
        -0.7440732186750174,
        -0.715705481727918,
        -0.701996053426422,
        -0.7286699326152632,
        -0.7290756129051368,
        1.0,
        -0.7294985409558061,
        -0.7188562036050011,
        -0.7559258896798453,
        -0.7273287724186912,
        -0.746296614259335,
        -0.6539212311941933,
        -0.757462358134233,
        -0.7621697016385878,
        -0.7427669723068203,
        -0.683339073342645,
        -0.7600330662434138,
        -0.7291754489998917,
        -0.728177074557443,
        -0.5873443516739645,
        -0.733484199227768,
        -0.7426675283099711,
        -0.7417759285531302,
        -0.7442792313315878,
        -0.7320490527394273,
        -0.7460095056288565,
        -0.7447855882568648,
        -0.7238254435134189,
        -0.7455998346459112,
        -0.7493159264029481,
        -0.7533746515449425,
        -0.742889011835797,
        -0.7450169064481584,
        -0.7242989721084833,
        -0.7097738510598648,
        -0.742367095637368,
        -0.7587337219652905,
        -0.7556525115866961,
        -0.7228935805500003,
        -0.7441210299717562
      ],
      [
        -0.7486446273388538,
        -0.7254456458292992,
        -0.7423024367059217,
        -0.7060726656360496,
        -0.7160454557219464,
        -0.7118505495456549,
        -0.7528546414354382,
        -0.7468824353715297,
        -0.7103460804957432,
        -0.7419223343964823,
        -0.7117154417591549,
        -0.7347777459623578,
        -0.742612033510684,
        -0.7182912534349876,
        -0.7262507456062809,
        -0.7294985409558061,
        1.0,
        -0.7312872837650404,
        -0.7287773723608848,
        -0.7234713235283932,
        -0.7375351956941121,
        -0.7206341116123023,
        -0.7486920004213757,
        -0.7532483227942981,
        -0.7419878590651092,
        -0.7573319656182942,
        -0.7400502806770942,
        -0.7230302897984651,
        -0.7314619273896215,
        -0.7505980208820763,
        -0.7032879844823835,
        -0.7470725580017881,
        -0.7248200246571302,
        -0.7219020593510247,
        -0.7519873480621171,
        -0.7218898148727756,
        -0.7466758795407245,
        -0.7376477940516233,
        -0.7160697232675548,
        -0.7266949318691938,
        -0.7531907103752684,
        -0.6821085780581939,
        -0.748782182626219,
        -0.7005511596565575,
        -0.7258408086783366,
        -0.7330766374781625,
        -0.7504405687371665,
        -0.7615360653309562,
        -0.6895195503273125,
        -0.7281332128965391
      ],
      [
        -0.6635469475665994,
        -0.7338511638968866,
        -0.7208809451960335,
        -0.6722967336319561,
        -0.7084366207704779,
        -0.7054227282340638,
        -0.7413494790990052,
        -0.5659437165068086,
        -0.6893158362231282,
        -0.49664298978335364,
        -0.7393335179903104,
        -0.669270003419897,
        -0.7056100447406113,
        -0.6871139363919193,
        -0.6871129521142483,
        -0.7188562036050011,
        -0.7312872837650404,
        1.0,
        -0.7478782126802822,
        -0.7079910867438459,
        -0.7205237145830532,
        -0.725360047259572,
        -0.7556252512187751,
        -0.7568633870171486,
        -0.7115714513655065,
        -0.7478117814607956,
        -0.7500744950394335,
        -0.733653598760897,
        -0.7479118809170926,
        -0.7519738532730421,
        -0.6939555211884608,
        -0.749491283924733,
        -0.7280510415963317,
        -0.727004073753202,
        -0.4692595689206096,
        -0.7057737581239425,
        -0.7430069129498911,
        -0.734061045802596,
        -0.7222668675865056,
        -0.715452380286587,
        -0.7534887920751289,
        -0.7240582211577604,
        -0.739791333472039,
        -0.7051360687268098,
        -0.6855668773328079,
        -0.6956434177099273,
        -0.736604068545774,
        -0.7588059766531529,
        -0.6687007913396433,
        -0.7128913111310489
      ],
      [
        -0.7553404071623129,
        -0.7270307575630532,
        -0.7360543312234937,
        -0.6922540038819213,
        -0.6805942075791245,
        -0.666247781077873,
        -0.6902452357535864,
        -0.7573724922762829,
        -0.6925034786235136,
        -0.7528101277857364,
        -0.7541589451894017,
        -0.7588009061503109,
        -0.7569731398329543,
        -0.7098965879164298,
        -0.7165626503637599,
        -0.7559258896798453,
        -0.7287773723608848,
        -0.7478782126802822,
        1.0,
        -0.7409590732042762,
        -0.7036384913484479,
        -0.7528546439636404,
        -0.7579518125830134,
        -0.7533309030525344,
        -0.7628436263815093,
        -0.7537403188338041,
        -0.7330246948789125,
        -0.7561738674603673,
        -0.7298987458478039,
        -0.7450818567377742,
        -0.6709232920444241,
        -0.7521968700013856,
        -0.7219541232127922,
        -0.7394198532643836,
        -0.7632098706556444,
        -0.7288079470306761,
        -0.751166958107752,
        -0.7591742742544805,
        -0.7468485400651415,
        -0.7404661524379097,
        -0.7530246380736755,
        -0.6980726905761199,
        -0.7625805095738316,
        -0.7323964218667095,
        -0.7518494699323335,
        -0.7063667304354344,
        -0.6112044997414505,
        -0.7632910103376069,
        -0.6515923543273459,
        -0.7341493300166445
      ],
      [
        -0.7273555789785392,
        -0.7186913173316676,
        -0.6804190400163173,
        -0.673359994825909,
        -0.67657161029422,
        -0.688046259968422,
        -0.7087340609905635,
        -0.7128566623428128,
        -0.6806933963093637,
        -0.7164526973943144,
        -0.6969767961464572,
        -0.7175239659804371,
        -0.7343135733787807,
        -0.6849383529065295,
        -0.6943789425480064,
        -0.7273287724186912,
        -0.7234713235283932,
        -0.7079910867438459,
        -0.7409590732042762,
        1.0,
        -0.7032121506909215,
        -0.7440509568098524,
        -0.7570952592699121,
        -0.732741163450199,
        -0.7471313121792372,
        -0.7489690288696744,
        -0.7231451608214499,
        -0.7329706990614101,
        -0.7326999618277231,
        -0.7552366198177771,
        -0.6607819594163151,
        -0.738143354873326,
        -0.716022955040055,
        -0.7133790125312989,
        -0.7456399080485123,
        -0.7039223779351047,
        -0.7398746564568971,
        -0.7392768565728344,
        -0.7322779909494052,
        -0.7222412755653813,
        -0.7161123135142535,
        -0.7023396552251179,
        -0.7246671144109751,
        -0.44692965111187144,
        -0.7040824653586621,
        -0.700352171776878,
        -0.7422093614439257,
        -0.7628049217742894,
        -0.6403289240001677,
        -0.7106587773355424
      ],
      [
        -0.7426442270258455,
        -0.619434772222808,
        -0.7216217118660945,
        -0.5414139673060921,
        -0.5514738379517781,
        -0.5367949247346943,
        -0.7309018433839698,
        -0.7353224039191895,
        -0.5557809890303796,
        -0.7353109050157755,
        -0.733905753604206,
        -0.7382837818538481,
        -0.7487866223748626,
        -0.6928019791247482,
        -0.6064415853723288,
        -0.746296614259335,
        -0.7375351956941121,
        -0.7205237145830532,
        -0.7036384913484479,
        -0.7032121506909215,
        1.0,
        -0.7471800505844406,
        -0.7568109791398198,
        -0.7571893994230603,
        -0.7573554593122058,
        -0.7568256878465229,
        -0.7316021016262471,
        -0.7493087539315904,
        -0.7210359742891458,
        -0.7527168643800524,
        -0.530117122620954,
        -0.7424280758332154,
        -0.5794936034914011,
        -0.7396575416565137,
        -0.7522712354025267,
        -0.7120480617341218,
        -0.7466004192465305,
        -0.7536823943524377,
        -0.7371785361830652,
        -0.7286682342107602,
        -0.74768439904689,
        -0.5438073098989136,
        -0.7497089798744678,
        -0.7032660390261833,
        -0.7367115811588416,
        -0.5615869649636576,
        -0.7263094328025559,
        -0.7643730692803448,
        -0.5040654680014952,
        -0.7290596046745637
      ],
      [
        -0.7429920694786252,
        -0.7085075608252152,
        -0.7454034408559794,
        -0.7315122164476338,
        -0.7353114517469337,
        -0.7412468279436921,
        -0.7546483201287781,
        -0.7505667636560649,
        -0.7098247460328047,
        -0.7528393886677431,
        -0.7510257848587307,
        -0.7349429226665991,
        -0.3150333712347136,
        -0.7354800735570486,
        -0.7378293846173674,
        -0.6539212311941933,
        -0.7206341116123023,
        -0.725360047259572,
        -0.7528546439636404,
        -0.7440509568098524,
        -0.7471800505844406,
        1.0,
        -0.7599166043126774,
        -0.7633555348008788,
        -0.744642476528629,
        -0.7207919235382889,
        -0.7593931226914216,
        -0.7418058010014639,
        -0.733783920150884,
        -0.7335315214606746,
        -0.7363247781905556,
        -0.7487370949826876,
        -0.7461590420787887,
        -0.7405136441590316,
        -0.7334402552184707,
        -0.7444367433023885,
        -0.7108930889195764,
        -0.6247809113999108,
        -0.7139636481035776,
        -0.7459506145555344,
        -0.7425648905930371,
        -0.7462442502870437,
        -0.7583491597483547,
        -0.7061838879133944,
        -0.7197199382027304,
        -0.749369897594163,
        -0.757800166469686,
        -0.7654522631918953,
        -0.7269672398569293,
        -0.746210886704592
      ],
      [
        -0.7627373527776786,
        -0.756558160308378,
        -0.7639566008163778,
        -0.7434893738222108,
        -0.7415527278708776,
        -0.7422680637695998,
        -0.7638564251434405,
        -0.7593536073586374,
        -0.7412092599974935,
        -0.7595133602403636,
        -0.7357369315333853,
        -0.7580423588254339,
        -0.7569055226589035,
        -0.7519161161030963,
        -0.7517174902485595,
        -0.757462358134233,
        -0.7486920004213757,
        -0.7556252512187751,
        -0.7579518125830134,
        -0.7570952592699121,
        -0.7568109791398198,
        -0.7599166043126774,
        1.0,
        -0.7649202130544263,
        -0.7514933779348607,
        -0.7640944069014759,
        -0.7446441209379784,
        -0.7544525863273255,
        -0.7557248499240239,
        -0.7470457431102583,
        -0.7370545725282872,
        -0.7534985036311185,
        -0.7523071205360902,
        -0.7554448759034937,
        -0.7606132972148221,
        -0.7494722518472162,
        -0.7563226903024842,
        -0.7573641335774469,
        -0.7481263445034623,
        -0.7382733953087564,
        -0.7632695613719622,
        -0.7492583155702124,
        -0.7588986582386594,
        -0.7495021179462391,
        -0.7486279303017914,
        -0.7542301419743107,
        -0.7642182197897365,
        -0.7645678896989299,
        -0.738477599577781,
        -0.7534832281754182
      ],
      [
        -0.7638030170467296,
        -0.7581538942733315,
        -0.7573103791713995,
        -0.7429888506134006,
        -0.746678561492139,
        -0.7342278685617324,
        -0.7600907347141037,
        -0.7571247199923389,
        -0.7478353003110817,
        -0.7278719838885044,
        -0.7563892912527272,
        -0.7574616901550443,
        -0.7645407033231302,
        -0.7415322898589433,
        -0.7473616752284085,
        -0.7621697016385878,
        -0.7532483227942981,
        -0.7568633870171486,
        -0.7533309030525344,
        -0.732741163450199,
        -0.7571893994230603,
        -0.7633555348008788,
        -0.7649202130544263,
        1.0,
        -0.7581480289892579,
        -0.7653686720188738,
        -0.7506284819486966,
        -0.748455807943972,
        -0.7542579229898563,
        -0.7515614309419865,
        -0.7415455953874416,
        -0.7594324286370502,
        -0.7524321120520612,
        -0.7535030208912097,
        -0.7623747490974588,
        -0.7216174100867045,
        -0.7630090845966756,
        -0.764063608761846,
        -0.7508029000942273,
        -0.7466073134632292,
        -0.765018235956532,
        -0.7465159919152482,
        -0.7635579890407626,
        -0.7498549771088266,
        -0.7557579526607361,
        -0.7492392548221353,
        -0.7538949007736597,
        -0.7646742607632296,
        -0.7378725317550284,
        -0.7542439616996761
      ],
      [
        -0.7373613090508633,
        -0.7190808045224273,
        -0.7548521129632291,
        -0.7454266366268298,
        -0.7491977464202184,
        -0.7539590326026436,
        -0.7579568788973516,
        -0.7491614353070206,
        -0.7487332219171563,
        -0.7391509969503854,
        -0.7523832195772606,
        -0.7341000219802319,
        -0.7470059313994675,
        -0.7398898596399754,
        -0.7412187293018287,
        -0.7427669723068203,
        -0.7419878590651092,
        -0.7115714513655065,
        -0.7628436263815093,
        -0.7471313121792372,
        -0.7573554593122058,
        -0.744642476528629,
        -0.7514933779348607,
        -0.7581480289892579,
        1.0,
        -0.7622833999103757,
        -0.7572126109603687,
        -0.6650357547977678,
        -0.7482439899927906,
        -0.7493086303449245,
        -0.7492735253401359,
        -0.7616747523424432,
        -0.7565572522854204,
        -0.6727753841318239,
        -0.7323441097122816,
        -0.7412457434859092,
        -0.745854369031728,
        -0.7464555316110049,
        -0.7530457847817622,
        -0.7453016623384854,
        -0.7607393607540207,
        -0.7445791044215583,
        -0.7565318529210041,
        -0.7426299863087058,
        -0.7323448011893454,
        -0.7500358790337016,
        -0.7592310856427774,
        -0.7644735539542016,
        -0.7431551053176054,
        -0.7396104391475722
      ],
      [
        -0.7063195153293261,
        -0.7526878158976413,
        -0.7548967507514975,
        -0.7473832924393625,
        -0.7511183909910053,
        -0.7527802415744753,
        -0.7227179175570208,
        -0.7598525297238095,
        -0.7442443447248603,
        -0.7594695708132444,
        -0.763142184300984,
        -0.7514146326476673,
        -0.7237850532119541,
        -0.7496667518287621,
        -0.7494998869260294,
        -0.683339073342645,
        -0.7573319656182942,
        -0.7478117814607956,
        -0.7537403188338041,
        -0.7489690288696744,
        -0.7568256878465229,
        -0.7207919235382889,
        -0.7640944069014759,
        -0.7653686720188738,
        -0.7622833999103757,
        1.0,
        -0.7609872344508408,
        -0.7553847122903365,
        -0.7544346000209377,
        -0.7435689417007151,
        -0.7486418036411009,
        -0.7587633084961607,
        -0.7563261291746655,
        -0.7560422135163203,
        -0.7510444051294871,
        -0.7596735595418038,
        -0.7448340357543047,
        -0.7492078014404046,
        -0.7580603585917413,
        -0.760407690317556,
        -0.7460078782212968,
        -0.7560778922711637,
        -0.7600918960294065,
        -0.7442138886180598,
        -0.7484443462627679,
        -0.7540384849944615,
        -0.7612025636813858,
        -0.7645533339364387,
        -0.7458562566242544,
        -0.7549387390937936
      ],
      [
        -0.7603894790704809,
        -0.7176401111361814,
        -0.7462356510909984,
        -0.6986204284104447,
        -0.6962460695681251,
        -0.6975216982718324,
        -0.7540646891184585,
        -0.7563288955767802,
        -0.7037716516761545,
        -0.7534098709900986,
        -0.746144778357372,
        -0.7478472351835226,
        -0.7605210564576608,
        -0.7316397738812253,
        -0.7201016103335869,
        -0.7600330662434138,
        -0.7400502806770942,
        -0.7500744950394335,
        -0.7330246948789125,
        -0.7231451608214499,
        -0.7316021016262471,
        -0.7593931226914216,
        -0.7446441209379784,
        -0.7506284819486966,
        -0.7572126109603687,
        -0.7609872344508408,
        1.0,
        -0.7511355151665484,
        -0.7457354148870554,
        -0.7463811426464544,
        -0.6595252021101101,
        -0.7575984040865038,
        -0.729962348864623,
        -0.7197963577565791,
        -0.7601397290750935,
        -0.6681247225248366,
        -0.7517443444103872,
        -0.7549819736494572,
        -0.7330057453474857,
        -0.5240012657268392,
        -0.7577653806055343,
        -0.7039982608798487,
        -0.7618180366931726,
        -0.7419865518829569,
        -0.7553091925331339,
        -0.726159694743122,
        -0.752558192639349,
        -0.7645130042616566,
        -0.6766925537296462,
        -0.7234241322896036
      ],
      [
        -0.7525895662255053,
        -0.7101815475050688,
        -0.733702737411273,
        -0.7294956771201087,
        -0.733588572465229,
        -0.7295934411344038,
        -0.7477443850730094,
        -0.7534616780587237,
        -0.7337920247655667,
        -0.7256287791691038,
        -0.7452213754825472,
        -0.7185942757178303,
        -0.7505751705978155,
        -0.7276479048815356,
        -0.7207652927901078,
        -0.7291754489998917,
        -0.7230302897984651,
        -0.733653598760897,
        -0.7561738674603673,
        -0.7329706990614101,
        -0.7493087539315904,
        -0.7418058010014639,
        -0.7544525863273255,
        -0.748455807943972,
        -0.6650357547977678,
        -0.7553847122903365,
        -0.7511355151665484,
        1.0,
        -0.7418419497844797,
        -0.7445048088529413,
        -0.7311147195748893,
        -0.7536254436628047,
        -0.7506479399301027,
        -0.6214630824997422,
        -0.7509906289679827,
        -0.7265480227946696,
        -0.6645140677893954,
        -0.7473578934532126,
        -0.7370461035369886,
        -0.7371446862543296,
        -0.7552345082679657,
        -0.7245363179137901,
        -0.7463629053333526,
        -0.7319367521839224,
        -0.7214810286384454,
        -0.7404966737799454,
        -0.7539350636301575,
        -0.7629301468792761,
        -0.7234048711628894,
        -0.7208591126581976
      ],
      [
        -0.7517967232276719,
        -0.74366077165654,
        -0.7439270387538989,
        -0.7223932827583455,
        -0.7255536805233072,
        -0.7335000751710572,
        -0.7080972031127851,
        -0.7548793543217625,
        -0.7252441490610146,
        -0.7508801376392964,
        -0.7448104459912394,
        -0.7314503300086541,
        -0.7436535495476212,
        -0.7346715668978334,
        -0.7294539964517228,
        -0.728177074557443,
        -0.7314619273896215,
        -0.7479118809170926,
        -0.7298987458478039,
        -0.7326999618277231,
        -0.7210359742891458,
        -0.733783920150884,
        -0.7557248499240239,
        -0.7542579229898563,
        -0.7482439899927906,
        -0.7544346000209377,
        -0.7457354148870554,
        -0.7418419497844797,
        1.0,
        -0.7531592917256673,
        -0.7235905445286831,
        -0.7395675878775243,
        -0.7364590764809956,
        -0.7196320201415212,
        -0.7588210501930968,
        -0.7316274109488539,
        -0.749093016809606,
        -0.7370220384705419,
        -0.7457783489421872,
        -0.7072463454864921,
        -0.7573726149615267,
        -0.7356779862995051,
        -0.7600896952757175,
        -0.7260449077033184,
        -0.7319698011311822,
        -0.7355169957200296,
        -0.7503766046553304,
        -0.7642255076641695,
        -0.7126810468044044,
        -0.7345638207600331
      ],
      [
        -0.7622667218151229,
        -0.7502141786329782,
        -0.759629606348571,
        -0.7368016924488426,
        -0.737936269280822,
        -0.7375741316676423,
        -0.7606997057035587,
        -0.7585551788027877,
        -0.7370303112517278,
        -0.7568014304687216,
        -0.7572489454973996,
        -0.7478587155858181,
        -0.750598517947582,
        -0.7517039061771034,
        -0.748646268222249,
        -0.5873443516739645,
        -0.7505980208820763,
        -0.7519738532730421,
        -0.7450818567377742,
        -0.7552366198177771,
        -0.7527168643800524,
        -0.7335315214606746,
        -0.7470457431102583,
        -0.7515614309419865,
        -0.7493086303449245,
        -0.7435689417007151,
        -0.7463811426464544,
        -0.7445048088529413,
        -0.7531592917256673,
        1.0,
        -0.731787941339163,
        -0.7579847145108742,
        -0.752810577488784,
        -0.752901968976004,
        -0.7575738324220156,
        -0.747315493599482,
        -0.7484471456316715,
        -0.7151584092444472,
        -0.7574495366068552,
        -0.7541382023208155,
        -0.7592089322244593,
        -0.7451684325680399,
        -0.758646810227956,
        -0.7487874365709735,
        -0.7432404354528723,
        -0.7418702925526598,
        -0.7605779951026678,
        -0.7631144983140973,
        -0.7297506063513164,
        -0.754995024450567
      ],
      [
        -0.7441556169393619,
        -0.5655544579161254,
        -0.7075594522624571,
        -0.12062941681093053,
        0.015039247705488548,
        -0.12674040515296991,
        -0.7291256912671444,
        -0.7252273523798566,
        -0.022213837246173748,
        -0.72216382107979,
        -0.712235005742262,
        -0.730120142738039,
        -0.7392607489105286,
        -0.6294357113859514,
        -0.5117814449955173,
        -0.733484199227768,
        -0.7032879844823835,
        -0.6939555211884608,
        -0.6709232920444241,
        -0.6607819594163151,
        -0.530117122620954,
        -0.7363247781905556,
        -0.7370545725282872,
        -0.7415455953874416,
        -0.7492735253401359,
        -0.7486418036411009,
        -0.6595252021101101,
        -0.7311147195748893,
        -0.7235905445286831,
        -0.731787941339163,
        1.0,
        -0.7246426907538372,
        -0.542356725862989,
        -0.6935802134631823,
        -0.7423450601963929,
        -0.6462409159522308,
        -0.7349247805532979,
        -0.7412623272390324,
        -0.7001572090288091,
        -0.6974814336382171,
        -0.738203267579873,
        -0.2304654695528177,
        -0.7484968745597568,
        -0.65426591410118,
        -0.7169879236569807,
        -0.503004651526846,
        -0.7067532490309064,
        -0.7610109718555514,
        0.16044135887867272,
        -0.6610349381969233
      ],
      [
        -0.760985767503713,
        -0.7369576787110699,
        -0.7432460977072948,
        -0.7293830689247688,
        -0.7337777410088553,
        -0.7106762999214469,
        -0.7569058199779384,
        -0.756917671916712,
        -0.7292310808834208,
        -0.7468243518275473,
        -0.7530521393088911,
        -0.7506337337301977,
        -0.7441265921438913,
        -0.7430934783935976,
        -0.7375412794608717,
        -0.7426675283099711,
        -0.7470725580017881,
        -0.749491283924733,
        -0.7521968700013856,
        -0.738143354873326,
        -0.7424280758332154,
        -0.7487370949826876,
        -0.7534985036311185,
        -0.7594324286370502,
        -0.7616747523424432,
        -0.7587633084961607,
        -0.7575984040865038,
        -0.7536254436628047,
        -0.7395675878775243,
        -0.7579847145108742,
        -0.7246426907538372,
        1.0,
        -0.7335117788484675,
        -0.7458964197842615,
        -0.7597038739881563,
        -0.7255434812322102,
        -0.7542752841384186,
        -0.7522835070310367,
        -0.7261123691088827,
        -0.7528156717136494,
        -0.7593812325315759,
        -0.7435130494733966,
        -0.7566915343622469,
        -0.7233726592100991,
        -0.7323628826671287,
        -0.7456955477277992,
        -0.7539760805221452,
        -0.7636034823119329,
        -0.7215043804480195,
        -0.7452446865370705
      ],
      [
        -0.7443522513625997,
        -0.5937059023921456,
        -0.7406115033699969,
        -0.5369193450397907,
        -0.5587230929949021,
        -0.5499360116872951,
        -0.7357144029876124,
        -0.7401972035863784,
        -0.5675979861845475,
        -0.7435135581020884,
        -0.7436208713373258,
        -0.7398454909402219,
        -0.7512414362131761,
        -0.705965811095147,
        -0.5932251309770336,
        -0.7417759285531302,
        -0.7248200246571302,
        -0.7280510415963317,
        -0.7219541232127922,
        -0.716022955040055,
        -0.5794936034914011,
        -0.7461590420787887,
        -0.7523071205360902,
        -0.7524321120520612,
        -0.7565572522854204,
        -0.7563261291746655,
        -0.729962348864623,
        -0.7506479399301027,
        -0.7364590764809956,
        -0.752810577488784,
        -0.542356725862989,
        -0.7335117788484675,
        1.0,
        -0.7272046937320846,
        -0.7507174137171047,
        -0.6103014350665606,
        -0.7498663149070082,
        -0.7530093720010715,
        -0.7369382566967381,
        -0.7304778293146157,
        -0.7441405940368269,
        -0.510321566990689,
        -0.7503676633662895,
        -0.6921496209558868,
        -0.7261856239043671,
        -0.5773342569876585,
        -0.7426588932582112,
        -0.7640570328783056,
        -0.5198099292817775,
        -0.7076434034779725
      ],
      [
        -0.7478956585715058,
        -0.7217934842584415,
        -0.7332483806561685,
        -0.7052144463195564,
        -0.7109461460640116,
        -0.6828739966902326,
        -0.7329648154034489,
        -0.7436994709739828,
        -0.7124809243850653,
        -0.7181122437058576,
        -0.7423089462371425,
        -0.7040200994779795,
        -0.7457817515845155,
        -0.717756196562084,
        -0.721358254145708,
        -0.7442792313315878,
        -0.7219020593510247,
        -0.727004073753202,
        -0.7394198532643836,
        -0.7133790125312989,
        -0.7396575416565137,
        -0.7405136441590316,
        -0.7554448759034937,
        -0.7535030208912097,
        -0.6727753841318239,
        -0.7560422135163203,
        -0.7197963577565791,
        -0.6214630824997422,
        -0.7196320201415212,
        -0.752901968976004,
        -0.6935802134631823,
        -0.7458964197842615,
        -0.7272046937320846,
        1.0,
        -0.7540226082273874,
        -0.6634263555998652,
        -0.7164572882084697,
        -0.7452353673540666,
        -0.7247580710611676,
        -0.7176405717914154,
        -0.7506079350808735,
        -0.7107490223864059,
        -0.7517940953088629,
        -0.7210492081576961,
        -0.7239106907167829,
        -0.7302322749089931,
        -0.7463454972815077,
        -0.7629830368947724,
        -0.6937034764685539,
        -0.4359601458935486
      ],
      [
        -0.7160446595742793,
        -0.7533156960720299,
        -0.7573873320893039,
        -0.7426611562924368,
        -0.7465472428607421,
        -0.7495021550708287,
        -0.761052636474992,
        -0.6907829420111504,
        -0.7365363022431155,
        -0.7261451125456057,
        -0.7575642591564252,
        -0.721290029372649,
        -0.7202955184851314,
        -0.7322377691381681,
        -0.747280735142434,
        -0.7320490527394273,
        -0.7519873480621171,
        -0.4692595689206096,
        -0.7632098706556444,
        -0.7456399080485123,
        -0.7522712354025267,
        -0.7334402552184707,
        -0.7606132972148221,
        -0.7623747490974588,
        -0.7323441097122816,
        -0.7510444051294871,
        -0.7601397290750935,
        -0.7509906289679827,
        -0.7588210501930968,
        -0.7575738324220156,
        -0.7423450601963929,
        -0.7597038739881563,
        -0.7507174137171047,
        -0.7540226082273874,
        1.0,
        -0.7450366329866799,
        -0.7567685050514479,
        -0.7444589062205588,
        -0.7420984066021484,
        -0.7494307275425733,
        -0.7584165959848282,
        -0.7490298997388343,
        -0.7497841084396188,
        -0.7414717823949637,
        -0.7348077114669248,
        -0.7435841569811656,
        -0.7599259727196986,
        -0.7637598864171382,
        -0.7371081897074385,
        -0.7479710487580251
      ],
      [
        -0.7512306327556177,
        -0.6951080849286841,
        -0.6599201369720704,
        -0.6669344096758146,
        -0.6772210066130282,
        -0.615877935206605,
        -0.736273004417428,
        -0.7411204393817886,
        -0.6731811359156901,
        -0.7154555656402161,
        -0.7233474482490152,
        -0.7287916201712498,
        -0.7542059824758474,
        -0.6996866848144707,
        -0.6850039009849589,
        -0.7460095056288565,
        -0.7218898148727756,
        -0.7057737581239425,
        -0.7288079470306761,
        -0.7039223779351047,
        -0.7120480617341218,
        -0.7444367433023885,
        -0.7494722518472162,
        -0.7216174100867045,
        -0.7412457434859092,
        -0.7596735595418038,
        -0.6681247225248366,
        -0.7265480227946696,
        -0.7316274109488539,
        -0.747315493599482,
        -0.6462409159522308,
        -0.7255434812322102,
        -0.6103014350665606,
        -0.6634263555998652,
        -0.7450366329866799,
        1.0,
        -0.7261212605457239,
        -0.7515537026070533,
        -0.7107667858257678,
        -0.6414467732719045,
        -0.7472951079989619,
        -0.696640687203637,
        -0.7473894393502691,
        -0.7074099083657086,
        -0.7235793034792687,
        -0.6987663220032168,
        -0.7284620379942073,
        -0.7615736446705992,
        -0.6364342039103328,
        -0.685359693095615
      ],
      [
        -0.7348280167760419,
        -0.7340870440903018,
        -0.7259431642226923,
        -0.7345299069229987,
        -0.7403995828086816,
        -0.7291576280399723,
        -0.7530650547270701,
        -0.75739454932285,
        -0.7355156223005679,
        -0.7462113371698568,
        -0.7470046347406553,
        -0.7401874590273595,
        -0.733618983166294,
        -0.735930169681654,
        -0.7375520343376363,
        -0.7447855882568648,
        -0.7466758795407245,
        -0.7430069129498911,
        -0.751166958107752,
        -0.7398746564568971,
        -0.7466004192465305,
        -0.7108930889195764,
        -0.7563226903024842,
        -0.7630090845966756,
        -0.745854369031728,
        -0.7448340357543047,
        -0.7517443444103872,
        -0.6645140677893954,
        -0.749093016809606,
        -0.7484471456316715,
        -0.7349247805532979,
        -0.7542752841384186,
        -0.7498663149070082,
        -0.7164572882084697,
        -0.7567685050514479,
        -0.7261212605457239,
        1.0,
        -0.7088622493593242,
        -0.7437890282749102,
        -0.7386121878064995,
        -0.7175557209530303,
        -0.7433538129742235,
        -0.7580865168787769,
        -0.7394882490363957,
        -0.7398409624686524,
        -0.7465243893734852,
        -0.7536395083374028,
        -0.7627842137814982,
        -0.7279586548418497,
        -0.7341328134422873
      ],
      [
        -0.7532167700918874,
        -0.7537486645379302,
        -0.7504959980768939,
        -0.7398682140114091,
        -0.7404574256898496,
        -0.7500601417482884,
        -0.7594063382587986,
        -0.7465305227416602,
        -0.7295090035994909,
        -0.7535149697614363,
        -0.7102807200529719,
        -0.7417681136533025,
        -0.658926699627628,
        -0.7479096853225262,
        -0.7448251509624966,
        -0.7238254435134189,
        -0.7376477940516233,
        -0.734061045802596,
        -0.7591742742544805,
        -0.7392768565728344,
        -0.7536823943524377,
        -0.6247809113999108,
        -0.7573641335774469,
        -0.764063608761846,
        -0.7464555316110049,
        -0.7492078014404046,
        -0.7549819736494572,
        -0.7473578934532126,
        -0.7370220384705419,
        -0.7151584092444472,
        -0.7412623272390324,
        -0.7522835070310367,
        -0.7530093720010715,
        -0.7452353673540666,
        -0.7444589062205588,
        -0.7515537026070533,
        -0.7088622493593242,
        1.0,
        -0.7093646492067514,
        -0.7520931857470938,
        -0.7248546522481172,
        -0.748007185497044,
        -0.7553488414374532,
        -0.6696022840983429,
        -0.7311330751643872,
        -0.7514859980145996,
        -0.7598003709642448,
        -0.7650134455163955,
        -0.7376267075783537,
        -0.7442175437460166
      ],
      [
        -0.7500689204049696,
        -0.7336676493627079,
        -0.747183311956837,
        -0.7156668087997122,
        -0.7188586902044859,
        -0.7034042230489574,
        -0.750595854994716,
        -0.7476641333176655,
        -0.7082618783172216,
        -0.7360276952482252,
        -0.7427949578362879,
        -0.7331036291714832,
        -0.7349538394720078,
        -0.730545155471934,
        -0.723999752786928,
        -0.7455998346459112,
        -0.7160697232675548,
        -0.7222668675865056,
        -0.7468485400651415,
        -0.7322779909494052,
        -0.7371785361830652,
        -0.7139636481035776,
        -0.7481263445034623,
        -0.7508029000942273,
        -0.7530457847817622,
        -0.7580603585917413,
        -0.7330057453474857,
        -0.7370461035369886,
        -0.7457783489421872,
        -0.7574495366068552,
        -0.7001572090288091,
        -0.7261123691088827,
        -0.7369382566967381,
        -0.7247580710611676,
        -0.7420984066021484,
        -0.7107667858257678,
        -0.7437890282749102,
        -0.7093646492067514,
        1.0,
        -0.7210686662085177,
        -0.7483085278294783,
        -0.7291156451121095,
        -0.750040080066244,
        -0.7183993522406515,
        -0.7353593251142151,
        -0.733558720273396,
        -0.7529448694526866,
        -0.7613581526142941,
        -0.6985866274718293,
        -0.7167928031360353
      ],
      [
        -0.7532504239357223,
        -0.7289976053555783,
        -0.7038764820352292,
        -0.6988320899998648,
        -0.7109063254078489,
        -0.7070838902776366,
        -0.7325803452508064,
        -0.7433328872016847,
        -0.7123844340769889,
        -0.7403080481404577,
        -0.7319938215146806,
        -0.7096817058823865,
        -0.7560536253814949,
        -0.7061350830366205,
        -0.7075969918441776,
        -0.7493159264029481,
        -0.7266949318691938,
        -0.715452380286587,
        -0.7404661524379097,
        -0.7222412755653813,
        -0.7286682342107602,
        -0.7459506145555344,
        -0.7382733953087564,
        -0.7466073134632292,
        -0.7453016623384854,
        -0.760407690317556,
        -0.5240012657268392,
        -0.7371446862543296,
        -0.7072463454864921,
        -0.7541382023208155,
        -0.6974814336382171,
        -0.7528156717136494,
        -0.7304778293146157,
        -0.7176405717914154,
        -0.7494307275425733,
        -0.6414467732719045,
        -0.7386121878064995,
        -0.7520931857470938,
        -0.7210686662085177,
        1.0,
        -0.7569244239316892,
        -0.7195696474268278,
        -0.7557904353820859,
        -0.7322127897237345,
        -0.7313740642324487,
        -0.7197668834483875,
        -0.7432961065571695,
        -0.7618077194192162,
        -0.6858356404156614,
        -0.7184675529317236
      ],
      [
        -0.7595067848535157,
        -0.750258794070841,
        -0.7429846205256299,
        -0.7387832549082647,
        -0.7446238621178823,
        -0.7398742840496761,
        -0.7547168612911492,
        -0.7593867167992729,
        -0.735854229932809,
        -0.7532018781295444,
        -0.7421568164179762,
        -0.7523045443153837,
        -0.7536129088488704,
        -0.7535845086114318,
        -0.7485752183406571,
        -0.7533746515449425,
        -0.7531907103752684,
        -0.7534887920751289,
        -0.7530246380736755,
        -0.7161123135142535,
        -0.74768439904689,
        -0.7425648905930371,
        -0.7632695613719622,
        -0.765018235956532,
        -0.7607393607540207,
        -0.7460078782212968,
        -0.7577653806055343,
        -0.7552345082679657,
        -0.7573726149615267,
        -0.7592089322244593,
        -0.738203267579873,
        -0.7593812325315759,
        -0.7441405940368269,
        -0.7506079350808735,
        -0.7584165959848282,
        -0.7472951079989619,
        -0.7175557209530303,
        -0.7248546522481172,
        -0.7483085278294783,
        -0.7569244239316892,
        1.0,
        -0.7462376287305246,
        -0.7535063453704265,
        -0.7191131602941746,
        -0.7560841951433505,
        -0.7499678432669704,
        -0.7579949586181649,
        -0.7613543080043289,
        -0.7310838265549808,
        -0.7517999950969603
      ],
      [
        -0.7516371414677991,
        -0.5653555334629528,
        -0.7408168131300612,
        -0.3142340448220653,
        -0.27308000826988577,
        -0.3513717843110711,
        -0.7418359508566865,
        -0.7412337577754394,
        -0.32137617112042793,
        -0.7438830992962427,
        -0.7384837617509715,
        -0.7418193703229353,
        -0.7484404337317079,
        -0.6763210022792858,
        -0.5508950643477473,
        -0.742889011835797,
        -0.6821085780581939,
        -0.7240582211577604,
        -0.6980726905761199,
        -0.7023396552251179,
        -0.5438073098989136,
        -0.7462442502870437,
        -0.7492583155702124,
        -0.7465159919152482,
        -0.7445791044215583,
        -0.7560778922711637,
        -0.7039982608798487,
        -0.7245363179137901,
        -0.7356779862995051,
        -0.7451684325680399,
        -0.2304654695528177,
        -0.7435130494733966,
        -0.510321566990689,
        -0.7107490223864059,
        -0.7490298997388343,
        -0.696640687203637,
        -0.7433538129742235,
        -0.748007185497044,
        -0.7291156451121095,
        -0.7195696474268278,
        -0.7462376287305246,
        1.0,
        -0.7530135567133256,
        -0.6963721205974926,
        -0.7331697579640576,
        -0.534493868897512,
        -0.731809308625138,
        -0.7586685942034861,
        -0.2686396257016946,
        -0.7037448114729141
      ],
      [
        -0.7552966053495354,
        -0.7526288407019648,
        -0.7564416780349486,
        -0.7471821018397506,
        -0.7461477504517109,
        -0.7521328999018333,
        -0.7601572134438117,
        -0.7329756701792192,
        -0.7514763022862556,
        -0.7323102211233636,
        -0.7413195283964331,
        -0.6664378059393304,
        -0.7581750800424549,
        -0.7493754804404821,
        -0.7467722876822352,
        -0.7450169064481584,
        -0.748782182626219,
        -0.739791333472039,
        -0.7625805095738316,
        -0.7246671144109751,
        -0.7497089798744678,
        -0.7583491597483547,
        -0.7588986582386594,
        -0.7635579890407626,
        -0.7565318529210041,
        -0.7600918960294065,
        -0.7618180366931726,
        -0.7463629053333526,
        -0.7600896952757175,
        -0.758646810227956,
        -0.7484968745597568,
        -0.7566915343622469,
        -0.7503676633662895,
        -0.7517940953088629,
        -0.7497841084396188,
        -0.7473894393502691,
        -0.7580865168787769,
        -0.7553488414374532,
        -0.750040080066244,
        -0.7557904353820859,
        -0.7535063453704265,
        -0.7530135567133256,
        1.0,
        -0.7323294809101109,
        -0.7427756585809557,
        -0.7535766625765092,
        -0.7631049853022892,
        -0.7611682520333332,
        -0.7404495804167782,
        -0.7491444209334381
      ],
      [
        -0.7133536262963088,
        -0.7071690384771544,
        -0.6905575487489184,
        -0.672774968459591,
        -0.6546900809236857,
        -0.6825090733517432,
        -0.7327427534108789,
        -0.7260956825381395,
        -0.6617969002692159,
        -0.7243273732298805,
        -0.723717468832684,
        -0.706987852315194,
        -0.6501358621917248,
        -0.697290655683771,
        -0.6891551817853645,
        -0.7242989721084833,
        -0.7005511596565575,
        -0.7051360687268098,
        -0.7323964218667095,
        -0.44692965111187144,
        -0.7032660390261833,
        -0.7061838879133944,
        -0.7495021179462391,
        -0.7498549771088266,
        -0.7426299863087058,
        -0.7442138886180598,
        -0.7419865518829569,
        -0.7319367521839224,
        -0.7260449077033184,
        -0.7487874365709735,
        -0.65426591410118,
        -0.7233726592100991,
        -0.6921496209558868,
        -0.7210492081576961,
        -0.7414717823949637,
        -0.7074099083657086,
        -0.7394882490363957,
        -0.6696022840983429,
        -0.7183993522406515,
        -0.7322127897237345,
        -0.7191131602941746,
        -0.6963721205974926,
        -0.7323294809101109,
        1.0,
        -0.6927579325888336,
        -0.7063387521162372,
        -0.737341713434658,
        -0.7615444276541642,
        -0.6354801142367019,
        -0.7178562282176268
      ],
      [
        -0.7166276422090869,
        -0.710197167746761,
        -0.7264293024691689,
        -0.6962406381060574,
        -0.6979816825380314,
        -0.7235689031674483,
        -0.7377516223568217,
        -0.7294462338173803,
        -0.7054712452705667,
        -0.7154199866328474,
        -0.7495278306085075,
        -0.6899440959655517,
        -0.716765824838608,
        -0.7033221181735998,
        -0.6844272502832462,
        -0.7097738510598648,
        -0.7258408086783366,
        -0.6855668773328079,
        -0.7518494699323335,
        -0.7040824653586621,
        -0.7367115811588416,
        -0.7197199382027304,
        -0.7486279303017914,
        -0.7557579526607361,
        -0.7323448011893454,
        -0.7484443462627679,
        -0.7553091925331339,
        -0.7214810286384454,
        -0.7319698011311822,
        -0.7432404354528723,
        -0.7169879236569807,
        -0.7323628826671287,
        -0.7261856239043671,
        -0.7239106907167829,
        -0.7348077114669248,
        -0.7235793034792687,
        -0.7398409624686524,
        -0.7311330751643872,
        -0.7353593251142151,
        -0.7313740642324487,
        -0.7560841951433505,
        -0.7331697579640576,
        -0.7427756585809557,
        -0.6927579325888336,
        1.0,
        -0.7136854502860852,
        -0.7509775091190622,
        -0.762009759763447,
        -0.6955563162604792,
        -0.7235957716428789
      ],
      [
        -0.7336686804534891,
        -0.6122964038197702,
        -0.6959942275142132,
        -0.3201674984886522,
        -0.5259511801657168,
        -0.5329213906751016,
        -0.7354745334449244,
        -0.7294815048822865,
        -0.5462901197331977,
        -0.7274485868194889,
        -0.736256251452946,
        -0.734498251925048,
        -0.7462445760231616,
        -0.6653681804890523,
        -0.5536822072472123,
        -0.742367095637368,
        -0.7330766374781625,
        -0.6956434177099273,
        -0.7063667304354344,
        -0.700352171776878,
        -0.5615869649636576,
        -0.749369897594163,
        -0.7542301419743107,
        -0.7492392548221353,
        -0.7500358790337016,
        -0.7540384849944615,
        -0.726159694743122,
        -0.7404966737799454,
        -0.7355169957200296,
        -0.7418702925526598,
        -0.503004651526846,
        -0.7456955477277992,
        -0.5773342569876585,
        -0.7302322749089931,
        -0.7435841569811656,
        -0.6987663220032168,
        -0.7465243893734852,
        -0.7514859980145996,
        -0.733558720273396,
        -0.7197668834483875,
        -0.7499678432669704,
        -0.534493868897512,
        -0.7535766625765092,
        -0.7063387521162372,
        -0.7136854502860852,
        1.0,
        -0.6012288013815028,
        -0.7647047988121207,
        -0.4844204829348886,
        -0.7044285249147393
      ],
      [
        -0.7543063188788711,
        -0.7424265885963013,
        -0.5380753404699451,
        -0.7202320105970121,
        -0.7170728432068956,
        -0.7019038269983802,
        -0.740430865789163,
        -0.7571973233873197,
        -0.7216784002132426,
        -0.7484220102778032,
        -0.7559672962600212,
        -0.7526141272608939,
        -0.7590042139062585,
        -0.7219863676139868,
        -0.7117802750564527,
        -0.7587337219652905,
        -0.7504405687371665,
        -0.736604068545774,
        -0.6112044997414505,
        -0.7422093614439257,
        -0.7263094328025559,
        -0.757800166469686,
        -0.7642182197897365,
        -0.7538949007736597,
        -0.7592310856427774,
        -0.7612025636813858,
        -0.752558192639349,
        -0.7539350636301575,
        -0.7503766046553304,
        -0.7605779951026678,
        -0.7067532490309064,
        -0.7539760805221452,
        -0.7426588932582112,
        -0.7463454972815077,
        -0.7599259727196986,
        -0.7284620379942073,
        -0.7536395083374028,
        -0.7598003709642448,
        -0.7529448694526866,
        -0.7432961065571695,
        -0.7579949586181649,
        -0.731809308625138,
        -0.7631049853022892,
        -0.737341713434658,
        -0.7509775091190622,
        -0.6012288013815028,
        1.0,
        -0.7640300888018021,
        -0.7062769753171882,
        -0.7397871880216129
      ],
      [
        -0.7654311541113826,
        -0.7652212655280105,
        -0.7650317209557191,
        -0.761627721076654,
        -0.7628441311336589,
        -0.7615440600376031,
        -0.7647688035235667,
        -0.7608168933364763,
        -0.7632525388782392,
        -0.7612336666876938,
        -0.7650134128731014,
        -0.7584648676295771,
        -0.7654515567539087,
        -0.7613766213894251,
        -0.763211639389344,
        -0.7556525115866961,
        -0.7615360653309562,
        -0.7588059766531529,
        -0.7632910103376069,
        -0.7628049217742894,
        -0.7643730692803448,
        -0.7654522631918953,
        -0.7645678896989299,
        -0.7646742607632296,
        -0.7644735539542016,
        -0.7645533339364387,
        -0.7645130042616566,
        -0.7629301468792761,
        -0.7642255076641695,
        -0.7631144983140973,
        -0.7610109718555514,
        -0.7636034823119329,
        -0.7640570328783056,
        -0.7629830368947724,
        -0.7637598864171382,
        -0.7615736446705992,
        -0.7627842137814982,
        -0.7650134455163955,
        -0.7613581526142941,
        -0.7618077194192162,
        -0.7613543080043289,
        -0.7586685942034861,
        -0.7611682520333332,
        -0.7615444276541642,
        -0.762009759763447,
        -0.7647047988121207,
        -0.7640300888018021,
        1.0,
        -0.7615798772333046,
        -0.7649284844737477
      ],
      [
        -0.7323276295171324,
        -0.5463774336986128,
        -0.7012026172205877,
        0.020028687972609857,
        -0.06413493878565524,
        -0.1517847810641803,
        -0.7119965772314758,
        -0.7120369454016013,
        -0.0848790171923358,
        -0.699433471373925,
        -0.7142061570344411,
        -0.7154638385797336,
        -0.7301181301839204,
        -0.6088232605445518,
        -0.481002657336014,
        -0.7228935805500003,
        -0.6895195503273125,
        -0.6687007913396433,
        -0.6515923543273459,
        -0.6403289240001677,
        -0.5040654680014952,
        -0.7269672398569293,
        -0.738477599577781,
        -0.7378725317550284,
        -0.7431551053176054,
        -0.7458562566242544,
        -0.6766925537296462,
        -0.7234048711628894,
        -0.7126810468044044,
        -0.7297506063513164,
        0.16044135887867272,
        -0.7215043804480195,
        -0.5198099292817775,
        -0.6937034764685539,
        -0.7371081897074385,
        -0.6364342039103328,
        -0.7279586548418497,
        -0.7376267075783537,
        -0.6985866274718293,
        -0.6858356404156614,
        -0.7310838265549808,
        -0.2686396257016946,
        -0.7404495804167782,
        -0.6354801142367019,
        -0.6955563162604792,
        -0.4844204829348886,
        -0.7062769753171882,
        -0.7615798772333046,
        1.0,
        -0.6626585929787353
      ],
      [
        -0.7353975684793493,
        -0.7118047613135826,
        -0.7091370560313035,
        -0.6807088283177578,
        -0.6878832862102301,
        -0.6732277731127874,
        -0.7199203823961577,
        -0.7226793668279445,
        -0.6893642882708869,
        -0.728189690014665,
        -0.7442071404198203,
        -0.7331101916648528,
        -0.7435232305078431,
        -0.7069115906331904,
        -0.70227303539689,
        -0.7441210299717562,
        -0.7281332128965391,
        -0.7128913111310489,
        -0.7341493300166445,
        -0.7106587773355424,
        -0.7290596046745637,
        -0.746210886704592,
        -0.7534832281754182,
        -0.7542439616996761,
        -0.7396104391475722,
        -0.7549387390937936,
        -0.7234241322896036,
        -0.7208591126581976,
        -0.7345638207600331,
        -0.754995024450567,
        -0.6610349381969233,
        -0.7452446865370705,
        -0.7076434034779725,
        -0.4359601458935486,
        -0.7479710487580251,
        -0.685359693095615,
        -0.7341328134422873,
        -0.7442175437460166,
        -0.7167928031360353,
        -0.7184675529317236,
        -0.7517999950969603,
        -0.7037448114729141,
        -0.7491444209334381,
        -0.7178562282176268,
        -0.7235957716428789,
        -0.7044285249147393,
        -0.7397871880216129,
        -0.7649284844737477,
        -0.6626585929787353,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        37,
        4,
        8,
        1,
        3,
        3,
        10,
        18,
        5,
        3,
        9,
        22,
        3,
        6,
        0,
        12,
        2,
        12,
        7,
        7,
        1,
        6,
        0,
        1,
        12,
        3,
        0,
        8,
        7,
        1,
        0,
        4,
        3,
        7,
        2,
        14,
        2,
        11,
        10,
        6,
        1,
        1,
        1,
        5,
        9,
        0,
        2,
        1,
        2,
        1
      ],
      "2020-02": [
        38,
        6,
        10,
        1,
        1,
        2,
        18,
        28,
        4,
        8,
        16,
        25,
        1,
        6,
        0,
        11,
        6,
        11,
        13,
        10,
        2,
        5,
        1,
        2,
        10,
        6,
        0,
        9,
        8,
        2,
        0,
        0,
        1,
        10,
        5,
        24,
        5,
        9,
        7,
        7,
        2,
        1,
        1,
        4,
        15,
        0,
        1,
        1,
        1,
        8
      ],
      "2020-03": [
        37,
        9,
        9,
        0,
        3,
        3,
        16,
        30,
        8,
        5,
        12,
        26,
        3,
        7,
        0,
        12,
        3,
        17,
        5,
        11,
        4,
        10,
        3,
        1,
        18,
        7,
        0,
        7,
        4,
        0,
        0,
        4,
        2,
        16,
        4,
        10,
        3,
        9,
        5,
        3,
        1,
        0,
        6,
        3,
        9,
        2,
        4,
        1,
        1,
        5
      ],
      "2020-04": [
        64,
        9,
        15,
        0,
        11,
        2,
        40,
        90,
        8,
        11,
        37,
        51,
        10,
        24,
        5,
        34,
        7,
        43,
        13,
        44,
        3,
        26,
        4,
        4,
        32,
        4,
        0,
        33,
        10,
        0,
        0,
        8,
        4,
        25,
        15,
        51,
        13,
        22,
        16,
        15,
        9,
        0,
        8,
        17,
        22,
        7,
        1,
        2,
        4,
        16
      ],
      "2020-05": [
        128,
        21,
        7,
        1,
        11,
        6,
        25,
        48,
        21,
        25,
        26,
        41,
        14,
        19,
        5,
        28,
        8,
        31,
        11,
        22,
        8,
        32,
        4,
        10,
        23,
        13,
        1,
        21,
        13,
        3,
        1,
        16,
        5,
        16,
        21,
        43,
        17,
        17,
        7,
        8,
        5,
        0,
        15,
        8,
        16,
        9,
        4,
        3,
        3,
        13
      ],
      "2020-06": [
        77,
        4,
        14,
        1,
        3,
        5,
        17,
        34,
        8,
        10,
        8,
        33,
        2,
        14,
        5,
        23,
        6,
        21,
        12,
        16,
        2,
        21,
        1,
        1,
        12,
        4,
        0,
        15,
        8,
        5,
        1,
        6,
        0,
        12,
        7,
        22,
        8,
        4,
        4,
        7,
        1,
        4,
        3,
        5,
        7,
        2,
        3,
        1,
        6,
        6
      ],
      "2020-07": [
        45,
        12,
        13,
        1,
        0,
        2,
        3,
        20,
        6,
        5,
        12,
        18,
        7,
        9,
        2,
        28,
        7,
        15,
        7,
        7,
        1,
        22,
        0,
        1,
        12,
        6,
        0,
        7,
        11,
        2,
        0,
        3,
        2,
        8,
        8,
        23,
        3,
        6,
        7,
        12,
        3,
        3,
        4,
        9,
        15,
        2,
        6,
        1,
        4,
        5
      ],
      "2020-08": [
        68,
        9,
        5,
        1,
        4,
        0,
        14,
        18,
        1,
        6,
        5,
        22,
        3,
        9,
        1,
        20,
        4,
        15,
        6,
        7,
        2,
        22,
        0,
        3,
        7,
        6,
        0,
        10,
        4,
        2,
        0,
        0,
        2,
        10,
        2,
        14,
        7,
        14,
        7,
        9,
        5,
        1,
        4,
        10,
        11,
        3,
        0,
        1,
        4,
        6
      ],
      "2020-09": [
        50,
        6,
        11,
        0,
        5,
        5,
        39,
        42,
        9,
        13,
        15,
        32,
        6,
        11,
        7,
        15,
        5,
        16,
        3,
        17,
        1,
        22,
        2,
        3,
        12,
        6,
        0,
        34,
        13,
        1,
        2,
        5,
        2,
        23,
        5,
        34,
        8,
        10,
        9,
        11,
        6,
        2,
        3,
        8,
        24,
        3,
        3,
        1,
        1,
        11
      ],
      "2020-10": [
        154,
        19,
        16,
        3,
        11,
        8,
        47,
        104,
        28,
        40,
        48,
        77,
        14,
        23,
        4,
        53,
        16,
        80,
        16,
        33,
        4,
        27,
        6,
        12,
        37,
        10,
        0,
        45,
        11,
        5,
        2,
        19,
        8,
        42,
        11,
        76,
        25,
        16,
        23,
        13,
        15,
        2,
        15,
        17,
        38,
        7,
        8,
        0,
        3,
        18
      ],
      "2020-11": [
        96,
        15,
        9,
        2,
        4,
        2,
        18,
        44,
        9,
        10,
        17,
        37,
        8,
        16,
        2,
        23,
        2,
        30,
        5,
        12,
        2,
        16,
        4,
        2,
        16,
        11,
        0,
        21,
        6,
        3,
        0,
        1,
        3,
        15,
        10,
        40,
        7,
        13,
        15,
        4,
        4,
        2,
        4,
        5,
        15,
        3,
        5,
        0,
        2,
        6
      ],
      "2020-12": [
        61,
        8,
        14,
        2,
        9,
        6,
        26,
        38,
        4,
        10,
        21,
        35,
        13,
        14,
        1,
        20,
        10,
        22,
        6,
        18,
        1,
        24,
        3,
        7,
        18,
        8,
        0,
        24,
        6,
        0,
        0,
        9,
        4,
        17,
        11,
        33,
        15,
        9,
        11,
        11,
        3,
        2,
        4,
        10,
        18,
        8,
        5,
        2,
        0,
        9
      ],
      "2021-01": [
        39,
        10,
        7,
        2,
        5,
        4,
        9,
        15,
        9,
        9,
        8,
        29,
        8,
        16,
        2,
        12,
        11,
        27,
        4,
        20,
        0,
        14,
        2,
        3,
        18,
        9,
        0,
        17,
        12,
        1,
        1,
        5,
        3,
        12,
        7,
        34,
        7,
        24,
        6,
        6,
        2,
        1,
        4,
        6,
        26,
        5,
        2,
        2,
        1,
        6
      ],
      "2021-02": [
        77,
        8,
        13,
        0,
        1,
        2,
        16,
        26,
        7,
        3,
        4,
        22,
        9,
        10,
        8,
        16,
        10,
        19,
        2,
        17,
        0,
        7,
        4,
        3,
        6,
        5,
        1,
        17,
        6,
        1,
        4,
        2,
        3,
        7,
        9,
        17,
        7,
        17,
        7,
        8,
        3,
        2,
        2,
        4,
        11,
        1,
        0,
        3,
        2,
        6
      ],
      "2021-03": [
        74,
        2,
        14,
        0,
        4,
        2,
        14,
        45,
        9,
        8,
        14,
        23,
        11,
        25,
        5,
        26,
        5,
        24,
        5,
        14,
        1,
        18,
        5,
        4,
        10,
        12,
        0,
        21,
        3,
        1,
        1,
        9,
        4,
        9,
        9,
        26,
        6,
        9,
        11,
        7,
        6,
        2,
        6,
        6,
        13,
        1,
        4,
        2,
        4,
        5
      ],
      "2021-04": [
        131,
        14,
        16,
        1,
        11,
        7,
        21,
        67,
        15,
        22,
        34,
        54,
        10,
        24,
        5,
        21,
        11,
        47,
        8,
        21,
        4,
        28,
        11,
        9,
        15,
        16,
        2,
        31,
        14,
        1,
        4,
        11,
        8,
        25,
        9,
        49,
        21,
        20,
        23,
        15,
        9,
        0,
        2,
        14,
        34,
        4,
        4,
        1,
        7,
        14
      ],
      "2021-05": [
        73,
        10,
        6,
        2,
        7,
        10,
        30,
        51,
        11,
        14,
        30,
        27,
        12,
        20,
        4,
        24,
        4,
        35,
        4,
        18,
        0,
        22,
        6,
        4,
        19,
        11,
        1,
        25,
        8,
        3,
        4,
        10,
        8,
        11,
        6,
        34,
        19,
        15,
        13,
        12,
        6,
        0,
        6,
        9,
        15,
        7,
        4,
        0,
        4,
        9
      ],
      "2021-06": [
        113,
        11,
        20,
        5,
        9,
        7,
        39,
        56,
        16,
        21,
        25,
        43,
        12,
        28,
        7,
        39,
        12,
        43,
        9,
        12,
        2,
        15,
        4,
        10,
        27,
        16,
        1,
        34,
        12,
        2,
        1,
        14,
        7,
        28,
        10,
        46,
        22,
        7,
        28,
        8,
        2,
        2,
        5,
        14,
        24,
        7,
        3,
        1,
        2,
        11
      ],
      "2021-07": [
        69,
        7,
        15,
        0,
        6,
        3,
        15,
        37,
        10,
        9,
        5,
        15,
        6,
        21,
        7,
        18,
        6,
        21,
        6,
        9,
        0,
        10,
        5,
        2,
        11,
        9,
        0,
        10,
        4,
        3,
        1,
        3,
        3,
        11,
        3,
        21,
        7,
        12,
        8,
        2,
        5,
        3,
        8,
        7,
        15,
        3,
        4,
        2,
        2,
        3
      ],
      "2021-08": [
        49,
        12,
        17,
        1,
        6,
        6,
        18,
        23,
        9,
        6,
        21,
        17,
        16,
        16,
        7,
        21,
        6,
        12,
        4,
        23,
        0,
        20,
        6,
        5,
        19,
        8,
        0,
        21,
        6,
        2,
        0,
        7,
        5,
        7,
        6,
        22,
        11,
        13,
        11,
        11,
        5,
        3,
        4,
        8,
        17,
        3,
        3,
        1,
        2,
        3
      ],
      "2021-09": [
        80,
        9,
        19,
        1,
        17,
        11,
        53,
        93,
        25,
        27,
        38,
        49,
        14,
        26,
        12,
        37,
        14,
        61,
        15,
        32,
        6,
        24,
        9,
        11,
        33,
        16,
        2,
        40,
        13,
        6,
        3,
        9,
        7,
        26,
        11,
        66,
        26,
        18,
        33,
        10,
        7,
        4,
        15,
        12,
        37,
        10,
        5,
        0,
        4,
        17
      ],
      "2021-10": [
        134,
        8,
        12,
        0,
        11,
        8,
        34,
        44,
        20,
        17,
        37,
        39,
        10,
        29,
        14,
        28,
        11,
        36,
        11,
        16,
        3,
        30,
        11,
        2,
        12,
        10,
        0,
        18,
        12,
        8,
        2,
        7,
        3,
        18,
        7,
        38,
        16,
        14,
        18,
        13,
        4,
        4,
        3,
        13,
        24,
        11,
        8,
        4,
        4,
        13
      ],
      "2021-11": [
        61,
        9,
        20,
        1,
        5,
        3,
        18,
        18,
        5,
        4,
        11,
        22,
        13,
        16,
        8,
        32,
        6,
        16,
        7,
        7,
        1,
        17,
        2,
        3,
        13,
        3,
        1,
        13,
        4,
        1,
        3,
        5,
        1,
        14,
        7,
        20,
        15,
        15,
        7,
        5,
        6,
        2,
        2,
        12,
        18,
        1,
        5,
        1,
        2,
        8
      ],
      "2021-12": [
        56,
        8,
        32,
        1,
        8,
        5,
        15,
        34,
        14,
        14,
        27,
        20,
        10,
        22,
        4,
        23,
        10,
        21,
        4,
        18,
        3,
        11,
        11,
        3,
        13,
        10,
        0,
        13,
        11,
        1,
        3,
        8,
        7,
        20,
        5,
        37,
        12,
        8,
        12,
        16,
        8,
        2,
        2,
        14,
        18,
        2,
        3,
        1,
        4,
        19
      ],
      "2022-01": [
        43,
        10,
        11,
        0,
        10,
        5,
        11,
        18,
        10,
        6,
        8,
        20,
        12,
        13,
        9,
        22,
        7,
        22,
        6,
        11,
        3,
        14,
        7,
        4,
        9,
        8,
        1,
        11,
        9,
        3,
        0,
        7,
        5,
        10,
        8,
        28,
        9,
        11,
        8,
        13,
        2,
        2,
        4,
        5,
        19,
        4,
        2,
        1,
        4,
        6
      ],
      "2022-02": [
        71,
        6,
        12,
        1,
        6,
        4,
        13,
        26,
        9,
        7,
        8,
        27,
        10,
        16,
        10,
        13,
        5,
        20,
        5,
        14,
        0,
        14,
        4,
        1,
        9,
        6,
        0,
        11,
        6,
        0,
        4,
        6,
        1,
        11,
        3,
        18,
        8,
        7,
        11,
        6,
        7,
        1,
        4,
        9,
        11,
        7,
        6,
        4,
        2,
        12
      ],
      "2022-03": [
        127,
        12,
        33,
        2,
        14,
        10,
        29,
        49,
        12,
        9,
        29,
        37,
        13,
        27,
        24,
        18,
        7,
        42,
        14,
        24,
        4,
        13,
        11,
        8,
        24,
        19,
        2,
        29,
        8,
        4,
        4,
        5,
        7,
        14,
        8,
        35,
        18,
        16,
        13,
        11,
        9,
        0,
        8,
        15,
        31,
        7,
        11,
        2,
        4,
        10
      ],
      "2022-04": [
        110,
        13,
        22,
        2,
        9,
        9,
        41,
        42,
        15,
        13,
        28,
        28,
        17,
        37,
        17,
        30,
        7,
        45,
        7,
        20,
        3,
        32,
        6,
        2,
        29,
        8,
        0,
        26,
        6,
        4,
        3,
        14,
        6,
        18,
        6,
        42,
        21,
        21,
        17,
        18,
        6,
        0,
        8,
        11,
        29,
        9,
        5,
        1,
        8,
        16
      ],
      "2022-05": [
        86,
        9,
        17,
        2,
        18,
        16,
        38,
        67,
        26,
        12,
        28,
        18,
        10,
        28,
        28,
        31,
        9,
        53,
        18,
        32,
        3,
        22,
        5,
        10,
        16,
        22,
        0,
        46,
        6,
        2,
        5,
        20,
        13,
        36,
        13,
        55,
        18,
        21,
        17,
        21,
        5,
        1,
        6,
        14,
        41,
        12,
        9,
        2,
        4,
        18
      ],
      "2022-06": [
        73,
        7,
        12,
        1,
        9,
        7,
        20,
        24,
        10,
        18,
        10,
        19,
        13,
        13,
        10,
        17,
        6,
        22,
        8,
        23,
        4,
        14,
        4,
        3,
        11,
        10,
        1,
        8,
        9,
        4,
        1,
        6,
        7,
        22,
        5,
        23,
        10,
        11,
        10,
        8,
        7,
        1,
        3,
        16,
        20,
        6,
        6,
        2,
        6,
        8
      ],
      "2022-07": [
        71,
        8,
        17,
        3,
        5,
        5,
        22,
        17,
        6,
        2,
        9,
        13,
        2,
        15,
        4,
        15,
        5,
        19,
        9,
        8,
        0,
        15,
        3,
        2,
        10,
        14,
        2,
        16,
        7,
        2,
        3,
        5,
        5,
        16,
        1,
        20,
        4,
        15,
        6,
        9,
        1,
        1,
        1,
        1,
        6,
        3,
        4,
        2,
        0,
        13
      ],
      "2022-08": [
        37,
        12,
        11,
        3,
        8,
        5,
        16,
        13,
        14,
        2,
        17,
        13,
        4,
        17,
        9,
        20,
        6,
        18,
        5,
        8,
        3,
        7,
        4,
        7,
        19,
        7,
        0,
        17,
        9,
        0,
        3,
        7,
        2,
        11,
        9,
        18,
        8,
        7,
        9,
        12,
        6,
        2,
        2,
        5,
        7,
        4,
        3,
        3,
        7,
        9
      ],
      "2022-09": [
        64,
        11,
        17,
        1,
        4,
        3,
        23,
        43,
        14,
        11,
        22,
        15,
        9,
        18,
        17,
        15,
        11,
        27,
        16,
        17,
        4,
        17,
        8,
        4,
        13,
        11,
        1,
        14,
        9,
        0,
        4,
        14,
        1,
        15,
        2,
        26,
        14,
        16,
        13,
        9,
        7,
        2,
        5,
        9,
        23,
        5,
        4,
        1,
        6,
        11
      ],
      "2022-10": [
        162,
        20,
        43,
        2,
        11,
        29,
        54,
        92,
        32,
        29,
        39,
        34,
        15,
        43,
        25,
        34,
        13,
        66,
        10,
        49,
        9,
        24,
        15,
        15,
        38,
        21,
        2,
        45,
        9,
        4,
        12,
        21,
        12,
        31,
        16,
        67,
        32,
        15,
        27,
        25,
        14,
        3,
        6,
        15,
        52,
        26,
        11,
        5,
        14,
        39
      ],
      "2022-11": [
        122,
        9,
        34,
        4,
        10,
        11,
        24,
        36,
        23,
        10,
        27,
        24,
        7,
        36,
        21,
        22,
        13,
        38,
        8,
        26,
        2,
        20,
        18,
        8,
        22,
        12,
        1,
        32,
        3,
        4,
        0,
        17,
        9,
        26,
        7,
        36,
        24,
        12,
        17,
        15,
        8,
        1,
        3,
        17,
        31,
        16,
        6,
        1,
        7,
        16
      ],
      "2022-12": [
        75,
        11,
        20,
        2,
        7,
        26,
        35,
        45,
        12,
        10,
        36,
        18,
        9,
        37,
        30,
        17,
        7,
        25,
        4,
        36,
        7,
        12,
        10,
        6,
        29,
        16,
        1,
        19,
        8,
        3,
        9,
        7,
        12,
        15,
        4,
        39,
        17,
        18,
        17,
        21,
        10,
        2,
        8,
        17,
        22,
        16,
        5,
        3,
        14,
        17
      ],
      "2023-01": [
        51,
        14,
        17,
        2,
        1,
        7,
        17,
        26,
        8,
        6,
        17,
        15,
        7,
        19,
        6,
        14,
        10,
        17,
        3,
        13,
        3,
        17,
        12,
        8,
        11,
        6,
        1,
        10,
        4,
        2,
        5,
        9,
        9,
        13,
        5,
        18,
        10,
        20,
        8,
        7,
        6,
        7,
        5,
        10,
        11,
        10,
        4,
        2,
        6,
        10
      ],
      "2023-02": [
        67,
        7,
        24,
        3,
        9,
        12,
        22,
        33,
        16,
        10,
        20,
        24,
        8,
        21,
        20,
        19,
        7,
        23,
        13,
        15,
        5,
        22,
        8,
        6,
        17,
        16,
        0,
        17,
        8,
        3,
        14,
        7,
        4,
        18,
        5,
        19,
        16,
        16,
        10,
        11,
        6,
        2,
        11,
        14,
        26,
        16,
        9,
        3,
        7,
        10
      ],
      "2023-03": [
        69,
        29,
        30,
        3,
        12,
        10,
        24,
        39,
        10,
        7,
        35,
        17,
        11,
        35,
        25,
        18,
        12,
        31,
        11,
        14,
        4,
        15,
        9,
        3,
        14,
        10,
        1,
        17,
        8,
        6,
        10,
        7,
        8,
        15,
        2,
        32,
        14,
        11,
        9,
        14,
        4,
        5,
        2,
        16,
        19,
        11,
        7,
        3,
        18,
        11
      ],
      "2023-04": [
        44,
        20,
        15,
        1,
        3,
        19,
        26,
        33,
        15,
        4,
        16,
        27,
        8,
        28,
        16,
        30,
        14,
        37,
        8,
        16,
        5,
        16,
        12,
        7,
        11,
        9,
        0,
        18,
        8,
        4,
        11,
        12,
        9,
        13,
        3,
        31,
        8,
        23,
        12,
        16,
        2,
        3,
        2,
        26,
        24,
        22,
        6,
        1,
        26,
        9
      ],
      "2023-05": [
        203,
        32,
        67,
        4,
        21,
        68,
        76,
        132,
        56,
        26,
        90,
        73,
        17,
        100,
        79,
        50,
        30,
        94,
        26,
        59,
        20,
        24,
        25,
        33,
        51,
        17,
        6,
        63,
        27,
        12,
        57,
        38,
        21,
        52,
        23,
        100,
        47,
        18,
        43,
        52,
        12,
        4,
        19,
        42,
        68,
        70,
        11,
        2,
        52,
        48
      ],
      "2023-06": [
        145,
        29,
        43,
        4,
        18,
        32,
        33,
        51,
        33,
        15,
        28,
        32,
        13,
        55,
        27,
        30,
        20,
        49,
        14,
        28,
        12,
        26,
        20,
        11,
        19,
        27,
        0,
        34,
        17,
        8,
        34,
        13,
        16,
        21,
        17,
        35,
        25,
        11,
        22,
        17,
        4,
        3,
        3,
        23,
        21,
        35,
        5,
        1,
        35,
        17
      ],
      "2023-07": [
        96,
        12,
        26,
        4,
        15,
        16,
        27,
        24,
        18,
        3,
        19,
        24,
        13,
        35,
        25,
        27,
        17,
        24,
        11,
        20,
        9,
        20,
        13,
        5,
        12,
        14,
        4,
        17,
        13,
        9,
        38,
        8,
        13,
        16,
        7,
        24,
        11,
        14,
        10,
        13,
        5,
        6,
        5,
        24,
        27,
        37,
        10,
        1,
        40,
        14
      ],
      "2023-08": [
        70,
        28,
        24,
        1,
        13,
        26,
        23,
        33,
        20,
        2,
        12,
        23,
        7,
        40,
        17,
        25,
        9,
        24,
        15,
        17,
        8,
        14,
        12,
        6,
        17,
        13,
        8,
        19,
        14,
        10,
        40,
        3,
        6,
        24,
        5,
        33,
        10,
        13,
        20,
        17,
        7,
        2,
        4,
        16,
        22,
        35,
        5,
        2,
        39,
        16
      ],
      "2023-09": [
        105,
        24,
        38,
        1,
        19,
        31,
        31,
        40,
        28,
        15,
        24,
        31,
        10,
        52,
        36,
        32,
        15,
        27,
        15,
        18,
        17,
        18,
        18,
        3,
        14,
        17,
        9,
        17,
        16,
        8,
        65,
        13,
        5,
        22,
        6,
        35,
        10,
        18,
        24,
        8,
        7,
        2,
        3,
        28,
        11,
        41,
        9,
        0,
        41,
        19
      ],
      "2023-10": [
        140,
        20,
        42,
        14,
        42,
        79,
        51,
        71,
        44,
        13,
        37,
        41,
        12,
        80,
        54,
        32,
        28,
        63,
        36,
        49,
        40,
        23,
        26,
        19,
        31,
        22,
        6,
        45,
        20,
        16,
        104,
        34,
        20,
        46,
        19,
        74,
        39,
        23,
        40,
        19,
        17,
        12,
        9,
        51,
        33,
        78,
        13,
        3,
        83,
        53
      ],
      "2023-11": [
        74,
        32,
        41,
        6,
        34,
        45,
        29,
        53,
        35,
        16,
        35,
        26,
        20,
        55,
        45,
        28,
        25,
        47,
        21,
        29,
        25,
        19,
        16,
        8,
        22,
        10,
        8,
        22,
        11,
        16,
        68,
        21,
        19,
        25,
        9,
        54,
        21,
        11,
        21,
        19,
        10,
        4,
        4,
        38,
        17,
        55,
        12,
        2,
        56,
        25
      ],
      "2023-12": [
        62,
        25,
        26,
        5,
        14,
        39,
        17,
        34,
        20,
        4,
        14,
        15,
        11,
        50,
        32,
        26,
        16,
        16,
        21,
        23,
        16,
        22,
        15,
        14,
        15,
        14,
        10,
        15,
        9,
        7,
        62,
        17,
        12,
        34,
        9,
        26,
        20,
        18,
        14,
        11,
        6,
        4,
        4,
        23,
        13,
        46,
        8,
        1,
        44,
        23
      ],
      "2024-01": [
        83,
        33,
        27,
        3,
        24,
        32,
        30,
        43,
        24,
        1,
        18,
        20,
        13,
        62,
        35,
        30,
        21,
        43,
        35,
        25,
        23,
        28,
        20,
        7,
        20,
        13,
        20,
        19,
        9,
        10,
        80,
        13,
        8,
        19,
        7,
        51,
        15,
        16,
        16,
        11,
        5,
        10,
        1,
        38,
        15,
        44,
        4,
        3,
        54,
        23
      ],
      "2024-02": [
        104,
        37,
        44,
        12,
        55,
        94,
        36,
        55,
        44,
        10,
        40,
        36,
        17,
        108,
        71,
        27,
        30,
        70,
        72,
        39,
        57,
        30,
        14,
        22,
        27,
        14,
        40,
        25,
        25,
        14,
        172,
        27,
        21,
        33,
        16,
        60,
        19,
        24,
        36,
        34,
        11,
        11,
        7,
        45,
        25,
        107,
        19,
        1,
        122,
        53
      ],
      "2024-03": [
        96,
        42,
        49,
        11,
        37,
        44,
        36,
        59,
        37,
        13,
        40,
        31,
        19,
        87,
        47,
        37,
        23,
        47,
        51,
        33,
        40,
        27,
        39,
        13,
        27,
        24,
        27,
        29,
        20,
        14,
        99,
        18,
        17,
        33,
        22,
        55,
        23,
        27,
        29,
        16,
        4,
        10,
        5,
        28,
        23,
        64,
        9,
        3,
        78,
        36
      ],
      "2024-04": [
        86,
        41,
        40,
        5,
        37,
        52,
        18,
        44,
        32,
        7,
        35,
        22,
        13,
        48,
        43,
        25,
        19,
        72,
        37,
        20,
        43,
        22,
        9,
        13,
        23,
        18,
        32,
        33,
        15,
        10,
        114,
        15,
        14,
        26,
        15,
        52,
        32,
        16,
        27,
        23,
        4,
        10,
        5,
        34,
        19,
        47,
        11,
        1,
        73,
        22
      ],
      "2024-05": [
        95,
        48,
        34,
        11,
        34,
        51,
        19,
        45,
        39,
        3,
        28,
        24,
        11,
        74,
        52,
        39,
        27,
        37,
        44,
        32,
        49,
        12,
        16,
        13,
        19,
        16,
        51,
        24,
        23,
        10,
        133,
        26,
        25,
        26,
        13,
        45,
        19,
        23,
        22,
        20,
        15,
        12,
        5,
        30,
        23,
        51,
        13,
        5,
        97,
        35
      ],
      "2024-06": [
        211,
        49,
        55,
        14,
        49,
        88,
        34,
        61,
        68,
        7,
        48,
        25,
        13,
        95,
        68,
        36,
        33,
        83,
        66,
        30,
        87,
        16,
        26,
        20,
        22,
        24,
        84,
        25,
        22,
        16,
        172,
        31,
        26,
        35,
        19,
        75,
        40,
        24,
        37,
        32,
        15,
        8,
        6,
        56,
        37,
        109,
        17,
        1,
        134,
        54
      ],
      "2024-07": [
        113,
        32,
        39,
        10,
        45,
        56,
        29,
        49,
        62,
        6,
        26,
        24,
        8,
        69,
        40,
        36,
        23,
        38,
        44,
        25,
        41,
        18,
        26,
        11,
        19,
        22,
        53,
        23,
        15,
        11,
        123,
        13,
        11,
        21,
        23,
        48,
        18,
        22,
        33,
        22,
        2,
        5,
        4,
        36,
        21,
        50,
        11,
        1,
        94,
        24
      ],
      "2024-08": [
        84,
        38,
        25,
        9,
        31,
        52,
        18,
        40,
        40,
        6,
        26,
        29,
        8,
        64,
        41,
        33,
        19,
        34,
        36,
        23,
        35,
        8,
        15,
        14,
        11,
        15,
        44,
        11,
        10,
        16,
        81,
        13,
        10,
        23,
        5,
        35,
        29,
        9,
        26,
        14,
        12,
        7,
        1,
        19,
        20,
        48,
        5,
        3,
        76,
        23
      ],
      "2024-09": [
        136,
        44,
        29,
        5,
        31,
        63,
        18,
        36,
        33,
        8,
        26,
        28,
        20,
        69,
        35,
        25,
        22,
        46,
        53,
        18,
        42,
        18,
        17,
        16,
        17,
        15,
        66,
        18,
        20,
        7,
        112,
        16,
        14,
        17,
        7,
        41,
        22,
        7,
        14,
        25,
        13,
        9,
        7,
        28,
        21,
        62,
        14,
        2,
        70,
        20
      ],
      "2024-10": [
        148,
        52,
        64,
        30,
        73,
        142,
        31,
        72,
        64,
        11,
        36,
        36,
        22,
        126,
        81,
        28,
        30,
        69,
        82,
        38,
        107,
        27,
        37,
        18,
        13,
        25,
        110,
        23,
        19,
        19,
        203,
        27,
        16,
        39,
        18,
        80,
        34,
        37,
        50,
        38,
        13,
        16,
        7,
        58,
        28,
        105,
        14,
        3,
        155,
        57
      ],
      "2024-11": [
        78,
        26,
        40,
        11,
        28,
        43,
        17,
        33,
        36,
        5,
        17,
        23,
        16,
        70,
        34,
        27,
        20,
        40,
        40,
        26,
        32,
        16,
        4,
        9,
        13,
        12,
        54,
        12,
        16,
        14,
        89,
        8,
        7,
        18,
        9,
        41,
        15,
        19,
        25,
        21,
        11,
        9,
        4,
        29,
        15,
        34,
        10,
        2,
        84,
        13
      ],
      "2024-12": [
        113,
        34,
        53,
        15,
        48,
        72,
        22,
        50,
        34,
        9,
        28,
        22,
        14,
        74,
        46,
        37,
        25,
        48,
        56,
        24,
        41,
        21,
        28,
        16,
        20,
        14,
        69,
        20,
        8,
        10,
        104,
        16,
        11,
        33,
        13,
        55,
        26,
        20,
        21,
        21,
        15,
        5,
        5,
        29,
        21,
        64,
        12,
        5,
        91,
        25
      ],
      "2025-01": [
        70,
        38,
        39,
        9,
        42,
        77,
        23,
        34,
        29,
        5,
        22,
        21,
        13,
        61,
        22,
        26,
        22,
        39,
        31,
        17,
        32,
        14,
        23,
        13,
        10,
        16,
        66,
        10,
        9,
        11,
        97,
        16,
        19,
        18,
        9,
        28,
        16,
        15,
        15,
        15,
        9,
        6,
        8,
        35,
        17,
        26,
        8,
        3,
        82,
        27
      ],
      "2025-02": [
        116,
        43,
        42,
        26,
        70,
        185,
        29,
        47,
        50,
        14,
        34,
        36,
        13,
        143,
        62,
        43,
        39,
        74,
        90,
        50,
        83,
        24,
        34,
        22,
        10,
        21,
        110,
        23,
        25,
        22,
        212,
        29,
        23,
        30,
        15,
        64,
        29,
        20,
        40,
        28,
        11,
        14,
        5,
        50,
        24,
        85,
        9,
        1,
        162,
        44
      ],
      "2025-03": [
        98,
        50,
        58,
        22,
        27,
        140,
        20,
        41,
        44,
        9,
        30,
        20,
        8,
        99,
        40,
        39,
        21,
        48,
        96,
        30,
        58,
        20,
        22,
        13,
        12,
        21,
        80,
        19,
        26,
        13,
        136,
        16,
        12,
        18,
        10,
        49,
        14,
        19,
        37,
        25,
        9,
        11,
        3,
        37,
        19,
        51,
        8,
        4,
        98,
        34
      ],
      "2025-04": [
        67,
        43,
        38,
        5,
        35,
        134,
        18,
        43,
        33,
        9,
        19,
        21,
        9,
        62,
        31,
        30,
        28,
        46,
        64,
        22,
        39,
        24,
        30,
        12,
        17,
        11,
        81,
        9,
        25,
        18,
        121,
        15,
        17,
        24,
        10,
        30,
        22,
        18,
        23,
        21,
        16,
        9,
        2,
        39,
        18,
        44,
        7,
        3,
        88,
        22
      ],
      "2025-05": [
        180,
        72,
        56,
        20,
        88,
        321,
        27,
        60,
        77,
        11,
        28,
        29,
        19,
        165,
        63,
        41,
        34,
        77,
        127,
        43,
        99,
        18,
        30,
        36,
        22,
        35,
        128,
        26,
        19,
        16,
        193,
        30,
        23,
        40,
        18,
        69,
        35,
        19,
        47,
        34,
        14,
        12,
        8,
        59,
        26,
        100,
        20,
        3,
        182,
        52
      ],
      "2025-06": [
        172,
        62,
        52,
        14,
        48,
        198,
        31,
        33,
        58,
        15,
        25,
        32,
        28,
        121,
        47,
        19,
        30,
        59,
        105,
        31,
        69,
        21,
        21,
        27,
        18,
        29,
        92,
        25,
        16,
        17,
        141,
        11,
        13,
        25,
        18,
        45,
        35,
        23,
        47,
        28,
        19,
        9,
        2,
        51,
        24,
        71,
        16,
        6,
        115,
        33
      ],
      "2025-07": [
        113,
        57,
        32,
        14,
        34,
        128,
        20,
        37,
        44,
        9,
        19,
        21,
        10,
        82,
        34,
        26,
        23,
        41,
        68,
        28,
        45,
        12,
        28,
        9,
        15,
        20,
        78,
        19,
        20,
        21,
        93,
        13,
        14,
        22,
        20,
        38,
        13,
        14,
        27,
        22,
        12,
        9,
        6,
        28,
        17,
        51,
        11,
        2,
        81,
        29
      ],
      "2025-08": [
        112,
        46,
        20,
        14,
        45,
        130,
        25,
        43,
        57,
        3,
        17,
        21,
        11,
        83,
        40,
        44,
        25,
        53,
        101,
        30,
        51,
        18,
        30,
        22,
        21,
        21,
        108,
        13,
        19,
        21,
        115,
        21,
        15,
        21,
        26,
        43,
        22,
        19,
        23,
        22,
        18,
        11,
        6,
        38,
        17,
        44,
        8,
        3,
        94,
        15
      ],
      "2025-09": [
        62,
        19,
        21,
        7,
        18,
        46,
        5,
        17,
        25,
        2,
        8,
        11,
        1,
        21,
        14,
        17,
        9,
        18,
        53,
        14,
        25,
        9,
        11,
        6,
        11,
        13,
        35,
        13,
        1,
        9,
        54,
        4,
        10,
        13,
        16,
        14,
        13,
        7,
        13,
        12,
        9,
        2,
        0,
        10,
        5,
        29,
        6,
        3,
        45,
        9
      ]
    },
    "papers": {
      "0": [
        {
          "title": "A Textless Metric for Speech-to-Speech Comparison",
          "year": "2022-10",
          "abstract": "In this paper, we introduce a new and simple method for comparing speech\nutterances without relying on text transcripts. Our speech-to-speech comparison\nmetric utilizes state-of-the-art speech2unit encoders like HuBERT to convert\nspeech utterances into discrete acoustic units. We then propose a simple and\neasily replicable neural architecture that learns a speech-based metric that\nclosely corresponds to its text-based counterpart. This textless metric has\nnumerous potential applications, including evaluating speech-to-speech\ntranslation for oral languages, languages without dependable ASR systems, or to\navoid the need for ASR transcription altogether. This paper also shows that for\nspeech-to-speech translation evaluation, ASR-BLEU (which consists in\nautomatically transcribing both speech hypothesis and reference and compute\nsentence-level BLEU between transcripts) is a poor proxy to real text-BLEU even\nwhen ASR system is strong.",
          "arxiv_id": "2210.11835v2"
        },
        {
          "title": "Simultaneous Speech-to-Speech Translation System with Neural Incremental ASR, MT, and TTS",
          "year": "2020-11",
          "abstract": "This paper presents a newly developed, simultaneous neural speech-to-speech\ntranslation system and its evaluation. The system consists of three\nfully-incremental neural processing modules for automatic speech recognition\n(ASR), machine translation (MT), and text-to-speech synthesis (TTS). We\ninvestigated its overall latency in the system's Ear-Voice Span and speaking\nlatency along with module-level performance.",
          "arxiv_id": "2011.04845v2"
        },
        {
          "title": "Gated Recurrent Fusion with Joint Training Framework for Robust End-to-End Speech Recognition",
          "year": "2020-11",
          "abstract": "The joint training framework for speech enhancement and recognition methods\nhave obtained quite good performances for robust end-to-end automatic speech\nrecognition (ASR). However, these methods only utilize the enhanced feature as\nthe input of the speech recognition component, which are affected by the speech\ndistortion problem. In order to address this problem, this paper proposes a\ngated recurrent fusion (GRF) method with joint training framework for robust\nend-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and\nenhanced features. Therefore, the GRF can not only remove the noise signals\nfrom the enhanced features, but also learn the raw fine structures from the\nnoisy features so that it can alleviate the speech distortion. The proposed\nmethod consists of speech enhancement, GRF and speech recognition. Firstly, the\nmask based speech enhancement network is applied to enhance the input speech.\nSecondly, the GRF is applied to address the speech distortion problem. Thirdly,\nto improve the performance of ASR, the state-of-the-art speech transformer\nalgorithm is used as the speech recognition component. Finally, the joint\ntraining framework is utilized to optimize these three components,\nsimultaneously. Our experiments are conducted on an open-source Mandarin speech\ncorpus called AISHELL-1. Experimental results show that the proposed method\nachieves the relative character error rate (CER) reduction of 10.04\\% over the\nconventional joint enhancement and transformer method only using the enhanced\nfeatures. Especially for the low signal-to-noise ratio (0 dB), our proposed\nmethod can achieves better performances with 12.67\\% CER reduction, which\nsuggests the potential of our proposed method.",
          "arxiv_id": "2011.04249v1"
        }
      ],
      "1": [
        {
          "title": "A Study of Social and Behavioral Determinants of Health in Lung Cancer Patients Using Transformers-based Natural Language Processing Models",
          "year": "2021-08",
          "abstract": "Social and behavioral determinants of health (SBDoH) have important roles in\nshaping people's health. In clinical research studies, especially comparative\neffectiveness studies, failure to adjust for SBDoH factors will potentially\ncause confounding issues and misclassification errors in either statistical\nanalyses and machine learning-based models. However, there are limited studies\nto examine SBDoH factors in clinical outcomes due to the lack of structured\nSBDoH information in current electronic health record (EHR) systems, while much\nof the SBDoH information is documented in clinical narratives. Natural language\nprocessing (NLP) is thus the key technology to extract such information from\nunstructured clinical text. However, there is not a mature clinical NLP system\nfocusing on SBDoH. In this study, we examined two state-of-the-art\ntransformer-based NLP models, including BERT and RoBERTa, to extract SBDoH\nconcepts from clinical narratives, applied the best performing model to extract\nSBDoH concepts on a lung cancer screening patient cohort, and examined the\ndifference of SBDoH information between NLP extracted results and structured\nEHRs (SBDoH information captured in standard vocabularies such as the\nInternational Classification of Diseases codes). The experimental results show\nthat the BERT-based NLP model achieved the best strict/lenient F1-score of\n0.8791 and 0.8999, respectively. The comparison between NLP extracted SBDoH\ninformation and structured EHRs in the lung cancer patient cohort of 864\npatients with 161,933 various types of clinical notes showed that much more\ndetailed information about smoking, education, and employment were only\ncaptured in clinical narratives and that it is necessary to use both clinical\nnarratives and structured EHRs to construct a more complete picture of\npatients' SBDoH factors.",
          "arxiv_id": "2108.04949v1"
        },
        {
          "title": "Generative Medical Event Models Improve with Scale",
          "year": "2025-08",
          "abstract": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes.",
          "arxiv_id": "2508.12104v1"
        },
        {
          "title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records",
          "year": "2022-02",
          "abstract": "There is an increasing interest in developing artificial intelligence (AI)\nsystems to process and interpret electronic health records (EHRs). Natural\nlanguage processing (NLP) powered by pretrained language models is the key\ntechnology for medical AI systems utilizing clinical narratives. However, there\nare few clinical language models, the largest of which trained in the clinical\ndomain is comparatively small at 110 million parameters (compared with billions\nof parameters in the general domain). It is not clear how large clinical\nlanguage models with billions of parameters can help medical AI systems utilize\nunstructured EHRs. In this study, we develop from scratch a large clinical\nlanguage model - GatorTron - using >90 billion words of text (including >82\nbillion words of de-identified clinical text) and systematically evaluate it on\n5 clinical NLP tasks including clinical concept extraction, medical relation\nextraction, semantic textual similarity, natural language inference (NLI), and\nmedical question answering (MQA). We examine how (1) scaling up the number of\nparameters and (2) scaling up the size of the training data could benefit these\nNLP tasks. GatorTron models scale up the clinical language model from 110\nmillion to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6%\nand 9.5% improvement in accuracy for NLI and MQA), which can be applied to\nmedical AI systems to improve healthcare delivery. The GatorTron models are\npublicly available at:\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.",
          "arxiv_id": "2203.03540v3"
        }
      ],
      "2": [
        {
          "title": "TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment",
          "year": "2024-05",
          "abstract": "Recent advancements in image understanding have benefited from the extensive\nuse of web image-text pairs. However, video understanding remains a challenge\ndespite the availability of substantial web video-text data. This difficulty\nprimarily arises from the inherent complexity of videos and the inefficient\nlanguage supervision in recent web-collected video-text datasets. In this\npaper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend\nlarge language models (LLMs) for video understanding, without the need for\npre-training on real video data. Specifically, we first employ an advanced LLM\nto automatically generate Textual Videos comprising continuous textual frames,\nalong with corresponding annotations to simulate real video-text data. Then,\nthese annotated textual videos are used to pre-align a language-only LLM with\nthe video modality. To bridge the gap between textual and real videos, we\nemploy the CLIP model as the feature extractor to align image and text\nmodalities. During text-only pre-alignment, the continuous textual frames,\nencoded as a sequence of CLIP text features, are analogous to continuous CLIP\nimage features, thus aligning the LLM with real video representation. Extensive\nexperiments, including zero-shot evaluation and finetuning on various video\nunderstanding tasks, demonstrate that TOPA is an effective and efficient\nframework for aligning video content with LLMs. In particular, without training\non any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%\non the challenging long-form video understanding benchmark, Egoschema. This\nperformance surpasses previous video-text pre-training approaches and proves\ncompetitive with recent GPT-3.5-based video agents.",
          "arxiv_id": "2405.13911v2"
        },
        {
          "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
          "year": "2022-08",
          "abstract": "A big convergence of language, vision, and multimodal pretraining is\nemerging. In this work, we introduce a general-purpose multimodal foundation\nmodel BEiT-3, which achieves state-of-the-art transfer performance on both\nvision and vision-language tasks. Specifically, we advance the big convergence\nfrom three aspects: backbone architecture, pretraining task, and model scaling\nup. We introduce Multiway Transformers for general-purpose modeling, where the\nmodular architecture enables both deep fusion and modality-specific encoding.\nBased on the shared backbone, we perform masked \"language\" modeling on images\n(Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a\nunified manner. Experimental results show that BEiT-3 obtains state-of-the-art\nperformance on object detection (COCO), semantic segmentation (ADE20K), image\nclassification (ImageNet), visual reasoning (NLVR2), visual question answering\n(VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).",
          "arxiv_id": "2208.10442v2"
        },
        {
          "title": "E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning",
          "year": "2021-06",
          "abstract": "Vision-language pre-training (VLP) on large-scale image-text pairs has\nachieved huge success for the cross-modal downstream tasks. The most existing\npre-training methods mainly adopt a two-step training procedure, which firstly\nemploys a pre-trained object detector to extract region-based visual features,\nthen concatenates the image representation and text embedding as the input of\nTransformer to train. However, these methods face problems of using\ntask-specific visual representation of the specific object detector for generic\ncross-modal understanding, and the computation inefficiency of two-stage\npipeline. In this paper, we propose the first end-to-end vision-language\npre-trained model for both V+L understanding and generation, namely E2E-VLP,\nwhere we build a unified Transformer framework to jointly learn visual\nrepresentation, and semantic alignments between image and text. We incorporate\nthe tasks of object detection and image captioning into pre-training with a\nunified Transformer encoder-decoder architecture for enhancing visual learning.\nAn extensive set of experiments have been conducted on well-established\nvision-language downstream tasks to demonstrate the effectiveness of this novel\nVLP paradigm.",
          "arxiv_id": "2106.01804v2"
        }
      ],
      "3": [
        {
          "title": "KVCrush: Key value cache size-reduction using similarity in head-behaviour",
          "year": "2025-02",
          "abstract": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
          "arxiv_id": "2503.00022v1"
        },
        {
          "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs",
          "year": "2025-07",
          "abstract": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
          "arxiv_id": "2507.19823v1"
        },
        {
          "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios",
          "year": "2024-09",
          "abstract": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
          "arxiv_id": "2409.10593v3"
        }
      ],
      "4": [
        {
          "title": "SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models",
          "year": "2024-05",
          "abstract": "Integrated Speech and Large Language Models (SLMs) that can follow speech\ninstructions and generate relevant text responses have gained popularity\nlately. However, the safety and robustness of these models remains largely\nunclear. In this work, we investigate the potential vulnerabilities of such\ninstruction-following speech-language models to adversarial attacks and\njailbreaking. Specifically, we design algorithms that can generate adversarial\nexamples to jailbreak SLMs in both white-box and black-box attack settings\nwithout human involvement. Additionally, we propose countermeasures to thwart\nsuch jailbreaking attacks. Our models, trained on dialog data with speech\ninstructions, achieve state-of-the-art performance on spoken question-answering\ntask, scoring over 80% on both safety and helpfulness metrics. Despite safety\nguardrails, experiments on jailbreaking demonstrate the vulnerability of SLMs\nto adversarial perturbations and transfer attacks, with average attack success\nrates of 90% and 10% respectively when evaluated on a dataset of carefully\ndesigned harmful questions spanning 12 different toxic categories. However, we\ndemonstrate that our proposed countermeasures reduce the attack success\nsignificantly.",
          "arxiv_id": "2405.08317v1"
        },
        {
          "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
          "year": "2024-02",
          "abstract": "As large language models (LLMs) become increasingly integrated into\nreal-world applications such as code generation and chatbot assistance,\nextensive efforts have been made to align LLM behavior with human values,\nincluding safety. Jailbreak attacks, aiming to provoke unintended and unsafe\nbehaviors from LLMs, remain a significant/leading LLM safety threat. In this\npaper, we aim to defend LLMs against jailbreak attacks by introducing\nSafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and\nharmless responses to user queries. Our insight in developing SafeDecoding is\nbased on the observation that, even though probabilities of tokens representing\nharmful contents outweigh those representing harmless responses, safety\ndisclaimers still appear among the top tokens after sorting tokens by\nprobability in descending order. This allows us to mitigate jailbreak attacks\nby identifying safety disclaimers and amplifying their token probabilities,\nwhile simultaneously attenuating the probabilities of token sequences that are\naligned with the objectives of jailbreak attacks. We perform extensive\nexperiments on five LLMs using six state-of-the-art jailbreak attacks and four\nbenchmark datasets. Our results show that SafeDecoding significantly reduces\nthe attack success rate and harmfulness of jailbreak attacks without\ncompromising the helpfulness of responses to benign user queries. SafeDecoding\noutperforms six defense methods.",
          "arxiv_id": "2402.08983v4"
        },
        {
          "title": "Uncovering Safety Risks of Large Language Models through Concept Activation Vector",
          "year": "2024-04",
          "abstract": "Despite careful safety alignment, current large language models (LLMs) remain\nvulnerable to various attacks. To further unveil the safety risks of LLMs, we\nintroduce a Safety Concept Activation Vector (SCAV) framework, which\neffectively guides the attacks by accurately interpreting LLMs' safety\nmechanisms. We then develop an SCAV-guided attack method that can generate both\nattack prompts and embedding-level attacks with automatically selected\nperturbation hyperparameters. Both automatic and human evaluations demonstrate\nthat our attack method significantly improves the attack success rate and\nresponse quality while requiring less training data. Additionally, we find that\nour generated attack prompts may be transferable to GPT-4, and the\nembedding-level attacks may also be transferred to other white-box LLMs whose\nparameters are known. Our experiments further uncover the safety risks present\nin current LLMs. For example, in our evaluation of seven open-source LLMs, we\nobserve an average attack success rate of 99.14%, based on the classic\nkeyword-matching criterion. Finally, we provide insights into the safety\nmechanism of LLMs. The code is available at\nhttps://github.com/SproutNan/AI-Safety_SCAV.",
          "arxiv_id": "2404.12038v5"
        }
      ],
      "5": [
        {
          "title": "Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization",
          "year": "2025-04",
          "abstract": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\n(Ada-R1) significantly reduces inference costs compared to other baseline\napproaches, while maintaining performance. Notably, on five mathematical\ndatasets, the average length of reasoning is reduced by more than 50%,\nhighlighting the potential of adaptive strategies to optimize reasoning\nefficiency in large language models. Our code is coming soon at\nhttps://github.com/StarDewXXX/AdaR1",
          "arxiv_id": "2504.21659v2"
        },
        {
          "title": "Training Large Language Models to Reason in a Continuous Latent Space",
          "year": "2024-12",
          "abstract": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research.",
          "arxiv_id": "2412.06769v2"
        },
        {
          "title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
          "year": "2023-10",
          "abstract": "Recent advances in large language models (LLMs) have revolutionized the\nlandscape of reasoning tasks. To enhance the capabilities of LLMs to emulate\nhuman reasoning, prior studies have focused on modeling reasoning steps using\nvarious thought structures like chains, trees, or graphs. However, LLM-based\nreasoning still encounters the following challenges: (1) Limited adaptability\nof preset structures to diverse tasks; (2) Insufficient precision in exploiting\nknown conditions to derive new ones; and (3) Inadequate consideration of\nhistorical reasoning experiences for subsequent reasoning steps. To this end,\nwe propose DetermLR, a novel perspective that rethinks the reasoning process as\nan evolution from indeterminacy to determinacy. First, we categorize known\nconditions into two types: determinate and indeterminate premises This provides\nan oveall direction for the reasoning process and guides LLMs in converting\nindeterminate data into progressively determinate insights. Subsequently, we\nleverage quantitative measurements to prioritize more relevant premises to\nexplore new insights. Furthermore, we automate the storage and extraction of\navailable premises and reasoning paths with reasoning memory, preserving\nhistorical reasoning details for subsequent reasoning steps. Comprehensive\nexperimental results demonstrate that DetermLR surpasses all baselines on\nvarious logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and\nLogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR\nachieves higher accuracy with fewer reasoning steps, highlighting its superior\nefficiency and effectiveness in solving logical reasoning tasks.",
          "arxiv_id": "2310.18659v2"
        }
      ],
      "6": [
        {
          "title": "Fusing task-oriented and open-domain dialogues in conversational agents",
          "year": "2021-09",
          "abstract": "The goal of building intelligent dialogue systems has largely been separately\npursued under two paradigms: task-oriented dialogue (TOD) systems, which\nperform goal-oriented functions, and open-domain dialogue (ODD) systems, which\nfocus on non-goal-oriented chitchat. The two dialogue modes can potentially be\nintertwined together seamlessly in the same conversation, as easily done by a\nfriendly human assistant. Such ability is desirable in conversational agents,\nas the integration makes them more accessible and useful. Our paper addresses\nthis problem of fusing TODs and ODDs in multi-turn dialogues. Based on the\npopular TOD dataset MultiWOZ, we build a new dataset FusedChat, by rewriting\nthe existing TOD turns and adding new ODD turns. This procedure constructs\nconversation sessions containing exchanges from both dialogue modes. It\nfeatures inter-mode contextual dependency, i.e., the dialogue turns from the\ntwo modes depend on each other. Rich dependency patterns including co-reference\nand ellipsis are features. The new dataset, with 60k new human-written ODD\nturns and 5k re-written TOD turns, offers a benchmark to test a dialogue\nmodel's ability to perform inter-mode conversations. This is a more challenging\ntask since the model has to determine the appropriate dialogue mode and\ngenerate the response based on the inter-mode context. But such models would\nbetter mimic human-level conversation capabilities. We evaluate baseline models\non this task, including classification-based two-stage models and two-in-one\nfused models. We publicly release FusedChat and the baselines to propel future\nwork on inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.",
          "arxiv_id": "2109.04137v3"
        },
        {
          "title": "Alexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems",
          "year": "2021-04",
          "abstract": "Traditional goal-oriented dialogue systems rely on various components such as\nnatural language understanding, dialogue state tracking, policy learning and\nresponse generation. Training each component requires annotations which are\nhard to obtain for every new domain, limiting scalability of such systems.\nSimilarly, rule-based dialogue systems require extensive writing and\nmaintenance of rules and do not scale either. End-to-End dialogue systems, on\nthe other hand, do not require module-specific annotations but need a large\namount of data for training. To overcome these problems, in this demo, we\npresent Alexa Conversations, a new approach for building goal-oriented dialogue\nsystems that is scalable, extensible as well as data efficient. The components\nof this system are trained in a data-driven manner, but instead of collecting\nannotated conversations for training, we generate them using a novel dialogue\nsimulator based on a few seed dialogues and specifications of APIs and entities\nprovided by the developer. Our approach provides out-of-the-box support for\nnatural conversational phenomena like entity sharing across turns or users\nchanging their mind during conversation without requiring developers to provide\nany such dialogue flows. We exemplify our approach using a simple pizza\nordering task and showcase its value in reducing the developer burden for\ncreating a robust experience. Finally, we evaluate our system using a typical\nmovie ticket booking task and show that the dialogue simulator is an essential\ncomponent of the system that leads to over $50\\%$ improvement in turn-level\naction signature prediction accuracy.",
          "arxiv_id": "2104.09088v1"
        },
        {
          "title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking",
          "year": "2022-08",
          "abstract": "As an essential component in task-oriented dialogue systems, dialogue state\ntracking (DST) aims to track human-machine interactions and generate state\nrepresentations for managing the dialogue. Representations of dialogue states\nare dependent on the domain ontology and the user's goals. In several\ntask-oriented dialogues with a limited scope of objectives, dialogue states can\nbe represented as a set of slot-value pairs. As the capabilities of dialogue\nsystems expand to support increasing naturalness in communication,\nincorporating dialogue act processing into dialogue model design becomes\nessential. The lack of such consideration limits the scalability of dialogue\nstate tracking models for dialogues having specific objectives and ontology. To\naddress this issue, we formulate and incorporate dialogue acts, and leverage\nrecent advances in machine reading comprehension to predict both categorical\nand non-categorical types of slots for multi-domain dialogue state tracking.\nExperimental results show that our models can improve the overall accuracy of\ndialogue state tracking on the MultiWOZ 2.1 dataset, and demonstrate that\nincorporating dialogue acts can guide dialogue state design for future\ntask-oriented dialogue systems.",
          "arxiv_id": "2208.02462v1"
        }
      ],
      "7": [
        {
          "title": "Neural Machine Translation for Extremely Low-Resource African Languages: A Case Study on Bambara",
          "year": "2020-11",
          "abstract": "Low-resource languages present unique challenges to (neural) machine\ntranslation. We discuss the case of Bambara, a Mande language for which\ntraining data is scarce and requires significant amounts of pre-processing.\nMore than the linguistic situation of Bambara itself, the socio-cultural\ncontext within which Bambara speakers live poses challenges for automated\nprocessing of this language. In this paper, we present the first parallel data\nset for machine translation of Bambara into and from English and French and the\nfirst benchmark results on machine translation to and from Bambara. We discuss\nchallenges in working with low-resource languages and propose strategies to\ncope with data scarcity in low-resource machine translation (MT).",
          "arxiv_id": "2011.05284v1"
        },
        {
          "title": "Exploiting Language Relatedness in Machine Translation Through Domain Adaptation Techniques",
          "year": "2023-03",
          "abstract": "One of the significant challenges of Machine Translation (MT) is the scarcity\nof large amounts of data, mainly parallel sentence aligned corpora. If the\nevaluation is as rigorous as resource-rich languages, both Neural Machine\nTranslation (NMT) and Statistical Machine Translation (SMT) can produce good\nresults with such large amounts of data. However, it is challenging to improve\nthe quality of MT output for low resource languages, especially in NMT and SMT.\nIn order to tackle the challenges faced by MT, we present a novel approach of\nusing a scaled similarity score of sentences, especially for related languages\nbased on a 5-gram KenLM language model with Kneser-ney smoothing technique for\nfiltering in-domain data from out-of-domain corpora that boost the translation\nquality of MT. Furthermore, we employ other domain adaptation techniques such\nas multi-domain, fine-tuning and iterative back-translation approach to compare\nour novel approach on the Hindi-Nepali language pair for NMT and SMT. Our\napproach succeeds in increasing ~2 BLEU point on multi-domain approach, ~3 BLEU\npoint on fine-tuning for NMT and ~2 BLEU point on iterative back-translation\napproach.",
          "arxiv_id": "2303.01793v1"
        },
        {
          "title": "Cross-lingual Supervision Improves Unsupervised Neural Machine Translation",
          "year": "2020-04",
          "abstract": "Neural machine translation~(NMT) is ineffective for zero-resource languages.\nRecent works exploring the possibility of unsupervised neural machine\ntranslation (UNMT) with only monolingual data can achieve promising results.\nHowever, there are still big gaps between UNMT and NMT with parallel\nsupervision. In this work, we introduce a multilingual unsupervised NMT\n(\\method) framework to leverage weakly supervised signals from high-resource\nlanguage pairs to zero-resource translation directions. More specifically, for\nunsupervised language pairs \\texttt{En-De}, we can make full use of the\ninformation from parallel dataset \\texttt{En-Fr} to jointly train the\nunsupervised translation directions all in one model. \\method is based on\nmultilingual models which require no changes to the standard unsupervised NMT.\nEmpirical results demonstrate that \\method significantly improves the\ntranslation quality by more than 3 BLEU score on six benchmark unsupervised\ntranslation directions.",
          "arxiv_id": "2004.03137v3"
        }
      ],
      "8": [
        {
          "title": "Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation",
          "year": "2023-11",
          "abstract": "Large Language Models (LLMs) can generate biased and toxic responses. Yet\nmost prior work on LLM gender bias evaluation requires predefined\ngender-related phrases or gender stereotypes, which are challenging to be\ncomprehensively collected and are limited to explicit bias evaluation. In\naddition, we believe that instances devoid of gender-related language or\nexplicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in\nthis work, we propose a conditional text generation mechanism without the need\nfor predefined gender phrases and stereotypes. This approach employs three\ntypes of inputs generated through three distinct strategies to probe LLMs,\naiming to show evidence of explicit and implicit gender biases in LLMs. We also\nutilize explicit and implicit evaluation metrics to evaluate gender bias in\nLLMs under different strategies. Our experiments demonstrate that an increased\nmodel size does not consistently lead to enhanced fairness and all tested LLMs\nexhibit explicit and/or implicit gender bias, even when explicit gender\nstereotypes are absent in the inputs.",
          "arxiv_id": "2311.00306v1"
        },
        {
          "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement",
          "year": "2025-06",
          "abstract": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.",
          "arxiv_id": "2506.22372v1"
        },
        {
          "title": "Gender Bias in Large Language Models across Multiple Languages",
          "year": "2024-03",
          "abstract": "With the growing deployment of large language models (LLMs) across various\napplications, assessing the influence of gender biases embedded in LLMs becomes\ncrucial. The topic of gender bias within the realm of natural language\nprocessing (NLP) has gained considerable focus, particularly in the context of\nEnglish. Nonetheless, the investigation of gender bias in languages other than\nEnglish is still relatively under-explored and insufficiently analyzed. In this\nwork, We examine gender bias in LLMs-generated outputs for different languages.\nWe use three measurements: 1) gender bias in selecting descriptive words given\nthe gender-related context. 2) gender bias in selecting gender-related pronouns\n(she/he) given the descriptive words. 3) gender bias in the topics of\nLLM-generated dialogues. We investigate the outputs of the GPT series of LLMs\nin various languages using our three measurement methods. Our findings revealed\nsignificant gender biases across all the languages we examined.",
          "arxiv_id": "2403.00277v1"
        }
      ],
      "9": [
        {
          "title": "Unsupervised and Few-shot Parsing from Pretrained Language Models",
          "year": "2022-06",
          "abstract": "Pretrained language models are generally acknowledged to be able to encode\nsyntax [Tenney et al., 2019, Jawahar et al., 2019, Hewitt and Manning, 2019].\nIn this article, we propose UPOA, an Unsupervised constituent Parsing model\nthat calculates an Out Association score solely based on the self-attention\nweight matrix learned in a pretrained language model as the syntactic distance\nfor span segmentation. We further propose an enhanced version, UPIO, which\nexploits both inside association and outside association scores for estimating\nthe likelihood of a span. Experiments with UPOA and UPIO disclose that the\nlinear projection matrices for the query and key in the self-attention\nmechanism play an important role in parsing. We therefore extend the\nunsupervised models to few-shot parsing models (FPOA, FPIO) that use a few\nannotated trees to learn better linear projection matrices for parsing.\nExperiments on the Penn Treebank demonstrate that our unsupervised parsing\nmodel UPIO achieves results comparable to the state of the art on short\nsentences (length <= 10). Our few-shot parsing model FPIO trained with only 20\nannotated trees outperforms a previous few-shot parsing method trained with 50\nannotated trees. Experiments on cross-lingual parsing show that both\nunsupervised and few-shot parsing methods are better than previous methods on\nmost languages of SPMRL [Seddah et al., 2013].",
          "arxiv_id": "2206.04980v1"
        },
        {
          "title": "Multipath parsing in the brain",
          "year": "2024-01",
          "abstract": "Humans understand sentences word-by-word, in the order that they hear them.\nThis incrementality entails resolving temporary ambiguities about syntactic\nrelationships. We investigate how humans process these syntactic ambiguities by\ncorrelating predictions from incremental generative dependency parsers with\ntimecourse data from people undergoing functional neuroimaging while listening\nto an audiobook. In particular, we compare competing hypotheses regarding the\nnumber of developing syntactic analyses in play during word-by-word\ncomprehension: one vs more than one. This comparison involves evaluating\nsyntactic surprisal from a state-of-the-art dependency parser with LLM-adapted\nencodings against an existing fMRI dataset. In both English and Chinese data,\nwe find evidence for multipath parsing. Brain regions associated with this\nmultipath effect include bilateral superior temporal gyrus.",
          "arxiv_id": "2401.18046v2"
        },
        {
          "title": "Compositional Generalization in Dependency Parsing",
          "year": "2021-10",
          "abstract": "Compositionality -- the ability to combine familiar units like words into\nnovel phrases and sentences -- has been the focus of intense interest in\nartificial intelligence in recent years. To test compositional generalization\nin semantic parsing, Keysers et al. (2020) introduced Compositional Freebase\nQueries (CFQ). This dataset maximizes the similarity between the test and train\ndistributions over primitive units, like words, while maximizing the compound\ndivergence: the dissimilarity between test and train distributions over larger\nstructures, like phrases. Dependency parsing, however, lacks a compositional\ngeneralization benchmark. In this work, we introduce a gold-standard set of\ndependency parses for CFQ, and use this to analyze the behavior of a\nstate-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We\nfind that increasing compound divergence degrades dependency parsing\nperformance, although not as dramatically as semantic parsing performance.\nAdditionally, we find the performance of the dependency parser does not\nuniformly degrade relative to compound divergence, and the parser performs\ndifferently on different splits with the same compound divergence. We explore a\nnumber of hypotheses for what causes the non-uniform degradation in dependency\nparsing performance, and identify a number of syntactic structures that drive\nthe dependency parser's lower performance on the most challenging splits.",
          "arxiv_id": "2110.06843v2"
        }
      ],
      "10": [
        {
          "title": "Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees",
          "year": "2022-09",
          "abstract": "Current abstractive summarization models either suffer from a lack of clear\ninterpretability or provide incomplete rationales by only highlighting parts of\nthe source document. To this end, we propose the Summarization Program (SP), an\ninterpretable modular framework consisting of an (ordered) list of binary\ntrees, each encoding the step-by-step generative process of an abstractive\nsummary sentence from the source document. A Summarization Program contains one\nroot node per summary sentence, and a distinct tree connects each summary\nsentence (root node) to the document sentences (leaf nodes) from which it is\nderived, with the connecting nodes containing intermediate generated sentences.\nEdges represent different modular operations involved in summarization such as\nsentence fusion, compression, and paraphrasing. We first propose an efficient\nbest-first search method over neural modules, SP-Search that identifies SPs for\nhuman summaries by directly optimizing for ROUGE scores. Next, using these\nprograms as automatic supervision, we propose seq2seq models that generate\nSummarization Programs, which are then executed to obtain final summaries. We\ndemonstrate that SP-Search effectively represents the generative process behind\nhuman summaries using modules that are typically faithful to their intended\nbehavior. We also conduct a simulation study to show that Summarization\nPrograms improve the interpretability of summarization models by allowing\nhumans to better simulate model reasoning. Summarization Programs constitute a\npromising step toward interpretable and modular abstractive summarization, a\ncomplex task previously addressed primarily through blackbox end-to-end neural\nsystems. Supporting code available at\nhttps://github.com/swarnaHub/SummarizationPrograms",
          "arxiv_id": "2209.10492v2"
        },
        {
          "title": "Automated News Summarization Using Transformers",
          "year": "2021-04",
          "abstract": "The amount of text data available online is increasing at a very fast pace\nhence text summarization has become essential. Most of the modern recommender\nand text classification systems require going through a huge amount of data.\nManually generating precise and fluent summaries of lengthy articles is a very\ntiresome and time-consuming task. Hence generating automated summaries for the\ndata and using it to train machine learning models will make these models space\nand time-efficient. Extractive summarization and abstractive summarization are\ntwo separate methods of generating summaries. The extractive technique\nidentifies the relevant sentences from the original document and extracts only\nthose from the text. Whereas in abstractive summarization techniques, the\nsummary is generated after interpreting the original text, hence making it more\ncomplicated. In this paper, we will be presenting a comprehensive comparison of\na few transformer architecture based pre-trained models for text summarization.\nFor analysis and comparison, we have used the BBC news dataset that contains\ntext data that can be used for summarization and human generated summaries for\nevaluating and comparing the summaries generated by machine learning models.",
          "arxiv_id": "2108.01064v1"
        },
        {
          "title": "MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization",
          "year": "2021-09",
          "abstract": "One of the most challenging aspects of current single-document news\nsummarization is that the summary often contains 'extrinsic hallucinations',\ni.e., facts that are not present in the source document, which are often\nderived via world knowledge. This causes summarization systems to act more like\nopen-ended language models tending to hallucinate facts that are erroneous. In\nthis paper, we mitigate this problem with the help of multiple supplementary\nresource documents assisting the task. We present a new dataset MiRANews and\nbenchmark existing summarization models. In contrast to multi-document\nsummarization, which addresses multiple events from several source documents,\nwe still aim at generating a summary for a single document. We show via data\nanalysis that it's not only the models which are to blame: more than 27% of\nfacts mentioned in the gold summaries of MiRANews are better grounded on\nassisting documents than in the main source articles. An error analysis of\ngenerated summaries from pretrained models fine-tuned on MiRANews reveals that\nthis has an even bigger effects on models: assisted summarization reduces 55%\nof hallucinations when compared to single-document summarization models trained\non the main article only. Our code and data are available at\nhttps://github.com/XinnuoXu/MiRANews.",
          "arxiv_id": "2109.10650v1"
        }
      ],
      "11": [
        {
          "title": "Learning Efficient Task-Specific Meta-Embeddings with Word Prisms",
          "year": "2020-11",
          "abstract": "Word embeddings are trained to predict word cooccurrence statistics, which\nleads them to possess different lexical properties (syntactic, semantic, etc.)\ndepending on the notion of context defined at training time. These properties\nmanifest when querying the embedding space for the most similar vectors, and\nwhen used at the input layer of deep neural networks trained to solve\ndownstream NLP problems. Meta-embeddings combine multiple sets of differently\ntrained word embeddings, and have been shown to successfully improve intrinsic\nand extrinsic performance over equivalent models which use just one set of\nsource embeddings. We introduce word prisms: a simple and efficient\nmeta-embedding method that learns to combine source embeddings according to the\ntask at hand. Word prisms learn orthogonal transformations to linearly combine\nthe input source embeddings, which allows them to be very efficient at\ninference time. We evaluate word prisms in comparison to other meta-embedding\nmethods on six extrinsic evaluations and observe that word prisms offer\nimprovements in performance on all tasks.",
          "arxiv_id": "2011.02944v1"
        },
        {
          "title": "EDS-MEMBED: Multi-sense embeddings based on enhanced distributional semantic structures via a graph walk over word senses",
          "year": "2021-02",
          "abstract": "Several language applications often require word semantics as a core part of\ntheir processing pipeline, either as precise meaning inference or semantic\nsimilarity. Multi-sense embeddings (M-SE) can be exploited for this important\nrequirement. M-SE seeks to represent each word by their distinct senses in\norder to resolve the conflation of meanings of words as used in different\ncontexts. Previous works usually approach this task by training a model on a\nlarge corpus and often ignore the effect and usefulness of the semantic\nrelations offered by lexical resources. However, even with large training data,\ncoverage of all possible word senses is still an issue. In addition, a\nconsiderable percentage of contextual semantic knowledge are never learned\nbecause a huge amount of possible distributional semantic structures are never\nexplored. In this paper, we leverage the rich semantic structures in WordNet\nusing a graph-theoretic walk technique over word senses to enhance the quality\nof multi-sense embeddings. This algorithm composes enriched texts from the\noriginal texts. Furthermore, we derive new distributional semantic similarity\nmeasures for M-SE from prior ones. We adapt these measures to word sense\ndisambiguation (WSD) aspect of our experiment. We report evaluation results on\n11 benchmark datasets involving WSD and Word Similarity tasks and show that our\nmethod for enhancing distributional semantic structures improves embeddings\nquality on the baselines. Despite the small training data, it achieves\nstate-of-the-art performance on some of the datasets.",
          "arxiv_id": "2103.00232v1"
        },
        {
          "title": "Towards a Theoretical Understanding of Word and Relation Representation",
          "year": "2022-02",
          "abstract": "Representing words by vectors, or embeddings, enables computational reasoning\nand is foundational to automating natural language tasks. For example, if word\nembeddings of similar words contain similar values, word similarity can be\nreadily assessed, whereas judging that from their spelling is often impossible\n(e.g. cat /feline) and to predetermine and store similarities between all words\nis prohibitively time-consuming, memory intensive and subjective. We focus on\nword embeddings learned from text corpora and knowledge graphs. Several\nwell-known algorithms learn word embeddings from text on an unsupervised basis\nby learning to predict those words that occur around each word, e.g. word2vec\nand GloVe. Parameters of such word embeddings are known to reflect word\nco-occurrence statistics, but how they capture semantic meaning has been\nunclear. Knowledge graph representation models learn representations both of\nentities (words, people, places, etc.) and relations between them, typically by\ntraining a model to predict known facts in a supervised manner. Despite steady\nimprovements in fact prediction accuracy, little is understood of the latent\nstructure that enables this.\n  The limited understanding of how latent semantic structure is encoded in the\ngeometry of word embeddings and knowledge graph representations makes a\nprincipled means of improving their performance, reliability or\ninterpretability unclear. To address this:\n  1. we theoretically justify the empirical observation that particular\ngeometric relationships between word embeddings learned by algorithms such as\nword2vec and GloVe correspond to semantic relations between words; and\n  2. we extend this correspondence between semantics and geometry to the\nentities and relations of knowledge graphs, providing a model for the latent\nstructure of knowledge graph representation linked to that of word embeddings.",
          "arxiv_id": "2202.00486v1"
        }
      ],
      "12": [
        {
          "title": "Hate Speech Detection via Dual Contrastive Learning",
          "year": "2023-07",
          "abstract": "The fast spread of hate speech on social media impacts the Internet\nenvironment and our society by increasing prejudice and hurting people.\nDetecting hate speech has aroused broad attention in the field of natural\nlanguage processing. Although hate speech detection has been addressed in\nrecent work, this task still faces two inherent unsolved challenges. The first\nchallenge lies in the complex semantic information conveyed in hate speech,\nparticularly the interference of insulting words in hate speech detection. The\nsecond challenge is the imbalanced distribution of hate speech and non-hate\nspeech, which may significantly deteriorate the performance of models. To\ntackle these challenges, we propose a novel dual contrastive learning (DCL)\nframework for hate speech detection. Our framework jointly optimizes the\nself-supervised and the supervised contrastive learning loss for capturing\nspan-level information beyond the token-level emotional semantics used in\nexisting models, particularly detecting speech containing abusive and insulting\nwords. Moreover, we integrate the focal loss into the dual contrastive learning\nframework to alleviate the problem of data imbalance. We conduct experiments on\ntwo publicly available English datasets, and experimental results show that the\nproposed model outperforms the state-of-the-art models and precisely detects\nhate speeches.",
          "arxiv_id": "2307.05578v1"
        },
        {
          "title": "ProvocationProbe: Instigating Hate Speech Dataset from Twitter",
          "year": "2024-10",
          "abstract": "In the recent years online social media platforms has been flooded with\nhateful remarks such as racism, sexism, homophobia etc. As a result, there have\nbeen many measures taken by various social media platforms to mitigate the\nspread of hate-speech over the internet. One particular concept within the\ndomain of hate speech is instigating hate, which involves provoking hatred\nagainst a particular community, race, colour, gender, religion or ethnicity. In\nthis work, we introduce \\textit{ProvocationProbe} - a dataset designed to\nexplore what distinguishes instigating hate speech from general hate speech.\nFor this study, we collected around twenty thousand tweets from Twitter,\nencompassing a total of nine global controversies. These controversies span\nvarious themes including racism, politics, and religion. In this paper, i) we\npresent an annotated dataset after comprehensive examination of all the\ncontroversies, ii) we also highlight the difference between hate speech and\ninstigating hate speech by identifying distinguishing features, such as\ntargeted identity attacks and reasons for hate.",
          "arxiv_id": "2410.19687v1"
        },
        {
          "title": "An Investigation of Large Language Models for Real-World Hate Speech Detection",
          "year": "2024-01",
          "abstract": "Hate speech has emerged as a major problem plaguing our social spaces today.\nWhile there have been significant efforts to address this problem, existing\nmethods are still significantly limited in effectively detecting hate speech\nonline. A major limitation of existing methods is that hate speech detection is\na highly contextual problem, and these methods cannot fully capture the context\nof hate speech to make accurate predictions. Recently, large language models\n(LLMs) have demonstrated state-of-the-art performance in several natural\nlanguage tasks. LLMs have undergone extensive training using vast amounts of\nnatural language data, enabling them to grasp intricate contextual details.\nHence, they could be used as knowledge bases for context-aware hate speech\ndetection. However, a fundamental problem with using LLMs to detect hate speech\nis that there are no studies on effectively prompting LLMs for context-aware\nhate speech detection. In this study, we conduct a large-scale study of hate\nspeech detection, employing five established hate speech datasets. We discover\nthat LLMs not only match but often surpass the performance of current benchmark\nmachine learning models in identifying hate speech. By proposing four diverse\nprompting strategies that optimize the use of LLMs in detecting hate speech.\nOur study reveals that a meticulously crafted reasoning prompt can effectively\ncapture the context of hate speech by fully utilizing the knowledge base in\nLLMs, significantly outperforming existing techniques. Furthermore, although\nLLMs can provide a rich knowledge base for the contextual detection of hate\nspeech, suitable prompting strategies play a crucial role in effectively\nleveraging this knowledge base for efficient detection.",
          "arxiv_id": "2401.03346v1"
        }
      ],
      "13": [
        {
          "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
          "year": "2020-09",
          "abstract": "Pre-trained models for programming language have achieved dramatic empirical\nimprovements on a variety of code-related tasks such as code search, code\ncompletion, code summarization, etc. However, existing pre-trained models\nregard a code snippet as a sequence of tokens, while ignoring the inherent\nstructure of code, which provides crucial code semantics and would enhance the\ncode understanding process. We present GraphCodeBERT, a pre-trained model for\nprogramming language that considers the inherent structure of code. Instead of\ntaking syntactic-level structure of code like abstract syntax tree (AST), we\nuse data flow in the pre-training stage, which is a semantic-level structure of\ncode that encodes the relation of \"where-the-value-comes-from\" between\nvariables. Such a semantic-level structure is neat and does not bring an\nunnecessarily deep hierarchy of AST, the property of which makes the model more\nefficient. We develop GraphCodeBERT based on Transformer. In addition to using\nthe task of masked language modeling, we introduce two structure-aware\npre-training tasks. One is to predict code structure edges, and the other is to\nalign representations between source code and code structure. We implement the\nmodel in an efficient way with a graph-guided masked attention function to\nincorporate the code structure. We evaluate our model on four tasks, including\ncode search, clone detection, code translation, and code refinement. Results\nshow that code structure and newly introduced pre-training tasks can improve\nGraphCodeBERT and achieves state-of-the-art performance on the four downstream\ntasks. We further show that the model prefers structure-level attentions over\ntoken-level attentions in the task of code search.",
          "arxiv_id": "2009.08366v4"
        },
        {
          "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding",
          "year": "2025-05",
          "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.",
          "arxiv_id": "2505.07768v1"
        },
        {
          "title": "Benchmarking LLM Code Generation for Audio Programming with Visual Dataflow Languages",
          "year": "2024-09",
          "abstract": "Node-based programming languages are increasingly popular in media arts\ncoding domains. These languages are designed to be accessible to users with\nlimited coding experience, allowing them to achieve creative output without an\nextensive programming background. Using LLM-based code generation to further\nlower the barrier to creative output is an exciting opportunity. However, the\nbest strategy for code generation for visual node-based programming languages\nis still an open question. In particular, such languages have multiple levels\nof representation in text, each of which may be used for code generation. In\nthis work, we explore the performance of LLM code generation in audio\nprogramming tasks in visual programming languages at multiple levels of\nrepresentation. We explore code generation through metaprogramming code\nrepresentations for these languages (i.e., coding the language using a\ndifferent high-level text-based programming language), as well as through\ndirect node generation with JSON. We evaluate code generated in this way for\ntwo visual languages for audio programming on a benchmark set of coding\nproblems. We measure both correctness and complexity of the generated code. We\nfind that metaprogramming results in more semantically correct generated code,\ngiven that the code is well-formed (i.e., is syntactically correct and runs).\nWe also find that prompting for richer metaprogramming using randomness and\nloops led to more complex code.",
          "arxiv_id": "2409.00856v1"
        }
      ],
      "14": [
        {
          "title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations",
          "year": "2023-05",
          "abstract": "Large language models (LLMs) have exhibited striking in-context learning\n(ICL) ability to adapt to target tasks with a few input-output demonstrations.\nFor better ICL, different methods are proposed to select representative\ndemonstrations from existing training corpora. However, such settings are not\naligned with real-world practices, as end-users usually query LMs without\naccess to demonstration pools. In this work, we introduce Self-ICL -- a simple\nframework which bootstraps LMs' intrinsic capabilities to perform zero-shot\nICL. Given a test input, Self-ICL first prompts the model to generate\npseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via\nzero-shot prompting. Finally, we perform ICL for the test input with the\npseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard\ntasks shows Self-ICL outperforms zero-shot baselines on both average accuracy\nand head-to-head comparison. Moreover, with zero-shot chain-of-thought,\nSelf-ICL achieves results comparable to using real demonstrations.\nAdditionally, we conduct a range of analyses to validate Self-ICL's\neffectiveness and provide insights for its behaviors under different settings.",
          "arxiv_id": "2305.15035v2"
        },
        {
          "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
          "year": "2023-07",
          "abstract": "The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.",
          "arxiv_id": "2307.12375v4"
        },
        {
          "title": "Many-Shot In-Context Learning",
          "year": "2024-04",
          "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. We also find that inference cost increases\nlinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL\nto varying degrees. Our analysis also reveals the limitations of next-token\nprediction loss as an indicator of downstream ICL performance.",
          "arxiv_id": "2404.11018v3"
        }
      ],
      "15": [
        {
          "title": "Sentiment analysis and opinion mining on educational data: A survey",
          "year": "2023-02",
          "abstract": "Sentiment analysis AKA opinion mining is one of the most widely used NLP\napplications to identify human intentions from their reviews. In the education\nsector, opinion mining is used to listen to student opinions and enhance their\nlearning-teaching practices pedagogically. With advancements in sentiment\nannotation techniques and AI methodologies, student comments can be labelled\nwith their sentiment orientation without much human intervention. In this\nreview article, (1) we consider the role of emotional analysis in education\nfrom four levels: document level, sentence level, entity level, and aspect\nlevel, (2) sentiment annotation techniques including lexicon-based and\ncorpus-based approaches for unsupervised annotations are explored, (3) the role\nof AI in sentiment analysis with methodologies like machine learning, deep\nlearning, and transformers are discussed, (4) the impact of sentiment analysis\non educational procedures to enhance pedagogy, decision-making, and evaluation\nare presented. Educational institutions have been widely invested to build\nsentiment analysis tools and process their student feedback to draw their\nopinions and insights. Applications built on sentiment analysis of student\nfeedback are reviewed in this study. Challenges in sentiment analysis like\nmulti-polarity, polysemous, negation words, and opinion spam detection are\nexplored and their trends in the research space are discussed. The future\ndirections of sentiment analysis in education are discussed.",
          "arxiv_id": "2302.04359v1"
        },
        {
          "title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis",
          "year": "2020-05",
          "abstract": "Recently, sentiment analysis has seen remarkable advance with the help of\npre-training approaches. However, sentiment knowledge, such as sentiment words\nand aspect-sentiment pairs, is ignored in the process of pre-training, despite\nthe fact that they are widely used in traditional sentiment analysis\napproaches. In this paper, we introduce Sentiment Knowledge Enhanced\nPre-training (SKEP) in order to learn a unified sentiment representation for\nmultiple sentiment analysis tasks. With the help of automatically-mined\nknowledge, SKEP conducts sentiment masking and constructs three sentiment\nknowledge prediction objectives, so as to embed sentiment information at the\nword, polarity and aspect level into pre-trained sentiment representation. In\nparticular, the prediction of aspect-sentiment pairs is converted into\nmulti-label classification, aiming to capture the dependency between words in a\npair. Experiments on three kinds of sentiment tasks show that SKEP\nsignificantly outperforms strong pre-training baseline, and achieves new\nstate-of-the-art results on most of the test datasets. We release our code at\nhttps://github.com/baidu/Senta.",
          "arxiv_id": "2005.05635v2"
        },
        {
          "title": "Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa",
          "year": "2022-07",
          "abstract": "Text sentiment analysis, also known as opinion mining, is research on the\ncalculation of people's views, evaluations, attitude and emotions expressed by\nentities. Text sentiment analysis can be divided into text-level sentiment\nanalysis, sen-tence-level sentiment analysis and aspect-level sentiment\nanalysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the\nfield of sentiment analysis, which aims to predict the polarity of aspects. The\nresearch of pre-training neural model has significantly improved the\nperformance of many natural language processing tasks. In recent years, pre\ntraining model (PTM) has been applied in ABSA. Therefore, there has been a\nquestion, which is whether PTMs contain sufficient syntactic information for\nABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced\nBERT with disentangled attention) to solve Aspect-Based Sentiment Analysis\nproblem. DeBERTa is a kind of neural language model based on transformer, which\nuses self-supervised learning to pre-train on a large number of original text\ncorpora. Based on the Local Context Focus (LCF) mechanism, by integrating\nDeBERTa model, we purpose a multi-task learning model for aspect-based\nsentiment analysis. The experiments result on the most commonly used the laptop\nand restaurant datasets of SemEval-2014 and the ACL twitter dataset show that\nLCF mechanism with DeBERTa has significant improvement.",
          "arxiv_id": "2207.02424v2"
        }
      ],
      "16": [
        {
          "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
          "year": "2024-08",
          "abstract": "One of the grand challenges of artificial general intelligence is developing\nagents capable of conducting scientific research and discovering new knowledge.\nWhile frontier models have already been used as aides to human scientists, e.g.\nfor brainstorming ideas, writing code, or prediction tasks, they still conduct\nonly a small part of the scientific process. This paper presents the first\ncomprehensive framework for fully automatic scientific discovery, enabling\nfrontier large language models to perform research independently and\ncommunicate their findings. We introduce The AI Scientist, which generates\nnovel research ideas, writes code, executes experiments, visualizes results,\ndescribes its findings by writing a full scientific paper, and then runs a\nsimulated review process for evaluation. In principle, this process can be\nrepeated to iteratively develop ideas in an open-ended fashion, acting like the\nhuman scientific community. We demonstrate its versatility by applying it to\nthree distinct subfields of machine learning: diffusion modeling,\ntransformer-based language modeling, and learning dynamics. Each idea is\nimplemented and developed into a full paper at a cost of less than $15 per\npaper. To evaluate the generated papers, we design and validate an automated\nreviewer, which we show achieves near-human performance in evaluating paper\nscores. The AI Scientist can produce papers that exceed the acceptance\nthreshold at a top machine learning conference as judged by our automated\nreviewer. This approach signifies the beginning of a new era in scientific\ndiscovery in machine learning: bringing the transformative benefits of AI\nagents to the entire research process of AI itself, and taking us closer to a\nworld where endless affordable creativity and innovation can be unleashed on\nthe world's most challenging problems. Our code is open-sourced at\nhttps://github.com/SakanaAI/AI-Scientist",
          "arxiv_id": "2408.06292v3"
        },
        {
          "title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
          "year": "2024-04",
          "abstract": "The pace of scientific research, vital for improving human life, is complex,\nslow, and needs specialized expertise. Meanwhile, novel, impactful research\noften stems from both a deep understanding of prior work, and a\ncross-pollination of ideas across domains and fields. To enhance the\nproductivity of researchers, we propose ResearchAgent, which leverages the\nencyclopedic knowledge and linguistic reasoning capabilities of Large Language\nModels (LLMs) to assist them in their work. This system automatically defines\nnovel problems, proposes methods and designs experiments, while iteratively\nrefining them based on the feedback from collaborative LLM-powered reviewing\nagents. Specifically, starting with a core scientific paper, ResearchAgent is\naugmented not only with relevant publications by connecting information over an\nacademic graph but also entities retrieved from a knowledge store derived from\nshared underlying concepts mined across numerous papers. Then, mimicking a\nscientific approach to improving ideas with peer discussions, we leverage\nmultiple LLM-based ReviewingAgents that provide reviews and feedback via\niterative revision processes. These reviewing agents are instantiated with\nhuman preference-aligned LLMs whose criteria for evaluation are elicited from\nactual human judgments via LLM prompting. We experimentally validate our\nResearchAgent on scientific publications across multiple disciplines, showing\nits effectiveness in generating novel, clear, and valid ideas based on both\nhuman and model-based evaluation results. Our initial foray into AI-mediated\nscientific research has important implications for the development of future\nsystems aimed at supporting researchers in their ideation and\noperationalization of novel work.",
          "arxiv_id": "2404.07738v2"
        },
        {
          "title": "Detecting and analyzing missing citations to published scientific entities",
          "year": "2022-10",
          "abstract": "Proper citation is of great importance in academic writing for it enables\nknowledge accumulation and maintains academic integrity. However, citing\nproperly is not an easy task. For published scientific entities, the\never-growing academic publications and over-familiarity of terms easily lead to\nmissing citations. To deal with this situation, we design a special method\nCitation Recommendation for Published Scientific Entity (CRPSE) based on the\ncooccurrences between published scientific entities and in-text citations in\nthe same sentences from previous researchers. Experimental outcomes show the\neffectiveness of our method in recommending the source papers for published\nscientific entities. We further conduct a statistical analysis on missing\ncitations among papers published in prestigious computer science conferences in\n2020. In the 12,278 papers collected, 475 published scientific entities of\ncomputer science and mathematics are found to have missing citations. Many\nentities mentioned without citations are found to be well-accepted research\nresults. On a median basis, the papers proposing these published scientific\nentities with missing citations were published 8 years ago, which can be\nconsidered the time frame for a published scientific entity to develop into a\nwell-accepted concept. For published scientific entities, we appeal for\naccurate and full citation of their source papers as required by academic\nstandards.",
          "arxiv_id": "2210.10073v1"
        }
      ],
      "17": [
        {
          "title": "A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs",
          "year": "2024-06",
          "abstract": "Low-resource languages, by its very definition, tend to be under represented\nin the pre-training corpora of Large Language Models. In this work, we\ninvestigate three low-resource cross-lingual approaches that enable an LLM\nadapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic\nlanguages, among many other language families, contribute to less than\n$0.005\\%$ of the total $2$ trillion token pre-training corpora. In this work,\nwe experiment with the English-dominated Llama-2 for cross-lingual transfer to\nthree Indic languages, Bengali, Hindi, and Tamil as target languages. We study\nthree approaches for cross-lingual transfer, under ICL and fine-tuning. One, we\nfind that adding additional supervisory signals via a dominant language in the\nLLM, leads to improvements, both under in-context learning and fine-tuning.\nTwo, adapting the target languages to word reordering may be beneficial under\nICL, but its impact diminishes with fine tuning. Finally, continued\npre-training in one low-resource language can improve model performance for\nother related low-resource languages.",
          "arxiv_id": "2406.17377v1"
        },
        {
          "title": "Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training",
          "year": "2021-04",
          "abstract": "Pre-trained multilingual language encoders, such as multilingual BERT and\nXLM-R, show great potential for zero-shot cross-lingual transfer. However,\nthese multilingual encoders do not precisely align words and phrases across\nlanguages. Especially, learning alignments in the multilingual embedding space\nusually requires sentence-level or word-level parallel corpora, which are\nexpensive to be obtained for low-resource languages. An alternative is to make\nthe multilingual encoders more robust; when fine-tuning the encoder using\ndownstream task, we train the encoder to tolerate noise in the contextual\nembedding spaces such that even if the representations of different languages\nare not aligned well, the model can still achieve good performance on zero-shot\ncross-lingual transfer. In this work, we propose a learning strategy for\ntraining robust models by drawing connections between adversarial examples and\nthe failure cases of zero-shot cross-lingual transfer. We adopt two widely used\nrobust training methods, adversarial training and randomized smoothing, to\ntrain the desired robust model. The experimental results demonstrate that\nrobust training improves zero-shot cross-lingual transfer on text\nclassification tasks. The improvement is more significant in the generalized\ncross-lingual transfer setting, where the pair of input sentences belong to two\ndifferent languages.",
          "arxiv_id": "2104.08645v2"
        },
        {
          "title": "Transfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese",
          "year": "2023-04",
          "abstract": "Multilingual language models have pushed state-of-the-art in cross-lingual\nNLP transfer. The majority of zero-shot cross-lingual transfer, however, use\none and the same massively multilingual transformer (e.g., mBERT or XLM-R) to\ntransfer to all target languages, irrespective of their typological,\netymological, and phylogenetic relations to other languages. In particular,\nreadily available data and models of resource-rich sibling languages are often\nignored. In this work, we empirically show, in a case study for Faroese -- a\nlow-resource language from a high-resource language family -- that by\nleveraging the phylogenetic information and departing from the\n'one-size-fits-all' paradigm, one can improve cross-lingual transfer to\nlow-resource languages. In particular, we leverage abundant resources of other\nScandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for\nthe benefit of Faroese. Our evaluation results show that we can substantially\nimprove the transfer performance to Faroese by exploiting data and models of\nclosely-related high-resource languages. Further, we release a new web corpus\nof Faroese and Faroese datasets for named entity recognition (NER), semantic\ntext similarity (STS), and new language models trained on all Scandinavian\nlanguages.",
          "arxiv_id": "2304.08823v1"
        }
      ],
      "18": [
        {
          "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments",
          "year": "2024-06",
          "abstract": "Building generalist agents that can handle diverse tasks and evolve\nthemselves across different environments is a long-term goal in the AI\ncommunity. Large language models (LLMs) are considered a promising foundation\nto build such agents due to their generalized capabilities. Current approaches\neither have LLM-based agents imitate expert-provided trajectories step-by-step,\nrequiring human supervision, which is hard to scale and limits environmental\nexploration; or they let agents explore and learn in isolated environments,\nresulting in specialist agents with limited generalization. In this paper, we\ntake the first step towards building generally-capable LLM-based agents with\nself-evolution ability. We identify a trinity of ingredients: 1) diverse\nenvironments for agent exploration and learning, 2) a trajectory set to equip\nagents with basic capabilities and prior knowledge, and 3) an effective and\nscalable evolution method. We propose AgentGym, a new framework featuring a\nvariety of environments and tasks for broad, real-time, uni-format, and\nconcurrent agent exploration. AgentGym also includes a database with expanded\ninstructions, a benchmark suite, and high-quality trajectories across\nenvironments. Next, we propose a novel method, AgentEvol, to investigate the\npotential of agent self-evolution beyond previously seen data across tasks and\nenvironments. Experimental results show that the evolved agents can achieve\nresults comparable to SOTA models. We release the AgentGym suite, including the\nplatform, dataset, benchmark, checkpoints, and algorithm implementations. The\nAgentGym suite is available on https://github.com/WooooDyy/AgentGym.",
          "arxiv_id": "2406.04151v1"
        },
        {
          "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
          "year": "2023-09",
          "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.",
          "arxiv_id": "2309.07864v3"
        },
        {
          "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL",
          "year": "2025-08",
          "abstract": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
          "arxiv_id": "2508.13167v1"
        }
      ],
      "19": [
        {
          "title": "Harnessing the Plug-and-Play Controller by Prompting",
          "year": "2024-02",
          "abstract": "Controllable text generation is a growing field within natural language\ngeneration (NLG) that focuses on producing text that meets specific constraints\nin real-world applications. Previous approaches, such as plug-and-play\ncontrollers (PPCs), aimed to steer the properties of generated text in a\nflexible manner. However, these methods often compromised the integrity of the\nlanguage model's decoding process, resulting in less smooth text generation.\nAlternatively, other techniques utilized multiple attribute prompts to align\nthe generated text with desired attributes, but this approach required prompt\ndesign for each attribute and was dependent on the size of the language model.\nThis paper introduces a novel method for flexible attribute control in text\ngeneration using pre-trained language models (PLMs). The proposed approach aims\nto enhance the fluency of generated text by guiding the generation process with\nPPCs. The key idea is to dynamically adjust the distribution of generated text\nby modifying prompts, effectively constraining the output space of the language\nmodel and influencing the desired attribute. To enable smooth cooperation\nbetween the PLM and the PPC, our work innovatively proposes a new model\nfine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback\n(RLDAF).This fine-tuning process adapts a small subset of the language model's\nparameters based on the generating actions taken during the PPC control\nprocess. The resulting harmonious collaboration between the PLM and PPC leads\nto improved smoothness in text generation during inference. Extensive\nexperiments were conducted on the SST2 dataset, and the proposed method\noutperformed previous approaches in various evaluation metrics, including text\nfluency and attribute consistency.",
          "arxiv_id": "2402.04160v1"
        },
        {
          "title": "SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes",
          "year": "2022-12",
          "abstract": "Is it possible to train a general metric for evaluating text generation\nquality without human annotated ratings? Existing learned metrics either\nperform unsatisfactorily across text generation tasks or require human ratings\nfor training on specific tasks. In this paper, we propose SESCORE2, a\nself-supervised approach for training a model-based metric for text generation\nevaluation. The key concept is to synthesize realistic model mistakes by\nperturbing sentences retrieved from a corpus. The primary advantage of the\nSESCORE2 is its ease of extension to many other languages while providing\nreliable severity estimation. We evaluate SESCORE2 and previous methods on four\ntext generation tasks across three languages. SESCORE2 outperforms unsupervised\nmetric PRISM on four text generation evaluation benchmarks, with a Kendall\nimprovement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised\nBLEURT and COMET on multiple text generation tasks. The code and data are\navailable at https://github.com/xu1998hz/SEScore2.",
          "arxiv_id": "2212.09305v2"
        },
        {
          "title": "CoNT: Contrastive Neural Text Generation",
          "year": "2022-05",
          "abstract": "Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.",
          "arxiv_id": "2205.14690v4"
        }
      ],
      "20": [
        {
          "title": "Robust Preference Optimization through Reward Model Distillation",
          "year": "2024-05",
          "abstract": "Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, the empirical evidence suggests that DPO\ntypically assigns implicit rewards that overfit, and trend towards infinite\nmagnitude. This frequently leads to degenerate policies, sometimes causing even\nthe probabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and use distillation to get a better proxy for the true\npreference distribution over generation pairs: we train the LM such that its\ninduced implicit reward, i.e., the scaled log-likelihood ratio of the model to\nthe reference model, matches an explicit reward model trained on the preference\ndata. Moreover, to account for uncertainty in the reward model we are\ndistilling from, we optimize against a family of reward models that, as a\nwhole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.",
          "arxiv_id": "2405.19316v2"
        },
        {
          "title": "LRHP: Learning Representations for Human Preferences via Preference Pairs",
          "year": "2024-10",
          "abstract": "To improve human-preference alignment training, current research has\ndeveloped numerous preference datasets consisting of preference pairs labeled\nas \"preferred\" or \"dispreferred\". These preference pairs are typically used to\nencode human preferences into a single numerical value through reward modeling,\nwhich acts as a reward signal during reinforcement learning from human feedback\n(RLHF). However, representing these human preferences as a numerical value\ncomplicates the analysis of these preferences and restricts their broader\napplications other than RLHF. In contrast, in this work, we introduce a\npreference representation learning task that aims to construct a richer and\nmore structured representation of human preferences. We further develop a more\ngeneralizable framework, Learning Representations for Human Preferences via\npreference pairs (namely LRHP), which extends beyond traditional reward\nmodeling to tackle this task. We verify the utility of preference\nrepresentations in two downstream tasks: preference data selection and\npreference margin prediction. Building upon the human preferences in\nrepresentations, we achieve strong performance in both tasks, significantly\noutperforming baselines.",
          "arxiv_id": "2410.04503v1"
        },
        {
          "title": "Statistical Rejection Sampling Improves Preference Optimization",
          "year": "2023-09",
          "abstract": "Improving the alignment of language models with human preferences remains an\nactive research challenge. Previous approaches have primarily utilized\nReinforcement Learning from Human Feedback (RLHF) via online RL methods such as\nProximal Policy Optimization (PPO). Recently, offline methods such as Sequence\nLikelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have\nemerged as attractive alternatives, offering improvements in stability and\nscalability while maintaining competitive performance. SLiC refines its loss\nfunction using sequence pairs sampled from a supervised fine-tuned (SFT)\npolicy, while DPO directly optimizes language models based on preference data,\nforegoing the need for a separate reward model. However, the maximum likelihood\nestimator (MLE) of the target optimal policy requires labeled preference pairs\nsampled from that policy. DPO's lack of a reward model constrains its ability\nto sample preference pairs from the optimal policy, and SLiC is restricted to\nsampling preference pairs only from the SFT policy. To address these\nlimitations, we introduce a novel approach called Statistical Rejection\nSampling Optimization (RSO) that aims to source preference data from the target\noptimal policy using rejection sampling, enabling a more accurate estimation of\nthe optimal policy. We also propose a unified framework that enhances the loss\nfunctions used in both SLiC and DPO from a preference modeling standpoint.\nThrough extensive experiments across three diverse tasks, we demonstrate that\nRSO consistently outperforms both SLiC and DPO on evaluations from both Large\nLanguage Model (LLM) and human raters.",
          "arxiv_id": "2309.06657v2"
        }
      ],
      "21": [
        {
          "title": "Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis",
          "year": "2023-02",
          "abstract": "The COVID-19 pandemic has introduced new opportunities for health\ncommunication, including an increase in the public use of online outlets for\nhealth-related emotions. People have turned to social media networks to share\nsentiments related to the impacts of the COVID-19 pandemic. In this paper we\nexamine the role of social messaging shared by Persons in the Public Eye (i.e.\nathletes, politicians, news personnel) in determining overall public discourse\ndirection. We harvested approximately 13 million tweets ranging from 1 January\n2020 to 1 March 2022. The sentiment was calculated for each tweet using a\nfine-tuned DistilRoBERTa model, which was used to compare COVID-19\nvaccine-related Twitter posts (tweets) that co-occurred with mentions of People\nin the Public Eye. Our findings suggest the presence of consistent patterns of\nemotional content co-occurring with messaging shared by Persons in the Public\nEye for the first two years of the COVID-19 pandemic influenced public opinion\nand largely stimulated online public discourse. We demonstrate that as the\npandemic progressed, public sentiment shared on social networks was shaped by\nrisk perceptions, political ideologies and health-protective behaviours shared\nby Persons in the Public Eye, often in a negative light.",
          "arxiv_id": "2303.16759v1"
        },
        {
          "title": "COVID-19 and Big Data: Multi-faceted Analysis for Spatio-temporal Understanding of the Pandemic with Social Media Conversations",
          "year": "2021-04",
          "abstract": "COVID-19 has been devastating the world since the end of 2019 and has\ncontinued to play a significant role in major national and worldwide events,\nand consequently, the news. In its wake, it has left no life unaffected. Having\nearned the world's attention, social media platforms have served as a vehicle\nfor the global conversation about COVID-19. In particular, many people have\nused these sites in order to express their feelings, experiences, and\nobservations about the pandemic. We provide a multi-faceted analysis of\ncritical properties exhibited by these conversations on social media regarding\nthe novel coronavirus pandemic. We present a framework for analysis, mining,\nand tracking the critical content and characteristics of social media\nconversations around the pandemic. Focusing on Twitter and Reddit, we have\ngathered a large-scale dataset on COVID-19 social media conversations. Our\nanalyses cover tracking potential reports on virus acquisition, symptoms,\nconversation topics, and language complexity measures through time and by\nregion across the United States. We also present a BERT-based model for\nrecognizing instances of hateful tweets in COVID-19 conversations, which\nachieves a lower error-rate than the state-of-the-art performance. Our results\nprovide empirical validation for the effectiveness of our proposed framework\nand further demonstrate that social media data can be efficiently leveraged to\nprovide public health experts with inexpensive but thorough insight over the\ncourse of an outbreak.",
          "arxiv_id": "2104.10807v1"
        },
        {
          "title": "Large language models for newspaper sentiment analysis during COVID-19: The Guardian",
          "year": "2024-05",
          "abstract": "During the COVID-19 pandemic, the news media coverage encompassed a wide\nrange of topics that includes viral transmission, allocation of medical\nresources, and government response measures. There have been studies on\nsentiment analysis of social media platforms during COVID-19 to understand the\npublic response given the rise of cases and government strategies implemented\nto control the spread of the virus. Sentiment analysis can provide a better\nunderstanding of changes in societal opinions and emotional trends during the\npandemic. Apart from social media, newspapers have played a vital role in the\ndissemination of information, including information from the government,\nexperts, and also the public about various topics. A study of sentiment\nanalysis of newspaper sources during COVID-19 for selected countries can give\nan overview of how the media covered the pandemic. In this study, we select The\nGuardian newspaper and provide a sentiment analysis during various stages of\nCOVID-19 that includes initial transmission, lockdowns and vaccination. We\nemploy novel large language models (LLMs) and refine them with expert-labelled\nsentiment analysis data. We also provide an analysis of sentiments experienced\npre-pandemic for comparison. The results indicate that during the early\npandemic stages, public sentiment prioritised urgent crisis response, later\nshifting focus to addressing the impact on health and the economy. In\ncomparison with related studies about social media sentiment analyses, we found\na discrepancy between The Guardian with dominance of negative sentiments (sad,\nannoyed, anxious and denial), suggesting that social media offers a more\ndiversified emotional reflection. We found a grim narrative in The Guardian\nwith overall dominance of negative sentiments, pre and during COVID-19 across\nnews sections including Australia, UK, World News, and Opinion",
          "arxiv_id": "2405.13056v2"
        }
      ],
      "22": [
        {
          "title": "Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation",
          "year": "2025-01",
          "abstract": "Interpreting the law is always essential for the law to adapt to the\never-changing society. It is a critical and challenging task even for legal\npractitioners, as it requires meticulous and professional annotations and\nsummarizations by legal experts, which are admittedly time-consuming and\nexpensive to collect at scale. To alleviate the burden on legal experts, we\npropose a method for automated legal interpretation. Specifically, by emulating\ndoctrinal legal research, we introduce a novel framework, ATRIE, to address\nLegal Concept Interpretation, a typical task in legal interpretation. ATRIE\nutilizes large language models (LLMs) to AuTomatically Retrieve concept-related\ninformation, Interpret legal concepts, and Evaluate generated interpretations,\neliminating dependence on legal experts. ATRIE comprises a legal concept\ninterpreter and a legal concept interpretation evaluator. The interpreter uses\nLLMs to retrieve relevant information from previous cases and interpret legal\nconcepts. The evaluator uses performance changes on Legal Concept Entailment, a\ndownstream task we propose, as a proxy of interpretation quality. Automated and\nmultifaceted human evaluations indicate that the quality of our interpretations\nis comparable to those written by legal experts, with superior\ncomprehensiveness and readability. Although there remains a slight gap in\naccuracy, it can already assist legal practitioners in improving the efficiency\nof legal interpretation.",
          "arxiv_id": "2501.01743v3"
        },
        {
          "title": "LeKUBE: A Legal Knowledge Update BEnchmark",
          "year": "2024-07",
          "abstract": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
          "arxiv_id": "2407.14192v2"
        },
        {
          "title": "DeliLaw: A Chinese Legal Counselling System Based on a Large Language Model",
          "year": "2024-08",
          "abstract": "Traditional legal retrieval systems designed to retrieve legal documents,\nstatutes, precedents, and other legal information are unable to give\nsatisfactory answers due to lack of semantic understanding of specific\nquestions. Large Language Models (LLMs) have achieved excellent results in a\nvariety of natural language processing tasks, which inspired us that we train a\nLLM in the legal domain to help legal retrieval. However, in the Chinese legal\ndomain, due to the complexity of legal questions and the rigour of legal\narticles, there is no legal large model with satisfactory practical application\nyet. In this paper, we present DeliLaw, a Chinese legal counselling system\nbased on a large language model. DeliLaw integrates a legal retrieval module\nand a case retrieval module to overcome the model hallucination. Users can\nconsult professional legal questions, search for legal articles and relevant\njudgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,\nDeliLaw supports the use of English for counseling. we provide the address of\nthe system: https://data.delilegal.com/lawQuestion.",
          "arxiv_id": "2408.00357v1"
        }
      ],
      "23": [
        {
          "title": "KeyInst: Keyword Instruction for Improving SQL Formulation in Text-to-SQL",
          "year": "2024-10",
          "abstract": "Text-to-SQL parsing involves the translation of natural language queries\n(NLQs) into their corresponding SQL commands. A principal challenge within this\ndomain is the formulation of SQL queries that are not only syntactically\ncorrect but also semantically aligned with the natural language input. However,\nthe intrinsic disparity between the NLQ and the SQL poses a significant\nchallenge. In this research, we introduce Keyword Instruction (KeyInst), a\nnovel method designed to enhance SQL formulation by Large Language Models\n(LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to\nbe part of the final query, thus facilitates a smoother SQL query formulation\nprocess. We explore two strategies for integrating KeyInst into Text-to-SQL\nparsing: a pipeline strategy and a single-pass strategy. The former first\ngenerates KeyInst for question, which are then used to prompt LLMs. The latter\nemploys a fine-tuned model to concurrently generate KeyInst and SQL in one\nstep. We developed StrucQL, a benchmark specifically designed for the\nevaluation of SQL formulation. Extensive experiments on StrucQL and other\nbenchmarks demonstrate that KeyInst significantly improves upon the existing\nText-to-SQL prompting techniques.",
          "arxiv_id": "2411.00788v1"
        },
        {
          "title": "HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing",
          "year": "2022-03",
          "abstract": "Recently, context-dependent text-to-SQL semantic parsing which translates\nnatural language into SQL in an interaction process has attracted a lot of\nattention. Previous works leverage context-dependence information either from\ninteraction history utterances or the previous predicted SQL queries but fail\nin taking advantage of both since of the mismatch between natural language and\nlogic-form SQL. In this work, we propose a History Information Enhanced\ntext-to-SQL model (HIE-SQL) to exploit context-dependence information from both\nhistory utterances and the last predicted SQL query. In view of the mismatch,\nwe treat natural language and SQL as two modalities and propose a bimodal\npre-trained model to bridge the gap between them. Besides, we design a\nschema-linking graph to enhance connections from utterances and the SQL query\nto the database schema. We show our history information enhanced methods\nimprove the performance of HIE-SQL by a significant margin, which achieves new\nstate-of-the-art results on the two context-dependent text-to-SQL benchmarks,\nthe SparC and CoSQL datasets, at the writing time.",
          "arxiv_id": "2203.07376v2"
        },
        {
          "title": "Abacus-SQL: A Text-to-SQL System Empowering Cross-Domain and Open-Domain Database Retrieval",
          "year": "2025-04",
          "abstract": "The existing text-to-SQL systems have made significant progress in SQL query\ngeneration, but they still face numerous challenges. Existing systems often\nlack retrieval capabilities for open-domain databases, requiring users to\nmanually filter relevant databases. Additionally, their cross-domain\ntransferability is limited, making it challenging to accommodate diverse query\nrequirements. To address these issues, we propose Abacus-SQL. Abacus-SQL\nutilizes database retrieval technology to accurately locate the required\ndatabases in an open-domain database environment. It also enhances the system\ncross-domain transfer ability through data augmentation methods. Moreover,\nAbacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the\naccuracy of SQL queries. Experimental results demonstrate that Abacus-SQL\nperforms excellently in multi-turn text-to-SQL tasks, effectively validating\nthe approach's effectiveness. Abacus-SQL is publicly accessible at\nhttps://huozi.8wss.com/abacus-sql/.",
          "arxiv_id": "2504.09824v1"
        }
      ],
      "24": [
        {
          "title": "KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical Named Entity Recognition",
          "year": "2025-03",
          "abstract": "Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing (NLP) that plays a crucial role in information extraction, question\nanswering, and knowledge-based systems. Traditional deep learning-based NER\nmodels often struggle with domain-specific generalization and suffer from data\nsparsity issues. In this work, we introduce Knowledge Graph distilled for Named\nEntity Recognition (KoGNER), a novel approach that integrates Knowledge Graph\n(KG) distillation into NER models to enhance entity recognition performance.\nOur framework leverages structured knowledge representations from KGs to enrich\ncontextual embeddings, thereby improving entity classification and reducing\nambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge\nDistillation, where external knowledge sources are distilled into a lightweight\nrepresentation for seamless integration with NER models, and (2) Entity-Aware\nAugmentation, which integrates contextual embeddings that have been enriched\nwith knowledge graph information directly into GNN, thereby improving the\nmodel's ability to understand and represent entity relationships. Experimental\nresults on benchmark datasets demonstrate that KoGNER achieves state-of-the-art\nperformance, outperforming finetuned NER models and LLMs by a significant\nmargin. These findings suggest that leveraging knowledge graphs as auxiliary\ninformation can significantly improve NER accuracy, making KoGNER a promising\ndirection for future research in knowledge-aware NLP.",
          "arxiv_id": "2503.15737v1"
        },
        {
          "title": "E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text",
          "year": "2022-12",
          "abstract": "Identifying named entities such as a person, location or organization, in\ndocuments can highlight key information to readers. Training Named Entity\nRecognition (NER) models requires an annotated data set, which can be a\ntime-consuming labour-intensive task. Nevertheless, there are publicly\navailable NER data sets for general English. Recently there has been interest\nin developing NER for legal text. However, prior work and experimental results\nreported here indicate that there is a significant degradation in performance\nwhen NER methods trained on a general English data set are applied to legal\ntext. We describe a publicly available legal NER data set, called E-NER, based\non legal company filings available from the US Securities and Exchange\nCommission's EDGAR data set. Training a number of different NER algorithms on\nthe general English CoNLL-2003 corpus but testing on our test collection\nconfirmed significant degradations in accuracy, as measured by the F1-score, of\nbetween 29.4\\% and 60.4\\%, compared to training and testing on the E-NER\ncollection.",
          "arxiv_id": "2212.09306v1"
        },
        {
          "title": "NaijaNER : Comprehensive Named Entity Recognition for 5 Nigerian Languages",
          "year": "2021-03",
          "abstract": "Most of the common applications of Named Entity Recognition (NER) is on\nEnglish and other highly available languages. In this work, we present our\nfindings on Named Entity Recognition for 5 Nigerian Languages (Nigerian\nEnglish, Nigerian Pidgin English, Igbo, Yoruba and Hausa). These languages are\nconsidered low-resourced, and very little openly available Natural Language\nProcessing work has been done in most of them. In this work, individual NER\nmodels were trained and metrics recorded for each of the languages. We also\nworked on a combined model that can handle Named Entity Recognition (NER) for\nany of the five languages. The combined model works well for Named Entity\nRecognition(NER) on each of the languages and with better performance compared\nto individual NER models trained specifically on annotated data for the\nspecific language. The aim of this work is to share our learning on how\ninformation extraction using Named Entity Recognition can be optimized for the\nlisted Nigerian Languages for inclusion, ease of deployment in production and\nreusability of models. Models developed during this project are available on\nGitHub https://git.io/JY0kk and an interactive web app\nhttps://nigner.herokuapp.com/.",
          "arxiv_id": "2105.00810v1"
        }
      ],
      "25": [
        {
          "title": "Think out Loud: Emotion Deducing Explanation in Dialogues",
          "year": "2024-06",
          "abstract": "Humans convey emotions through daily dialogues, making emotion understanding\na crucial step of affective intelligence. To understand emotions in dialogues,\nmachines are asked to recognize the emotion for an utterance (Emotion\nRecognition in Dialogues, ERD); based on the emotion, then find causal\nutterances for the emotion (Emotion Cause Extraction in Dialogues, ECED). The\nsetting of the two tasks requires first ERD and then ECED, ignoring the mutual\ncomplement between emotion and cause. To fix this, some new tasks are proposed\nto extract them simultaneously. Although the current research on these tasks\nhas excellent achievements, simply identifying emotion-related factors by\nclassification modeling lacks realizing the specific thinking process of causes\nstimulating the emotion in an explainable way. This thinking process especially\nreflected in the reasoning ability of Large Language Models (LLMs) is\nunder-explored. To this end, we propose a new task \"Emotion Deducing\nExplanation in Dialogues\" (EDEN). EDEN recognizes emotion and causes in an\nexplicitly thinking way. That is, models need to generate an explanation text,\nwhich first summarizes the causes; analyzes the inner activities of the\nspeakers triggered by the causes using common sense; then guesses the emotion\naccordingly. To support the study of EDEN, based on the existing resources in\nECED, we construct two EDEN datasets by human effort. We further evaluate\ndifferent models on EDEN and find that LLMs are more competent than\nconventional PLMs. Besides, EDEN can help LLMs achieve better recognition of\nemotions and causes, which explores a new research direction of explainable\nemotion understanding in dialogues.",
          "arxiv_id": "2406.04758v1"
        },
        {
          "title": "EmoVerse: Exploring Multimodal Large Language Models for Sentiment and Emotion Understanding",
          "year": "2024-12",
          "abstract": "Sentiment and emotion understanding are essential to applications such as\nhuman-computer interaction and depression detection. While Multimodal Large\nLanguage Models (MLLMs) demonstrate robust general capabilities, they face\nconsiderable challenges in the field of affective computing, particularly in\ndetecting subtle facial expressions and handling complex emotion-related tasks,\nsuch as emotion reason inference and understanding emotions in long-context\nscenarios. Furthermore, there is a lack of a unified MLLM that can effectively\nhandle both sentiment and emotion-related tasks. To address these challenges,\nwe explore multi-task training strategies for MLLMs in affective computing and\nintroduce Emotion Universe (EmoVerse), an MLLM designed to handle a broad\nspectrum of sentiment and emotion-related tasks. In addition, EmoVerse is\ncapable of deeply analyzing the underlying causes of emotional states. We also\nintroduce the Affective Multitask (AMT) Dataset, which supports multimodal\nsentiment analysis, multimodal emotion recognition, facial expression\nrecognition, emotion reason inference, and emotion cause-pair extraction tasks.\nExtensive experiments demonstrate that EmoVerse outperforms existing methods,\nachieving state-of-the-art results in sentiment and emotion-related tasks. The\ncode is available at https://github.com/liaolea/EmoVerse.",
          "arxiv_id": "2412.08049v3"
        },
        {
          "title": "Emotion Correlation Mining Through Deep Learning Models on Natural Language Text",
          "year": "2020-07",
          "abstract": "Emotion analysis has been attracting researchers' attention. Most previous\nworks in the artificial intelligence field focus on recognizing emotion rather\nthan mining the reason why emotions are not or wrongly recognized. Correlation\namong emotions contributes to the failure of emotion recognition. In this\npaper, we try to fill the gap between emotion recognition and emotion\ncorrelation mining through natural language text from web news. Correlation\namong emotions, expressed as the confusion and evolution of emotion, is\nprimarily caused by human emotion cognitive bias. To mine emotion correlation\nfrom emotion recognition through text, three kinds of features and two deep\nneural network models are presented. The emotion confusion law is extracted\nthrough orthogonal basis. The emotion evolution law is evaluated from three\nperspectives, one-step shift, limited-step shifts, and shortest path transfer.\nThe method is validated using three datasets-the titles, the bodies, and the\ncomments of news articles, covering both objective and subjective texts in\nvarying lengths (long and short). The experimental results show that, in\nsubjective comments, emotions are easily mistaken as anger. Comments tend to\narouse emotion circulations of love-anger and sadness-anger. In objective news,\nit is easy to recognize text emotion as love and cause fear-joy circulation.\nThat means, journalists may try to attract attention using fear and joy words\nbut arouse the emotion love instead; After news release, netizens generate\nemotional comments to express their intense emotions, i.e., anger, sadness, and\nlove. These findings could provide insights for applications regarding\naffective interaction such as network public sentiment, social media\ncommunication, and human-computer interaction.",
          "arxiv_id": "2007.14071v1"
        }
      ],
      "26": [
        {
          "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
          "year": "2023-09",
          "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating\nthe hallucination of large language models (LLMs). However, existing research\nlacks rigorous evaluation of the impact of retrieval-augmented generation on\ndifferent large language models, which make it challenging to identify the\npotential bottlenecks in the capabilities of RAG for different LLMs. In this\npaper, we systematically investigate the impact of Retrieval-Augmented\nGeneration on large language models. We analyze the performance of different\nlarge language models in 4 fundamental abilities required for RAG, including\nnoise robustness, negative rejection, information integration, and\ncounterfactual robustness. To this end, we establish Retrieval-Augmented\nGeneration Benchmark (RGB), a new corpus for RAG evaluation in both English and\nChinese. RGB divides the instances within the benchmark into 4 separate\ntestbeds based on the aforementioned fundamental abilities required to resolve\nthe case. Then we evaluate 6 representative LLMs on RGB to diagnose the\nchallenges of current LLMs when applying RAG. Evaluation reveals that while\nLLMs exhibit a certain degree of noise robustness, they still struggle\nsignificantly in terms of negative rejection, information integration, and\ndealing with false information. The aforementioned assessment outcomes indicate\nthat there is still a considerable journey ahead to effectively apply RAG to\nLLMs.",
          "arxiv_id": "2309.01431v2"
        },
        {
          "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation",
          "year": "2025-07",
          "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge, where the LLM's ability to generate responses\nbased on the combination of a given query and retrieved documents is crucial.\nHowever, most benchmarks focus on overall RAG system performance, rarely\nassessing LLM-specific capabilities. Current benchmarks emphasize broad aspects\nsuch as noise robustness, but lack a systematic and granular evaluation\nframework on document utilization. To this end, we introduce\n\\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,\nemphasizing the following progressive dimensions: (1) multi-level filtering\nabilities, (2) combination abilities, and (3) reference reasoning. To provide a\nmore nuanced understanding of LLMs' roles in RAG systems, we formulate an\ninnovative placeholder-based approach to decouple the contributions of the\nLLM's parametric knowledge and the external knowledge. Experiments demonstrate\nthe limitations of representative LLMs in the RAG system's generation\ncapabilities, particularly in error resilience and context faithfulness. Our\nbenchmark provides a reproducible framework for developing more reliable and\nefficient RAG systems. Our code is available in\nhttps://github.com/Alipay-Med/PRGB.",
          "arxiv_id": "2507.22927v1"
        },
        {
          "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving",
          "year": "2025-03",
          "abstract": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
          "arxiv_id": "2503.14649v2"
        }
      ],
      "27": [
        {
          "title": "Relation Extraction with Contextualized Relation Embedding (CRE)",
          "year": "2020-11",
          "abstract": "Relation extraction is the task of identifying relation instance between two\nentities given a corpus whereas Knowledge base modeling is the task of\nrepresenting a knowledge base, in terms of relations between entities. This\npaper proposes an architecture for the relation extraction task that integrates\nsemantic information with knowledge base modeling in a novel manner. Existing\napproaches for relation extraction either do not utilize knowledge base\nmodelling or use separately trained KB models for the RE task. We present a\nmodel architecture that internalizes KB modeling in relation extraction. This\nmodel applies a novel approach to encode sentences into contextualized relation\nembeddings, which can then be used together with parameterized entity\nembeddings to score relation instances. The proposed CRE model achieves state\nof the art performance on datasets derived from The New York Times Annotated\nCorpus and FreeBase. The source code has been made available.",
          "arxiv_id": "2011.09658v1"
        },
        {
          "title": "Multi-Attribute Relation Extraction (MARE) -- Simplifying the Application of Relation Extraction",
          "year": "2021-11",
          "abstract": "Natural language understanding's relation extraction makes innovative and\nencouraging novel business concepts possible and facilitates new digitilized\ndecision-making processes. Current approaches allow the extraction of relations\nwith a fixed number of entities as attributes. Extracting relations with an\narbitrary amount of attributes requires complex systems and costly\nrelation-trigger annotations to assist these systems. We introduce\nmulti-attribute relation extraction (MARE) as an assumption-less problem\nformulation with two approaches, facilitating an explicit mapping from business\nuse cases to the data annotations. Avoiding elaborated annotation constraints\nsimplifies the application of relation extraction approaches. The evaluation\ncompares our models to current state-of-the-art event extraction and binary\nrelation extraction methods. Our approaches show improvement compared to these\non the extraction of general multi-attribute relations.",
          "arxiv_id": "2111.09035v1"
        },
        {
          "title": "Complex Relation Extraction: Challenges and Opportunities",
          "year": "2020-12",
          "abstract": "Relation extraction aims to identify the target relations of entities in\ntexts. Relation extraction is very important for knowledge base construction\nand text understanding. Traditional binary relation extraction, including\nsupervised, semi-supervised and distant supervised ones, has been extensively\nstudied and significant results are achieved. In recent years, many complex\nrelation extraction tasks, i.e., the variants of simple binary relation\nextraction, are proposed to meet the complex applications in practice. However,\nthere is no literature to fully investigate and summarize these complex\nrelation extraction works so far. In this paper, we first report the recent\nprogress in traditional simple binary relation extraction. Then we summarize\nthe existing complex relation extraction tasks and present the definition,\nrecent progress, challenges and opportunities for each task.",
          "arxiv_id": "2012.04821v1"
        }
      ],
      "28": [
        {
          "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)",
          "year": "2022-03",
          "abstract": "For a long time, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language can describe almost anything and language grounding\nis a powerful medium to represent various problems or tasks, we present a\nflexible and unified text-to-text paradigm called \"Pretrain, Personalized\nPrompt, and Predict Paradigm\" (P5) for recommendation, which unifies various\nrecommendation tasks in a shared framework. In P5, all data such as user-item\ninteractions, user descriptions, item metadata, and user reviews are converted\nto a common format -- natural language sequences. The rich information from\nnatural language assists P5 to capture deeper semantics for personalization and\nrecommendation. Specifically, P5 learns different tasks with the same language\nmodeling objective during pretraining. Thus, it serves as the foundation model\nfor various downstream recommendation tasks, allows easy integration with other\nmodalities, and enables instruction-based recommendation based on prompts. P5\nadvances recommender systems from shallow model to deep model to big model, and\nwill revolutionize the technical form of recommender systems towards universal\nrecommendation engine. With adaptive personalized prompt for different users,\nP5 is able to make predictions in a zero-shot or few-shot manner and largely\nreduces the necessity for extensive fine-tuning. On several recommendation\nbenchmarks, we conduct experiments to show the effectiveness of P5. We release\nthe source code at https://github.com/jeykigung/P5.",
          "arxiv_id": "2203.13366v7"
        },
        {
          "title": "User-Inspired Posterior Network for Recommendation Reason Generation",
          "year": "2021-02",
          "abstract": "Recommendation reason generation, aiming at showing the selling points of\nproducts for customers, plays a vital role in attracting customers' attention\nas well as improving user experience. A simple and effective way is to extract\nkeywords directly from the knowledge-base of products, i.e., attributes or\ntitle, as the recommendation reason. However, generating recommendation reason\nfrom product knowledge doesn't naturally respond to users' interests.\nFortunately, on some E-commerce websites, there exists more and more\nuser-generated content (user-content for short), i.e., product\nquestion-answering (QA) discussions, which reflect user-cared aspects.\nTherefore, in this paper, we consider generating the recommendation reason by\ntaking into account not only the product attributes but also the\ncustomer-generated product QA discussions. In reality, adequate user-content is\nonly possible for the most popular commodities, whereas large sums of long-tail\nproducts or new products cannot gather a sufficient number of user-content. To\ntackle this problem, we propose a user-inspired multi-source posterior\ntransformer (MSPT), which induces the model reflecting the users' interests\nwith a posterior multiple QA discussions module, and generating recommendation\nreasons containing the product attributes as well as the user-cared aspects.\nExperimental results show that our model is superior to traditional generative\nmodels. Additionally, the analysis also shows that our model can focus more on\nthe user-cared aspects than baselines.",
          "arxiv_id": "2102.07919v1"
        },
        {
          "title": "GenRec: Large Language Model for Generative Recommendation",
          "year": "2023-07",
          "abstract": "In recent years, large language models (LLM) have emerged as powerful tools\nfor diverse natural language processing tasks. However, their potential for\nrecommender systems under the generative recommendation paradigm remains\nrelatively unexplored. This paper presents an innovative approach to\nrecommendation systems using large language models (LLMs) based on text data.\nIn this paper, we present a novel LLM for generative recommendation (GenRec)\nthat utilized the expressive power of LLM to directly generate the target item\nto recommend, rather than calculating ranking score for each candidate item one\nby one as in traditional discriminative recommendation. GenRec uses LLM's\nunderstanding ability to interpret context, learn user preferences, and\ngenerate relevant recommendation. Our proposed approach leverages the vast\nknowledge encoded in large language models to accomplish recommendation tasks.\nWe first we formulate specialized prompts to enhance the ability of LLM to\ncomprehend recommendation tasks. Subsequently, we use these prompts to\nfine-tune the LLaMA backbone LLM on a dataset of user-item interactions,\nrepresented by textual data, to capture user preferences and item\ncharacteristics. Our research underscores the potential of LLM-based generative\nrecommendation in revolutionizing the domain of recommendation systems and\noffers a foundational framework for future explorations in this field. We\nconduct extensive experiments on benchmark datasets, and the experiments shows\nthat our GenRec has significant better results on large dataset.",
          "arxiv_id": "2307.00457v2"
        }
      ],
      "29": [
        {
          "title": "Financial data analysis application via multi-strategy text processing",
          "year": "2022-04",
          "abstract": "Maintaining financial system stability is critical to economic development,\nand early identification of risks and opportunities is essential. The financial\nindustry contains a wide variety of data, such as financial statements,\ncustomer information, stock trading data, news, etc. Massive heterogeneous data\ncalls for intelligent algorithms for machines to process and understand. This\npaper mainly focuses on the stock trading data and news about China A-share\ncompanies. We present a financial data analysis application, Financial Quotient\nPorter, designed to combine textual and numerical data by using a\nmulti-strategy data mining approach. Additionally, we present our efforts and\nplans in deep learning financial text processing application scenarios using\nnatural language processing (NLP) and knowledge graph (KG) technologies. Based\non KG technology, risks and opportunities can be identified from heterogeneous\ndata. NLP technology can be used to extract entities, relations, and events\nfrom unstructured text, and analyze market sentiment. Experimental results show\nmarket sentiments towards a company and an industry, as well as news-level\nassociations between companies.",
          "arxiv_id": "2204.11394v1"
        },
        {
          "title": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights",
          "year": "2024-01",
          "abstract": "In recent years, Large Language Models (LLMs) like ChatGPT have seen\nconsiderable advancements and have been applied in diverse fields. Built on the\nTransformer architecture, these models are trained on extensive datasets,\nenabling them to understand and generate human language effectively. In the\nfinancial domain, the deployment of LLMs is gaining momentum. These models are\nbeing utilized for automating financial report generation, forecasting market\ntrends, analyzing investor sentiment, and offering personalized financial\nadvice. Leveraging their natural language processing capabilities, LLMs can\ndistill key insights from vast financial data, aiding institutions in making\ninformed investment choices and enhancing both operational efficiency and\ncustomer satisfaction. In this study, we provide a comprehensive overview of\nthe emerging integration of LLMs into various financial tasks. Additionally, we\nconducted holistic tests on multiple financial tasks through the combination of\nnatural language instructions. Our findings show that GPT-4 effectively follow\nprompt instructions across various financial tasks. This survey and evaluation\nof LLMs in the financial domain aim to deepen the understanding of LLMs'\ncurrent role in finance for both financial practitioners and LLM researchers,\nidentify new research and application prospects, and highlight how these\ntechnologies can be leveraged to solve practical challenges in the finance\nindustry.",
          "arxiv_id": "2401.11641v2"
        },
        {
          "title": "Tracking Turbulence Through Financial News During COVID-19",
          "year": "2021-09",
          "abstract": "Grave human toll notwithstanding, the COVID-19 pandemic created uniquely\nunstable conditions in financial markets. In this work we uncover and discuss\nrelationships involving sentiment in financial publications during the 2020\npandemic-motivated U.S. financial crash. First, we introduce a set of expert\nannotations of financial sentiment for articles from major American financial\nnews publishers. After an exploratory data analysis, we then describe a\nCNN-based architecture to address the task of predicting financial sentiment in\nthis anomalous, tumultuous setting. Our best performing model achieves a\nmaximum weighted F1 score of 0.746, establishing a strong performance\nbenchmark. Using predictions from our top performing model, we close by\nconducting a statistical correlation study with real stock market data, finding\ninteresting and strong relationships between financial news and the S\\&P 500\nindex, trading volume, market volatility, and different single-factor ETFs.",
          "arxiv_id": "2109.04369v1"
        }
      ],
      "30": [
        {
          "title": "Unified Hallucination Detection for Multimodal Large Language Models",
          "year": "2024-02",
          "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.",
          "arxiv_id": "2402.03190v4"
        },
        {
          "title": "HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation",
          "year": "2024-06",
          "abstract": "Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), achieving remarkable performance across diverse\ntasks and enabling widespread real-world applications. However, LLMs are prone\nto hallucination, generating content that either conflicts with established\nknowledge or is unfaithful to the original sources. Existing hallucination\nbenchmarks primarily focus on sentence- or passage-level hallucination\ndetection, neglecting dialogue-level evaluation, hallucination localization,\nand rationale provision. They also predominantly target factuality\nhallucinations while underestimating faithfulness hallucinations, often relying\non labor-intensive or non-specialized evaluators. To address these limitations,\nwe propose HalluDial, the first comprehensive large-scale benchmark for\nautomatic dialogue-level hallucination evaluation. HalluDial encompasses both\nspontaneous and induced hallucination scenarios, covering factuality and\nfaithfulness hallucinations. The benchmark includes 4,094 dialogues with a\ntotal of 146,856 samples. Leveraging HalluDial, we conduct a comprehensive\nmeta-evaluation of LLMs' hallucination evaluation capabilities in\ninformation-seeking dialogues and introduce a specialized judge language model,\nHalluJudge. The high data quality of HalluDial enables HalluJudge to achieve\nsuperior or competitive performance in hallucination evaluation, facilitating\nthe automatic assessment of dialogue-level hallucinations in LLMs and providing\nvaluable insights into this phenomenon. The dataset and the code are available\nat https://github.com/FlagOpen/HalluDial.",
          "arxiv_id": "2406.07070v1"
        },
        {
          "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
          "year": "2024-07",
          "abstract": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.",
          "arxiv_id": "2407.04693v2"
        }
      ],
      "31": [
        {
          "title": "New Faithfulness-Centric Interpretability Paradigms for Natural Language Processing",
          "year": "2024-11",
          "abstract": "As machine learning becomes more widespread and is used in more critical\napplications, it's important to provide explanations for these models, to\nprevent unintended behavior. Unfortunately, many current interpretability\nmethods struggle with faithfulness. Therefore, this Ph.D. thesis investigates\nthe question \"How to provide and ensure faithful explanations for complex\ngeneral-purpose neural NLP models?\" The main thesis is that we should develop\nnew paradigms in interpretability. This is achieved by first developing solid\nfaithfulness metrics and then applying the lessons learned from this\ninvestigation to develop new paradigms. The two new paradigms explored are\nfaithfulness measurable models (FMMs) and self-explanations. The idea in\nself-explanations is to have large language models explain themselves, we\nidentify that current models are not capable of doing this consistently.\nHowever, we suggest how this could be achieved. The idea of FMMs is to create\nmodels that are designed such that measuring faithfulness is cheap and precise.\nThis makes it possible to optimize an explanation towards maximum faithfulness,\nwhich makes FMMs designed to be explained. We find that FMMs yield explanations\nthat are near theoretical optimal in terms of faithfulness. Overall, from all\ninvestigations of faithfulness, results show that post-hoc and intrinsic\nexplanations are by default model and task-dependent. However, this was not the\ncase when using FMMs, even with the same post-hoc explanation methods. This\nshows, that even simple modifications to the model, such as randomly masking\nthe training dataset, as was done in FMMs, can drastically change the situation\nand result in consistently faithful explanations. This answers the question of\nhow to provide and ensure faithful explanations.",
          "arxiv_id": "2411.17992v1"
        },
        {
          "title": "MaNtLE: Model-agnostic Natural Language Explainer",
          "year": "2023-05",
          "abstract": "Understanding the internal reasoning behind the predictions of machine\nlearning systems is increasingly vital, given their rising adoption and\nacceptance. While previous approaches, such as LIME, generate algorithmic\nexplanations by attributing importance to input features for individual\nexamples, recent research indicates that practitioners prefer examining\nlanguage explanations that explain sub-groups of examples. In this paper, we\nintroduce MaNtLE, a model-agnostic natural language explainer that analyzes\nmultiple classifier predictions and generates faithful natural language\nexplanations of classifier rationale for structured classification tasks.\nMaNtLE uses multi-task training on thousands of synthetic classification tasks\nto generate faithful explanations. Simulated user studies indicate that, on\naverage, MaNtLE-generated explanations are at least 11% more faithful compared\nto LIME and Anchors explanations across three tasks. Human evaluations\ndemonstrate that users can better predict model behavior using explanations\nfrom MaNtLE compared to other techniques",
          "arxiv_id": "2305.12995v1"
        },
        {
          "title": "NILE : Natural Language Inference with Faithful Natural Language Explanations",
          "year": "2020-05",
          "abstract": "The recent growth in the popularity and success of deep learning models on\nNLP classification tasks has accompanied the need for generating some form of\nnatural language explanation of the predicted labels. Such generated natural\nlanguage (NL) explanations are expected to be faithful, i.e., they should\ncorrelate well with the model's internal decision making. In this work, we\nfocus on the task of natural language inference (NLI) and address the following\nquestion: can we build NLI systems which produce labels with high accuracy,\nwhile also generating faithful explanations of its decisions? We propose\nNatural-language Inference over Label-specific Explanations (NILE), a novel NLI\nmethod which utilizes auto-generated label-specific NL explanations to produce\nlabels along with its faithful explanation. We demonstrate NILE's effectiveness\nover previously reported methods through automated and human evaluation of the\nproduced labels and explanations. Our evaluation of NILE also supports the\nclaim that accurate systems capable of providing testable explanations of their\ndecisions can be designed. We discuss the faithfulness of NILE's explanations\nin terms of sensitivity of the decisions to the corresponding explanations. We\nargue that explicit evaluation of faithfulness, in addition to label and\nexplanation accuracy, is an important step in evaluating model's explanations.\nFurther, we demonstrate that task-specific probes are necessary to establish\nsuch sensitivity.",
          "arxiv_id": "2005.12116v1"
        }
      ],
      "32": [
        {
          "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
          "year": "2024-10",
          "abstract": "Generative AI, particularly Language Models (LMs), has the potential to\ntransform real-world domains with societal impact, particularly where access to\nexperts is limited. For example, in education, training novice educators with\nexpert guidance is important for effectiveness but expensive, creating\nsignificant barriers to improving education quality at scale. This challenge\ndisproportionately harms students from under-served communities, who stand to\ngain the most from high-quality education. We introduce Tutor CoPilot, a novel\nHuman-AI approach that leverages a model of expert thinking to provide\nexpert-like guidance to tutors as they tutor. This study is the first\nrandomized controlled trial of a Human-AI system in live tutoring, involving\n900 tutors and 1,800 K-12 students from historically under-served communities.\nFollowing a preregistered analysis plan, we find that students working with\ntutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more\nlikely to master topics (p<0.01). Notably, students of lower-rated tutors\nexperienced the greatest benefit, improving mastery by 9 p.p. We find that\nTutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages\nusing classifiers to identify pedagogical strategies, and find that tutors with\naccess to Tutor CoPilot are more likely to use high-quality strategies to\nfoster student understanding (e.g., asking guiding questions) and less likely\nto give away the answer to the student. Tutor interviews highlight how Tutor\nCoPilot's guidance helps tutors to respond to student needs, though they flag\nissues in Tutor CoPilot, such as generating suggestions that are not\ngrade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates\nhow Human-AI systems can scale expertise in real-world domains, bridge gaps in\nskills and create a future where high-quality education is accessible to all\nstudents.",
          "arxiv_id": "2410.03017v2"
        },
        {
          "title": "Automatic Short Math Answer Grading via In-context Meta-learning",
          "year": "2022-05",
          "abstract": "Automatic short answer grading is an important research direction in the\nexploration of how to use artificial intelligence (AI)-based tools to improve\neducation. Current state-of-the-art approaches use neural language models to\ncreate vectorized representations of students responses, followed by\nclassifiers to predict the score. However, these approaches have several key\nlimitations, including i) they use pre-trained language models that are not\nwell-adapted to educational subject domains and/or student-generated text and\nii) they almost always train one model per question, ignoring the linkage\nacross a question and result in a significant model storage problem due to the\nsize of advanced language models. In this paper, we study the problem of\nautomatic short answer grading for students' responses to math questions and\npropose a novel framework for this task. First, we use MathBERT, a variant of\nthe popular language model BERT adapted to mathematical content, as our base\nmodel and fine-tune it for the downstream task of student response grading.\nSecond, we use an in-context learning approach that provides scoring examples\nas input to the language model to provide additional context information and\npromote generalization to previously unseen questions. We evaluate our\nframework on a real-world dataset of student responses to open-ended math\nquestions and show that our framework (often significantly) outperforms\nexisting approaches, especially for new questions that are not seen during\ntraining.",
          "arxiv_id": "2205.15219v3"
        },
        {
          "title": "Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between Language Models and Human Error Patterns",
          "year": "2025-02",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious educational tasks, yet their alignment with human learning patterns,\nparticularly in predicting which incorrect options students are most likely to\nselect in multiple-choice questions (MCQs), remains underexplored. Our work\ninvestigates the relationship between LLM generation likelihood and student\nresponse distributions in MCQs with a specific focus on distractor selections.\nWe collect a comprehensive dataset of MCQs with real-world student response\ndistributions to explore two fundamental research questions: (1). RQ1 - Do the\ndistractors that students more frequently select correspond to those that LLMs\nassign higher generation likelihood to? (2). RQ2 - When an LLM selects a\nincorrect choice, does it choose the same distractor that most students pick?\nOur experiments reveals moderate correlations between LLM-assigned\nprobabilities and student selection patterns for distractors in MCQs.\nAdditionally, when LLMs make mistakes, they are more likley to select the same\nincorrect answers that commonly mislead students, which is a pattern consistent\nacross both small and large language models. Our work provides empirical\nevidence that despite LLMs' strong performance on generating educational\ncontent, there remains a gap between LLM's underlying reasoning process and\nhuman cognitive processes in identifying confusing distractors. Our findings\nalso have significant implications for educational assessment development. The\nsmaller language models could be efficiently utilized for automated distractor\ngeneration as they demonstrate similar patterns in identifying confusing answer\nchoices as larger language models. This observed alignment between LLMs and\nstudent misconception patterns opens new opportunities for generating\nhigh-quality distractors that complement traditional human-designed\ndistractors.",
          "arxiv_id": "2502.15140v1"
        }
      ],
      "33": [
        {
          "title": "Two-view Graph Neural Networks for Knowledge Graph Completion",
          "year": "2021-12",
          "abstract": "We present an effective graph neural network (GNN)-based knowledge graph\nembedding model, which we name WGE, to capture entity- and relation-focused\ngraph structures. Given a knowledge graph, WGE builds a single undirected\nentity-focused graph that views entities as nodes. WGE also constructs another\nsingle undirected graph from relation-focused constraints, which views entities\nand relations as nodes. WGE then proposes a GNN-based architecture to better\nlearn vector representations of entities and relations from these two single\nentity- and relation-focused graphs. WGE feeds the learned entity and relation\nrepresentations into a weighted score function to return the triple scores for\nknowledge graph completion. Experimental results show that WGE outperforms\nstrong baselines on seven benchmark datasets for knowledge graph completion.",
          "arxiv_id": "2112.09231v4"
        },
        {
          "title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding",
          "year": "2021-10",
          "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become\nwidespread recently, and many knowledge graph embedding (KGE) methods are\nproposed to embed entities and relations of a KG into continuous vector spaces.\nSuch embedding methods simplify the operations of conducting various in-KG\ntasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering).\nThey can be viewed as general solutions for representing KGs. However, existing\nKGE methods are not applicable to inductive settings, where a model trained on\nsource KGs will be tested on target KGs with entities unseen during model\ntraining. Existing works focusing on KGs in inductive settings can only solve\nthe inductive relation prediction task. They can not handle other out-of-KG\ntasks as general as KGE methods since they don't produce embeddings for\nentities. In this paper, to achieve inductive knowledge graph embedding, we\npropose a model MorsE, which does not learn embeddings for entities but learns\ntransferable meta-knowledge that can be used to produce entity embeddings. Such\nmeta-knowledge is modeled by entity-independent modules and learned by\nmeta-learning. Experimental results show that our model significantly\noutperforms corresponding baselines for in-KG and out-of-KG tasks in inductive\nsettings.",
          "arxiv_id": "2110.14170v3"
        },
        {
          "title": "FedE: Embedding Knowledge Graphs in Federated Setting",
          "year": "2020-10",
          "abstract": "Knowledge graphs (KGs) consisting of triples are always incomplete, so it's\nimportant to do Knowledge Graph Completion (KGC) by predicting missing triples.\nMulti-Source KG is a common situation in real KG applications which can be\nviewed as a set of related individual KGs where different KGs contains\nrelations of different aspects of entities. It's intuitive that, for each\nindividual KG, its completion could be greatly contributed by the triples\ndefined and labeled in other ones. However, because of the data privacy and\nsensitivity, a set of relevant knowledge graphs cannot complement each other's\nKGC by just collecting data from different knowledge graphs together.\nTherefore, in this paper, we introduce federated setting to keep their privacy\nwithout triple transferring between KGs and apply it in embedding knowledge\ngraph, a typical method which have proven effective for KGC in the past decade.\nWe propose a Federated Knowledge Graph Embedding framework FedE, focusing on\nlearning knowledge graph embeddings by aggregating locally-computed updates.\nFinally, we conduct extensive experiments on datasets derived from KGE\nbenchmark datasets and results show the effectiveness of our proposed FedE.",
          "arxiv_id": "2010.12882v1"
        }
      ],
      "34": [
        {
          "title": "Automatic Standardization of Arabic Dialects for Machine Translation",
          "year": "2023-01",
          "abstract": "Based on an annotated multimedia corpus, television series Mar{\\=a}y{\\=a}\n2013, we dig into the question of ''automatic standardization'' of Arabic\ndialects for machine translation. Here we distinguish between rule-based\nmachine translation and statistical machine translation. Machine translation\nfrom Arabic most of the time takes standard or modern Arabic as the source\nlanguage and produces quite satisfactory translations thanks to the\navailability of the translation memories necessary for training the models. The\ncase is different for the translation of Arabic dialects. The productions are\nmuch less efficient. In our research we try to apply machine translation\nmethods to a dialect/standard (or modern) Arabic pair to automatically produce\na standard Arabic text from a dialect input, a process we call ''automatic\nstandardization''. we opt here for the application of ''statistical models''\nbecause ''automatic standardization'' based on rules is more hard with the lack\nof ''diglossic'' dictionaries on the one hand and the difficulty of creating\nlinguistic rules for each dialect on the other. Carrying out this research\ncould then lead to combining ''automatic standardization'' software and\nautomatic translation software so that we take the output of the first software\nand introduce it as input into the second one to obtain at the end a quality\nmachine translation. This approach may also have educational applications such\nas the development of applications to help understand different Arabic dialects\nby transforming dialectal texts into standard Arabic.",
          "arxiv_id": "2301.03447v1"
        },
        {
          "title": "Interpreting Arabic Transformer Models",
          "year": "2022-01",
          "abstract": "Arabic is a Semitic language which is widely spoken with many dialects. Given\nthe success of pre-trained language models, many transformer models trained on\nArabic and its dialects have surfaced. While these models have been compared\nwith respect to downstream NLP tasks, no evaluation has been carried out to\ndirectly compare the internal representations. We probe how linguistic\ninformation is encoded in Arabic pretrained models, trained on different\nvarieties of Arabic language. We perform a layer and neuron analysis on the\nmodels using three intrinsic tasks: two morphological tagging tasks based on\nMSA (modern standard Arabic) and dialectal POS-tagging and a dialectal\nidentification task. Our analysis enlightens interesting findings such as: i)\nword morphology is learned at the lower and middle layers ii) dialectal\nidentification necessitate more knowledge and hence preserved even in the final\nlayers, iii) despite a large overlap in their vocabulary, the MSA-based models\nfail to capture the nuances of Arabic dialects, iv) we found that neurons in\nembedding layers are polysemous in nature, while the neurons in middle layers\nare exclusive to specific properties.",
          "arxiv_id": "2201.07434v2"
        },
        {
          "title": "Large Language Models and Arabic Content: A Review",
          "year": "2025-05",
          "abstract": "Over the past three years, the rapid advancement of Large Language Models\n(LLMs) has had a profound impact on multiple areas of Artificial Intelligence\n(AI), particularly in Natural Language Processing (NLP) across diverse\nlanguages, including Arabic. Although Arabic is considered one of the most\nwidely spoken languages across 27 countries in the Arabic world and used as a\nsecond language in some other non-Arabic countries as well, there is still a\nscarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face\nvarious challenges due to the complexities of the Arabic language, including\nits rich morphology, intricate structure, and diverse writing standards, among\nother factors. Researchers have been actively addressing these challenges,\ndemonstrating that pre-trained Large Language Models (LLMs) trained on\nmultilingual corpora achieve significant success in various Arabic NLP tasks.\nThis study provides an overview of using large language models (LLMs) for the\nArabic language, highlighting early pre-trained Arabic Language models across\nvarious NLP applications and their ability to handle diverse Arabic content\ntasks and dialects. It also provides an overview of how techniques like\nfinetuning and prompt engineering can enhance the performance of these models.\nAdditionally, the study summarizes common Arabic benchmarks and datasets while\npresenting our observations on the persistent upward trend in the adoption of\nLLMs.",
          "arxiv_id": "2505.08004v1"
        }
      ],
      "35": [
        {
          "title": "AmQA: Amharic Question Answering Dataset",
          "year": "2023-03",
          "abstract": "Question Answering (QA) returns concise answers or answer lists from natural\nlanguage text given a context document. Many resources go into curating QA\ndatasets to advance robust models' development. There is a surge of QA datasets\nfor languages like English, however, this is not true for Amharic. Amharic, the\nofficial language of Ethiopia, is the second most spoken Semitic language in\nthe world. There is no published or publicly available Amharic QA dataset.\nHence, to foster the research in Amharic QA, we present the first Amharic QA\n(AmQA) dataset. We crowdsourced 2628 question-answer pairs over 378 Wikipedia\narticles. Additionally, we run an XLMR Large-based baseline model to spark\nopen-domain QA research interest. The best-performing baseline achieves an\nF-score of 69.58 and 71.74 in reader-retriever QA and reading comprehension\nsettings respectively.",
          "arxiv_id": "2303.03290v2"
        },
        {
          "title": "Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering",
          "year": "2021-09",
          "abstract": "In this paper we propose a novel approach towards improving the efficiency of\nQuestion Answering (QA) systems by filtering out questions that will not be\nanswered by them. This is based on an interesting new finding: the answer\nconfidence scores of state-of-the-art QA systems can be approximated well by\nmodels solely using the input question text. This enables preemptive filtering\nof questions that are not answered by the system due to their answer confidence\nscores being lower than the system threshold. Specifically, we learn\nTransformer-based question models by distilling Transformer-based answering\nmodels. Our experiments on three popular QA datasets and one industrial QA\nbenchmark demonstrate the ability of our question models to approximate the\nPrecision/Recall curves of the target QA system well. These question models,\nwhen used as filters, can effectively trade off lower computation cost of QA\nsystems for lower Recall, e.g., reducing computation by ~60%, while only losing\n~3-4% of Recall.",
          "arxiv_id": "2109.07009v1"
        },
        {
          "title": "OneStop QAMaker: Extract Question-Answer Pairs from Text in a One-Stop Approach",
          "year": "2021-02",
          "abstract": "Large-scale question-answer (QA) pairs are critical for advancing research\nareas like machine reading comprehension and question answering. To construct\nQA pairs from documents requires determining how to ask a question and what is\nthe corresponding answer. Existing methods for QA pair generation usually\nfollow a pipeline approach. Namely, they first choose the most likely candidate\nanswer span and then generate the answer-specific question. This pipeline\napproach, however, is undesired in mining the most appropriate QA pairs from\ndocuments since it ignores the connection between question generation and\nanswer extraction, which may lead to incompatible QA pair generation, i.e., the\nselected answer span is inappropriate for question generation. However, for\nhuman annotators, we take the whole QA pair into account and consider the\ncompatibility between question and answer. Inspired by such motivation, instead\nof the conventional pipeline approach, we propose a model named OneStop\ngenerate QA pairs from documents in a one-stop approach. Specifically,\nquestions and their corresponding answer span is extracted simultaneously and\nthe process of question generation and answer extraction mutually affect each\nother. Additionally, OneStop is much more efficient to be trained and deployed\nin industrial scenarios since it involves only one model to solve the complex\nQA generation task. We conduct comprehensive experiments on three large-scale\nmachine reading comprehension datasets: SQuAD, NewsQA, and DuReader. The\nexperimental results demonstrate that our OneStop model outperforms the\nbaselines significantly regarding the quality of generated questions, quality\nof generated question-answer pairs, and model efficiency.",
          "arxiv_id": "2102.12128v1"
        }
      ],
      "36": [
        {
          "title": "MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation",
          "year": "2023-11",
          "abstract": "Understanding events in texts is a core objective of natural language\nunderstanding, which requires detecting event occurrences, extracting event\narguments, and analyzing inter-event relationships. However, due to the\nannotation challenges brought by task complexity, a large-scale dataset\ncovering the full process of event understanding has long been absent. In this\npaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\nargument annotations, making the first all-in-one dataset supporting event\ndetection, event argument extraction (EAE), and event relation extraction. As\nan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\nschema covering 162 event types and 612 argument roles, all with expert-written\ndefinitions and examples; (2) a large data scale, containing 98,591 events and\n290,613 arguments obtained with laborious human annotation; (3) the exhaustive\nannotation supporting all task variants of EAE, which annotates both entity and\nnon-entity event arguments in document level. Experiments indicate that\nMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\nlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\nall-in-one dataset, we preliminarily explore a potential application, future\nevent prediction, with LLMs. MAVEN-Arg and codes can be obtained from\nhttps://github.com/THU-KEG/MAVEN-Argument.",
          "arxiv_id": "2311.09105v2"
        },
        {
          "title": "EventGraph: Event Extraction as Semantic Graph Parsing",
          "year": "2022-10",
          "abstract": "Event extraction involves the detection and extraction of both the event\ntriggers and corresponding event arguments. Existing systems often decompose\nevent extraction into multiple subtasks, without considering their possible\ninteractions. In this paper, we propose EventGraph, a joint framework for event\nextraction, which encodes events as graphs. We represent event triggers and\narguments as nodes in a semantic graph. Event extraction therefore becomes a\ngraph parsing problem, which provides the following advantages: 1) performing\nevent detection and argument extraction jointly; 2) detecting and extracting\nmultiple events from a piece of text; and 3) capturing the complicated\ninteraction between event arguments and triggers. Experimental results on\nACE2005 show that our model is competitive to state-of-the-art systems and has\nsubstantially improved the results on argument extraction. Additionally, we\ncreate two new datasets from ACE2005 where we keep the entire text spans for\nevent arguments, instead of just the head word(s). Our code and models are\nreleased as open-source.",
          "arxiv_id": "2210.08646v1"
        },
        {
          "title": "EventPlus: A Temporal Event Understanding Pipeline",
          "year": "2021-01",
          "abstract": "We present EventPlus, a temporal event understanding pipeline that integrates\nvarious state-of-the-art event understanding components including event trigger\nand type detection, event argument detection, event duration and temporal\nrelation extraction. Event information, especially event temporal knowledge, is\na type of common sense knowledge that helps people understand how stories\nevolve and provides predictive hints for future events. EventPlus as the first\ncomprehensive temporal event understanding pipeline provides a convenient tool\nfor users to quickly obtain annotations about events and their temporal\ninformation for any user-provided document. Furthermore, we show EventPlus can\nbe easily adapted to other domains (e.g., biomedical domain). We make EventPlus\npublicly available to facilitate event-related information extraction and\ndownstream applications.",
          "arxiv_id": "2101.04922v2"
        }
      ],
      "37": [
        {
          "title": "Dataset of Fake News Detection and Fact Verification: A Survey",
          "year": "2021-11",
          "abstract": "The rapid increase in fake news, which causes significant damage to society,\ntriggers many fake news related studies, including the development of fake news\ndetection and fact verification techniques. The resources for these studies are\nmainly available as public datasets taken from Web data. We surveyed 118\ndatasets related to fake news research on a large scale from three\nperspectives: (1) fake news detection, (2) fact verification, and (3) other\ntasks; for example, the analysis of fake news and satire detection. We also\ndescribe in detail their utilization tasks and their characteristics. Finally,\nwe highlight the challenges in the fake news dataset construction and some\nresearch opportunities that address these challenges. Our survey facilitates\nfake news research by helping researchers find suitable datasets without\nreinventing the wheel, and thereby, improves fake news studies in depth.",
          "arxiv_id": "2111.03299v1"
        },
        {
          "title": "Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection",
          "year": "2024-12",
          "abstract": "Social platforms, while facilitating access to information, have also become\nsaturated with a plethora of fake news, resulting in negative consequences.\nAutomatic multimodal fake news detection is a worthwhile pursuit. Existing\nmultimodal fake news datasets only provide binary labels of real or fake.\nHowever, real news is alike, while each fake news is fake in its own way. These\ndatasets fail to reflect the mixed nature of various types of multimodal fake\nnews. To bridge the gap, we construct an attributing multi-granularity\nmultimodal fake news detection dataset \\amg, revealing the inherent fake\npattern. Furthermore, we propose a multi-granularity clue alignment model \\our\nto achieve multimodal fake news detection and attribution. Experimental results\ndemonstrate that \\amg is a challenging dataset, and its attribution setting\nopens up new avenues for future research.",
          "arxiv_id": "2412.14686v1"
        },
        {
          "title": "Fake News Detection and Behavioral Analysis: Case of COVID-19",
          "year": "2023-05",
          "abstract": "While the world has been combating COVID-19 for over three years, an ongoing\n\"Infodemic\" due to the spread of fake news regarding the pandemic has also been\na global issue. The existence of the fake news impact different aspect of our\ndaily lives, including politics, public health, economic activities, etc.\nReaders could mistake fake news for real news, and consequently have less\naccess to authentic information. This phenomenon will likely cause confusion of\ncitizens and conflicts in society. Currently, there are major challenges in\nfake news research. It is challenging to accurately identify fake news data in\nsocial media posts. In-time human identification is infeasible as the amount of\nthe fake news data is overwhelming. Besides, topics discussed in fake news are\nhard to identify due to their similarity to real news. The goal of this paper\nis to identify fake news on social media to help stop the spread. We present\nDeep Learning approaches and an ensemble approach for fake news detection. Our\ndetection models achieved higher accuracy than previous studies. The ensemble\napproach further improved the detection performance. We discovered feature\ndifferences between fake news and real news items. When we added them into the\nsentence embeddings, we found that they affected the model performance. We\napplied a hybrid method and built models for recognizing topics from posts. We\nfound half of the identified topics were overlapping in fake news and real\nnews, which could increase confusion in the population.",
          "arxiv_id": "2305.16057v1"
        }
      ],
      "38": [
        {
          "title": "RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict",
          "year": "2024-03",
          "abstract": "Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.",
          "arxiv_id": "2403.16662v2"
        },
        {
          "title": "Evidence-based Interpretable Open-domain Fact-checking with Large Language Models",
          "year": "2023-12",
          "abstract": "Universal fact-checking systems for real-world claims face significant\nchallenges in gathering valid and sufficient real-time evidence and making\nreasoned decisions. In this work, we introduce the Open-domain Explainable\nFact-checking (OE-Fact) system for claim-checking in real-world scenarios. The\nOE-Fact system can leverage the powerful understanding and reasoning\ncapabilities of large language models (LLMs) to validate claims and generate\ncausal explanations for fact-checking decisions. To adapt the traditional\nthree-module fact-checking framework to the open domain setting, we first\nretrieve claim-related information as relevant evidence from open websites.\nAfter that, we retain the evidence relevant to the claim through LLM and\nsimilarity calculation for subsequent verification. We evaluate the performance\nof our adapted three-module OE-Fact system on the Fact Extraction and\nVerification (FEVER) dataset. Experimental results show that our OE-Fact system\noutperforms general fact-checking baseline systems in both closed- and\nopen-domain scenarios, ensuring stable and accurate verdicts while providing\nconcise and convincing real-time explanations for fact-checking decisions.",
          "arxiv_id": "2312.05834v1"
        },
        {
          "title": "Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation",
          "year": "2022-10",
          "abstract": "Misinformation emerges in times of uncertainty when credible information is\nlimited. This is challenging for NLP-based fact-checking as it relies on\ncounter-evidence, which may not yet be available. Despite increasing interest\nin automatic fact-checking, it is still unclear if automated approaches can\nrealistically refute harmful real-world misinformation. Here, we contrast and\ncompare NLP fact-checking with how professional fact-checkers combat\nmisinformation in the absence of counter-evidence. In our analysis, we show\nthat, by design, existing NLP task definitions for fact-checking cannot refute\nmisinformation as professional fact-checkers do for the majority of claims. We\nthen define two requirements that the evidence in datasets must fulfill for\nrealistic fact-checking: It must be (1) sufficient to refute the claim and (2)\nnot leaked from existing fact-checking articles. We survey existing\nfact-checking datasets and find that all of them fail to satisfy both criteria.\nFinally, we perform experiments to demonstrate that models trained on a\nlarge-scale fact-checking dataset rely on leaked evidence, which makes them\nunsuitable in real-world scenarios. Taken together, we show that current NLP\nfact-checking cannot realistically combat real-world misinformation because it\ndepends on unrealistic assumptions about counter-evidence in the data.",
          "arxiv_id": "2210.13865v1"
        }
      ],
      "39": [
        {
          "title": "GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval",
          "year": "2023-10",
          "abstract": "Given a query and a document corpus, the information retrieval (IR) task is\nto output a ranked list of relevant documents. Combining large language models\n(LLMs) with embedding-based retrieval models, recent work shows promising\nresults on the zero-shot retrieval problem, i.e., no access to labeled data\nfrom the target domain. Two such popular paradigms are generation-augmented\nretrieval or GAR (generate additional context for the query and then retrieve),\nand retrieval-augmented generation or RAG (retrieve relevant documents as\ncontext and then generate answers). The success of these paradigms hinges on\n(i) high-recall retrieval models, which are difficult to obtain in the\nzero-shot setting, and (ii) high-precision (re-)ranking models which typically\nneed a good initialization. In this work, we propose a novel GAR-meets-RAG\nrecurrence formulation that overcomes the challenges of existing paradigms. Our\nmethod iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in\nthe zero-shot setting. A key design principle is that the rewrite-retrieval\nstages improve the recall of the system and a final re-ranking stage improves\nthe precision. We conduct extensive experiments on zero-shot passage retrieval\nbenchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in\nthe BEIR benchmark, outperforming previous best results in Recall@100 and\nnDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the\nprevious best.",
          "arxiv_id": "2310.20158v1"
        },
        {
          "title": "CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion",
          "year": "2022-12",
          "abstract": "The dual-encoder has become the de facto architecture for dense retrieval.\nTypically, it computes the latent representations of the query and document\nindependently, thus failing to fully capture the interactions between the query\nand document. To alleviate this, recent research has focused on obtaining\nquery-informed document representations. During training, it expands the\ndocument with a real query, but during inference, it replaces the real query\nwith a generated one. This inconsistency between training and inference causes\nthe dense retrieval model to prioritize query information while disregarding\nthe document when computing the document representation. Consequently, it\nperforms even worse than the vanilla dense retrieval model because its\nperformance heavily relies on the relevance between the generated queries and\nthe real query.In this paper, we propose a curriculum sampling strategy that\nutilizes pseudo queries during training and progressively enhances the\nrelevance between the generated query and the real query. By doing so, the\nretrieval model learns to extend its attention from the document alone to both\nthe document and query, resulting in high-quality query-informed document\nrepresentations. Experimental results on both in-domain and out-of-domain\ndatasets demonstrate that our approach outperforms previous dense retrieval\nmodels.",
          "arxiv_id": "2212.09114v2"
        },
        {
          "title": "Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval",
          "year": "2023-11",
          "abstract": "There has been limited success for dense retrieval models in multilingual\nretrieval, due to uneven and scarce training data available across multiple\nlanguages. Synthetic training data generation is promising (e.g., InPars or\nPromptagator), but has been investigated only for English. Therefore, to study\nmodel capabilities across both cross-lingual and monolingual retrieval tasks,\nwe develop SWIM-IR, a synthetic retrieval training dataset containing 33 (high\nto very-low resource) languages for fine-tuning multilingual dense retrievers\nwithout requiring any human supervision. To construct SWIM-IR, we propose SAP\n(summarize-then-ask prompting), where the large language model (LLM) generates\na textual summary prior to the query generation step. SAP assists the LLM in\ngenerating informative queries in the target language. Using SWIM-IR, we\nexplore synthetic fine-tuning of multilingual dense retrieval models and\nevaluate them robustly on three retrieval benchmarks: XOR-Retrieve\n(cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual). Our\nmodels, called SWIM-X, are competitive with human-supervised dense retrieval\nmodels, e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for\nexpensive human-labeled retrieval training data. SWIM-IR dataset and SWIM-X\nmodels are available at https://github.com/google-research-datasets/SWIM-IR.",
          "arxiv_id": "2311.05800v2"
        }
      ],
      "40": [
        {
          "title": "Whose story is it? Personalizing story generation by inferring author styles",
          "year": "2025-02",
          "abstract": "Personalization is critical for improving user experience in interactive\nwriting and educational applications, yet remains understudied in story\ngeneration. We study the task of personalizing story generation, where our goal\nis to mimic an author's writing style, given other stories written by them. We\ncollect Mythos, a dataset of 3.6k stories from 112 authors, with an average of\n16 stories per author, across five distinct sources reflecting diverse\nstory-writing settings. We propose a two-stage pipeline for personalized story\ngeneration: first, we infer authors' implicit writing characteristics and\norganize them into an Author Writing Sheet, which is validated by humans to be\nof high quality; second, we simulate the author's persona using tailored\npersona descriptions and personalized story rules. We find that stories\npersonalized using the Author Writing Sheet outperform a non-personalized\nbaseline, achieving a 78% win-rate in capturing authors' past style and 59% in\nsimilarity to ground-truth author stories. Human evaluation supports these\nfindings and further highlights trends, such as Reddit stories being easier to\npersonalize, and the Creativity and Language Use aspects of stories being\neasier to personalize than the Plot.",
          "arxiv_id": "2502.13028v2"
        },
        {
          "title": "A Character-Centric Creative Story Generation via Imagination",
          "year": "2024-09",
          "abstract": "Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/",
          "arxiv_id": "2409.16667v3"
        },
        {
          "title": "StoryWriter: A Multi-Agent Framework for Long Story Generation",
          "year": "2025-06",
          "abstract": "Long story generation remains a challenge for existing large language models\n(LLMs), primarily due to two main factors: (1) discourse coherence, which\nrequires plot consistency, logical coherence, and completeness in the long-form\ngeneration, and (2) narrative complexity, which requires an interwoven and\nengaging narrative. To address these challenges, we propose StoryWriter, a\nmulti-agent story generation framework, which consists of three main modules:\n(1) outline agent, which generates event-based outlines containing rich event\nplots, character, and event-event relationships. (2) planning agent, which\nfurther details events and plans which events should be written in each chapter\nto maintain an interwoven and engaging story. (3) writing agent, which\ndynamically compresses the story history based on the current event to generate\nand reflect new plots, ensuring the coherence of the generated story. We\nconduct both human and automated evaluation, and StoryWriter significantly\noutperforms existing story generation baselines in both story quality and\nlength. Furthermore, we use StoryWriter to generate a dataset, which contains\nabout $6,000$ high-quality long stories, with an average length of $8,000$\nwords. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning\non LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which\ndemonstrates advanced performance in long story generation.",
          "arxiv_id": "2506.16445v1"
        }
      ],
      "41": [
        {
          "title": "MatTools: Benchmarking Large Language Models for Materials Science Tools",
          "year": "2025-05",
          "abstract": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research.",
          "arxiv_id": "2505.10852v1"
        },
        {
          "title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation",
          "year": "2025-03",
          "abstract": "Revolutionizing drug discovery demands more than just understanding molecular\ninteractions - it requires generative models that can design novel ligands\ntailored to specific biological targets. While chemical Language Models (cLMs)\nhave made strides in learning molecular properties, most fail to incorporate\ntarget-specific insights, restricting their ability to drive de-novo ligand\ngeneration. Chem42, a cutting-edge family of generative chemical Language\nModels, is designed to bridge this gap. By integrating atomic-level\ninteractions with multimodal inputs from Prot42, a complementary protein\nLanguage Model, Chem42 achieves a sophisticated cross-modal representation of\nmolecular structures, interactions, and binding patterns. This innovative\nframework enables the creation of structurally valid, synthetically accessible\nligands with enhanced target specificity. Evaluations across diverse protein\ntargets confirm that Chem42 surpasses existing approaches in chemical validity,\ntarget-aware design, and predicted binding affinity. By reducing the search\nspace of viable drug candidates, Chem42 could accelerate the drug discovery\npipeline, offering a powerful generative AI tool for precision medicine. Our\nChem42 models set a new benchmark in molecule property prediction, conditional\nmolecule generation, and target-aware ligand design. The models are publicly\navailable at huggingface.co/inceptionai.",
          "arxiv_id": "2503.16563v2"
        },
        {
          "title": "Computational Protein Science in the Era of Large Language Models (LLMs)",
          "year": "2025-01",
          "abstract": "Considering the significance of proteins, computational protein science has\nalways been a critical scientific field, dedicated to revealing knowledge and\ndeveloping applications within the protein sequence-structure-function\nparadigm. In the last few decades, Artificial Intelligence (AI) has made\nsignificant impacts in computational protein science, leading to notable\nsuccesses in specific protein modeling tasks. However, those previous AI models\nstill meet limitations, such as the difficulty in comprehending the semantics\nof protein sequences, and the inability to generalize across a wide range of\nprotein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to\ntheir unprecedented language processing & generalization capability. They can\npromote comprehensive progress in fields rather than solving individual tasks.\nAs a result, researchers have actively introduced LLM techniques in\ncomputational protein science, developing protein Language Models (pLMs) that\nskillfully grasp the foundational knowledge of proteins and can be effectively\ngeneralized to solve a diversity of sequence-structure-function reasoning\nproblems. While witnessing prosperous developments, it's necessary to present a\nsystematic overview of computational protein science empowered by LLM\ntechniques. First, we summarize existing pLMs into categories based on their\nmastered protein knowledge, i.e., underlying sequence patterns, explicit\nstructural and functional information, and external scientific languages.\nSecond, we introduce the utilization and adaptation of pLMs, highlighting their\nremarkable achievements in promoting protein structure prediction, protein\nfunction prediction, and protein design studies. Then, we describe the\npractical application of pLMs in antibody design, enzyme design, and drug\ndiscovery. Finally, we specifically discuss the promising future directions in\nthis fast-growing field.",
          "arxiv_id": "2501.10282v2"
        }
      ],
      "42": [
        {
          "title": "SimpLex: a lexical text simplification architecture",
          "year": "2023-04",
          "abstract": "Text simplification (TS) is the process of generating easy-to-understand\nsentences from a given sentence or piece of text. The aim of TS is to reduce\nboth the lexical (which refers to vocabulary complexity and meaning) and\nsyntactic (which refers to the sentence structure) complexity of a given text\nor sentence without the loss of meaning or nuance. In this paper, we present\n\\textsc{SimpLex}, a novel simplification architecture for generating simplified\nEnglish sentences. To generate a simplified sentence, the proposed architecture\nuses either word embeddings (i.e., Word2Vec) and perplexity, or sentence\ntransformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The\nsolution is incorporated into a user-friendly and simple-to-use software. We\nevaluate our system using two metrics, i.e., SARI, and Perplexity Decrease.\nExperimentally, we observe that the transformer models outperform the other\nmodels in terms of the SARI score. However, in terms of Perplexity, the\nWord-Embeddings-based models achieve the biggest decrease. Thus, the main\ncontributions of this paper are: (1) We propose a new Word Embedding and\nTransformer based algorithm for text simplification; (2) We design\n\\textsc{SimpLex} -- a modular novel text simplification system -- that can\nprovide a baseline for further research; and (3) We perform an in-depth\nanalysis of our solution and compare our results with two state-of-the-art\nmodels, i.e., LightLS [19] and NTS-w2v [44]. We also make the code publicly\navailable online.",
          "arxiv_id": "2304.07002v1"
        },
        {
          "title": "Elaborative Simplification: Content Addition and Explanation Generation in Text Simplification",
          "year": "2020-10",
          "abstract": "Much of modern-day text simplification research focuses on sentence-level\nsimplification, transforming original, more complex sentences into simplified\nversions. However, adding content can often be useful when difficult concepts\nand reasoning need to be explained. In this work, we present the first\ndata-driven study of content addition in text simplification, which we call\nelaborative simplification. We introduce a new annotated dataset of 1.3K\ninstances of elaborative simplification in the Newsela corpus, and analyze how\nentities, ideas, and concepts are elaborated through the lens of contextual\nspecificity. We establish baselines for elaboration generation using\nlarge-scale pre-trained language models, and demonstrate that considering\ncontextual specificity during generation can improve performance. Our results\nillustrate the complexities of elaborative simplification, suggesting many\ninteresting directions for future work.",
          "arxiv_id": "2010.10035v3"
        },
        {
          "title": "MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases",
          "year": "2020-05",
          "abstract": "Progress in sentence simplification has been hindered by a lack of labeled\nparallel simplification data, particularly in languages other than English. We\nintroduce MUSS, a Multilingual Unsupervised Sentence Simplification system that\ndoes not require labeled simplification data. MUSS uses a novel approach to\nsentence simplification that trains strong models using sentence-level\nparaphrase data instead of proper simplification data. These models leverage\nunsupervised pretraining and controllable generation mechanisms to flexibly\nadjust attributes such as length and lexical complexity at inference time. We\nfurther present a method to mine such paraphrase data in any language from\nCommon Crawl using semantic sentence embeddings, thus removing the need for\nlabeled data. We evaluate our approach on English, French, and Spanish\nsimplification benchmarks and closely match or outperform the previous best\nsupervised results, despite not using any labeled simplification data. We push\nthe state of the art further by incorporating labeled simplification data.",
          "arxiv_id": "2005.00352v2"
        }
      ],
      "43": [
        {
          "title": "Testing of Detection Tools for AI-Generated Text",
          "year": "2023-06",
          "abstract": "Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AI-generated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AI-generated\ntext. Furthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.",
          "arxiv_id": "2306.15666v2"
        },
        {
          "title": "Who Said That? Benchmarking Social Media AI Detection",
          "year": "2023-10",
          "abstract": "AI-generated text has proliferated across various online platforms, offering\nboth transformative prospects and posing significant risks related to\nmisinformation and manipulation. Addressing these challenges, this paper\nintroduces SAID (Social media AI Detection), a novel benchmark developed to\nassess AI-text detection models' capabilities in real social media platforms.\nIt incorporates real AI-generate text from popular social media platforms like\nZhihu and Quora. Unlike existing benchmarks, SAID deals with content that\nreflects the sophisticated strategies employed by real AI users on the Internet\nwhich may evade detection or gain visibility, providing a more realistic and\nchallenging evaluation landscape. A notable finding of our study, based on the\nZhihu dataset, reveals that annotators can distinguish between AI-generated and\nhuman-generated texts with an average accuracy rate of 96.5%. This finding\nnecessitates a re-evaluation of human capability in recognizing AI-generated\ntext in today's widely AI-influenced environment. Furthermore, we present a new\nuser-oriented AI-text detection challenge focusing on the practicality and\neffectiveness of identifying AI-generated text based on user information and\nmultiple responses. The experimental results demonstrate that conducting\ndetection tasks on actual social media platforms proves to be more challenging\ncompared to traditional simulated AI-text detection, resulting in a decreased\naccuracy. On the other hand, user-oriented AI-generated text detection\nsignificantly improve the accuracy of detection.",
          "arxiv_id": "2310.08240v1"
        },
        {
          "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
          "year": "2023-10",
          "abstract": "Large Language Models (LLMs) have revolutionized the domain of natural\nlanguage processing (NLP) with remarkable capabilities of generating human-like\ntext responses. However, despite these advancements, several works in the\nexisting literature have raised serious concerns about the potential misuse of\nLLMs such as spreading misinformation, generating fake news, plagiarism in\nacademia, and contaminating the web. To address these concerns, a consensus\namong the research community is to develop algorithmic solutions to detect\nAI-generated text. The basic idea is that whenever we can tell if the given\ntext is either written by a human or an AI, we can utilize this information to\naddress the above-mentioned concerns. To that end, a plethora of detection\nframeworks have been proposed, highlighting the possibilities of AI-generated\ntext detection. But in parallel to the development of detection frameworks,\nresearchers have also concentrated on designing strategies to elude detection,\ni.e., focusing on the impossibilities of AI-generated text detection. This is a\ncrucial step in order to make sure the detection frameworks are robust enough\nand it is not too easy to fool a detector. Despite the huge interest and the\nflurry of research in this domain, the community currently lacks a\ncomprehensive analysis of recent developments. In this survey, we aim to\nprovide a concise categorization and overview of current work encompassing both\nthe prospects and the limitations of AI-generated text detection. To enrich the\ncollective knowledge, we engage in an exhaustive discussion on critical and\nchallenging open questions related to ongoing research on AI-generated text\ndetection.",
          "arxiv_id": "2310.15264v1"
        }
      ],
      "44": [
        {
          "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification",
          "year": "2022-05",
          "abstract": "Weakly supervised text classification methods typically train a deep neural\nclassifier based on pseudo-labels. The quality of pseudo-labels is crucial to\nfinal performance but they are inevitably noisy due to their heuristic nature,\nso selecting the correct ones has a huge potential for performance boost. One\nstraightforward solution is to select samples based on the softmax probability\nscores in the neural classifier corresponding to their pseudo-labels. However,\nwe show through our experiments that such solutions are ineffective and\nunstable due to the erroneously high-confidence predictions from poorly\ncalibrated models. Recent studies on the memorization effects of deep neural\nmodels suggest that these models first memorize training samples with clean\nlabels and then those with noisy labels. Inspired by this observation, we\npropose a novel pseudo-label selection method LOPS that takes learning order of\nsamples into consideration. We hypothesize that the learning order reflects the\nprobability of wrong annotation in terms of ranking, and therefore, propose to\nselect the samples that are learnt earlier. LOPS can be viewed as a strong\nperformance-boost plug-in to most of existing weakly-supervised text\nclassification methods, as confirmed in extensive experiments on four\nreal-world datasets.",
          "arxiv_id": "2205.12528v2"
        },
        {
          "title": "LIME: Weakly-Supervised Text Classification Without Seeds",
          "year": "2022-10",
          "abstract": "In weakly-supervised text classification, only label names act as sources of\nsupervision. Predominant approaches to weakly-supervised text classification\nutilize a two-phase framework, where test samples are first assigned\npseudo-labels and are then used to train a neural text classifier. In most\nprevious work, the pseudo-labeling step is dependent on obtaining seed words\nthat best capture the relevance of each class label. We present LIME, a\nframework for weakly-supervised text classification that entirely replaces the\nbrittle seed-word generation process with entailment-based\npseudo-classification. We find that combining weakly-supervised classification\nand textual entailment mitigates shortcomings of both, resulting in a more\nstreamlined and effective classification pipeline. With just an off-the-shelf\ntextual entailment model, LIME outperforms recent baselines in\nweakly-supervised text classification and achieves state-of-the-art in 4\nbenchmarks. We open source our code at https://github.com/seongminp/LIME.",
          "arxiv_id": "2210.06720v1"
        },
        {
          "title": "XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision",
          "year": "2023-10",
          "abstract": "Text classification aims to effectively categorize documents into pre-defined\ncategories. Traditional methods for text classification often rely on large\namounts of manually annotated training data, making the process time-consuming\nand labor-intensive. To address this issue, recent studies have focused on\nweakly-supervised and extremely weakly-supervised settings, which require\nminimal or no human annotation, respectively. In previous methods of weakly\nsupervised text classification, pseudo-training data is generated by assigning\npseudo-labels to documents based on their alignment (e.g., keyword matching)\nwith specific classes. However, these methods ignore the importance of\nincorporating the explanations of the generated pseudo-labels, or saliency of\nindividual words, as additional guidance during the text classification\ntraining process. To address this limitation, we propose XAI-CLASS, a novel\nexplanation-enhanced extremely weakly-supervised text classification method\nthat incorporates word saliency prediction as an auxiliary task. XAI-CLASS\nbegins by employing a multi-round question-answering process to generate\npseudo-training data that promotes the mutual enhancement of class labels and\ncorresponding explanation word generation. This pseudo-training data is then\nused to train a multi-task framework that simultaneously learns both text\nclassification and word saliency prediction. Extensive experiments on several\nweakly-supervised text classification datasets show that XAI-CLASS outperforms\nother weakly-supervised text classification methods significantly. Moreover,\nexperiments demonstrate that XAI-CLASS enhances both model performance and\nexplainability.",
          "arxiv_id": "2311.00189v1"
        }
      ],
      "45": [
        {
          "title": "A Survey on Data Selection for LLM Instruction Tuning",
          "year": "2024-02",
          "abstract": "Instruction tuning is a vital step of training large language models (LLMs),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLMs. Therefore,\nrecently a lot of studies focus on exploring the methods of selecting\nhigh-quality subset from instruction datasets, aiming to reduce training costs\nand enhance the instruction-following capabilities of LLMs. This paper presents\na comprehensive survey on data selection for LLM instruction tuning. Firstly,\nwe introduce the wildly used instruction datasets. Then, we propose a new\ntaxonomy of the data selection methods and provide a detailed introduction of\nrecent advances, and the evaluation strategies and results of data selection\nmethods are also elaborated in detail. Finally, we emphasize the open\nchallenges and present new frontiers of this task.",
          "arxiv_id": "2402.05123v3"
        },
        {
          "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
          "year": "2023-05",
          "abstract": "Instruction tuning has emerged to enhance the capabilities of large language\nmodels (LLMs) to comprehend instructions and generate appropriate responses.\nExisting methods either manually annotate or employ LLM (e.g., GPT-series) to\ngenerate data for instruction tuning. However, they often overlook associating\ninstructions with existing annotated datasets. In this paper, we propose\nDynosaur, a dynamic growth paradigm for the automatic curation of\ninstruction-tuning data. Based on the metadata of existing datasets, we use\nLLMs to automatically construct instruction-tuning data by identifying relevant\ndata fields and generating appropriate instructions.\n  By leveraging the existing annotated datasets, Dynosaur offers several\nadvantages: 1) it reduces the API cost for generating instructions (e.g., it\ncosts less than $12 USD by calling GPT-3.5-turbo for generating 800K\ninstruction tuning samples; 2) it provides high-quality data for instruction\ntuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform\nwith comparable data sizes); and 3) it supports the continuous improvement of\nmodels by generating instruction-tuning data when a new annotated dataset\nbecomes available. We further investigate a continual learning scheme for\nlearning with the ever-growing instruction-tuning dataset, and demonstrate that\nreplaying tasks with diverse instruction embeddings not only helps mitigate\nforgetting issues but generalizes to unseen tasks better.\n  Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
          "arxiv_id": "2305.14327v2"
        },
        {
          "title": "Instruction Following without Instruction Tuning",
          "year": "2024-09",
          "abstract": "Instruction tuning commonly means finetuning a language model on\ninstruction-response pairs. We discover two forms of adaptation (tuning) that\nare deficient compared to instruction tuning, yet still yield instruction\nfollowing; we call this implicit instruction tuning. We first find that\ninstruction-response pairs are not necessary: training solely on responses,\nwithout any corresponding instructions, yields instruction following. This\nsuggests pretrained models have an instruction-response mapping which is\nrevealed by teaching the model the desired distribution of responses. However,\nwe then find it's not necessary to teach the desired distribution of responses:\ninstruction-response training on narrow-domain data like poetry still leads to\nbroad instruction-following behavior like recipe generation. In particular,\nwhen instructions are very different from those in the narrow finetuning\ndomain, models' responses do not adhere to the style of the finetuning domain.\nTo begin to explain implicit instruction tuning, we hypothesize that very\nsimple changes to a language model's distribution yield instruction following.\nWe support this by hand-writing a rule-based language model which yields\ninstruction following in a product-of-experts with a pretrained model. The\nrules are to slowly increase the probability of ending the sequence, penalize\nrepetition, and uniformly change 15 words' probabilities. In summary,\nadaptations made without being designed to yield instruction following can do\nso implicitly.",
          "arxiv_id": "2409.14254v1"
        }
      ],
      "46": [
        {
          "title": "Sim-to-Real Transfer for Vision-and-Language Navigation",
          "year": "2020-11",
          "abstract": "We study the challenging problem of releasing a robot in a previously unseen\nenvironment, and having it follow unconstrained natural language navigation\ninstructions. Recent work on the task of Vision-and-Language Navigation (VLN)\nhas achieved significant progress in simulation. To assess the implications of\nthis work for robotics, we transfer a VLN agent trained in simulation to a\nphysical robot. To bridge the gap between the high-level discrete action space\nlearned by the VLN agent, and the robot's low-level continuous action space, we\npropose a subgoal model to identify nearby waypoints, and use domain\nrandomization to mitigate visual domain differences. For accurate sim and real\ncomparisons in parallel environments, we annotate a 325m2 office space with\n1.3km of navigation instructions, and create a digitized replica in simulation.\nWe find that sim-to-real transfer to an environment not seen in training is\nsuccessful if an occupancy map and navigation graph can be collected and\nannotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more\nchallenging in the hardest setting with no prior mapping at all (success rate\nof 22.5%).",
          "arxiv_id": "2011.03807v1"
        },
        {
          "title": "InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment",
          "year": "2024-06",
          "abstract": "Enabling robots to navigate following diverse language instructions in\nunexplored environments is an attractive goal for human-robot interaction.\nHowever, this goal is challenging because different navigation tasks require\ndifferent strategies. The scarcity of instruction navigation data hinders\ntraining an instruction navigation model with varied strategies. Therefore,\nprevious methods are all constrained to one specific type of navigation\ninstruction. In this work, we propose InstructNav, a generic instruction\nnavigation system. InstructNav makes the first endeavor to handle various\ninstruction navigation tasks without any navigation training or pre-built maps.\nTo reach this goal, we introduce Dynamic Chain-of-Navigation (DCoN) to unify\nthe planning process for different types of navigation instructions.\nFurthermore, we propose Multi-sourced Value Maps to model key elements in\ninstruction navigation so that linguistic DCoN planning can be converted into\nrobot actionable trajectories. With InstructNav, we complete the R2R-CE task in\na zero-shot way for the first time and outperform many task-training methods.\nBesides, InstructNav also surpasses the previous SOTA method by 10.48% on the\nzero-shot Habitat ObjNav and by 86.34% on demand-driven navigation DDN. Real\nrobot experiments on diverse indoor scenes further demonstrate our method's\nrobustness in coping with the environment and instruction variations.",
          "arxiv_id": "2406.04882v1"
        },
        {
          "title": "ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments",
          "year": "2023-04",
          "abstract": "Vision-language navigation is a task that requires an agent to follow\ninstructions to navigate in environments. It becomes increasingly crucial in\nthe field of embodied AI, with potential applications in autonomous navigation,\nsearch and rescue, and human-robot interaction. In this paper, we propose to\naddress a more practical yet challenging counterpart setting - vision-language\nnavigation in continuous environments (VLN-CE). To develop a robust VLN-CE\nagent, we propose a new navigation framework, ETPNav, which focuses on two\ncritical skills: 1) the capability to abstract environments and generate\nlong-range navigation plans, and 2) the ability of obstacle-avoiding control in\ncontinuous environments. ETPNav performs online topological mapping of\nenvironments by self-organizing predicted waypoints along a traversed path,\nwithout prior environmental experience. It privileges the agent to break down\nthe navigation procedure into high-level planning and low-level control.\nConcurrently, ETPNav utilizes a transformer-based cross-modal planner to\ngenerate navigation plans based on topological maps and instructions. The plan\nis then performed through an obstacle-avoiding controller that leverages a\ntrial-and-error heuristic to prevent navigation from getting stuck in\nobstacles. Experimental results demonstrate the effectiveness of the proposed\nmethod. ETPNav yields more than 10% and 20% improvements over prior\nstate-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is\navailable at https://github.com/MarSaKi/ETPNav.",
          "arxiv_id": "2304.03047v3"
        }
      ],
      "47": [
        {
          "title": "Kindergarden quantum mechanics graduates (...or how I learned to stop gluing LEGO together and love the ZX-calculus)",
          "year": "2021-02",
          "abstract": "This paper is a `spiritual child' of the 2005 lecture notes Kindergarten\nQuantum Mechanics, which showed how a simple, pictorial extension of Dirac\nnotation allowed several quantum features to be easily expressed and derived,\nusing language even a kindergartner can understand. Central to that approach\nwas the use of pictures and pictorial transformation rules to understand and\nderive features of quantum theory and computation. However, this approach left\nmany wondering `where's the beef?' In other words, was this new approach\ncapable of producing new results, or was it simply an aesthetically pleasing\nway to restate stuff we already know?\n  The aim of this sequel paper is to say `here's the beef!', and highlight some\nof the major results of the approach advocated in Kindergarten Quantum\nMechanics, and how they are being applied to tackle practical problems on real\nquantum computers. We will focus mainly on what has become the Swiss army knife\nof the pictorial formalism: the ZX-calculus. First we look at some of the ideas\nbehind the ZX-calculus, comparing and contrasting it with the usual quantum\ncircuit formalism. We then survey results from the past 2 years falling into\nthree categories: (1) completeness of the rules of the ZX-calculus, (2)\nstate-of-the-art quantum circuit optimisation results in commercial and\nopen-source quantum compilers relying on ZX, and (3) the use of ZX in\ntranslating real-world stuff like natural language into quantum circuits that\ncan be run on today's (very limited) quantum hardware.\n  We also take the title literally, and outline an ongoing experiment aiming to\nshow that ZX-calculus enables children to do cutting-edge quantum computing\nstuff. If anything, this would truly confirm that `kindergarten quantum\nmechanics' wasn't just a joke.",
          "arxiv_id": "2102.10984v1"
        },
        {
          "title": "Grammar-aware sentence classification on quantum computers",
          "year": "2020-12",
          "abstract": "Natural language processing (NLP) is at the forefront of great advances in\ncontemporary AI, and it is arguably one of the most challenging areas of the\nfield. At the same time, in the area of Quantum Computing (QC), with the steady\ngrowth of quantum hardware and notable improvements towards implementations of\nquantum algorithms, we are approaching an era when quantum computers perform\ntasks that cannot be done on classical computers with a reasonable amount of\nresources. This provides a new range of opportunities for AI, and for NLP\nspecifically. In this work, we work with the Categorical Distributional\nCompositional (DisCoCat) model of natural language meaning, whose underlying\nmathematical underpinnings make it amenable to quantum instantiations. Earlier\nwork on fault-tolerant quantum algorithms has already demonstrated potential\nquantum advantage for NLP, notably employing DisCoCat. In this work, we focus\non the capabilities of noisy intermediate-scale quantum (NISQ) hardware and\nperform the first implementation of an NLP task on a NISQ processor, using the\nDisCoCat framework. Sentences are instantiated as parameterised quantum\ncircuits; word-meanings are embedded in quantum states using parameterised\nquantum-circuits and the sentence's grammatical structure faithfully manifests\nas a pattern of entangling operations which compose the word-circuits into a\nsentence-circuit. The circuits' parameters are trained using a classical\noptimiser in a supervised NLP task of binary classification. Our novel QNLP\nmodel shows concrete promise for scalability as the quality of the quantum\nhardware improves in the near future and solidifies a novel branch of\nexperimental research at the intersection of QC and AI.",
          "arxiv_id": "2012.03756v2"
        },
        {
          "title": "Foundations for Near-Term Quantum Natural Language Processing",
          "year": "2020-12",
          "abstract": "We provide conceptual and mathematical foundations for near-term quantum\nnatural language processing (QNLP), and do so in quantum computer scientist\nfriendly terms. We opted for an expository presentation style, and provide\nreferences for supporting empirical evidence and formal statements concerning\nmathematical generality.\n  We recall how the quantum model for natural language that we employ\ncanonically combines linguistic meanings with rich linguistic structure, most\nnotably grammar. In particular, the fact that it takes a quantum-like model to\ncombine meaning and structure, establishes QNLP as quantum-native, on par with\nsimulation of quantum systems. Moreover, the now leading Noisy\nIntermediate-Scale Quantum (NISQ) paradigm for encoding classical data on\nquantum hardware, variational quantum circuits, makes NISQ exceptionally\nQNLP-friendly: linguistic structure can be encoded as a free lunch, in contrast\nto the apparently exponentially expensive classical encoding of grammar.\n  Quantum speed-up for QNLP tasks has already been established in previous work\nwith Will Zeng. Here we provide a broader range of tasks which all enjoy the\nsame advantage.\n  Diagrammatic reasoning is at the heart of QNLP. Firstly, the quantum model\ninterprets language as quantum processes via the diagrammatic formalism of\ncategorical quantum mechanics. Secondly, these diagrams are via ZX-calculus\ntranslated into quantum circuits. Parameterisations of meanings then become the\ncircuit variables to be learned.\n  Our encoding of linguistic structure within quantum circuits also embodies a\nnovel approach for establishing word-meanings that goes beyond the current\nstandards in mainstream AI, by placing linguistic structure at the heart of\nWittgenstein's meaning-is-context.",
          "arxiv_id": "2012.03755v1"
        }
      ],
      "48": [
        {
          "title": "The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?",
          "year": "2024-12",
          "abstract": "The pursuit of leaderboard rankings in Large Language Models (LLMs) has\ncreated a fundamental paradox: models excel at standardized tests while failing\nto demonstrate genuine language understanding and adaptability. Our systematic\nanalysis of NLP evaluation frameworks reveals pervasive vulnerabilities across\nthe evaluation spectrum, from basic metrics to complex benchmarks like GLUE and\nMMLU. These vulnerabilities manifest through benchmark exploitation, dataset\ncontamination, and evaluation bias, creating a false perception of progress in\nlanguage understanding capabilities. Through extensive review of contemporary\nevaluation approaches, we identify significant limitations in static benchmark\ndesigns, human evaluation protocols, and LLM-as-judge frameworks, all of which\ncompromise the reliability of current performance assessments. As LLM\ncapabilities evolve and existing benchmarks become redundant, we lay the\ngroundwork for new evaluation methods that resist manipulation, minimize data\ncontamination, and assess domain-specific tasks. This requires frameworks that\nare adapted dynamically, addressing current limitations and providing a more\naccurate reflection of LLM performance.",
          "arxiv_id": "2412.03597v1"
        },
        {
          "title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
          "year": "2024-04",
          "abstract": "The rapid development of large language model (LLM) evaluation methodologies\nand datasets has led to a profound challenge: integrating state-of-the-art\nevaluation techniques cost-effectively while ensuring reliability,\nreproducibility, and efficiency. Currently, there is a notable absence of a\nunified and adaptable framework that seamlessly integrates various evaluation\napproaches. Moreover, the reliability of evaluation findings is often\nquestionable due to potential data contamination, with the evaluation\nefficiency commonly overlooked when facing the substantial costs associated\nwith LLM inference. In response to these challenges, we introduce FreeEval, a\nmodular and scalable framework crafted to enable trustworthy and efficient\nautomatic evaluations of LLMs. Firstly, FreeEval's unified abstractions\nsimplify the integration and improve the transparency of diverse evaluation\nmethodologies, encompassing dynamic evaluation that demand sophisticated LLM\ninteractions. Secondly, the framework integrates meta-evaluation techniques\nlike human evaluation and data contamination detection, which, along with\ndynamic evaluation modules in the platform, enhance the fairness of the\nevaluation outcomes. Lastly, FreeEval is designed with a high-performance\ninfrastructure, including distributed computation and caching strategies,\nenabling extensive evaluations across multi-node, multi-GPU clusters for\nopen-source and proprietary LLMs.",
          "arxiv_id": "2404.06003v1"
        },
        {
          "title": "Evaluating Scoring Bias in LLM-as-a-Judge",
          "year": "2025-06",
          "abstract": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.",
          "arxiv_id": "2506.22316v3"
        }
      ],
      "49": [
        {
          "title": "Knowledge Graph Enhanced Large Language Model Editing",
          "year": "2024-02",
          "abstract": "Large language models (LLMs) are pivotal in advancing natural language\nprocessing (NLP) tasks, yet their efficacy is hampered by inaccuracies and\noutdated knowledge. Model editing emerges as a promising solution to address\nthese challenges. However, existing editing methods struggle to track and\nincorporate changes in knowledge associated with edits, which limits the\ngeneralization ability of postedit LLMs in processing edited knowledge. To\ntackle these problems, we propose a novel model editing method that leverages\nknowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we\nfirst utilize a knowledge graph augmentation module to uncover associated\nknowledge that has changed due to editing, obtaining its internal\nrepresentations within LLMs. This approach allows knowledge alterations within\nLLMs to be reflected through an external graph structure. Subsequently, we\ndesign a graph-based knowledge edit module to integrate structured knowledge\ninto the model editing. This ensures that the updated parameters reflect not\nonly the modifications of the edited knowledge but also the changes in other\nassociated knowledge resulting from the editing process. Comprehensive\nexperiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME\nsignificantly improves the generalization capabilities of post-edit LLMs in\nemploying edited knowledge.",
          "arxiv_id": "2402.13593v1"
        },
        {
          "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
          "year": "2024-02",
          "abstract": "Model editing aims to precisely alter the behaviors of large language models\n(LLMs) in relation to specific knowledge, while leaving unrelated knowledge\nintact. This approach has proven effective in addressing issues of\nhallucination and outdated information in LLMs. However, the potential of using\nmodel editing to modify knowledge in the medical field remains largely\nunexplored, even though resolving hallucination is a pressing need in this\narea. Our observations indicate that current methods face significant\nchallenges in dealing with specialized and complex knowledge in medical domain.\nTherefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for\nmedical model editing. MedLaSA harnesses the strengths of both adding extra\nparameters and locate-then-edit methods for medical model editing. We utilize\ncausal tracing to identify the association of knowledge in neurons across\ndifferent layers, and generate a corresponding scale set from the association\nvalue for each piece of knowledge. Subsequently, we incorporate scalable\nadapters into the dense layers of LLMs. These adapters are assigned scaling\nvalues based on the corresponding specific knowledge, which allows for the\nadjustment of the adapter's weight and rank. The more similar the content, the\nmore consistent the scale between them. This ensures precise editing of\nsemantically identical knowledge while avoiding impact on unrelated knowledge.\nTo evaluate the editing impact on the behaviours of LLMs, we propose two model\nediting studies for medical domain: (1) editing factual knowledge for medical\nspecialization and (2) editing the explanatory ability for complex knowledge.\nWe build two novel medical benchmarking datasets and introduce a series of\nchallenging and comprehensive metrics. Extensive experiments on medical LLMs\ndemonstrate the editing efficiency of MedLaSA, without affecting unrelated\nknowledge.",
          "arxiv_id": "2402.18099v3"
        },
        {
          "title": "Event-level Knowledge Editing",
          "year": "2024-02",
          "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs)\nto prevent them from becoming outdated. Existing work edits LLMs at the level\nof factual knowledge triplets. However, natural knowledge updates in the real\nworld come from the occurrences of new events rather than direct changes in\nfactual triplets. In this paper, we propose a new task setting: event-level\nknowledge editing, which directly edits new events into LLMs and improves over\nconventional triplet-level editing on (1) Efficiency. A single event edit leads\nto updates in multiple entailed knowledge triplets. (2) Completeness. Beyond\nupdating factual knowledge, event-level editing also requires considering the\nevent influences and updating LLMs' knowledge about future trends. We construct\na high-quality event-level editing benchmark ELKEN, consisting of 1,515 event\nedits, 6,449 questions about factual knowledge, and 10,150 questions about\nfuture tendencies. We systematically evaluate the performance of various\nknowledge editing methods and LLMs on this benchmark. We find that ELKEN poses\nsignificant challenges to existing knowledge editing approaches. Our codes and\ndataset are publicly released to facilitate further research.",
          "arxiv_id": "2402.13093v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T18:48:15Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}