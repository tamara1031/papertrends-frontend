{
  "topics": {
    "data": {
      "0": {
        "name": "0_policy_RL_learning_agent",
        "keywords": [
          [
            "policy",
            0.01370673519054752
          ],
          [
            "RL",
            0.013523304437951182
          ],
          [
            "learning",
            0.013142974418340616
          ],
          [
            "agent",
            0.012765464022017368
          ],
          [
            "agents",
            0.011154964586071046
          ],
          [
            "reinforcement",
            0.010901883811198823
          ],
          [
            "reinforcement learning",
            0.010830928242646616
          ],
          [
            "robot",
            0.010777269856664093
          ],
          [
            "Learning",
            0.0095854017604569
          ],
          [
            "planning",
            0.009202389932176248
          ]
        ],
        "count": 14037
      },
      "1": {
        "name": "1_medical_clinical_data_models",
        "keywords": [
          [
            "medical",
            0.014264137189639148
          ],
          [
            "clinical",
            0.012901161828826047
          ],
          [
            "data",
            0.009002955780207964
          ],
          [
            "models",
            0.007927829407311427
          ],
          [
            "model",
            0.0077200847453068565
          ],
          [
            "patient",
            0.007412200025621448
          ],
          [
            "learning",
            0.007162605493543823
          ],
          [
            "segmentation",
            0.006896571119831053
          ],
          [
            "images",
            0.006699069892142773
          ],
          [
            "performance",
            0.006437438944313627
          ]
        ],
        "count": 10619
      },
      "2": {
        "name": "2_learning_neural_networks_training",
        "keywords": [
          [
            "learning",
            0.011749630331424935
          ],
          [
            "neural",
            0.011369239489834745
          ],
          [
            "networks",
            0.010484079041133191
          ],
          [
            "training",
            0.010143566356114804
          ],
          [
            "network",
            0.008664102959021314
          ],
          [
            "data",
            0.008442147094295397
          ],
          [
            "neural networks",
            0.008054537752924862
          ],
          [
            "performance",
            0.00799586410628975
          ],
          [
            "model",
            0.007330126719122404
          ],
          [
            "deep",
            0.007118168571966355
          ]
        ],
        "count": 8320
      },
      "3": {
        "name": "3_visual_video_image_multimodal",
        "keywords": [
          [
            "visual",
            0.022974786142972638
          ],
          [
            "video",
            0.015812356061359098
          ],
          [
            "image",
            0.015234745683051047
          ],
          [
            "multimodal",
            0.015040210422447034
          ],
          [
            "text",
            0.013135190994105237
          ],
          [
            "language",
            0.01156522112479537
          ],
          [
            "vision",
            0.011159974920463403
          ],
          [
            "modal",
            0.010410321436837322
          ],
          [
            "models",
            0.009765550257243977
          ],
          [
            "MLLMs",
            0.009720729506328601
          ]
        ],
        "count": 3130
      },
      "4": {
        "name": "4_speech_audio_Speech_speaker",
        "keywords": [
          [
            "speech",
            0.044443874890863655
          ],
          [
            "audio",
            0.03082164618248991
          ],
          [
            "Speech",
            0.01770993423582065
          ],
          [
            "speaker",
            0.016148246060038212
          ],
          [
            "ASR",
            0.015135017492148512
          ],
          [
            "Audio",
            0.011126072709970706
          ],
          [
            "model",
            0.010054396060648261
          ],
          [
            "acoustic",
            0.009564336010004986
          ],
          [
            "recognition",
            0.009518779813518308
          ],
          [
            "models",
            0.008835526094850506
          ]
        ],
        "count": 2962
      },
      "5": {
        "name": "5_traffic_driving_autonomous_prediction",
        "keywords": [
          [
            "traffic",
            0.028333318538382356
          ],
          [
            "driving",
            0.026129102920564116
          ],
          [
            "autonomous",
            0.014540763123892365
          ],
          [
            "prediction",
            0.012682091015139821
          ],
          [
            "vehicles",
            0.012440739365403065
          ],
          [
            "vehicle",
            0.012333270667858295
          ],
          [
            "trajectory",
            0.011398689653807876
          ],
          [
            "road",
            0.01049587280793286
          ],
          [
            "autonomous driving",
            0.01012921169951855
          ],
          [
            "urban",
            0.010062310703730863
          ]
        ],
        "count": 2946
      },
      "6": {
        "name": "6_reasoning_LLMs_Reasoning_mathematical",
        "keywords": [
          [
            "reasoning",
            0.0488420712675953
          ],
          [
            "LLMs",
            0.018255934825436443
          ],
          [
            "Reasoning",
            0.014826311890278571
          ],
          [
            "mathematical",
            0.012682629392795037
          ],
          [
            "language",
            0.01199638793446412
          ],
          [
            "models",
            0.011828267984647794
          ],
          [
            "language models",
            0.009658626389211526
          ],
          [
            "tasks",
            0.009358202557421305
          ],
          [
            "Large",
            0.00916111920004173
          ],
          [
            "problems",
            0.008946691980831608
          ]
        ],
        "count": 2377
      },
      "7": {
        "name": "7_graph_Graph_node_GNNs",
        "keywords": [
          [
            "graph",
            0.054670456913630816
          ],
          [
            "Graph",
            0.030000189906382698
          ],
          [
            "node",
            0.026827105282190095
          ],
          [
            "GNNs",
            0.024726164297431647
          ],
          [
            "graphs",
            0.023222377665411742
          ],
          [
            "nodes",
            0.017104503323144327
          ],
          [
            "GNN",
            0.017053536783832857
          ],
          [
            "Networks",
            0.01180467679595092
          ],
          [
            "networks",
            0.01179745755909655
          ],
          [
            "Neural",
            0.011213789030154481
          ]
        ],
        "count": 2179
      },
      "8": {
        "name": "8_AI_systems_human_intelligence",
        "keywords": [
          [
            "AI",
            0.07042869501691285
          ],
          [
            "systems",
            0.019838602456525106
          ],
          [
            "human",
            0.01598425029994919
          ],
          [
            "intelligence",
            0.014559847404169622
          ],
          [
            "ethical",
            0.013856892297619793
          ],
          [
            "Artificial",
            0.012978075633351852
          ],
          [
            "Intelligence",
            0.012594690043866494
          ],
          [
            "artificial",
            0.01125205006558448
          ],
          [
            "risks",
            0.010866530354444919
          ],
          [
            "research",
            0.009987951406490187
          ]
        ],
        "count": 2129
      },
      "9": {
        "name": "9_recommendation_user_item_recommender",
        "keywords": [
          [
            "recommendation",
            0.03611440591833301
          ],
          [
            "user",
            0.0288126323255046
          ],
          [
            "item",
            0.022144293294884716
          ],
          [
            "recommender",
            0.019507497713080437
          ],
          [
            "items",
            0.01718618242397992
          ],
          [
            "users",
            0.016449372633906376
          ],
          [
            "recommender systems",
            0.014899057274451294
          ],
          [
            "Recommendation",
            0.014424313859843245
          ],
          [
            "systems",
            0.012404400333172881
          ],
          [
            "recommendations",
            0.011410487986660342
          ]
        ],
        "count": 2115
      },
      "10": {
        "name": "10_explanations_XAI_explanation_AI",
        "keywords": [
          [
            "explanations",
            0.03167453694953358
          ],
          [
            "XAI",
            0.02422811933629752
          ],
          [
            "explanation",
            0.018575342388088036
          ],
          [
            "AI",
            0.014347505694452905
          ],
          [
            "methods",
            0.011072113561451994
          ],
          [
            "explainability",
            0.010444004081725597
          ],
          [
            "decision",
            0.009988623264122348
          ],
          [
            "model",
            0.009930320445136245
          ],
          [
            "interpretability",
            0.009655178295923588
          ],
          [
            "counterfactual",
            0.009147647378055632
          ]
        ],
        "count": 2105
      },
      "11": {
        "name": "11_code_software_Code_LLMs",
        "keywords": [
          [
            "code",
            0.05317303237110505
          ],
          [
            "software",
            0.023078027472777103
          ],
          [
            "Code",
            0.016605998039803758
          ],
          [
            "LLMs",
            0.01623802146389895
          ],
          [
            "code generation",
            0.01400799872719579
          ],
          [
            "generation",
            0.01234191236544622
          ],
          [
            "LLM",
            0.01041315960363574
          ],
          [
            "programming",
            0.010180668653564122
          ],
          [
            "language",
            0.009284351178270792
          ],
          [
            "program",
            0.00925228756286476
          ]
        ],
        "count": 2030
      },
      "12": {
        "name": "12_image_diffusion_generation_Diffusion",
        "keywords": [
          [
            "image",
            0.031237742065968063
          ],
          [
            "diffusion",
            0.027068099369019712
          ],
          [
            "generation",
            0.021186193056447748
          ],
          [
            "Diffusion",
            0.019709408858283234
          ],
          [
            "text",
            0.01767874622992506
          ],
          [
            "diffusion models",
            0.017109856092981435
          ],
          [
            "images",
            0.015485572514746634
          ],
          [
            "video",
            0.015178889101802274
          ],
          [
            "models",
            0.014451179651639919
          ],
          [
            "text image",
            0.012663911084823258
          ]
        ],
        "count": 1942
      },
      "13": {
        "name": "13_LLMs_attacks_safety_attack",
        "keywords": [
          [
            "LLMs",
            0.02208889760381968
          ],
          [
            "attacks",
            0.020981706522111326
          ],
          [
            "safety",
            0.019119291806159754
          ],
          [
            "attack",
            0.018331095194580167
          ],
          [
            "LLM",
            0.015697922066878103
          ],
          [
            "jailbreak",
            0.014421821647145405
          ],
          [
            "adversarial",
            0.013349876754482985
          ],
          [
            "security",
            0.012009107664029281
          ],
          [
            "harmful",
            0.011570757592005635
          ],
          [
            "Large",
            0.010978613546575871
          ]
        ],
        "count": 1828
      },
      "14": {
        "name": "14_optimization_problem_problems_algorithm",
        "keywords": [
          [
            "optimization",
            0.0210550128253512
          ],
          [
            "problem",
            0.019926501103891842
          ],
          [
            "problems",
            0.019243955011323186
          ],
          [
            "algorithm",
            0.01679055002862158
          ],
          [
            "algorithms",
            0.01417464576493552
          ],
          [
            "solutions",
            0.012297972175184633
          ],
          [
            "search",
            0.012217436619792573
          ],
          [
            "instances",
            0.011776547418633569
          ],
          [
            "solution",
            0.010791195943956474
          ],
          [
            "heuristic",
            0.009552312652995198
          ]
        ],
        "count": 1647
      },
      "15": {
        "name": "15_memory_quantization_attention_LoRA",
        "keywords": [
          [
            "memory",
            0.016636912579786722
          ],
          [
            "quantization",
            0.016240341251400607
          ],
          [
            "attention",
            0.015348285645568626
          ],
          [
            "LoRA",
            0.013319972662137754
          ],
          [
            "LLMs",
            0.01297475712323694
          ],
          [
            "inference",
            0.012561653527842177
          ],
          [
            "MoE",
            0.012400077726248727
          ],
          [
            "tuning",
            0.011893790228356274
          ],
          [
            "models",
            0.011231393648683084
          ],
          [
            "training",
            0.011048765098777252
          ]
        ],
        "count": 1619
      },
      "16": {
        "name": "16_3D_point_object_view",
        "keywords": [
          [
            "3D",
            0.052693325396697695
          ],
          [
            "point",
            0.022894281144115083
          ],
          [
            "object",
            0.01625223427492997
          ],
          [
            "view",
            0.015930657800367294
          ],
          [
            "point cloud",
            0.014820861765494972
          ],
          [
            "scene",
            0.014793071360562873
          ],
          [
            "cloud",
            0.012831076551296764
          ],
          [
            "camera",
            0.011609817543920691
          ],
          [
            "2D",
            0.011373163737927586
          ],
          [
            "LiDAR",
            0.0111188353828521
          ]
        ],
        "count": 1487
      },
      "17": {
        "name": "17_RAG_retrieval_LLMs_QA",
        "keywords": [
          [
            "RAG",
            0.02967750460068605
          ],
          [
            "retrieval",
            0.01942271557951707
          ],
          [
            "LLMs",
            0.017672169149360005
          ],
          [
            "QA",
            0.016366332957607765
          ],
          [
            "question",
            0.015260684323863823
          ],
          [
            "Retrieval",
            0.015102852334164784
          ],
          [
            "knowledge",
            0.014457660315803476
          ],
          [
            "LLM",
            0.011856688228713235
          ],
          [
            "answer",
            0.011397819630344112
          ],
          [
            "Augmented",
            0.010529602868905653
          ]
        ],
        "count": 1484
      },
      "18": {
        "name": "18_series_time series_time_forecasting",
        "keywords": [
          [
            "series",
            0.048308108825810187
          ],
          [
            "time series",
            0.0466988799786808
          ],
          [
            "time",
            0.03300063337311367
          ],
          [
            "forecasting",
            0.026065078659270557
          ],
          [
            "Time",
            0.021702196287937042
          ],
          [
            "Series",
            0.019230491822700778
          ],
          [
            "series forecasting",
            0.013533250753397211
          ],
          [
            "data",
            0.013138277451601583
          ],
          [
            "time series forecasting",
            0.011951520996899423
          ],
          [
            "temporal",
            0.01065091539031915
          ]
        ],
        "count": 1455
      },
      "19": {
        "name": "19_FL_Federated_clients_federated",
        "keywords": [
          [
            "FL",
            0.05118527405406165
          ],
          [
            "Federated",
            0.04373669010582585
          ],
          [
            "clients",
            0.03466247132536851
          ],
          [
            "federated",
            0.02976966768730279
          ],
          [
            "client",
            0.02356738883495281
          ],
          [
            "privacy",
            0.023371569734176416
          ],
          [
            "federated learning",
            0.02233133258432972
          ],
          [
            "data",
            0.019819075798117848
          ],
          [
            "Learning",
            0.019545087805226073
          ],
          [
            "local",
            0.01825181852656168
          ]
        ],
        "count": 1396
      },
      "20": {
        "name": "20_network_wireless_channel_communication",
        "keywords": [
          [
            "network",
            0.02069754887528742
          ],
          [
            "wireless",
            0.018194727955889087
          ],
          [
            "channel",
            0.015909444192911626
          ],
          [
            "communication",
            0.014430735364826332
          ],
          [
            "networks",
            0.01301424286052889
          ],
          [
            "CSI",
            0.010627428284351592
          ],
          [
            "resource",
            0.00854602822595478
          ],
          [
            "edge",
            0.008199466802654438
          ],
          [
            "learning",
            0.008112458364750198
          ],
          [
            "AI",
            0.008079881530036599
          ]
        ],
        "count": 1245
      },
      "21": {
        "name": "21_students_AI_student_education",
        "keywords": [
          [
            "students",
            0.044005457818008444
          ],
          [
            "AI",
            0.025970828287862166
          ],
          [
            "student",
            0.025476870224375805
          ],
          [
            "education",
            0.023484642251034134
          ],
          [
            "educational",
            0.019565619473735422
          ],
          [
            "learning",
            0.015516758206527165
          ],
          [
            "ChatGPT",
            0.01273813314146657
          ],
          [
            "feedback",
            0.01121585800474045
          ],
          [
            "teaching",
            0.010684791865331025
          ],
          [
            "course",
            0.010194680979507905
          ]
        ],
        "count": 1150
      },
      "22": {
        "name": "22_logic_SAT_ASP_semantics",
        "keywords": [
          [
            "logic",
            0.02805249335913835
          ],
          [
            "SAT",
            0.02389263889848333
          ],
          [
            "ASP",
            0.02195215351622003
          ],
          [
            "semantics",
            0.021416016085201762
          ],
          [
            "argumentation",
            0.016096538541050042
          ],
          [
            "solvers",
            0.014217247886490763
          ],
          [
            "Programming",
            0.01337745612342613
          ],
          [
            "Logic",
            0.013124961650094583
          ],
          [
            "problem",
            0.013051303637976495
          ],
          [
            "reasoning",
            0.013024016308014119
          ]
        ],
        "count": 1149
      },
      "23": {
        "name": "23_news_detection_hate_media",
        "keywords": [
          [
            "news",
            0.032062679151397475
          ],
          [
            "detection",
            0.023724873914498408
          ],
          [
            "hate",
            0.02123080850247287
          ],
          [
            "media",
            0.02079271280333715
          ],
          [
            "social",
            0.02059714461495018
          ],
          [
            "fake",
            0.019358376275262684
          ],
          [
            "content",
            0.017675767553533717
          ],
          [
            "social media",
            0.016960874107592193
          ],
          [
            "speech",
            0.013730115494740626
          ],
          [
            "Detection",
            0.013064959107565029
          ]
        ],
        "count": 1032
      },
      "24": {
        "name": "24_action_recognition_pose_video",
        "keywords": [
          [
            "action",
            0.01684056878476694
          ],
          [
            "recognition",
            0.01683982787198019
          ],
          [
            "pose",
            0.015106111028050045
          ],
          [
            "video",
            0.014854637746055324
          ],
          [
            "human",
            0.014819936801507885
          ],
          [
            "HAR",
            0.014361103043070658
          ],
          [
            "3D",
            0.013658141928451216
          ],
          [
            "temporal",
            0.011761395261148387
          ],
          [
            "Human",
            0.01175026085078948
          ],
          [
            "activity",
            0.010968249890566044
          ]
        ],
        "count": 1025
      },
      "25": {
        "name": "25_face_facial_images_recognition",
        "keywords": [
          [
            "face",
            0.02975677169308048
          ],
          [
            "facial",
            0.021388770516546855
          ],
          [
            "images",
            0.018610016739348118
          ],
          [
            "recognition",
            0.01663172753740276
          ],
          [
            "deepfake",
            0.014355299893701453
          ],
          [
            "image",
            0.014200360156064908
          ],
          [
            "Face",
            0.01373569364864816
          ],
          [
            "detection",
            0.013490508779723461
          ],
          [
            "face recognition",
            0.011999502723216964
          ],
          [
            "dataset",
            0.009102821242791534
          ]
        ],
        "count": 939
      },
      "26": {
        "name": "26_dialogue_Dialogue_conversational_dialog",
        "keywords": [
          [
            "dialogue",
            0.05657794249452586
          ],
          [
            "Dialogue",
            0.02232918469518509
          ],
          [
            "conversational",
            0.016722228680610896
          ],
          [
            "dialog",
            0.016477118923380694
          ],
          [
            "conversation",
            0.014294275275137438
          ],
          [
            "user",
            0.014112826023624512
          ],
          [
            "responses",
            0.013043619077616838
          ],
          [
            "dialogue systems",
            0.013009130832546035
          ],
          [
            "dialogues",
            0.012878422978160692
          ],
          [
            "response",
            0.012678134581587622
          ]
        ],
        "count": 913
      },
      "27": {
        "name": "27_equations_neural_PDE_PDEs",
        "keywords": [
          [
            "equations",
            0.01997947973001633
          ],
          [
            "neural",
            0.019099668389368617
          ],
          [
            "PDE",
            0.01688937881559943
          ],
          [
            "PDEs",
            0.01612333114654982
          ],
          [
            "physics",
            0.015547534074167438
          ],
          [
            "differential",
            0.014735681870516522
          ],
          [
            "Neural",
            0.014292010080568223
          ],
          [
            "differential equations",
            0.013710206786551372
          ],
          [
            "physical",
            0.013696697865012403
          ],
          [
            "PINNs",
            0.013385351651996785
          ]
        ],
        "count": 880
      },
      "28": {
        "name": "28_causal_Causal_variables_observational",
        "keywords": [
          [
            "causal",
            0.09070530797659952
          ],
          [
            "Causal",
            0.030985252233673787
          ],
          [
            "variables",
            0.024760212658674827
          ],
          [
            "observational",
            0.016949899735181826
          ],
          [
            "effect",
            0.016794808814217452
          ],
          [
            "causal inference",
            0.01432610990372847
          ],
          [
            "data",
            0.013776269103940078
          ],
          [
            "effects",
            0.012974075451028638
          ],
          [
            "causal discovery",
            0.01257682294530924
          ],
          [
            "discovery",
            0.012370007907461998
          ]
        ],
        "count": 873
      },
      "29": {
        "name": "29_images_remote sensing_remote_sensing",
        "keywords": [
          [
            "images",
            0.016876387120303978
          ],
          [
            "remote sensing",
            0.014754048303846414
          ],
          [
            "remote",
            0.014093792685863518
          ],
          [
            "sensing",
            0.013535935474970957
          ],
          [
            "detection",
            0.013508899679968753
          ],
          [
            "segmentation",
            0.012206905317719901
          ],
          [
            "classification",
            0.010738959554561027
          ],
          [
            "species",
            0.010501567767048211
          ],
          [
            "image",
            0.010440601687864225
          ],
          [
            "dataset",
            0.010248079893416301
          ]
        ],
        "count": 853
      },
      "30": {
        "name": "30_adversarial_attacks_robustness_Adversarial",
        "keywords": [
          [
            "adversarial",
            0.06865059608941665
          ],
          [
            "attacks",
            0.034738830532224055
          ],
          [
            "robustness",
            0.02945993889451772
          ],
          [
            "Adversarial",
            0.026041734802069276
          ],
          [
            "attack",
            0.02467249552950497
          ],
          [
            "adversarial examples",
            0.019793798983132015
          ],
          [
            "adversarial attacks",
            0.019402832797505272
          ],
          [
            "examples",
            0.017187745613324517
          ],
          [
            "perturbations",
            0.016375395460264192
          ],
          [
            "adversarial training",
            0.013003138659151724
          ]
        ],
        "count": 821
      },
      "31": {
        "name": "31_translation_languages_language_Translation",
        "keywords": [
          [
            "translation",
            0.03972612531236496
          ],
          [
            "languages",
            0.022924219075873258
          ],
          [
            "language",
            0.021396802272113623
          ],
          [
            "Translation",
            0.0189802560588997
          ],
          [
            "English",
            0.017847004959167207
          ],
          [
            "machine translation",
            0.017396290766830823
          ],
          [
            "multilingual",
            0.014999045171400925
          ],
          [
            "NMT",
            0.014273473278571033
          ],
          [
            "models",
            0.01414409826028566
          ],
          [
            "MT",
            0.01361087948968755
          ]
        ],
        "count": 798
      },
      "32": {
        "name": "32_agents_agent_GUI_LLM",
        "keywords": [
          [
            "agents",
            0.03400937033488594
          ],
          [
            "agent",
            0.030076091376535406
          ],
          [
            "GUI",
            0.020973009318959786
          ],
          [
            "LLM",
            0.016176630638933646
          ],
          [
            "Agent",
            0.015192423227541047
          ],
          [
            "Agents",
            0.013298070187199064
          ],
          [
            "web",
            0.013084329079869204
          ],
          [
            "tasks",
            0.012369602103523699
          ],
          [
            "multi",
            0.01023053722178741
          ],
          [
            "LLMs",
            0.009950740191597973
          ]
        ],
        "count": 786
      },
      "33": {
        "name": "33_segmentation_object_object detection_semantic segmentation",
        "keywords": [
          [
            "segmentation",
            0.03213704501898982
          ],
          [
            "object",
            0.027419879984253406
          ],
          [
            "object detection",
            0.017347285070162487
          ],
          [
            "semantic segmentation",
            0.016931802333950863
          ],
          [
            "detection",
            0.01681348605358964
          ],
          [
            "semantic",
            0.015540641247798762
          ],
          [
            "Segmentation",
            0.01479130064853364
          ],
          [
            "Object",
            0.012887521114942527
          ],
          [
            "objects",
            0.012452242392729046
          ],
          [
            "image",
            0.0117547665011769
          ]
        ],
        "count": 761
      },
      "34": {
        "name": "34_knowledge_Knowledge_KG_graph",
        "keywords": [
          [
            "knowledge",
            0.03149872353173713
          ],
          [
            "Knowledge",
            0.030928617875800493
          ],
          [
            "KG",
            0.02702738310094306
          ],
          [
            "graph",
            0.026741253142746575
          ],
          [
            "entities",
            0.024771593969861307
          ],
          [
            "KGs",
            0.024662728442431153
          ],
          [
            "knowledge graph",
            0.024070657869385774
          ],
          [
            "graphs",
            0.021019791241955414
          ],
          [
            "knowledge graphs",
            0.020508784758076822
          ],
          [
            "Graph",
            0.01709922267461613
          ]
        ],
        "count": 761
      },
      "35": {
        "name": "35_quantum_Quantum_classical_quantum computing",
        "keywords": [
          [
            "quantum",
            0.14272871264409984
          ],
          [
            "Quantum",
            0.057364474532349635
          ],
          [
            "classical",
            0.03172089971885177
          ],
          [
            "quantum computing",
            0.01854571203590413
          ],
          [
            "computing",
            0.017270372872816374
          ],
          [
            "circuits",
            0.017091978334812918
          ],
          [
            "circuit",
            0.015391680453102534
          ],
          [
            "machine",
            0.013223207991654492
          ],
          [
            "machine learning",
            0.012817897136386366
          ],
          [
            "learning",
            0.012385486008244299
          ]
        ],
        "count": 688
      },
      "36": {
        "name": "36_fairness_Fairness_fair_bias",
        "keywords": [
          [
            "fairness",
            0.09505902309493368
          ],
          [
            "Fairness",
            0.027792524933252613
          ],
          [
            "fair",
            0.026667089610992675
          ],
          [
            "bias",
            0.019707981195932277
          ],
          [
            "algorithmic",
            0.013551624646584038
          ],
          [
            "sensitive",
            0.01304928289775224
          ],
          [
            "groups",
            0.012986534424297631
          ],
          [
            "discrimination",
            0.012765607734752998
          ],
          [
            "decision",
            0.01246958669010987
          ],
          [
            "ML",
            0.012188392642130876
          ]
        ],
        "count": 655
      },
      "37": {
        "name": "37_energy_power_grid_load",
        "keywords": [
          [
            "energy",
            0.04412338702086798
          ],
          [
            "power",
            0.033614292083660166
          ],
          [
            "grid",
            0.022326135540012806
          ],
          [
            "load",
            0.0157770547031172
          ],
          [
            "electricity",
            0.01524540281622061
          ],
          [
            "renewable",
            0.014174379695269365
          ],
          [
            "forecasting",
            0.0121289134550424
          ],
          [
            "smart",
            0.011924962802383088
          ],
          [
            "consumption",
            0.011894552307135908
          ],
          [
            "Energy",
            0.011378936436343221
          ]
        ],
        "count": 607
      },
      "38": {
        "name": "38_bias_biases_LLMs_gender",
        "keywords": [
          [
            "bias",
            0.03684942775961852
          ],
          [
            "biases",
            0.03130194205082627
          ],
          [
            "LLMs",
            0.02783985370885547
          ],
          [
            "gender",
            0.02538988677272867
          ],
          [
            "language",
            0.01616032363099863
          ],
          [
            "Bias",
            0.014157647795332892
          ],
          [
            "Language",
            0.01383450382786684
          ],
          [
            "political",
            0.013586969775153218
          ],
          [
            "LLM",
            0.013152811312122927
          ],
          [
            "social",
            0.012983228917212477
          ]
        ],
        "count": 600
      },
      "39": {
        "name": "39_preference_reward_RLHF_DPO",
        "keywords": [
          [
            "preference",
            0.0439317972626651
          ],
          [
            "reward",
            0.034037573296642344
          ],
          [
            "RLHF",
            0.03393483186763263
          ],
          [
            "DPO",
            0.028411830255133923
          ],
          [
            "alignment",
            0.02753340172742866
          ],
          [
            "preferences",
            0.026708035970634293
          ],
          [
            "Preference",
            0.02499167041816431
          ],
          [
            "human",
            0.02228442781337807
          ],
          [
            "human preferences",
            0.019858225018272623
          ],
          [
            "feedback",
            0.016110403151712787
          ]
        ],
        "count": 546
      },
      "40": {
        "name": "40_scientific_papers_research_citation",
        "keywords": [
          [
            "scientific",
            0.033912042690800996
          ],
          [
            "papers",
            0.025088250440416068
          ],
          [
            "research",
            0.02323313266375702
          ],
          [
            "citation",
            0.016424144344094695
          ],
          [
            "review",
            0.013815034880146744
          ],
          [
            "academic",
            0.013780187293144017
          ],
          [
            "literature",
            0.01356254935847763
          ],
          [
            "LLMs",
            0.013408707514267626
          ],
          [
            "AI",
            0.012654071105830828
          ],
          [
            "Scientific",
            0.01173011950295065
          ]
        ],
        "count": 533
      },
      "41": {
        "name": "41_IoT_detection_traffic_attacks",
        "keywords": [
          [
            "IoT",
            0.030258488251182592
          ],
          [
            "detection",
            0.02841705304205862
          ],
          [
            "traffic",
            0.02313526656550797
          ],
          [
            "attacks",
            0.022960478498038263
          ],
          [
            "network",
            0.021526602561549705
          ],
          [
            "intrusion",
            0.020774900541827908
          ],
          [
            "security",
            0.01864536143158611
          ],
          [
            "IDS",
            0.01852669817879011
          ],
          [
            "Intrusion",
            0.01831078336116476
          ],
          [
            "intrusion detection",
            0.017545165881153277
          ]
        ],
        "count": 488
      },
      "42": {
        "name": "42_privacy_attacks_data_private",
        "keywords": [
          [
            "privacy",
            0.05109529729809239
          ],
          [
            "attacks",
            0.020521422467775673
          ],
          [
            "data",
            0.018976221918564688
          ],
          [
            "private",
            0.01649168698984196
          ],
          [
            "unlearning",
            0.01632536170712137
          ],
          [
            "Privacy",
            0.014150864115774231
          ],
          [
            "inference",
            0.013458409646916839
          ],
          [
            "model",
            0.012933303843942087
          ],
          [
            "training",
            0.012470453398810637
          ],
          [
            "differential privacy",
            0.011275066854913057
          ]
        ],
        "count": 440
      },
      "43": {
        "name": "43_summarization_summaries_Summarization_summary",
        "keywords": [
          [
            "summarization",
            0.07007537378167107
          ],
          [
            "summaries",
            0.0417288231249328
          ],
          [
            "Summarization",
            0.032990415362947265
          ],
          [
            "summary",
            0.02495787925458159
          ],
          [
            "document",
            0.02052697797459887
          ],
          [
            "abstractive",
            0.018123040862978786
          ],
          [
            "text",
            0.014985843052413145
          ],
          [
            "text summarization",
            0.013746315608362937
          ],
          [
            "news",
            0.010561672291281899
          ],
          [
            "documents",
            0.010545963313462914
          ]
        ],
        "count": 410
      },
      "44": {
        "name": "44_market_stock_financial_trading",
        "keywords": [
          [
            "market",
            0.04716278503680894
          ],
          [
            "stock",
            0.03808291557925859
          ],
          [
            "financial",
            0.037162723216914474
          ],
          [
            "trading",
            0.036166889357141754
          ],
          [
            "price",
            0.022306538074286387
          ],
          [
            "investment",
            0.017870552559696353
          ],
          [
            "portfolio",
            0.017525198341367176
          ],
          [
            "markets",
            0.016904554047514637
          ],
          [
            "risk",
            0.012345368489822475
          ],
          [
            "volatility",
            0.01160485575630144
          ]
        ],
        "count": 410
      },
      "45": {
        "name": "45_emotion_Emotion_emotions_emotional",
        "keywords": [
          [
            "emotion",
            0.05954574033562547
          ],
          [
            "Emotion",
            0.031730857178033034
          ],
          [
            "emotions",
            0.02777599612815219
          ],
          [
            "emotional",
            0.023669090365466404
          ],
          [
            "multimodal",
            0.019120571857603266
          ],
          [
            "emotion recognition",
            0.0183370629762399
          ],
          [
            "recognition",
            0.016790042437156018
          ],
          [
            "modality",
            0.0167655959183984
          ],
          [
            "Multimodal",
            0.01600125872049591
          ],
          [
            "affective",
            0.014765069444557478
          ]
        ],
        "count": 389
      },
      "46": {
        "name": "46_legal_Legal_law_case",
        "keywords": [
          [
            "legal",
            0.10669840547600871
          ],
          [
            "Legal",
            0.03976841343351954
          ],
          [
            "law",
            0.01651979506543372
          ],
          [
            "case",
            0.015450818625768501
          ],
          [
            "legal domain",
            0.012722057131313413
          ],
          [
            "LLMs",
            0.011793723407120557
          ],
          [
            "documents",
            0.01154109504609519
          ],
          [
            "reasoning",
            0.011373605102965272
          ],
          [
            "domain",
            0.010747713383827205
          ],
          [
            "cases",
            0.010718736673112097
          ]
        ],
        "count": 377
      },
      "47": {
        "name": "47_weather_climate_forecasting_precipitation",
        "keywords": [
          [
            "weather",
            0.03785721631202225
          ],
          [
            "climate",
            0.028472542350363755
          ],
          [
            "forecasting",
            0.026556436147592295
          ],
          [
            "precipitation",
            0.026079084292521263
          ],
          [
            "resolution",
            0.017043964897049298
          ],
          [
            "forecast",
            0.01575458854762491
          ],
          [
            "forecasts",
            0.015033343067711783
          ],
          [
            "weather forecasting",
            0.014120142808030107
          ],
          [
            "data",
            0.013605233975292886
          ],
          [
            "Weather",
            0.013566273917006084
          ]
        ],
        "count": 367
      }
    },
    "correlations": [
      [
        1.0,
        -0.7548813241823873,
        -0.5755672580512049,
        -0.726057240518099,
        -0.7605512299636725,
        -0.7085169819167432,
        -0.7130295669695247,
        -0.734273433417358,
        -0.7212532388225679,
        -0.7372705448931118,
        -0.7529001173832698,
        -0.7113673166910188,
        -0.73725969210321,
        -0.7175837393479365,
        -0.628417025487219,
        -0.722176743355929,
        -0.7392152316560194,
        -0.735651461943668,
        -0.7143535021238276,
        -0.751181893866431,
        -0.7202221966011519,
        -0.745898149191424,
        -0.7462376369684469,
        -0.7538549813919853,
        -0.7027176370632308,
        -0.7419163897536021,
        -0.754180081058996,
        -0.7205948171348011,
        -0.7461446845329192,
        -0.7446621672857452,
        -0.723951456017963,
        -0.7430617295245486,
        -0.45270303363990394,
        -0.7426410317467265,
        -0.7276127643852109,
        -0.7505926135582754,
        -0.7569632519002565,
        -0.7211165302759314,
        -0.7212692593715253,
        -0.6248754128292371,
        -0.7303215248260192,
        -0.7383934512530191,
        -0.7252251379710122,
        -0.7571265232460268,
        -0.7440077157324145,
        -0.7616543428094578,
        -0.7628203372596114,
        -0.7466090802601986
      ],
      [
        -0.7548813241823873,
        1.0,
        -0.7434240878893051,
        -0.72412138441977,
        -0.758355775653687,
        -0.7597922107354068,
        -0.7324288949944036,
        -0.7521036965566985,
        -0.7106245511958033,
        -0.7528250739427522,
        -0.7431295609235333,
        -0.7285517784937662,
        -0.7226098357880049,
        -0.7330000268771337,
        -0.7515712037050544,
        -0.7300569087022762,
        -0.7317347081351642,
        -0.728892264325004,
        -0.7379973974985093,
        -0.7498377897055069,
        -0.7507196390089657,
        -0.7504205904515195,
        -0.7594880097083335,
        -0.7494251101378725,
        -0.7515107718542943,
        -0.7234118724263374,
        -0.7564554157374876,
        -0.7494796453076207,
        -0.7551997065917657,
        -0.705829980088949,
        -0.7502923113369657,
        -0.7399443881993364,
        -0.7543337406545458,
        -0.6749706698248853,
        -0.7385348357679381,
        -0.7631073603269719,
        -0.7561209798224826,
        -0.7570218061122236,
        -0.726749718052018,
        -0.7485209365444909,
        -0.7355917108382779,
        -0.7420283758321775,
        -0.7121760149725361,
        -0.7492790387016656,
        -0.7603287643117549,
        -0.7587538121186567,
        -0.7610499893801342,
        -0.7402244960125798
      ],
      [
        -0.5755672580512049,
        -0.7434240878893051,
        1.0,
        -0.7331761016104339,
        -0.7442449838919292,
        -0.741500409409094,
        -0.7397796721005107,
        -0.7072590491884945,
        -0.7272379995400671,
        -0.7444652970856722,
        -0.7282135158756755,
        -0.7211069147039311,
        -0.7248952577336899,
        -0.741305386912434,
        -0.6942932070192835,
        -0.7174659783464452,
        -0.7370492051920711,
        -0.7512580860653648,
        -0.7131797915999514,
        -0.7470593791049591,
        -0.5765891087632047,
        -0.7423858414277,
        -0.7428080322634649,
        -0.7474248166653338,
        -0.7338692032460128,
        -0.7248696472098268,
        -0.7608049204354992,
        -0.019173992937889604,
        -0.748725837349117,
        -0.7041239468883747,
        -0.704828679831523,
        -0.7420424798685405,
        -0.7278835187244403,
        -0.7291372378743481,
        -0.7256998856485045,
        -0.7450502991229951,
        -0.7563708693309246,
        -0.6997768527190759,
        -0.7325243939676135,
        -0.7379118237206754,
        -0.7340292539081288,
        -0.7028345179851048,
        -0.7000695962640506,
        -0.7601426884499315,
        -0.7550022330134084,
        -0.758455741547784,
        -0.7630272242558825,
        -0.7167741320678609
      ],
      [
        -0.726057240518099,
        -0.72412138441977,
        -0.7331761016104339,
        1.0,
        -0.7262355078572289,
        -0.7422483490186857,
        -0.6219603973258565,
        -0.7462606357070339,
        -0.7256204105036717,
        -0.744840217983167,
        -0.7445464113299358,
        -0.6821046000790781,
        -0.5196382798730378,
        -0.6474272477853125,
        -0.7427551136561755,
        -0.6525343821776095,
        -0.7151433959581492,
        -0.672165455371756,
        -0.7321673702594723,
        -0.7596578425610677,
        -0.7437621120560113,
        -0.7482040171400741,
        -0.7505611853678448,
        -0.7288940312746204,
        -0.6363289998103434,
        -0.6234710733563933,
        -0.749077534185823,
        -0.7436203874030702,
        -0.7507157805048215,
        -0.6960685287218537,
        -0.7291330998754043,
        -0.6726148052116707,
        -0.7238794848757057,
        -0.6719983714761486,
        -0.7277667618321282,
        -0.7627492632431715,
        -0.759713383719838,
        -0.7482088138954577,
        -0.6473732539046921,
        -0.7097249943213099,
        -0.7266042859673955,
        -0.7357789191877873,
        -0.7342485080262633,
        -0.7476825030657115,
        -0.760142046997315,
        -0.71843841010263,
        -0.7632022539592869,
        -0.7484031111012921
      ],
      [
        -0.7605512299636725,
        -0.758355775653687,
        -0.7442449838919292,
        -0.7262355078572289,
        1.0,
        -0.7612837074326988,
        -0.7477824630597313,
        -0.7583717245535793,
        -0.7376134761541651,
        -0.7492812351568876,
        -0.7589028212025505,
        -0.7345218629232529,
        -0.7209542077827359,
        -0.7376839022818976,
        -0.7562544274031023,
        -0.7322819942512746,
        -0.7528572898689985,
        -0.7438352850677548,
        -0.7440150719526011,
        -0.7618166788377841,
        -0.7472106850283414,
        -0.7508132302186066,
        -0.760285046656504,
        -0.6832000275517349,
        -0.735843295238892,
        -0.7228536031818826,
        -0.7404539905072651,
        -0.7474960838081979,
        -0.75992468369895,
        -0.7510628548648293,
        -0.741473743686532,
        -0.700836623784493,
        -0.7547172053713409,
        -0.7503804939388383,
        -0.7533596182687572,
        -0.762768321762741,
        -0.7604291918828053,
        -0.7519570543176275,
        -0.7332281062291313,
        -0.7451882079898238,
        -0.7451243824022302,
        -0.7432466722346607,
        -0.7425349841538609,
        -0.7560455928129031,
        -0.7621621447660414,
        -0.6681219714320774,
        -0.762923370476084,
        -0.7574910308768416
      ],
      [
        -0.7085169819167432,
        -0.7597922107354068,
        -0.741500409409094,
        -0.7422483490186857,
        -0.7612837074326988,
        1.0,
        -0.7422492655075819,
        -0.7294647514929188,
        -0.7306224513432853,
        -0.7487344936796285,
        -0.7504811403468914,
        -0.734748230904359,
        -0.7448447406433207,
        -0.7226633452527388,
        -0.7368640189864006,
        -0.748256929168401,
        -0.7128242706271883,
        -0.7503105082066674,
        -0.7197992930602624,
        -0.7542197305851317,
        -0.7243140197433378,
        -0.756729560202906,
        -0.756057578513067,
        -0.7510396910792195,
        -0.7402307176536971,
        -0.7424612515001401,
        -0.7624466663520492,
        -0.7402670721311292,
        -0.7531108442956225,
        -0.742052054531819,
        -0.7310644663511997,
        -0.7573579761567664,
        -0.7027769984554515,
        -0.7120746538426704,
        -0.7460028787131396,
        -0.7607647485264317,
        -0.7606307791520166,
        -0.7347662211037076,
        -0.7494258072306051,
        -0.7405191130051698,
        -0.7444502907428093,
        -0.6088393776046965,
        -0.7360512999572324,
        -0.7617577567885323,
        -0.755680334191856,
        -0.7595319225619396,
        -0.7608121662082934,
        -0.7020395766033567
      ],
      [
        -0.7130295669695247,
        -0.7324288949944036,
        -0.7397796721005107,
        -0.6219603973258565,
        -0.7477824630597313,
        -0.7422492655075819,
        1.0,
        -0.7262098496365275,
        -0.6739942447514387,
        -0.7281296906825256,
        -0.7197510578565646,
        -0.4854715137309811,
        -0.721773083340112,
        -0.1388312782879179,
        -0.7254291703941782,
        -0.32561245981460074,
        -0.7445619156492059,
        -0.2539233577185598,
        -0.7284091773572363,
        -0.7583068396289179,
        -0.7516264973805742,
        -0.7227361537194321,
        -0.6916335507065889,
        -0.7299396314525982,
        -0.7343193762463216,
        -0.7344867292456435,
        -0.7314624454958856,
        -0.7453642287063897,
        -0.7290216447103111,
        -0.7490933580236,
        -0.7316943394170079,
        -0.49150372910550794,
        -0.5783889166754241,
        -0.7406954818321619,
        -0.6784687094144358,
        -0.7582567362837188,
        -0.7533649913750138,
        -0.7429697216542315,
        -0.21911680530986485,
        -0.5288834617708856,
        -0.5178170010953047,
        -0.7427791846487899,
        -0.7269245254350015,
        -0.7319805785634941,
        -0.7440384366083421,
        -0.7483177541870419,
        -0.7400166608897483,
        -0.7531209287243417
      ],
      [
        -0.734273433417358,
        -0.7521036965566985,
        -0.7072590491884945,
        -0.7462606357070339,
        -0.7583717245535793,
        -0.7294647514929188,
        -0.7262098496365275,
        1.0,
        -0.7486477815788842,
        -0.7131365133827133,
        -0.740663804383757,
        -0.7152209197582278,
        -0.7404088153442479,
        -0.7345593081292573,
        -0.7055176053598498,
        -0.7350932974874886,
        -0.7389864746818122,
        -0.7331321873618276,
        -0.7216658770281903,
        -0.7501747653530226,
        -0.7196697514810879,
        -0.7555049042058899,
        -0.7340377956331403,
        -0.7345712658632122,
        -0.7465317425675291,
        -0.7433213416745559,
        -0.7583356825049266,
        -0.7072045565409665,
        -0.7296739746012317,
        -0.7467623004189756,
        -0.7340061546901164,
        -0.745758409813323,
        -0.7342442295210126,
        -0.7432414903313671,
        -0.49513418115821284,
        -0.7522451865675922,
        -0.7549041657215738,
        -0.7258938520003722,
        -0.7338916024626092,
        -0.7460419851454481,
        -0.736521802966631,
        -0.7132177333917417,
        -0.7256283542282564,
        -0.7528467469086991,
        -0.7469644529166302,
        -0.7583532480931918,
        -0.763037260716054,
        -0.7395348988230745
      ],
      [
        -0.7212532388225679,
        -0.7106245511958033,
        -0.7272379995400671,
        -0.7256204105036717,
        -0.7376134761541651,
        -0.7306224513432853,
        -0.6739942447514387,
        -0.7486477815788842,
        1.0,
        -0.7127968989906158,
        -0.6740641308446702,
        -0.6920134701841919,
        -0.7177972040455903,
        -0.6710308226475061,
        -0.7252489383427683,
        -0.6997951766104828,
        -0.7407902277161629,
        -0.6912173494645532,
        -0.7232479049488327,
        -0.7489646091505735,
        -0.722158737208098,
        -0.5025831978391595,
        -0.7421958041480565,
        -0.7109614083735272,
        -0.7185622745587932,
        -0.7180973323747875,
        -0.7424512726965777,
        -0.7337033996584654,
        -0.7471273846590629,
        -0.7305230559402405,
        -0.7302047497276589,
        -0.715277137989595,
        -0.6566547898906259,
        -0.7408399868165774,
        -0.7309878612750491,
        -0.7543904789629505,
        -0.7145460062957292,
        -0.7068428500188335,
        -0.6695675233940411,
        -0.69390405867889,
        -0.6490400592607454,
        -0.7139171812879364,
        -0.6910871218542867,
        -0.7499159917184746,
        -0.7272834567802446,
        -0.7459166854641361,
        -0.7175265267531293,
        -0.7310743579786079
      ],
      [
        -0.7372705448931118,
        -0.7528250739427522,
        -0.7444652970856722,
        -0.744840217983167,
        -0.7492812351568876,
        -0.7487344936796285,
        -0.7281296906825256,
        -0.7131365133827133,
        -0.7127968989906158,
        1.0,
        -0.7214116058344312,
        -0.7241251973137691,
        -0.7400432337038094,
        -0.7187630158607344,
        -0.7289885416236522,
        -0.7284382484085161,
        -0.7556511364448151,
        -0.7209827751453328,
        -0.7342506773328603,
        -0.7513308195849473,
        -0.7355195245511705,
        -0.7361931379047173,
        -0.7520806627430672,
        -0.7122286982027223,
        -0.7425998957805664,
        -0.7408646459513193,
        -0.7255236669290329,
        -0.7517728036464095,
        -0.7508265767012874,
        -0.750769009648698,
        -0.7478119869875337,
        -0.7402333987248315,
        -0.7266545763396589,
        -0.753357873240007,
        -0.7160757912746172,
        -0.7613934067988146,
        -0.7353638368546995,
        -0.7491704763637305,
        -0.7130187158401464,
        -0.688017591175123,
        -0.7270284922981685,
        -0.7406646641111694,
        -0.7228282411625746,
        -0.7512635522454838,
        -0.7455047017995026,
        -0.7518327938534795,
        -0.7591189018535877,
        -0.7546082797627627
      ],
      [
        -0.7529001173832698,
        -0.7431295609235333,
        -0.7282135158756755,
        -0.7445464113299358,
        -0.7589028212025505,
        -0.7504811403468914,
        -0.7197510578565646,
        -0.740663804383757,
        -0.6740641308446702,
        -0.7214116058344312,
        1.0,
        -0.7424728664114189,
        -0.7465076578337742,
        -0.7386466603409202,
        -0.7449298351806433,
        -0.7450362706390885,
        -0.7587567475174524,
        -0.7407826601903896,
        -0.7433345754368883,
        -0.7624615156921748,
        -0.7522763963818515,
        -0.733471522410267,
        -0.7336168646295067,
        -0.747489885021131,
        -0.7508397929899502,
        -0.7462672560319128,
        -0.7584140101710217,
        -0.7345249968281427,
        -0.7361809418772363,
        -0.7484701336630938,
        -0.742977097849638,
        -0.7507231094160172,
        -0.7460912493377219,
        -0.7530697257133359,
        -0.7427739381372997,
        -0.7617276897365688,
        -0.7416523873895924,
        -0.7554084736808729,
        -0.734843135738581,
        -0.7434693283706328,
        -0.7334549608332313,
        -0.7421021754028658,
        -0.7427266079519801,
        -0.758216655329925,
        -0.7535464112390968,
        -0.7587898461369773,
        -0.7480580478302463,
        -0.7526105427047823
      ],
      [
        -0.7113673166910188,
        -0.7285517784937662,
        -0.7211069147039311,
        -0.6821046000790781,
        -0.7345218629232529,
        -0.734748230904359,
        -0.4854715137309811,
        -0.7152209197582278,
        -0.6920134701841919,
        -0.7241251973137691,
        -0.7424728664114189,
        1.0,
        -0.6884059578964582,
        -0.3952453272849583,
        -0.7174984776215245,
        -0.49795664641867426,
        -0.7213165538476576,
        -0.4646628722741674,
        -0.7149775656854194,
        -0.7527425359147173,
        -0.7376733812640683,
        -0.7219184933599281,
        -0.739216855535813,
        -0.7272325042094524,
        -0.7238170029118021,
        -0.715860398875084,
        -0.741865051828335,
        -0.7297405288835123,
        -0.7500083864190215,
        -0.7289819395490336,
        -0.7179065849460149,
        -0.6191252653962576,
        -0.6219343947636009,
        -0.709403360526668,
        -0.7091801486134162,
        -0.7577637890757873,
        -0.7546171406357539,
        -0.735206085590699,
        -0.44531524507598336,
        -0.6454096726319427,
        -0.5845416359059361,
        -0.7194539457343283,
        -0.7088377107421857,
        -0.7223443024188752,
        -0.74781436468727,
        -0.747070915239791,
        -0.7520416508138221,
        -0.7409029126730706
      ],
      [
        -0.73725969210321,
        -0.7226098357880049,
        -0.7248952577336899,
        -0.5196382798730378,
        -0.7209542077827359,
        -0.7448447406433207,
        -0.721773083340112,
        -0.7404088153442479,
        -0.7177972040455903,
        -0.7400432337038094,
        -0.7465076578337742,
        -0.6884059578964582,
        1.0,
        -0.7153012099491249,
        -0.7298348812162818,
        -0.7021177171222164,
        -0.687462153137974,
        -0.7206140769304783,
        -0.7245524367789045,
        -0.7587579943597148,
        -0.7308583066917431,
        -0.7474551043650752,
        -0.7512066823244894,
        -0.7178982764173172,
        -0.7045807225951616,
        -0.5092416214725181,
        -0.7553217445888998,
        -0.7249748873717903,
        -0.7529110347277881,
        -0.5918229181829624,
        -0.7047972275349647,
        -0.7125883869037495,
        -0.742285570151304,
        -0.6731229827964806,
        -0.7299496435611296,
        -0.7584539483718196,
        -0.758016013677707,
        -0.7358772458330656,
        -0.7115775953013102,
        -0.7098775184560879,
        -0.7299267410014914,
        -0.7260179434869568,
        -0.7153058201096669,
        -0.751364770905792,
        -0.758494278456385,
        -0.7501896313309337,
        -0.7603235360281577,
        -0.734262057281142
      ],
      [
        -0.7175837393479365,
        -0.7330000268771337,
        -0.741305386912434,
        -0.6474272477853125,
        -0.7376839022818976,
        -0.7226633452527388,
        -0.1388312782879179,
        -0.7345593081292573,
        -0.6710308226475061,
        -0.7187630158607344,
        -0.7386466603409202,
        -0.3952453272849583,
        -0.7153012099491249,
        1.0,
        -0.7311915746395699,
        -0.16841881449291912,
        -0.7496548576040305,
        -0.08272889611325228,
        -0.7243576500733745,
        -0.7425562780959453,
        -0.7469038190048731,
        -0.7189133813259301,
        -0.7457184002297488,
        -0.7130289888357078,
        -0.738558100775318,
        -0.7279047708594941,
        -0.7267774013900439,
        -0.7483331210337288,
        -0.7443609861299756,
        -0.7496213082875958,
        -0.5260273408252374,
        -0.4429268291504218,
        -0.542345978918385,
        -0.746058546502038,
        -0.7048920473687914,
        -0.759904837015627,
        -0.7478051708167222,
        -0.7368020822577213,
        0.08641696264189504,
        -0.48971471255907467,
        -0.4596384732341402,
        -0.6673839720053346,
        -0.6738593153875055,
        -0.720831468636932,
        -0.7365542891285441,
        -0.7460559493120209,
        -0.7404030166371689,
        -0.7506398837298288
      ],
      [
        -0.628417025487219,
        -0.7515712037050544,
        -0.6942932070192835,
        -0.7427551136561755,
        -0.7562544274031023,
        -0.7368640189864006,
        -0.7254291703941782,
        -0.7055176053598498,
        -0.7252489383427683,
        -0.7289885416236522,
        -0.7449298351806433,
        -0.7174984776215245,
        -0.7298348812162818,
        -0.7311915746395699,
        1.0,
        -0.723885652752154,
        -0.7369136865638694,
        -0.7391153602621843,
        -0.7022760504846017,
        -0.749850109117123,
        -0.7170635832182614,
        -0.7479205873919221,
        -0.7178300963949902,
        -0.7470199274407952,
        -0.734333439930285,
        -0.7393455984832207,
        -0.7605649265354392,
        -0.7035640376552805,
        -0.7388997539667201,
        -0.7372592765502375,
        -0.7300766588379264,
        -0.7411664969230631,
        -0.6928084052560273,
        -0.7432231283456693,
        -0.7190439066776618,
        -0.7376549360586626,
        -0.7480378543085417,
        -0.7186509485341752,
        -0.7301860374548441,
        -0.7127700390153395,
        -0.7248864385275968,
        -0.7337580195485192,
        -0.7115287241229931,
        -0.7571669409491546,
        -0.744437158255575,
        -0.7616429388088186,
        -0.7621075759889437,
        -0.7424458033009962
      ],
      [
        -0.722176743355929,
        -0.7300569087022762,
        -0.7174659783464452,
        -0.6525343821776095,
        -0.7322819942512746,
        -0.748256929168401,
        -0.32561245981460074,
        -0.7350932974874886,
        -0.6997951766104828,
        -0.7284382484085161,
        -0.7450362706390885,
        -0.49795664641867426,
        -0.7021177171222164,
        -0.16841881449291912,
        -0.723885652752154,
        1.0,
        -0.7444349729065296,
        -0.2981676266108396,
        -0.7221998717196664,
        -0.7504602054926756,
        -0.7367673712177388,
        -0.7309738941850861,
        -0.7509306443259272,
        -0.7253889603639034,
        -0.7384638515665103,
        -0.7281601216090896,
        -0.7320067267119945,
        -0.7351649310808082,
        -0.7495609565804566,
        -0.7436765664577341,
        -0.7226745687586524,
        -0.4539396873867723,
        -0.6147469856391842,
        -0.7346570724189667,
        -0.7116785795019205,
        -0.7595045677868438,
        -0.7542335421536835,
        -0.7243678277490391,
        -0.2387649666108032,
        -0.5302416912115369,
        -0.5252750148983236,
        -0.7316128093507809,
        -0.7079919378067729,
        -0.7283844423669663,
        -0.7467792594942837,
        -0.7488497577120559,
        -0.7505228603555929,
        -0.7485013810083199
      ],
      [
        -0.7392152316560194,
        -0.7317347081351642,
        -0.7370492051920711,
        -0.7151433959581492,
        -0.7528572898689985,
        -0.7128242706271883,
        -0.7445619156492059,
        -0.7389864746818122,
        -0.7407902277161629,
        -0.7556511364448151,
        -0.7587567475174524,
        -0.7213165538476576,
        -0.687462153137974,
        -0.7496548576040305,
        -0.7369136865638694,
        -0.7444349729065296,
        1.0,
        -0.7526416677389595,
        -0.7408124760856842,
        -0.7622793542955597,
        -0.7365578998536711,
        -0.7552329689104476,
        -0.7583324244483903,
        -0.7484910151237276,
        -0.6377161140093519,
        -0.7041850016250153,
        -0.7628925156078714,
        -0.733940226233774,
        -0.759261182414374,
        -0.7195945710440586,
        -0.7424023215816629,
        -0.7494523166852348,
        -0.7405328233329855,
        -0.5954876544970865,
        -0.7473746771814009,
        -0.7598032415649378,
        -0.7619292896271153,
        -0.7425524061150608,
        -0.7484654622602323,
        -0.7486634566311177,
        -0.7487546397787044,
        -0.7354485853636226,
        -0.7415863160801609,
        -0.7628817053967836,
        -0.7605117430722887,
        -0.7595162539987019,
        -0.7638175474662741,
        -0.7391440710697335
      ],
      [
        -0.735651461943668,
        -0.728892264325004,
        -0.7512580860653648,
        -0.672165455371756,
        -0.7438352850677548,
        -0.7503105082066674,
        -0.2539233577185598,
        -0.7331321873618276,
        -0.6912173494645532,
        -0.7209827751453328,
        -0.7407826601903896,
        -0.4646628722741674,
        -0.7206140769304783,
        -0.08272889611325228,
        -0.7391153602621843,
        -0.2981676266108396,
        -0.7526416677389595,
        1.0,
        -0.7309527687102695,
        -0.7577561320540399,
        -0.7545252558358506,
        -0.7195148505522122,
        -0.7457910632499033,
        -0.724627650186475,
        -0.7413717598598941,
        -0.7355680767333646,
        -0.7228819095665222,
        -0.7546020692603503,
        -0.7477717703198942,
        -0.751325477258866,
        -0.7286555145685509,
        -0.49529151912068237,
        -0.5956337191179085,
        -0.7496414565858402,
        -0.6811537370847538,
        -0.759852707622705,
        -0.7519596313608947,
        -0.7462482159247066,
        -0.18243528464527117,
        -0.5271795923778853,
        -0.48800288408224357,
        -0.7423245530660273,
        -0.7208562979948916,
        -0.7047857258341034,
        -0.7412654085276519,
        -0.7497356263105319,
        -0.7417921051383976,
        -0.754134266781497
      ],
      [
        -0.7143535021238276,
        -0.7379973974985093,
        -0.7131797915999514,
        -0.7321673702594723,
        -0.7440150719526011,
        -0.7197992930602624,
        -0.7284091773572363,
        -0.7216658770281903,
        -0.7232479049488327,
        -0.7342506773328603,
        -0.7433345754368883,
        -0.7149775656854194,
        -0.7245524367789045,
        -0.7243576500733745,
        -0.7022760504846017,
        -0.7221998717196664,
        -0.7408124760856842,
        -0.7309527687102695,
        1.0,
        -0.755780703461951,
        -0.7238299085279714,
        -0.7489520475957336,
        -0.748477928139539,
        -0.7358411908412409,
        -0.7275212913845579,
        -0.7339521523915613,
        -0.7575926186658195,
        -0.7117630763445342,
        -0.7303368728011852,
        -0.7291991162702138,
        -0.7354388289048666,
        -0.7379518853326392,
        -0.7234245289318695,
        -0.7323425174714754,
        -0.7287408347484429,
        -0.7541759162220668,
        -0.7582888065069082,
        -0.7047878331585009,
        -0.7234770812771232,
        -0.7357333133083177,
        -0.7281545588421648,
        -0.695383728206018,
        -0.7090349711895989,
        -0.7517645118939382,
        -0.7204197621621795,
        -0.7558431153782881,
        -0.7616176045963637,
        -0.6049985401858048
      ],
      [
        -0.751181893866431,
        -0.7498377897055069,
        -0.7470593791049591,
        -0.7596578425610677,
        -0.7618166788377841,
        -0.7542197305851317,
        -0.7583068396289179,
        -0.7501747653530226,
        -0.7489646091505735,
        -0.7513308195849473,
        -0.7624615156921748,
        -0.7527425359147173,
        -0.7587579943597148,
        -0.7425562780959453,
        -0.749850109117123,
        -0.7504602054926756,
        -0.7622793542955597,
        -0.7577561320540399,
        -0.755780703461951,
        1.0,
        -0.724315113203156,
        -0.7598788192374031,
        -0.7635240486360637,
        -0.7610210339121999,
        -0.7578742531660708,
        -0.7553637600094254,
        -0.7637026596968663,
        -0.7573535280828931,
        -0.7617584121780063,
        -0.7555137111178647,
        -0.734648356676153,
        -0.7593138363078795,
        -0.7529325181724851,
        -0.7583136252369419,
        -0.7538810444201114,
        -0.7595083610750409,
        -0.7406568546947513,
        -0.7448562450300091,
        -0.7517407268162767,
        -0.7579292120522474,
        -0.756768147069139,
        -0.7263219902093515,
        -0.5576368556075153,
        -0.7633987481540858,
        -0.7566596640690992,
        -0.7621809642853912,
        -0.7593672347072533,
        -0.758236182926751
      ],
      [
        -0.7202221966011519,
        -0.7507196390089657,
        -0.5765891087632047,
        -0.7437621120560113,
        -0.7472106850283414,
        -0.7243140197433378,
        -0.7516264973805742,
        -0.7196697514810879,
        -0.722158737208098,
        -0.7355195245511705,
        -0.7522763963818515,
        -0.7376733812640683,
        -0.7308583066917431,
        -0.7469038190048731,
        -0.7170635832182614,
        -0.7367673712177388,
        -0.7365578998536711,
        -0.7545252558358506,
        -0.7238299085279714,
        -0.724315113203156,
        1.0,
        -0.7542275532024947,
        -0.7539880150125873,
        -0.7449971327592717,
        -0.7378052482892917,
        -0.7342918287108879,
        -0.7611858128530705,
        -0.5754605059974298,
        -0.7563663576794417,
        -0.7244578946195452,
        -0.7314135510173386,
        -0.7521646828303106,
        -0.7298999685332737,
        -0.7328223158750474,
        -0.7356166116543421,
        -0.7551562645336933,
        -0.7581812898687301,
        -0.7029971391997354,
        -0.7489837822244916,
        -0.749405177589965,
        -0.7455917594805548,
        -0.49987012849134327,
        -0.7229544304556578,
        -0.7621072757811963,
        -0.7564212482576711,
        -0.7593079577192144,
        -0.7637930796872943,
        -0.7396273962411177
      ],
      [
        -0.745898149191424,
        -0.7504205904515195,
        -0.7423858414277,
        -0.7482040171400741,
        -0.7508132302186066,
        -0.756729560202906,
        -0.7227361537194321,
        -0.7555049042058899,
        -0.5025831978391595,
        -0.7361931379047173,
        -0.733471522410267,
        -0.7219184933599281,
        -0.7474551043650752,
        -0.7189133813259301,
        -0.7479205873919221,
        -0.7309738941850861,
        -0.7552329689104476,
        -0.7195148505522122,
        -0.7489520475957336,
        -0.7598788192374031,
        -0.7542275532024947,
        1.0,
        -0.7547251732493256,
        -0.7460433937337134,
        -0.7454018347036673,
        -0.7447200234226128,
        -0.7472968675019145,
        -0.7520326778771782,
        -0.7578498849204358,
        -0.7541273748904582,
        -0.7535342012927457,
        -0.7359335829953736,
        -0.7357287014505745,
        -0.7528509602624937,
        -0.7346258641697384,
        -0.7622769711628881,
        -0.7444837081955311,
        -0.7521545338132031,
        -0.7148357026123267,
        -0.733877521338761,
        -0.7143168230101369,
        -0.7515359504312551,
        -0.7387672723916769,
        -0.754673934152275,
        -0.7547258580715113,
        -0.7526780013856567,
        -0.7507357640141232,
        -0.7559727650817046
      ],
      [
        -0.7462376369684469,
        -0.7594880097083335,
        -0.7428080322634649,
        -0.7505611853678448,
        -0.760285046656504,
        -0.756057578513067,
        -0.6916335507065889,
        -0.7340377956331403,
        -0.7421958041480565,
        -0.7520806627430672,
        -0.7336168646295067,
        -0.739216855535813,
        -0.7512066823244894,
        -0.7457184002297488,
        -0.7178300963949902,
        -0.7509306443259272,
        -0.7583324244483903,
        -0.7457910632499033,
        -0.748477928139539,
        -0.7635240486360637,
        -0.7539880150125873,
        -0.7547251732493256,
        1.0,
        -0.7568795839551514,
        -0.7538933589101595,
        -0.7580629505174372,
        -0.7608827252725426,
        -0.7472951492802483,
        -0.7483723569198018,
        -0.7592408821443097,
        -0.7562772449603372,
        -0.7322788298854332,
        -0.740097675286699,
        -0.7509083878207741,
        -0.728952133817321,
        -0.7531438875875366,
        -0.7616141310446262,
        -0.7513647948251119,
        -0.7477516587436684,
        -0.7464525386038818,
        -0.7503163101994911,
        -0.7558236986653013,
        -0.7539198022552025,
        -0.7606040972263894,
        -0.7615745016072745,
        -0.7623457703757317,
        -0.7560014570037432,
        -0.7608522331767948
      ],
      [
        -0.7538549813919853,
        -0.7494251101378725,
        -0.7474248166653338,
        -0.7288940312746204,
        -0.6832000275517349,
        -0.7510396910792195,
        -0.7299396314525982,
        -0.7345712658632122,
        -0.7109614083735272,
        -0.7122286982027223,
        -0.747489885021131,
        -0.7272325042094524,
        -0.7178982764173172,
        -0.7130289888357078,
        -0.7470199274407952,
        -0.7253889603639034,
        -0.7484910151237276,
        -0.724627650186475,
        -0.7358411908412409,
        -0.7610210339121999,
        -0.7449971327592717,
        -0.7460433937337134,
        -0.7568795839551514,
        1.0,
        -0.7398317467843957,
        -0.699033363211555,
        -0.7531348416274848,
        -0.7509060703148933,
        -0.7545056898397765,
        -0.7067081294192019,
        -0.7358172294738883,
        -0.7153389705568576,
        -0.7389151347971408,
        -0.6703428481006215,
        -0.7343652690352018,
        -0.7618974765825532,
        -0.7537922741282935,
        -0.752225722707929,
        -0.7037134271525638,
        -0.7411953330796235,
        -0.7185611522592659,
        -0.5387304623379131,
        -0.7287043082026822,
        -0.7357281684448131,
        -0.729336433345111,
        -0.7285429492501138,
        -0.7522864263063294,
        -0.7459706034016564
      ],
      [
        -0.7027176370632308,
        -0.7515107718542943,
        -0.7338692032460128,
        -0.6363289998103434,
        -0.735843295238892,
        -0.7402307176536971,
        -0.7343193762463216,
        -0.7465317425675291,
        -0.7185622745587932,
        -0.7425998957805664,
        -0.7508397929899502,
        -0.7238170029118021,
        -0.7045807225951616,
        -0.738558100775318,
        -0.734333439930285,
        -0.7384638515665103,
        -0.6377161140093519,
        -0.7413717598598941,
        -0.7275212913845579,
        -0.7578742531660708,
        -0.7378052482892917,
        -0.7454018347036673,
        -0.7538933589101595,
        -0.7398317467843957,
        1.0,
        -0.6508663498580183,
        -0.7537394978538929,
        -0.7445401365662586,
        -0.7550018998768857,
        -0.7309264947584657,
        -0.7423595307638733,
        -0.7403076750478328,
        -0.7146506623522242,
        -0.696212081595791,
        -0.7427884318723745,
        -0.7598620595707596,
        -0.7604691956853378,
        -0.7442352127485312,
        -0.7381085851307227,
        -0.6401291366642194,
        -0.7410585119388086,
        -0.7343357249292675,
        -0.7303437816776583,
        -0.7597685206546627,
        -0.7602477434361121,
        -0.7462746496479629,
        -0.7620523659887795,
        -0.7478743771250003
      ],
      [
        -0.7419163897536021,
        -0.7234118724263374,
        -0.7248696472098268,
        -0.6234710733563933,
        -0.7228536031818826,
        -0.7424612515001401,
        -0.7344867292456435,
        -0.7433213416745559,
        -0.7180973323747875,
        -0.7408646459513193,
        -0.7462672560319128,
        -0.715860398875084,
        -0.5092416214725181,
        -0.7279047708594941,
        -0.7393455984832207,
        -0.7281601216090896,
        -0.7041850016250153,
        -0.7355680767333646,
        -0.7339521523915613,
        -0.7553637600094254,
        -0.7342918287108879,
        -0.7447200234226128,
        -0.7580629505174372,
        -0.699033363211555,
        -0.6508663498580183,
        1.0,
        -0.7570595609167143,
        -0.7353000958079818,
        -0.759279354802121,
        -0.4307945704983366,
        -0.7150092435774678,
        -0.737654045984472,
        -0.7419637873086993,
        -0.6810975055193633,
        -0.7398940138954324,
        -0.7597890303528693,
        -0.7474352014437834,
        -0.7440448970390962,
        -0.7190132794757254,
        -0.740021089040648,
        -0.7347004315682164,
        -0.6985982283228966,
        -0.7156048728034126,
        -0.7583647021293087,
        -0.7540710692607424,
        -0.7209985432433745,
        -0.7606413213967855,
        -0.7320900557890442
      ],
      [
        -0.754180081058996,
        -0.7564554157374876,
        -0.7608049204354992,
        -0.749077534185823,
        -0.7404539905072651,
        -0.7624466663520492,
        -0.7314624454958856,
        -0.7583356825049266,
        -0.7424512726965777,
        -0.7255236669290329,
        -0.7584140101710217,
        -0.741865051828335,
        -0.7553217445888998,
        -0.7267774013900439,
        -0.7605649265354392,
        -0.7320067267119945,
        -0.7628925156078714,
        -0.7228819095665222,
        -0.7575926186658195,
        -0.7637026596968663,
        -0.7611858128530705,
        -0.7472968675019145,
        -0.7608827252725426,
        -0.7531348416274848,
        -0.7537394978538929,
        -0.7570595609167143,
        1.0,
        -0.763384314673698,
        -0.7601347125127476,
        -0.7629266727723467,
        -0.7580947627849934,
        -0.7364104566589691,
        -0.733286539131207,
        -0.760278435680972,
        -0.7471383248061791,
        -0.7648664072546315,
        -0.7634070154536376,
        -0.7635048568702147,
        -0.7300758166045321,
        -0.7372333579393174,
        -0.7464922019713627,
        -0.7612443794941155,
        -0.7558303768896044,
        -0.7302815154208182,
        -0.7604693249565135,
        -0.7235799318493038,
        -0.7628469515900067,
        -0.7632153727477897
      ],
      [
        -0.7205948171348011,
        -0.7494796453076207,
        -0.019173992937889604,
        -0.7436203874030702,
        -0.7474960838081979,
        -0.7402670721311292,
        -0.7453642287063897,
        -0.7072045565409665,
        -0.7337033996584654,
        -0.7517728036464095,
        -0.7345249968281427,
        -0.7297405288835123,
        -0.7249748873717903,
        -0.7483331210337288,
        -0.7035640376552805,
        -0.7351649310808082,
        -0.733940226233774,
        -0.7546020692603503,
        -0.7117630763445342,
        -0.7573535280828931,
        -0.5754605059974298,
        -0.7520326778771782,
        -0.7472951492802483,
        -0.7509060703148933,
        -0.7445401365662586,
        -0.7353000958079818,
        -0.763384314673698,
        1.0,
        -0.7497864001560693,
        -0.7220366820269499,
        -0.7215795316770977,
        -0.7485765312459047,
        -0.746265958993262,
        -0.7386409768346314,
        -0.7343750636732902,
        -0.7429675211300135,
        -0.7591208536812657,
        -0.7080311759809041,
        -0.7403241915802949,
        -0.7503589338608824,
        -0.7290119385056169,
        -0.7161934755615265,
        -0.7223660145148361,
        -0.7618551809885825,
        -0.756605825162477,
        -0.7609447095893221,
        -0.7641562534973823,
        -0.716249457113361
      ],
      [
        -0.7461446845329192,
        -0.7551997065917657,
        -0.748725837349117,
        -0.7507157805048215,
        -0.75992468369895,
        -0.7531108442956225,
        -0.7290216447103111,
        -0.7296739746012317,
        -0.7471273846590629,
        -0.7508265767012874,
        -0.7361809418772363,
        -0.7500083864190215,
        -0.7529110347277881,
        -0.7443609861299756,
        -0.7388997539667201,
        -0.7495609565804566,
        -0.759261182414374,
        -0.7477717703198942,
        -0.7303368728011852,
        -0.7617584121780063,
        -0.7563663576794417,
        -0.7578498849204358,
        -0.7483723569198018,
        -0.7545056898397765,
        -0.7550018998768857,
        -0.759279354802121,
        -0.7601347125127476,
        -0.7497864001560693,
        1.0,
        -0.7593829250668209,
        -0.755160599452098,
        -0.7523729115993767,
        -0.7430857260824433,
        -0.7579375792818068,
        -0.7442609009739305,
        -0.7582234191616106,
        -0.7407513549274694,
        -0.7533926223612915,
        -0.7280025133229537,
        -0.749561937496688,
        -0.7457343589139647,
        -0.7564626313432379,
        -0.7460924323708904,
        -0.7613194506774879,
        -0.7555174027229064,
        -0.7596810798472462,
        -0.7597321335022735,
        -0.7481772532433194
      ],
      [
        -0.7446621672857452,
        -0.705829980088949,
        -0.7041239468883747,
        -0.6960685287218537,
        -0.7510628548648293,
        -0.742052054531819,
        -0.7490933580236,
        -0.7467623004189756,
        -0.7305230559402405,
        -0.750769009648698,
        -0.7484701336630938,
        -0.7289819395490336,
        -0.5918229181829624,
        -0.7496213082875958,
        -0.7372592765502375,
        -0.7436765664577341,
        -0.7195945710440586,
        -0.751325477258866,
        -0.7291991162702138,
        -0.7555137111178647,
        -0.7244578946195452,
        -0.7541273748904582,
        -0.7592408821443097,
        -0.7067081294192019,
        -0.7309264947584657,
        -0.4307945704983366,
        -0.7629266727723467,
        -0.7220366820269499,
        -0.7593829250668209,
        1.0,
        -0.7339131126679278,
        -0.7457984473256118,
        -0.7563128985618908,
        -0.6104770452655032,
        -0.7440802702822814,
        -0.759159642835396,
        -0.7601374871140744,
        -0.740318362542526,
        -0.7423606297964928,
        -0.7530733916026168,
        -0.7363454520959998,
        -0.6899574954757282,
        -0.7157366292502217,
        -0.7597283240107024,
        -0.7570896958941655,
        -0.7551366837510428,
        -0.7626732536976596,
        -0.4669951468818231
      ],
      [
        -0.723951456017963,
        -0.7502923113369657,
        -0.704828679831523,
        -0.7291330998754043,
        -0.741473743686532,
        -0.7310644663511997,
        -0.7316943394170079,
        -0.7340061546901164,
        -0.7302047497276589,
        -0.7478119869875337,
        -0.742977097849638,
        -0.7179065849460149,
        -0.7047972275349647,
        -0.5260273408252374,
        -0.7300766588379264,
        -0.7226745687586524,
        -0.7424023215816629,
        -0.7286555145685509,
        -0.7354388289048666,
        -0.734648356676153,
        -0.7314135510173386,
        -0.7535342012927457,
        -0.7562772449603372,
        -0.7358172294738883,
        -0.7423595307638733,
        -0.7150092435774678,
        -0.7580947627849934,
        -0.7215795316770977,
        -0.755160599452098,
        -0.7339131126679278,
        1.0,
        -0.7385431833224236,
        -0.7270899949810421,
        -0.7320301969434546,
        -0.7373006197631253,
        -0.758371921026687,
        -0.7476241422904517,
        -0.7417084882866279,
        -0.7165758080728919,
        -0.7329621567604008,
        -0.7422493853012814,
        -0.6358184785594911,
        -0.6743267037814432,
        -0.7602758406304906,
        -0.7554369644452059,
        -0.7587387772905194,
        -0.7601148157248414,
        -0.746051164213609
      ],
      [
        -0.7430617295245486,
        -0.7399443881993364,
        -0.7420424798685405,
        -0.6726148052116707,
        -0.700836623784493,
        -0.7573579761567664,
        -0.49150372910550794,
        -0.745758409813323,
        -0.715277137989595,
        -0.7402333987248315,
        -0.7507231094160172,
        -0.6191252653962576,
        -0.7125883869037495,
        -0.4429268291504218,
        -0.7411664969230631,
        -0.4539396873867723,
        -0.7494523166852348,
        -0.49529151912068237,
        -0.7379518853326392,
        -0.7593138363078795,
        -0.7521646828303106,
        -0.7359335829953736,
        -0.7322788298854332,
        -0.7153389705568576,
        -0.7403076750478328,
        -0.737654045984472,
        -0.7364104566589691,
        -0.7485765312459047,
        -0.7523729115993767,
        -0.7457984473256118,
        -0.7385431833224236,
        1.0,
        -0.6804956762963974,
        -0.7446335200733383,
        -0.7204796275960053,
        -0.7597970802971924,
        -0.7570275493542549,
        -0.7476521297077416,
        -0.44086959810044773,
        -0.5582724193971977,
        -0.6079224823803866,
        -0.7457437131171649,
        -0.7259003999097975,
        -0.7222447786236939,
        -0.7518190471899842,
        -0.7494815536876591,
        -0.749945837617226,
        -0.7535411252391658
      ],
      [
        -0.45270303363990394,
        -0.7543337406545458,
        -0.7278835187244403,
        -0.7238794848757057,
        -0.7547172053713409,
        -0.7027769984554515,
        -0.5783889166754241,
        -0.7342442295210126,
        -0.6566547898906259,
        -0.7266545763396589,
        -0.7460912493377219,
        -0.6219343947636009,
        -0.742285570151304,
        -0.542345978918385,
        -0.6928084052560273,
        -0.6147469856391842,
        -0.7405328233329855,
        -0.5956337191179085,
        -0.7234245289318695,
        -0.7529325181724851,
        -0.7298999685332737,
        -0.7357287014505745,
        -0.740097675286699,
        -0.7389151347971408,
        -0.7146506623522242,
        -0.7419637873086993,
        -0.733286539131207,
        -0.746265958993262,
        -0.7430857260824433,
        -0.7563128985618908,
        -0.7270899949810421,
        -0.6804956762963974,
        1.0,
        -0.7484758211025183,
        -0.7235660570345247,
        -0.7551744222367314,
        -0.747110372249599,
        -0.7284758715059954,
        -0.5918677514391861,
        -0.6127900622064638,
        -0.6366167033136563,
        -0.7387246408242281,
        -0.7343856206720929,
        -0.747522342491231,
        -0.7362222027078313,
        -0.7537891859280863,
        -0.7558719215061933,
        -0.7522657636409431
      ],
      [
        -0.7426410317467265,
        -0.6749706698248853,
        -0.7291372378743481,
        -0.6719983714761486,
        -0.7503804939388383,
        -0.7120746538426704,
        -0.7406954818321619,
        -0.7432414903313671,
        -0.7408399868165774,
        -0.753357873240007,
        -0.7530697257133359,
        -0.709403360526668,
        -0.6731229827964806,
        -0.746058546502038,
        -0.7432231283456693,
        -0.7346570724189667,
        -0.5954876544970865,
        -0.7496414565858402,
        -0.7323425174714754,
        -0.7583136252369419,
        -0.7328223158750474,
        -0.7528509602624937,
        -0.7509083878207741,
        -0.6703428481006215,
        -0.696212081595791,
        -0.6810975055193633,
        -0.760278435680972,
        -0.7386409768346314,
        -0.7579375792818068,
        -0.6104770452655032,
        -0.7320301969434546,
        -0.7446335200733383,
        -0.7484758211025183,
        1.0,
        -0.740073652069793,
        -0.7623001124944306,
        -0.762100382213762,
        -0.7424403007903657,
        -0.7422488303686969,
        -0.7483997940964947,
        -0.7459182507899116,
        -0.6414596432237861,
        -0.7372481225971896,
        -0.7582767848221665,
        -0.7609345548343738,
        -0.7594157582327401,
        -0.763690566963177,
        -0.7278902751578749
      ],
      [
        -0.7276127643852109,
        -0.7385348357679381,
        -0.7256998856485045,
        -0.7277667618321282,
        -0.7533596182687572,
        -0.7460028787131396,
        -0.6784687094144358,
        -0.49513418115821284,
        -0.7309878612750491,
        -0.7160757912746172,
        -0.7427739381372997,
        -0.7091801486134162,
        -0.7299496435611296,
        -0.7048920473687914,
        -0.7190439066776618,
        -0.7116785795019205,
        -0.7473746771814009,
        -0.6811537370847538,
        -0.7287408347484429,
        -0.7538810444201114,
        -0.7356166116543421,
        -0.7346258641697384,
        -0.728952133817321,
        -0.7343652690352018,
        -0.7427884318723745,
        -0.7398940138954324,
        -0.7471383248061791,
        -0.7343750636732902,
        -0.7442609009739305,
        -0.7440802702822814,
        -0.7373006197631253,
        -0.7204796275960053,
        -0.7235660570345247,
        -0.740073652069793,
        1.0,
        -0.7596123535957009,
        -0.7583939198026148,
        -0.7431749958070095,
        -0.7074461881393845,
        -0.7320214826047045,
        -0.7163851457770749,
        -0.7338256168637274,
        -0.7244383498148073,
        -0.7464221422917537,
        -0.7538624180248745,
        -0.7583750935433187,
        -0.7576663965102657,
        -0.7486570403368493
      ],
      [
        -0.7505926135582754,
        -0.7631073603269719,
        -0.7450502991229951,
        -0.7627492632431715,
        -0.762768321762741,
        -0.7607647485264317,
        -0.7582567362837188,
        -0.7522451865675922,
        -0.7543904789629505,
        -0.7613934067988146,
        -0.7617276897365688,
        -0.7577637890757873,
        -0.7584539483718196,
        -0.759904837015627,
        -0.7376549360586626,
        -0.7595045677868438,
        -0.7598032415649378,
        -0.759852707622705,
        -0.7541759162220668,
        -0.7595083610750409,
        -0.7551562645336933,
        -0.7622769711628881,
        -0.7531438875875366,
        -0.7618974765825532,
        -0.7598620595707596,
        -0.7597890303528693,
        -0.7648664072546315,
        -0.7429675211300135,
        -0.7582234191616106,
        -0.759159642835396,
        -0.758371921026687,
        -0.7597970802971924,
        -0.7551744222367314,
        -0.7623001124944306,
        -0.7596123535957009,
        1.0,
        -0.7639669458689171,
        -0.7490970298036939,
        -0.7591140709192654,
        -0.7595384267249211,
        -0.7565578585931689,
        -0.7557465132325389,
        -0.7545168527497984,
        -0.7643621508979811,
        -0.7591853189072892,
        -0.7626907790925348,
        -0.7653029980130653,
        -0.7568853252763081
      ],
      [
        -0.7569632519002565,
        -0.7561209798224826,
        -0.7563708693309246,
        -0.759713383719838,
        -0.7604291918828053,
        -0.7606307791520166,
        -0.7533649913750138,
        -0.7549041657215738,
        -0.7145460062957292,
        -0.7353638368546995,
        -0.7416523873895924,
        -0.7546171406357539,
        -0.758016013677707,
        -0.7478051708167222,
        -0.7480378543085417,
        -0.7542335421536835,
        -0.7619292896271153,
        -0.7519596313608947,
        -0.7582888065069082,
        -0.7406568546947513,
        -0.7581812898687301,
        -0.7444837081955311,
        -0.7616141310446262,
        -0.7537922741282935,
        -0.7604691956853378,
        -0.7474352014437834,
        -0.7634070154536376,
        -0.7591208536812657,
        -0.7407513549274694,
        -0.7601374871140744,
        -0.7476241422904517,
        -0.7570275493542549,
        -0.747110372249599,
        -0.762100382213762,
        -0.7583939198026148,
        -0.7639669458689171,
        1.0,
        -0.7567573018410889,
        -0.6775743956167343,
        -0.7475800804104749,
        -0.7488502322438269,
        -0.7581632183771267,
        -0.7301898396477526,
        -0.7598859513510345,
        -0.7546097560217518,
        -0.7615969169724381,
        -0.7290584083962138,
        -0.7615861609609043
      ],
      [
        -0.7211165302759314,
        -0.7570218061122236,
        -0.6997768527190759,
        -0.7482088138954577,
        -0.7519570543176275,
        -0.7347662211037076,
        -0.7429697216542315,
        -0.7258938520003722,
        -0.7068428500188335,
        -0.7491704763637305,
        -0.7554084736808729,
        -0.735206085590699,
        -0.7358772458330656,
        -0.7368020822577213,
        -0.7186509485341752,
        -0.7243678277490391,
        -0.7425524061150608,
        -0.7462482159247066,
        -0.7047878331585009,
        -0.7448562450300091,
        -0.7029971391997354,
        -0.7521545338132031,
        -0.7513647948251119,
        -0.752225722707929,
        -0.7442352127485312,
        -0.7440448970390962,
        -0.7635048568702147,
        -0.7080311759809041,
        -0.7533926223612915,
        -0.740318362542526,
        -0.7417084882866279,
        -0.7476521297077416,
        -0.7284758715059954,
        -0.7424403007903657,
        -0.7431749958070095,
        -0.7490970298036939,
        -0.7567573018410889,
        1.0,
        -0.738973970887679,
        -0.7451757147724158,
        -0.7387728165650445,
        -0.7100688355878755,
        -0.7258500872848976,
        -0.7610559178596805,
        -0.7345601137316706,
        -0.761087149112309,
        -0.7628891312616246,
        -0.7043541036493421
      ],
      [
        -0.7212692593715253,
        -0.726749718052018,
        -0.7325243939676135,
        -0.6473732539046921,
        -0.7332281062291313,
        -0.7494258072306051,
        -0.21911680530986485,
        -0.7338916024626092,
        -0.6695675233940411,
        -0.7130187158401464,
        -0.734843135738581,
        -0.44531524507598336,
        -0.7115775953013102,
        0.08641696264189504,
        -0.7301860374548441,
        -0.2387649666108032,
        -0.7484654622602323,
        -0.18243528464527117,
        -0.7234770812771232,
        -0.7517407268162767,
        -0.7489837822244916,
        -0.7148357026123267,
        -0.7477516587436684,
        -0.7037134271525638,
        -0.7381085851307227,
        -0.7190132794757254,
        -0.7300758166045321,
        -0.7403241915802949,
        -0.7280025133229537,
        -0.7423606297964928,
        -0.7165758080728919,
        -0.44086959810044773,
        -0.5918677514391861,
        -0.7422488303686969,
        -0.7074461881393845,
        -0.7591140709192654,
        -0.6775743956167343,
        -0.738973970887679,
        1.0,
        -0.5135642815919585,
        -0.489090064727276,
        -0.7352630884754474,
        -0.7043910459823518,
        -0.7255976099335437,
        -0.740771516962533,
        -0.7421192889449422,
        -0.7400431537654284,
        -0.7446901567715749
      ],
      [
        -0.6248754128292371,
        -0.7485209365444909,
        -0.7379118237206754,
        -0.7097249943213099,
        -0.7451882079898238,
        -0.7405191130051698,
        -0.5288834617708856,
        -0.7460419851454481,
        -0.69390405867889,
        -0.688017591175123,
        -0.7434693283706328,
        -0.6454096726319427,
        -0.7098775184560879,
        -0.48971471255907467,
        -0.7127700390153395,
        -0.5302416912115369,
        -0.7486634566311177,
        -0.5271795923778853,
        -0.7357333133083177,
        -0.7579292120522474,
        -0.749405177589965,
        -0.733877521338761,
        -0.7464525386038818,
        -0.7411953330796235,
        -0.6401291366642194,
        -0.740021089040648,
        -0.7372333579393174,
        -0.7503589338608824,
        -0.749561937496688,
        -0.7530733916026168,
        -0.7329621567604008,
        -0.5582724193971977,
        -0.6127900622064638,
        -0.7483997940964947,
        -0.7320214826047045,
        -0.7595384267249211,
        -0.7475800804104749,
        -0.7451757147724158,
        -0.5135642815919585,
        1.0,
        -0.6200565338686717,
        -0.7481183636528131,
        -0.7335904261003892,
        -0.7373873033580455,
        -0.7504397763782986,
        -0.7533935487400705,
        -0.7566211844785524,
        -0.7573015955401252
      ],
      [
        -0.7303215248260192,
        -0.7355917108382779,
        -0.7340292539081288,
        -0.7266042859673955,
        -0.7451243824022302,
        -0.7444502907428093,
        -0.5178170010953047,
        -0.736521802966631,
        -0.6490400592607454,
        -0.7270284922981685,
        -0.7334549608332313,
        -0.5845416359059361,
        -0.7299267410014914,
        -0.4596384732341402,
        -0.7248864385275968,
        -0.5252750148983236,
        -0.7487546397787044,
        -0.48800288408224357,
        -0.7281545588421648,
        -0.756768147069139,
        -0.7455917594805548,
        -0.7143168230101369,
        -0.7503163101994911,
        -0.7185611522592659,
        -0.7410585119388086,
        -0.7347004315682164,
        -0.7464922019713627,
        -0.7290119385056169,
        -0.7457343589139647,
        -0.7363454520959998,
        -0.7422493853012814,
        -0.6079224823803866,
        -0.6366167033136563,
        -0.7459182507899116,
        -0.7163851457770749,
        -0.7565578585931689,
        -0.7488502322438269,
        -0.7387728165650445,
        -0.489090064727276,
        -0.6200565338686717,
        1.0,
        -0.7342060216780014,
        -0.7178500658308054,
        -0.7214913746977574,
        -0.7435339586797998,
        -0.7502346951066192,
        -0.751187798912907,
        -0.7389951029470172
      ],
      [
        -0.7383934512530191,
        -0.7420283758321775,
        -0.7028345179851048,
        -0.7357789191877873,
        -0.7432466722346607,
        -0.6088393776046965,
        -0.7427791846487899,
        -0.7132177333917417,
        -0.7139171812879364,
        -0.7406646641111694,
        -0.7421021754028658,
        -0.7194539457343283,
        -0.7260179434869568,
        -0.6673839720053346,
        -0.7337580195485192,
        -0.7316128093507809,
        -0.7354485853636226,
        -0.7423245530660273,
        -0.695383728206018,
        -0.7263219902093515,
        -0.49987012849134327,
        -0.7515359504312551,
        -0.7558236986653013,
        -0.5387304623379131,
        -0.7343357249292675,
        -0.6985982283228966,
        -0.7612443794941155,
        -0.7161934755615265,
        -0.7564626313432379,
        -0.6899574954757282,
        -0.6358184785594911,
        -0.7457437131171649,
        -0.7387246408242281,
        -0.6414596432237861,
        -0.7338256168637274,
        -0.7557465132325389,
        -0.7581632183771267,
        -0.7100688355878755,
        -0.7352630884754474,
        -0.7481183636528131,
        -0.7342060216780014,
        1.0,
        -0.6702169772967386,
        -0.7587439849773446,
        -0.7461223814040365,
        -0.7573640926523746,
        -0.7613350513808187,
        -0.7274280892846596
      ],
      [
        -0.7252251379710122,
        -0.7121760149725361,
        -0.7000695962640506,
        -0.7342485080262633,
        -0.7425349841538609,
        -0.7360512999572324,
        -0.7269245254350015,
        -0.7256283542282564,
        -0.6910871218542867,
        -0.7228282411625746,
        -0.7427266079519801,
        -0.7088377107421857,
        -0.7153058201096669,
        -0.6738593153875055,
        -0.7115287241229931,
        -0.7079919378067729,
        -0.7415863160801609,
        -0.7208562979948916,
        -0.7090349711895989,
        -0.5576368556075153,
        -0.7229544304556578,
        -0.7387672723916769,
        -0.7539198022552025,
        -0.7287043082026822,
        -0.7303437816776583,
        -0.7156048728034126,
        -0.7558303768896044,
        -0.7223660145148361,
        -0.7460924323708904,
        -0.7157366292502217,
        -0.6743267037814432,
        -0.7259003999097975,
        -0.7343856206720929,
        -0.7372481225971896,
        -0.7244383498148073,
        -0.7545168527497984,
        -0.7301898396477526,
        -0.7258500872848976,
        -0.7043910459823518,
        -0.7335904261003892,
        -0.7178500658308054,
        -0.6702169772967386,
        1.0,
        -0.7517939234678499,
        -0.7386418308279628,
        -0.7561511255160914,
        -0.7460253915493057,
        -0.7254826826834435
      ],
      [
        -0.7571265232460268,
        -0.7492790387016656,
        -0.7601426884499315,
        -0.7476825030657115,
        -0.7560455928129031,
        -0.7617577567885323,
        -0.7319805785634941,
        -0.7528467469086991,
        -0.7499159917184746,
        -0.7512635522454838,
        -0.758216655329925,
        -0.7223443024188752,
        -0.751364770905792,
        -0.720831468636932,
        -0.7571669409491546,
        -0.7283844423669663,
        -0.7628817053967836,
        -0.7047857258341034,
        -0.7517645118939382,
        -0.7633987481540858,
        -0.7621072757811963,
        -0.754673934152275,
        -0.7606040972263894,
        -0.7357281684448131,
        -0.7597685206546627,
        -0.7583647021293087,
        -0.7302815154208182,
        -0.7618551809885825,
        -0.7613194506774879,
        -0.7597283240107024,
        -0.7602758406304906,
        -0.7222447786236939,
        -0.747522342491231,
        -0.7582767848221665,
        -0.7464221422917537,
        -0.7643621508979811,
        -0.7598859513510345,
        -0.7610559178596805,
        -0.7255976099335437,
        -0.7373873033580455,
        -0.7214913746977574,
        -0.7587439849773446,
        -0.7517939234678499,
        1.0,
        -0.7569541122768108,
        -0.7611554477412901,
        -0.7466792572593619,
        -0.7603564638041704
      ],
      [
        -0.7440077157324145,
        -0.7603287643117549,
        -0.7550022330134084,
        -0.760142046997315,
        -0.7621621447660414,
        -0.755680334191856,
        -0.7440384366083421,
        -0.7469644529166302,
        -0.7272834567802446,
        -0.7455047017995026,
        -0.7535464112390968,
        -0.74781436468727,
        -0.758494278456385,
        -0.7365542891285441,
        -0.744437158255575,
        -0.7467792594942837,
        -0.7605117430722887,
        -0.7412654085276519,
        -0.7204197621621795,
        -0.7566596640690992,
        -0.7564212482576711,
        -0.7547258580715113,
        -0.7615745016072745,
        -0.729336433345111,
        -0.7602477434361121,
        -0.7540710692607424,
        -0.7604693249565135,
        -0.756605825162477,
        -0.7555174027229064,
        -0.7570896958941655,
        -0.7554369644452059,
        -0.7518190471899842,
        -0.7362222027078313,
        -0.7609345548343738,
        -0.7538624180248745,
        -0.7591853189072892,
        -0.7546097560217518,
        -0.7345601137316706,
        -0.740771516962533,
        -0.7504397763782986,
        -0.7435339586797998,
        -0.7461223814040365,
        -0.7386418308279628,
        -0.7569541122768108,
        1.0,
        -0.7626772324630257,
        -0.7516316119368696,
        -0.7353139905286374
      ],
      [
        -0.7616543428094578,
        -0.7587538121186567,
        -0.758455741547784,
        -0.71843841010263,
        -0.6681219714320774,
        -0.7595319225619396,
        -0.7483177541870419,
        -0.7583532480931918,
        -0.7459166854641361,
        -0.7518327938534795,
        -0.7587898461369773,
        -0.747070915239791,
        -0.7501896313309337,
        -0.7460559493120209,
        -0.7616429388088186,
        -0.7488497577120559,
        -0.7595162539987019,
        -0.7497356263105319,
        -0.7558431153782881,
        -0.7621809642853912,
        -0.7593079577192144,
        -0.7526780013856567,
        -0.7623457703757317,
        -0.7285429492501138,
        -0.7462746496479629,
        -0.7209985432433745,
        -0.7235799318493038,
        -0.7609447095893221,
        -0.7596810798472462,
        -0.7551366837510428,
        -0.7587387772905194,
        -0.7494815536876591,
        -0.7537891859280863,
        -0.7594157582327401,
        -0.7583750935433187,
        -0.7626907790925348,
        -0.7615969169724381,
        -0.761087149112309,
        -0.7421192889449422,
        -0.7533935487400705,
        -0.7502346951066192,
        -0.7573640926523746,
        -0.7561511255160914,
        -0.7611554477412901,
        -0.7626772324630257,
        1.0,
        -0.7641743645168741,
        -0.7611621241137845
      ],
      [
        -0.7628203372596114,
        -0.7610499893801342,
        -0.7630272242558825,
        -0.7632022539592869,
        -0.762923370476084,
        -0.7608121662082934,
        -0.7400166608897483,
        -0.763037260716054,
        -0.7175265267531293,
        -0.7591189018535877,
        -0.7480580478302463,
        -0.7520416508138221,
        -0.7603235360281577,
        -0.7404030166371689,
        -0.7621075759889437,
        -0.7505228603555929,
        -0.7638175474662741,
        -0.7417921051383976,
        -0.7616176045963637,
        -0.7593672347072533,
        -0.7637930796872943,
        -0.7507357640141232,
        -0.7560014570037432,
        -0.7522864263063294,
        -0.7620523659887795,
        -0.7606413213967855,
        -0.7628469515900067,
        -0.7641562534973823,
        -0.7597321335022735,
        -0.7626732536976596,
        -0.7601148157248414,
        -0.749945837617226,
        -0.7558719215061933,
        -0.763690566963177,
        -0.7576663965102657,
        -0.7653029980130653,
        -0.7290584083962138,
        -0.7628891312616246,
        -0.7400431537654284,
        -0.7566211844785524,
        -0.751187798912907,
        -0.7613350513808187,
        -0.7460253915493057,
        -0.7466792572593619,
        -0.7516316119368696,
        -0.7641743645168741,
        1.0,
        -0.7636879116214528
      ],
      [
        -0.7466090802601986,
        -0.7402244960125798,
        -0.7167741320678609,
        -0.7484031111012921,
        -0.7574910308768416,
        -0.7020395766033567,
        -0.7531209287243417,
        -0.7395348988230745,
        -0.7310743579786079,
        -0.7546082797627627,
        -0.7526105427047823,
        -0.7409029126730706,
        -0.734262057281142,
        -0.7506398837298288,
        -0.7424458033009962,
        -0.7485013810083199,
        -0.7391440710697335,
        -0.754134266781497,
        -0.6049985401858048,
        -0.758236182926751,
        -0.7396273962411177,
        -0.7559727650817046,
        -0.7608522331767948,
        -0.7459706034016564,
        -0.7478743771250003,
        -0.7320900557890442,
        -0.7632153727477897,
        -0.716249457113361,
        -0.7481772532433194,
        -0.4669951468818231,
        -0.746051164213609,
        -0.7535411252391658,
        -0.7522657636409431,
        -0.7278902751578749,
        -0.7486570403368493,
        -0.7568853252763081,
        -0.7615861609609043,
        -0.7043541036493421,
        -0.7446901567715749,
        -0.7573015955401252,
        -0.7389951029470172,
        -0.7274280892846596,
        -0.7254826826834435,
        -0.7603564638041704,
        -0.7353139905286374,
        -0.7611621241137845,
        -0.7636879116214528,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        55,
        3,
        2,
        4,
        4,
        6,
        12,
        15,
        29,
        7,
        11,
        8,
        3,
        0,
        25,
        0,
        5,
        2,
        7,
        1,
        1,
        2,
        15,
        5,
        4,
        3,
        5,
        1,
        5,
        1,
        6,
        8,
        34,
        2,
        11,
        2,
        7,
        8,
        4,
        6,
        5,
        1,
        6,
        3,
        5,
        2,
        1,
        0
      ],
      "2020-02": [
        71,
        2,
        5,
        2,
        8,
        11,
        11,
        24,
        26,
        4,
        12,
        9,
        4,
        1,
        32,
        4,
        4,
        4,
        9,
        2,
        7,
        7,
        14,
        5,
        1,
        7,
        6,
        0,
        12,
        1,
        11,
        10,
        27,
        6,
        21,
        8,
        16,
        2,
        8,
        5,
        4,
        1,
        11,
        2,
        3,
        2,
        2,
        2
      ],
      "2020-03": [
        62,
        4,
        3,
        6,
        4,
        17,
        13,
        13,
        24,
        9,
        12,
        10,
        4,
        0,
        18,
        3,
        5,
        3,
        16,
        0,
        3,
        1,
        6,
        2,
        2,
        3,
        3,
        3,
        8,
        2,
        11,
        11,
        32,
        5,
        23,
        1,
        3,
        11,
        4,
        8,
        4,
        3,
        13,
        1,
        5,
        3,
        4,
        2
      ],
      "2020-04": [
        52,
        0,
        4,
        3,
        1,
        14,
        11,
        10,
        24,
        10,
        9,
        10,
        7,
        1,
        31,
        4,
        6,
        11,
        16,
        2,
        0,
        3,
        6,
        7,
        7,
        2,
        18,
        7,
        5,
        0,
        12,
        13,
        39,
        4,
        24,
        4,
        3,
        15,
        6,
        8,
        8,
        3,
        9,
        2,
        4,
        2,
        1,
        1
      ],
      "2020-05": [
        73,
        4,
        7,
        3,
        3,
        15,
        19,
        9,
        23,
        15,
        12,
        14,
        7,
        2,
        33,
        0,
        5,
        3,
        15,
        1,
        4,
        10,
        12,
        6,
        5,
        3,
        13,
        2,
        13,
        2,
        11,
        6,
        37,
        5,
        22,
        5,
        9,
        5,
        10,
        9,
        5,
        6,
        9,
        3,
        9,
        7,
        1,
        3
      ],
      "2020-06": [
        91,
        2,
        6,
        4,
        3,
        20,
        15,
        19,
        28,
        14,
        15,
        18,
        4,
        1,
        36,
        2,
        4,
        4,
        14,
        1,
        7,
        7,
        15,
        2,
        3,
        5,
        9,
        10,
        21,
        1,
        14,
        2,
        51,
        3,
        21,
        7,
        11,
        12,
        10,
        10,
        12,
        5,
        14,
        2,
        6,
        0,
        3,
        1
      ],
      "2020-07": [
        100,
        2,
        7,
        1,
        6,
        23,
        18,
        19,
        30,
        20,
        11,
        13,
        4,
        1,
        21,
        2,
        6,
        3,
        19,
        1,
        4,
        3,
        18,
        1,
        1,
        2,
        3,
        6,
        16,
        3,
        11,
        5,
        46,
        8,
        14,
        1,
        8,
        6,
        6,
        14,
        9,
        4,
        11,
        1,
        2,
        5,
        6,
        1
      ],
      "2020-08": [
        55,
        2,
        7,
        1,
        7,
        17,
        12,
        15,
        36,
        22,
        7,
        8,
        7,
        3,
        24,
        2,
        7,
        5,
        14,
        6,
        4,
        4,
        24,
        10,
        8,
        8,
        9,
        2,
        10,
        0,
        12,
        6,
        35,
        12,
        14,
        5,
        6,
        18,
        5,
        10,
        8,
        3,
        11,
        2,
        2,
        2,
        6,
        2
      ],
      "2020-09": [
        81,
        1,
        14,
        5,
        12,
        17,
        22,
        34,
        36,
        23,
        27,
        24,
        13,
        3,
        38,
        5,
        12,
        5,
        25,
        4,
        12,
        7,
        23,
        6,
        3,
        15,
        15,
        3,
        15,
        6,
        29,
        16,
        30,
        17,
        45,
        3,
        14,
        18,
        10,
        8,
        15,
        10,
        23,
        9,
        6,
        6,
        5,
        5
      ],
      "2020-10": [
        130,
        4,
        27,
        7,
        29,
        13,
        22,
        42,
        42,
        22,
        24,
        36,
        30,
        4,
        40,
        9,
        24,
        16,
        38,
        9,
        12,
        10,
        13,
        17,
        7,
        17,
        29,
        12,
        28,
        5,
        54,
        46,
        48,
        19,
        53,
        3,
        28,
        22,
        20,
        7,
        14,
        10,
        30,
        11,
        4,
        4,
        6,
        5
      ],
      "2020-11": [
        115,
        1,
        19,
        8,
        17,
        25,
        19,
        47,
        40,
        23,
        24,
        25,
        14,
        0,
        43,
        5,
        26,
        6,
        30,
        11,
        11,
        7,
        6,
        10,
        16,
        13,
        11,
        15,
        11,
        2,
        24,
        17,
        51,
        22,
        23,
        4,
        12,
        16,
        11,
        10,
        8,
        14,
        17,
        5,
        6,
        10,
        2,
        9
      ],
      "2020-12": [
        104,
        3,
        19,
        9,
        20,
        17,
        16,
        39,
        49,
        28,
        25,
        24,
        20,
        1,
        45,
        9,
        25,
        10,
        25,
        15,
        7,
        10,
        18,
        15,
        18,
        9,
        13,
        12,
        19,
        4,
        38,
        25,
        48,
        22,
        44,
        7,
        19,
        17,
        7,
        13,
        14,
        12,
        24,
        10,
        12,
        7,
        2,
        10
      ],
      "2021-01": [
        81,
        3,
        16,
        4,
        9,
        21,
        8,
        32,
        45,
        19,
        15,
        20,
        12,
        1,
        28,
        5,
        14,
        8,
        25,
        12,
        11,
        9,
        14,
        24,
        13,
        11,
        7,
        7,
        12,
        3,
        26,
        11,
        27,
        24,
        22,
        12,
        9,
        20,
        9,
        12,
        13,
        12,
        23,
        2,
        8,
        5,
        3,
        5
      ],
      "2021-02": [
        113,
        4,
        21,
        11,
        31,
        28,
        9,
        50,
        50,
        23,
        17,
        28,
        22,
        1,
        57,
        7,
        17,
        3,
        32,
        6,
        11,
        5,
        15,
        15,
        5,
        14,
        10,
        16,
        21,
        3,
        60,
        14,
        51,
        32,
        34,
        3,
        14,
        18,
        17,
        12,
        15,
        11,
        30,
        9,
        10,
        4,
        5,
        3
      ],
      "2021-03": [
        107,
        1,
        20,
        7,
        25,
        22,
        11,
        36,
        52,
        35,
        16,
        53,
        23,
        0,
        50,
        5,
        40,
        5,
        33,
        12,
        11,
        5,
        14,
        18,
        9,
        16,
        9,
        13,
        10,
        11,
        51,
        38,
        47,
        37,
        18,
        7,
        10,
        24,
        16,
        15,
        12,
        20,
        16,
        3,
        10,
        7,
        7,
        5
      ],
      "2021-04": [
        110,
        4,
        17,
        13,
        32,
        29,
        12,
        42,
        50,
        20,
        24,
        28,
        20,
        1,
        51,
        6,
        23,
        16,
        40,
        14,
        12,
        9,
        9,
        18,
        13,
        19,
        12,
        16,
        8,
        6,
        48,
        28,
        39,
        31,
        36,
        16,
        11,
        27,
        17,
        9,
        17,
        18,
        18,
        12,
        5,
        7,
        6,
        8
      ],
      "2021-05": [
        105,
        2,
        17,
        6,
        14,
        22,
        19,
        50,
        55,
        28,
        24,
        32,
        29,
        2,
        37,
        9,
        21,
        8,
        23,
        14,
        13,
        8,
        14,
        15,
        10,
        21,
        19,
        11,
        20,
        3,
        30,
        25,
        47,
        28,
        25,
        7,
        10,
        18,
        10,
        11,
        25,
        18,
        30,
        14,
        8,
        8,
        5,
        5
      ],
      "2021-06": [
        150,
        4,
        20,
        10,
        29,
        21,
        27,
        84,
        47,
        26,
        35,
        29,
        19,
        1,
        59,
        10,
        25,
        10,
        54,
        16,
        12,
        14,
        15,
        23,
        9,
        14,
        15,
        17,
        28,
        7,
        60,
        38,
        52,
        44,
        45,
        9,
        25,
        29,
        14,
        16,
        14,
        16,
        42,
        10,
        13,
        10,
        6,
        7
      ],
      "2021-07": [
        104,
        5,
        27,
        15,
        12,
        26,
        12,
        54,
        50,
        32,
        21,
        39,
        23,
        0,
        55,
        5,
        30,
        4,
        34,
        13,
        17,
        12,
        15,
        8,
        15,
        19,
        15,
        7,
        18,
        6,
        45,
        20,
        33,
        38,
        19,
        11,
        15,
        31,
        10,
        14,
        16,
        16,
        17,
        7,
        9,
        10,
        6,
        9
      ],
      "2021-08": [
        71,
        5,
        13,
        8,
        27,
        18,
        24,
        35,
        40,
        38,
        22,
        28,
        22,
        1,
        36,
        4,
        33,
        7,
        33,
        17,
        12,
        9,
        27,
        14,
        10,
        15,
        10,
        8,
        15,
        6,
        38,
        18,
        24,
        37,
        19,
        2,
        11,
        31,
        14,
        7,
        14,
        17,
        17,
        6,
        13,
        7,
        5,
        6
      ],
      "2021-09": [
        108,
        4,
        13,
        11,
        28,
        33,
        29,
        54,
        39,
        35,
        19,
        34,
        28,
        2,
        42,
        9,
        29,
        10,
        36,
        10,
        5,
        5,
        31,
        15,
        8,
        19,
        20,
        13,
        15,
        2,
        38,
        53,
        66,
        44,
        32,
        10,
        13,
        25,
        21,
        10,
        14,
        19,
        24,
        7,
        6,
        12,
        3,
        6
      ],
      "2021-10": [
        131,
        6,
        29,
        9,
        35,
        36,
        30,
        72,
        58,
        40,
        24,
        39,
        35,
        3,
        55,
        5,
        37,
        5,
        44,
        24,
        15,
        4,
        17,
        22,
        8,
        17,
        18,
        19,
        23,
        4,
        47,
        14,
        67,
        52,
        30,
        16,
        15,
        36,
        27,
        18,
        10,
        15,
        41,
        7,
        16,
        8,
        7,
        6
      ],
      "2021-11": [
        97,
        6,
        22,
        11,
        17,
        22,
        10,
        50,
        63,
        24,
        16,
        34,
        18,
        3,
        40,
        13,
        40,
        3,
        55,
        18,
        11,
        5,
        9,
        20,
        8,
        13,
        8,
        27,
        23,
        10,
        43,
        19,
        41,
        34,
        24,
        5,
        13,
        32,
        19,
        13,
        18,
        15,
        32,
        6,
        7,
        6,
        5,
        7
      ],
      "2021-12": [
        121,
        2,
        22,
        13,
        20,
        30,
        11,
        63,
        55,
        27,
        14,
        31,
        25,
        3,
        39,
        11,
        44,
        9,
        30,
        16,
        12,
        11,
        10,
        18,
        7,
        13,
        10,
        13,
        13,
        10,
        40,
        26,
        59,
        46,
        44,
        6,
        12,
        23,
        14,
        18,
        16,
        13,
        22,
        12,
        9,
        6,
        8,
        5
      ],
      "2022-01": [
        95,
        2,
        15,
        10,
        20,
        27,
        22,
        50,
        57,
        21,
        15,
        31,
        15,
        2,
        44,
        8,
        21,
        4,
        41,
        21,
        14,
        10,
        3,
        14,
        5,
        17,
        5,
        18,
        10,
        5,
        35,
        13,
        39,
        24,
        36,
        6,
        12,
        22,
        13,
        14,
        5,
        15,
        18,
        6,
        13,
        4,
        7,
        3
      ],
      "2022-02": [
        131,
        5,
        29,
        11,
        32,
        23,
        25,
        66,
        53,
        29,
        20,
        51,
        15,
        4,
        33,
        12,
        32,
        9,
        42,
        21,
        18,
        9,
        15,
        13,
        14,
        13,
        8,
        11,
        20,
        6,
        43,
        14,
        47,
        25,
        35,
        4,
        20,
        26,
        21,
        12,
        9,
        15,
        34,
        5,
        9,
        5,
        5,
        6
      ],
      "2022-03": [
        119,
        2,
        28,
        12,
        44,
        21,
        24,
        56,
        60,
        37,
        12,
        57,
        31,
        3,
        52,
        14,
        52,
        8,
        49,
        15,
        15,
        9,
        8,
        17,
        17,
        16,
        15,
        13,
        15,
        10,
        57,
        35,
        50,
        66,
        40,
        6,
        14,
        24,
        18,
        14,
        19,
        16,
        29,
        9,
        10,
        12,
        10,
        8
      ],
      "2022-04": [
        100,
        5,
        9,
        12,
        34,
        18,
        15,
        53,
        63,
        39,
        20,
        41,
        33,
        1,
        39,
        11,
        28,
        10,
        31,
        21,
        17,
        8,
        15,
        25,
        10,
        15,
        23,
        11,
        19,
        9,
        40,
        31,
        42,
        36,
        34,
        6,
        16,
        27,
        14,
        18,
        18,
        16,
        27,
        11,
        8,
        7,
        2,
        5
      ],
      "2022-05": [
        139,
        10,
        34,
        12,
        25,
        22,
        31,
        87,
        60,
        37,
        28,
        53,
        38,
        5,
        53,
        18,
        49,
        12,
        37,
        21,
        14,
        11,
        18,
        24,
        10,
        16,
        15,
        19,
        31,
        9,
        63,
        42,
        62,
        35,
        50,
        12,
        27,
        45,
        33,
        13,
        16,
        15,
        33,
        12,
        7,
        16,
        7,
        5
      ],
      "2022-06": [
        148,
        10,
        29,
        11,
        31,
        28,
        22,
        65,
        63,
        28,
        31,
        50,
        33,
        2,
        50,
        14,
        45,
        6,
        49,
        32,
        13,
        16,
        10,
        11,
        19,
        13,
        13,
        10,
        36,
        15,
        59,
        21,
        75,
        60,
        41,
        15,
        18,
        29,
        20,
        19,
        16,
        21,
        36,
        4,
        11,
        7,
        4,
        3
      ],
      "2022-07": [
        128,
        3,
        20,
        11,
        23,
        27,
        12,
        43,
        41,
        36,
        19,
        56,
        25,
        2,
        38,
        12,
        48,
        6,
        55,
        21,
        15,
        11,
        12,
        17,
        15,
        19,
        14,
        17,
        21,
        14,
        59,
        18,
        40,
        47,
        42,
        12,
        20,
        33,
        17,
        3,
        13,
        24,
        27,
        8,
        11,
        18,
        5,
        4
      ],
      "2022-08": [
        79,
        4,
        7,
        11,
        20,
        19,
        16,
        54,
        52,
        42,
        17,
        27,
        18,
        2,
        26,
        6,
        31,
        3,
        48,
        18,
        7,
        6,
        14,
        18,
        8,
        15,
        10,
        19,
        26,
        6,
        31,
        15,
        41,
        27,
        31,
        13,
        13,
        24,
        18,
        14,
        22,
        12,
        24,
        5,
        8,
        11,
        0,
        5
      ],
      "2022-09": [
        130,
        10,
        19,
        7,
        41,
        24,
        19,
        57,
        49,
        42,
        21,
        41,
        38,
        7,
        36,
        12,
        38,
        10,
        45,
        26,
        15,
        7,
        12,
        20,
        9,
        14,
        9,
        10,
        22,
        7,
        41,
        21,
        42,
        37,
        52,
        6,
        19,
        37,
        29,
        13,
        12,
        15,
        29,
        10,
        11,
        12,
        11,
        11
      ],
      "2022-10": [
        201,
        12,
        27,
        20,
        61,
        44,
        44,
        94,
        55,
        30,
        31,
        62,
        62,
        5,
        53,
        21,
        53,
        13,
        52,
        35,
        25,
        21,
        14,
        22,
        17,
        23,
        23,
        23,
        44,
        12,
        56,
        44,
        58,
        64,
        53,
        8,
        19,
        35,
        45,
        25,
        23,
        22,
        31,
        11,
        16,
        14,
        4,
        3
      ],
      "2022-11": [
        150,
        10,
        24,
        23,
        42,
        27,
        27,
        87,
        55,
        24,
        30,
        42,
        44,
        3,
        54,
        15,
        30,
        15,
        48,
        23,
        14,
        12,
        18,
        17,
        11,
        23,
        18,
        12,
        33,
        9,
        44,
        31,
        58,
        55,
        48,
        16,
        20,
        40,
        24,
        11,
        19,
        17,
        42,
        8,
        17,
        9,
        6,
        17
      ],
      "2022-12": [
        105,
        3,
        17,
        6,
        27,
        25,
        30,
        44,
        60,
        23,
        22,
        43,
        31,
        2,
        49,
        10,
        42,
        12,
        40,
        18,
        18,
        12,
        13,
        18,
        14,
        14,
        15,
        14,
        28,
        13,
        35,
        33,
        40,
        37,
        27,
        16,
        18,
        34,
        29,
        10,
        19,
        15,
        25,
        15,
        13,
        9,
        4,
        6
      ],
      "2023-01": [
        99,
        2,
        19,
        7,
        21,
        20,
        14,
        42,
        66,
        24,
        17,
        47,
        40,
        1,
        44,
        6,
        39,
        6,
        43,
        18,
        13,
        15,
        9,
        21,
        15,
        11,
        17,
        15,
        29,
        8,
        41,
        15,
        37,
        34,
        25,
        11,
        13,
        27,
        26,
        9,
        15,
        13,
        25,
        6,
        9,
        5,
        9,
        6
      ],
      "2023-02": [
        128,
        7,
        28,
        14,
        32,
        21,
        33,
        73,
        68,
        40,
        31,
        58,
        54,
        8,
        56,
        11,
        51,
        9,
        56,
        26,
        12,
        17,
        12,
        26,
        10,
        16,
        11,
        18,
        18,
        5,
        49,
        26,
        63,
        39,
        51,
        10,
        25,
        38,
        33,
        23,
        19,
        17,
        44,
        15,
        11,
        10,
        6,
        7
      ],
      "2023-03": [
        138,
        4,
        26,
        23,
        40,
        38,
        34,
        55,
        111,
        18,
        40,
        80,
        60,
        4,
        58,
        14,
        68,
        5,
        69,
        31,
        12,
        19,
        21,
        15,
        13,
        18,
        17,
        18,
        27,
        9,
        64,
        34,
        60,
        82,
        40,
        14,
        29,
        44,
        39,
        23,
        20,
        19,
        33,
        20,
        10,
        6,
        6,
        6
      ],
      "2023-04": [
        107,
        2,
        22,
        15,
        23,
        28,
        38,
        43,
        79,
        23,
        21,
        60,
        53,
        14,
        39,
        17,
        53,
        8,
        39,
        21,
        10,
        22,
        12,
        26,
        16,
        28,
        10,
        13,
        12,
        8,
        44,
        30,
        52,
        57,
        42,
        14,
        18,
        43,
        47,
        12,
        21,
        26,
        34,
        16,
        12,
        6,
        4,
        13
      ],
      "2023-05": [
        183,
        7,
        28,
        26,
        71,
        44,
        104,
        95,
        138,
        51,
        40,
        127,
        112,
        26,
        68,
        24,
        45,
        28,
        66,
        29,
        22,
        24,
        23,
        28,
        17,
        27,
        30,
        28,
        47,
        15,
        61,
        84,
        73,
        62,
        67,
        14,
        22,
        41,
        133,
        38,
        39,
        12,
        54,
        44,
        20,
        16,
        9,
        14
      ],
      "2023-06": [
        188,
        10,
        31,
        18,
        50,
        36,
        54,
        104,
        115,
        41,
        42,
        82,
        87,
        8,
        37,
        28,
        56,
        14,
        57,
        27,
        12,
        28,
        15,
        20,
        11,
        11,
        12,
        21,
        51,
        10,
        68,
        48,
        73,
        54,
        47,
        14,
        21,
        39,
        81,
        21,
        25,
        22,
        47,
        18,
        18,
        13,
        8,
        12
      ],
      "2023-07": [
        133,
        6,
        18,
        22,
        34,
        30,
        47,
        71,
        112,
        44,
        39,
        70,
        54,
        12,
        42,
        17,
        46,
        9,
        54,
        34,
        15,
        20,
        20,
        15,
        19,
        15,
        11,
        19,
        35,
        15,
        48,
        18,
        66,
        57,
        50,
        12,
        19,
        34,
        65,
        27,
        30,
        20,
        47,
        19,
        10,
        11,
        15,
        17
      ],
      "2023-08": [
        103,
        10,
        17,
        16,
        50,
        54,
        51,
        85,
        102,
        71,
        19,
        91,
        68,
        23,
        41,
        18,
        62,
        17,
        50,
        39,
        22,
        20,
        23,
        14,
        9,
        37,
        15,
        31,
        22,
        8,
        49,
        28,
        65,
        62,
        58,
        15,
        11,
        46,
        89,
        21,
        27,
        17,
        41,
        7,
        25,
        19,
        10,
        8
      ],
      "2023-09": [
        131,
        11,
        27,
        21,
        66,
        49,
        59,
        72,
        99,
        42,
        40,
        74,
        77,
        21,
        39,
        23,
        68,
        22,
        75,
        30,
        13,
        20,
        8,
        27,
        11,
        27,
        15,
        18,
        23,
        9,
        49,
        38,
        52,
        59,
        46,
        21,
        12,
        37,
        127,
        21,
        20,
        22,
        59,
        16,
        20,
        17,
        17,
        14
      ],
      "2023-10": [
        196,
        8,
        31,
        32,
        60,
        41,
        99,
        106,
        134,
        46,
        40,
        136,
        89,
        42,
        64,
        38,
        62,
        37,
        88,
        42,
        19,
        29,
        12,
        29,
        19,
        25,
        37,
        31,
        46,
        12,
        67,
        60,
        73,
        59,
        70,
        14,
        17,
        53,
        173,
        59,
        40,
        27,
        54,
        32,
        27,
        10,
        14,
        21
      ],
      "2023-11": [
        150,
        16,
        17,
        33,
        44,
        47,
        82,
        69,
        146,
        34,
        25,
        88,
        95,
        30,
        49,
        22,
        52,
        32,
        69,
        38,
        17,
        29,
        9,
        40,
        9,
        29,
        24,
        26,
        42,
        10,
        51,
        35,
        49,
        73,
        36,
        24,
        20,
        43,
        132,
        32,
        32,
        36,
        43,
        19,
        18,
        15,
        11,
        17
      ],
      "2023-12": [
        131,
        11,
        25,
        28,
        49,
        44,
        62,
        95,
        133,
        45,
        32,
        90,
        131,
        37,
        42,
        20,
        76,
        17,
        72,
        37,
        22,
        34,
        15,
        21,
        15,
        37,
        14,
        26,
        45,
        12,
        40,
        27,
        87,
        59,
        53,
        10,
        23,
        43,
        110,
        33,
        25,
        20,
        58,
        11,
        14,
        18,
        9,
        26
      ],
      "2024-01": [
        124,
        13,
        27,
        28,
        58,
        34,
        58,
        75,
        127,
        34,
        35,
        92,
        71,
        31,
        44,
        21,
        53,
        30,
        70,
        40,
        11,
        28,
        15,
        29,
        15,
        19,
        19,
        26,
        23,
        13,
        41,
        44,
        84,
        49,
        41,
        13,
        23,
        39,
        120,
        29,
        36,
        35,
        43,
        20,
        22,
        15,
        17,
        20
      ],
      "2024-02": [
        168,
        6,
        25,
        31,
        55,
        44,
        140,
        114,
        168,
        48,
        32,
        143,
        93,
        78,
        62,
        48,
        52,
        52,
        82,
        43,
        9,
        33,
        16,
        33,
        13,
        22,
        25,
        36,
        48,
        11,
        67,
        66,
        113,
        58,
        60,
        25,
        30,
        53,
        232,
        84,
        40,
        24,
        72,
        26,
        26,
        19,
        10,
        19
      ],
      "2024-03": [
        188,
        11,
        23,
        34,
        49,
        48,
        86,
        107,
        150,
        64,
        41,
        152,
        127,
        45,
        49,
        42,
        90,
        34,
        95,
        35,
        22,
        27,
        13,
        37,
        20,
        28,
        14,
        18,
        50,
        8,
        62,
        47,
        83,
        84,
        48,
        20,
        22,
        38,
        161,
        68,
        41,
        30,
        67,
        28,
        16,
        17,
        24,
        10
      ],
      "2024-04": [
        146,
        14,
        24,
        26,
        41,
        43,
        71,
        62,
        144,
        43,
        34,
        111,
        132,
        40,
        44,
        25,
        78,
        48,
        59,
        50,
        5,
        34,
        7,
        20,
        10,
        30,
        8,
        27,
        24,
        15,
        62,
        63,
        83,
        72,
        48,
        20,
        25,
        63,
        156,
        45,
        25,
        22,
        50,
        32,
        11,
        17,
        12,
        16
      ],
      "2024-05": [
        197,
        9,
        36,
        35,
        63,
        50,
        108,
        102,
        186,
        51,
        51,
        130,
        129,
        60,
        69,
        54,
        69,
        56,
        123,
        55,
        20,
        39,
        16,
        29,
        8,
        30,
        19,
        34,
        45,
        13,
        76,
        44,
        95,
        63,
        57,
        27,
        32,
        58,
        184,
        65,
        44,
        27,
        48,
        31,
        20,
        15,
        8,
        28
      ],
      "2024-06": [
        177,
        17,
        29,
        43,
        124,
        43,
        134,
        131,
        193,
        50,
        31,
        166,
        152,
        81,
        57,
        52,
        92,
        91,
        114,
        35,
        15,
        40,
        16,
        30,
        20,
        29,
        26,
        28,
        48,
        23,
        66,
        76,
        116,
        68,
        41,
        21,
        29,
        54,
        263,
        81,
        41,
        34,
        76,
        36,
        29,
        24,
        13,
        18
      ],
      "2024-07": [
        154,
        21,
        20,
        31,
        78,
        65,
        79,
        100,
        172,
        53,
        35,
        148,
        117,
        67,
        42,
        44,
        77,
        66,
        91,
        50,
        24,
        29,
        16,
        39,
        14,
        37,
        13,
        22,
        44,
        16,
        50,
        50,
        75,
        77,
        50,
        21,
        31,
        74,
        184,
        37,
        39,
        33,
        68,
        29,
        20,
        25,
        16,
        14
      ],
      "2024-08": [
        157,
        11,
        21,
        33,
        78,
        47,
        73,
        87,
        130,
        44,
        36,
        132,
        91,
        46,
        46,
        35,
        72,
        59,
        98,
        25,
        17,
        29,
        29,
        19,
        16,
        25,
        19,
        28,
        38,
        14,
        55,
        40,
        89,
        72,
        45,
        17,
        22,
        63,
        164,
        53,
        31,
        30,
        64,
        15,
        15,
        25,
        12,
        18
      ],
      "2024-09": [
        166,
        19,
        22,
        44,
        128,
        63,
        93,
        73,
        154,
        41,
        31,
        129,
        111,
        45,
        36,
        34,
        79,
        73,
        77,
        44,
        14,
        22,
        9,
        23,
        19,
        41,
        24,
        30,
        30,
        10,
        71,
        56,
        102,
        83,
        39,
        28,
        20,
        63,
        192,
        49,
        44,
        46,
        63,
        26,
        27,
        24,
        16,
        11
      ],
      "2024-10": [
        225,
        19,
        37,
        64,
        114,
        64,
        204,
        118,
        201,
        65,
        39,
        204,
        199,
        96,
        84,
        57,
        96,
        110,
        129,
        50,
        21,
        40,
        7,
        42,
        21,
        21,
        22,
        39,
        59,
        10,
        92,
        75,
        155,
        96,
        80,
        26,
        48,
        72,
        280,
        116,
        48,
        38,
        85,
        36,
        30,
        19,
        37,
        22
      ],
      "2024-11": [
        152,
        12,
        20,
        51,
        67,
        51,
        107,
        98,
        198,
        48,
        24,
        144,
        133,
        53,
        43,
        54,
        86,
        52,
        83,
        42,
        24,
        21,
        12,
        31,
        15,
        33,
        23,
        17,
        39,
        22,
        62,
        47,
        94,
        72,
        47,
        18,
        26,
        66,
        147,
        45,
        33,
        32,
        60,
        20,
        30,
        12,
        8,
        19
      ],
      "2024-12": [
        161,
        10,
        19,
        55,
        76,
        69,
        147,
        106,
        225,
        54,
        22,
        173,
        171,
        60,
        64,
        46,
        101,
        74,
        108,
        42,
        13,
        36,
        10,
        31,
        13,
        38,
        17,
        36,
        47,
        19,
        73,
        57,
        116,
        101,
        56,
        19,
        27,
        68,
        202,
        72,
        41,
        31,
        53,
        30,
        22,
        16,
        22,
        25
      ],
      "2025-01": [
        147,
        13,
        22,
        41,
        76,
        48,
        132,
        82,
        200,
        44,
        34,
        123,
        88,
        55,
        41,
        31,
        77,
        63,
        83,
        42,
        10,
        37,
        11,
        27,
        15,
        33,
        18,
        25,
        35,
        10,
        64,
        31,
        85,
        70,
        39,
        27,
        16,
        65,
        163,
        54,
        31,
        23,
        60,
        20,
        31,
        13,
        16,
        14
      ],
      "2025-02": [
        200,
        16,
        23,
        47,
        89,
        57,
        292,
        116,
        265,
        57,
        51,
        218,
        162,
        109,
        53,
        76,
        95,
        121,
        115,
        47,
        22,
        40,
        24,
        28,
        17,
        36,
        22,
        30,
        51,
        14,
        85,
        85,
        139,
        71,
        47,
        32,
        29,
        76,
        279,
        133,
        41,
        36,
        70,
        31,
        39,
        28,
        27,
        14
      ],
      "2025-03": [
        195,
        13,
        24,
        62,
        74,
        71,
        226,
        87,
        266,
        50,
        36,
        179,
        181,
        58,
        43,
        65,
        139,
        77,
        79,
        57,
        10,
        26,
        9,
        25,
        17,
        49,
        22,
        27,
        60,
        20,
        78,
        41,
        142,
        96,
        42,
        27,
        25,
        69,
        223,
        79,
        39,
        28,
        54,
        26,
        23,
        21,
        20,
        16
      ],
      "2025-04": [
        179,
        18,
        21,
        38,
        69,
        49,
        222,
        74,
        273,
        59,
        30,
        160,
        109,
        59,
        44,
        39,
        99,
        95,
        84,
        45,
        16,
        35,
        9,
        33,
        13,
        32,
        15,
        32,
        35,
        12,
        80,
        55,
        113,
        80,
        48,
        29,
        28,
        71,
        175,
        47,
        42,
        39,
        57,
        20,
        23,
        22,
        21,
        20
      ],
      "2025-05": [
        266,
        27,
        41,
        88,
        145,
        61,
        493,
        139,
        334,
        63,
        46,
        247,
        202,
        103,
        55,
        98,
        101,
        131,
        132,
        58,
        28,
        44,
        14,
        29,
        18,
        50,
        32,
        52,
        63,
        30,
        121,
        60,
        236,
        76,
        80,
        41,
        53,
        81,
        346,
        133,
        63,
        40,
        105,
        36,
        42,
        30,
        30,
        31
      ],
      "2025-06": [
        207,
        19,
        25,
        58,
        134,
        72,
        395,
        103,
        237,
        48,
        30,
        222,
        163,
        69,
        55,
        49,
        113,
        94,
        118,
        50,
        19,
        49,
        23,
        19,
        31,
        23,
        26,
        26,
        84,
        10,
        87,
        54,
        196,
        96,
        43,
        40,
        36,
        73,
        247,
        98,
        58,
        35,
        77,
        30,
        36,
        20,
        19,
        26
      ],
      "2025-07": [
        208,
        10,
        26,
        41,
        106,
        58,
        265,
        87,
        281,
        62,
        36,
        190,
        128,
        59,
        53,
        46,
        109,
        90,
        85,
        39,
        28,
        30,
        37,
        25,
        18,
        39,
        25,
        29,
        47,
        14,
        65,
        51,
        175,
        85,
        57,
        36,
        24,
        68,
        195,
        78,
        48,
        41,
        85,
        22,
        37,
        31,
        31,
        34
      ],
      "2025-08": [
        204,
        18,
        25,
        60,
        122,
        58,
        346,
        115,
        265,
        76,
        52,
        186,
        118,
        78,
        47,
        66,
        90,
        120,
        111,
        46,
        26,
        44,
        22,
        32,
        18,
        46,
        31,
        23,
        41,
        12,
        80,
        51,
        198,
        109,
        46,
        42,
        31,
        79,
        244,
        87,
        46,
        45,
        73,
        22,
        39,
        43,
        40,
        20
      ],
      "2025-09": [
        83,
        5,
        11,
        20,
        47,
        26,
        127,
        41,
        130,
        11,
        19,
        71,
        51,
        35,
        22,
        19,
        50,
        51,
        37,
        14,
        8,
        10,
        8,
        10,
        9,
        12,
        12,
        10,
        22,
        15,
        34,
        23,
        99,
        43,
        24,
        18,
        20,
        36,
        95,
        36,
        23,
        23,
        48,
        12,
        19,
        8,
        15,
        18
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Simple Agent, Complex Environment: Efficient Reinforcement Learning with Agent States",
          "year": "2021-02",
          "abstract": "We design a simple reinforcement learning (RL) agent that implements an\noptimistic version of $Q$-learning and establish through regret analysis that\nthis agent can operate with some level of competence in any environment. While\nwe leverage concepts from the literature on provably efficient RL, we consider\na general agent-environment interface and provide a novel agent design and\nanalysis. This level of generality positions our results to inform the design\nof future agents for operation in complex real environments. We establish that,\nas time progresses, our agent performs competitively relative to policies that\nrequire longer times to evaluate. The time it takes to approach asymptotic\nperformance is polynomial in the complexity of the agent's state representation\nand the time required to evaluate the best policy that the agent can represent.\nNotably, there is no dependence on the complexity of the environment. The\nultimate per-period performance loss of the agent is bounded by a constant\nmultiple of a measure of distortion introduced by the agent's state\nrepresentation. This work is the first to establish that an algorithm\napproaches this asymptotic condition within a tractable time frame.",
          "arxiv_id": "2102.05261v7"
        },
        {
          "title": "Constraints Penalized Q-learning for Safe Offline Reinforcement Learning",
          "year": "2021-07",
          "abstract": "We study the problem of safe offline reinforcement learning (RL), the goal is\nto learn a policy that maximizes long-term reward while satisfying safety\nconstraints given only offline data, without further interaction with the\nenvironment. This problem is more appealing for real world RL applications, in\nwhich data collection is costly or dangerous. Enforcing constraint satisfaction\nis non-trivial, especially in offline settings, as there is a potential large\ndiscrepancy between the policy distribution and the data distribution, causing\nerrors in estimating the value of safety constraints. We show that na\\\"ive\napproaches that combine techniques from safe RL and offline RL can only learn\nsub-optimal solutions. We thus develop a simple yet effective algorithm,\nConstraints Penalized Q-Learning (CPQ), to solve the problem. Our method admits\nthe use of data generated by mixed behavior policies. We present a theoretical\nanalysis and demonstrate empirically that our approach can learn robustly\nacross a variety of benchmark control tasks, outperforming several baselines.",
          "arxiv_id": "2107.09003v3"
        },
        {
          "title": "An advantage based policy transfer algorithm for reinforcement learning with measures of transferability",
          "year": "2023-11",
          "abstract": "Reinforcement learning (RL) enables sequential decision-making in complex and\nhigh-dimensional environments through interaction with the environment. In most\nreal-world applications, however, a high number of interactions are infeasible.\nIn these environments, transfer RL algorithms, which can be used for the\ntransfer of knowledge from one or multiple source environments to a target\nenvironment, have been shown to increase learning speed and improve initial and\nasymptotic performance. However, most existing transfer RL algorithms are\non-policy and sample inefficient, fail in adversarial target tasks, and often\nrequire heuristic choices in algorithm design. This paper proposes an\noff-policy Advantage-based Policy Transfer algorithm, APT-RL, for fixed domain\nenvironments. Its novelty is in using the popular notion of ``advantage'' as a\nregularizer, to weigh the knowledge that should be transferred from the source,\nrelative to new knowledge learned in the target, removing the need for\nheuristic choices. Further, we propose a new transfer performance measure to\nevaluate the performance of our algorithm and unify existing transfer RL\nframeworks. Finally, we present a scalable, theoretically-backed task\nsimilarity measurement algorithm to illustrate the alignments between our\nproposed transferability measure and similarities between source and target\nenvironments. We compare APT-RL with several baselines, including existing\ntransfer-RL algorithms, in three high-dimensional continuous control tasks. Our\nexperiments demonstrate that APT-RL outperforms existing transfer RL algorithms\nand is at least as good as learning from scratch in adversarial tasks.",
          "arxiv_id": "2311.06731v2"
        }
      ],
      "1": [
        {
          "title": "MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for Efficient and Interpretable Medical Image Segmentation",
          "year": "2024-10",
          "abstract": "Medical image segmentation is pivotal in healthcare, enhancing diagnostic\naccuracy, informing treatment strategies, and tracking disease progression.\nThis process allows clinicians to extract critical information from visual\ndata, enabling personalized patient care. However, developing neural networks\nfor segmentation remains challenging, especially when preserving image\nresolution, which is essential in detecting subtle details that influence\ndiagnoses. Moreover, the lack of transparency in these deep learning models has\nslowed their adoption in clinical practice. Efforts in model interpretability\nare increasingly focused on making these models' decision-making processes more\ntransparent. In this paper, we introduce MAPUNetR, a novel architecture that\nsynergizes the strengths of transformer models with the proven U-Net framework\nfor medical image segmentation. Our model addresses the resolution preservation\nchallenge and incorporates attention maps highlighting segmented regions,\nincreasing accuracy and interpretability. Evaluated on the BraTS 2020 dataset,\nMAPUNetR achieved a dice score of 0.88 and a dice coefficient of 0.92 on the\nISIC 2018 dataset. Our experiments show that the model maintains stable\nperformance and potential as a powerful tool for medical image segmentation in\nclinical practice.",
          "arxiv_id": "2410.22223v1"
        },
        {
          "title": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis",
          "year": "2025-06",
          "abstract": "Medical document analysis plays a crucial role in extracting essential\nclinical insights from unstructured healthcare records, supporting critical\ntasks such as differential diagnosis. Determining the most probable condition\namong overlapping symptoms requires precise evaluation and deep medical\nexpertise. While recent advancements in large language models (LLMs) have\nsignificantly enhanced performance in medical document analysis, privacy\nconcerns related to sensitive patient data limit the use of online LLMs\nservices in clinical settings. To address these challenges, we propose a\ntrustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using\nlow-rank adaptation, specifically optimized for differential diagnosis tasks.\nOur approach utilizes DDXPlus, the largest benchmark dataset for differential\ndiagnosis, and demonstrates superior performance in pathology prediction and\nvariable-length differential diagnosis compared to existing methods. The\ndeveloped web-based platform allows users to submit their own unstructured\nmedical documents and receive accurate, explainable diagnostic results. By\nincorporating advanced explainability techniques, the system ensures\ntransparent and reliable predictions, fostering user trust and confidence.\nExtensive evaluations confirm that the proposed method surpasses current\nstate-of-the-art models in predictive accuracy while offering practical utility\nin clinical settings. This work addresses the urgent need for reliable,\nexplainable, and privacy-preserving artificial intelligence solutions,\nrepresenting a significant advancement in intelligent medical document analysis\nfor real-world healthcare applications. The code can be found at\n\\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.",
          "arxiv_id": "2506.19702v1"
        },
        {
          "title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model for Advanced Medical Decision Support",
          "year": "2025-02",
          "abstract": "Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions. To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making.",
          "arxiv_id": "2502.18274v2"
        }
      ],
      "2": [
        {
          "title": "Information Bottleneck-Based Hebbian Learning Rule Naturally Ties Working Memory and Synaptic Updates",
          "year": "2021-11",
          "abstract": "Artificial neural networks have successfully tackled a large variety of\nproblems by training extremely deep networks via back-propagation. A direct\napplication of back-propagation to spiking neural networks contains\nbiologically implausible components, like the weight transport problem or\nseparate inference and learning phases. Various methods address different\ncomponents individually, but a complete solution remains intangible. Here, we\ntake an alternate approach that avoids back-propagation and its associated\nissues entirely. Recent work in deep learning proposed independently training\neach layer of a network via the information bottleneck (IB). Subsequent studies\nnoted that this layer-wise approach circumvents error propagation across\nlayers, leading to a biologically plausible paradigm. Unfortunately, the IB is\ncomputed using a batch of samples. The prior work addresses this with a weight\nupdate that only uses two samples (the current and previous sample). Our work\ntakes a different approach by decomposing the weight update into a local and\nglobal component. The local component is Hebbian and only depends on the\ncurrent sample. The global component computes a layer-wise modulatory signal\nthat depends on a batch of samples. We show that this modulatory signal can be\nlearned by an auxiliary circuit with working memory (WM) like a reservoir.\nThus, we can use batch sizes greater than two, and the batch size determines\nthe required capacity of the WM. To the best of our knowledge, our rule is the\nfirst biologically plausible mechanism to directly couple synaptic updates with\na WM of the task. We evaluate our rule on synthetic datasets and image\nclassification datasets like MNIST, and we explore the effect of the WM\ncapacity on learning performance. We hope our work is a first-step towards\nunderstanding the mechanistic role of memory in learning.",
          "arxiv_id": "2111.13187v1"
        },
        {
          "title": "Continual Learning with Deep Learning Methods in an Application-Oriented Context",
          "year": "2022-07",
          "abstract": "Abstract knowledge is deeply grounded in many computer-based applications. An\nimportant research area of Artificial Intelligence (AI) deals with the\nautomatic derivation of knowledge from data. Machine learning offers the\naccording algorithms. One area of research focuses on the development of\nbiologically inspired learning algorithms. The respective machine learning\nmethods are based on neurological concepts so that they can systematically\nderive knowledge from data and store it. One type of machine learning\nalgorithms that can be categorized as \"deep learning\" model is referred to as\nDeep Neural Networks (DNNs). DNNs consist of multiple artificial neurons\narranged in layers that are trained by using the backpropagation algorithm.\nThese deep learning methods exhibit amazing capabilities for inferring and\nstoring complex knowledge from high-dimensional data. However, DNNs are\naffected by a problem that prevents new knowledge from being added to an\nexisting base. The ability to continuously accumulate knowledge is an important\nfactor that contributed to evolution and is therefore a prerequisite for the\ndevelopment of strong AIs. The so-called \"catastrophic forgetting\" (CF) effect\ncauses DNNs to immediately loose already derived knowledge after a few training\niterations on a new data distribution. Only an energetically expensive\nretraining with the joint data distribution of past and new data enables the\nabstraction of the entire new set of knowledge. In order to counteract the\neffect, various techniques have been and are still being developed with the\ngoal to mitigate or even solve the CF problem. These published CF avoidance\nstudies usually imply the effectiveness of their approaches for various\ncontinual learning tasks. This dissertation is set in the context of continual\nmachine learning with deep learning methods. The first part deals with the\ndevelopment of an ...",
          "arxiv_id": "2207.06233v1"
        },
        {
          "title": "Going Deeper With Directly-Trained Larger Spiking Neural Networks",
          "year": "2020-10",
          "abstract": "Spiking neural networks (SNNs) are promising in a bio-plausible coding for\nspatio-temporal information and event-driven signal processing, which is very\nsuited for energy-efficient implementation in neuromorphic hardware. However,\nthe unique working mode of SNNs makes them more difficult to train than\ntraditional networks. Currently, there are two main routes to explore the\ntraining of deep SNNs with high performance. The first is to convert a\npre-trained ANN model to its SNN version, which usually requires a long coding\nwindow for convergence and cannot exploit the spatio-temporal features during\ntraining for solving temporal tasks. The other is to directly train SNNs in the\nspatio-temporal domain. But due to the binary spike activity of the firing\nfunction and the problem of gradient vanishing or explosion, current methods\nare restricted to shallow architectures and thereby difficult in harnessing\nlarge-scale datasets (e.g. ImageNet). To this end, we propose a\nthreshold-dependent batch normalization (tdBN) method based on the emerging\nspatio-temporal backpropagation, termed \"STBP-tdBN\", enabling direct training\nof a very deep SNN and the efficient implementation of its inference on\nneuromorphic hardware. With the proposed method and elaborated shortcut\nconnection, we significantly extend directly-trained SNNs from a shallow\nstructure ( < 10 layer) to a very deep structure (50 layers). Furthermore, we\ntheoretically analyze the effectiveness of our method based on \"Block Dynamical\nIsometry\" theory. Finally, we report superior accuracy results including 93.15\n% on CIFAR-10, 67.8 % on DVS-CIFAR10, and 67.05% on ImageNet with very few\ntimesteps. To our best knowledge, it's the first time to explore the\ndirectly-trained deep SNNs with high performance on ImageNet.",
          "arxiv_id": "2011.05280v2"
        }
      ],
      "3": [
        {
          "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better",
          "year": "2025-06",
          "abstract": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
          "arxiv_id": "2506.09040v1"
        },
        {
          "title": "Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models",
          "year": "2024-05",
          "abstract": "Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models",
          "arxiv_id": "2405.13872v2"
        },
        {
          "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection",
          "year": "2025-03",
          "abstract": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*.",
          "arxiv_id": "2503.11794v1"
        }
      ],
      "4": [
        {
          "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for -Tsang, Amdo and Kham Speech Dataset Generation",
          "year": "2025-05",
          "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.",
          "arxiv_id": "2505.14351v3"
        },
        {
          "title": "Recent Progress in the CUHK Dysarthric Speech Recognition System",
          "year": "2022-01",
          "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\nin the past few decades, recognition of disordered speech remains a highly\nchallenging task to date. Disordered speech presents a wide spectrum of\nchallenges to current data intensive deep neural networks (DNNs) based ASR\ntechnologies that predominantly target normal speech. This paper presents\nrecent research efforts at the Chinese University of Hong Kong (CUHK) to\nimprove the performance of disordered speech recognition systems on the largest\npublicly available UASpeech dysarthric speech corpus. A set of novel modelling\ntechniques including neural architectural search, data augmentation using\nspectra-temporal perturbation, model based speaker adaptation and cross-domain\ngeneration of visual features within an audio-visual speech recognition (AVSR)\nsystem framework were employed to address the above challenges. The combination\nof these techniques produced the lowest published word error rate (WER) of\n25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER\nreduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric\nspeech recognition system featuring a 6-way DNN system combination and cross\nadaptation of out-of-domain normal speech data trained systems. Bayesian model\nadaptation further allows rapid adaptation to individual dysarthric speakers to\nbe performed using as little as 3.06 seconds of speech. The efficacy of these\ntechniques were further demonstrated on a CUDYS Cantonese dysarthric speech\nrecognition task.",
          "arxiv_id": "2201.05845v2"
        },
        {
          "title": "Moshi: a speech-text foundation model for real-time dialogue",
          "year": "2024-09",
          "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.",
          "arxiv_id": "2410.00037v2"
        }
      ],
      "5": [
        {
          "title": "Calibration of Human Driving Behavior and Preference Using Naturalistic Traffic Data",
          "year": "2021-05",
          "abstract": "Understanding human driving behaviors quantitatively is critical even in the\nera when connected and autonomous vehicles and smart infrastructure are\nbecoming ever more prevalent. This is particularly so as that mixed traffic\nsettings, where autonomous vehicles and human driven vehicles co-exist, are\nexpected to persist for quite some time. Towards this end it is necessary that\nwe have a comprehensive modeling framework for decision-making within which\nhuman driving preferences can be inferred statistically from observed driving\nbehaviors in realistic and naturalistic traffic settings. Leveraging a recently\nproposed computational framework for smart vehicles in a smart world using\nmulti-agent based simulation and optimization, we first recapitulate how the\nforward problem of driving decision-making is modeled as a state space model.\nWe then show how the model can be inverted to estimate driver preferences from\nnaturalistic traffic data using the standard Kalman filter technique. We\nexplicitly illustrate our approach using the vehicle trajectory data from\nSugiyama experiment that was originally meant to demonstrate how stop-and-go\nshockwave can arise spontaneously without bottlenecks. Not only the estimated\nstate filter can fit the observed data well for each individual vehicle, the\ninferred utility functions can also re-produce quantitatively similar pattern\nof the observed collective behaviors. One distinct advantage of our approach is\nthe drastically reduced computational burden. This is possible because our\nforward model treats driving decision process, which is intrinsically dynamic\nwith multi-agent interactions, as a sequence of independent static optimization\nproblems contingent on the state with a finite look ahead anticipation.\nConsequently we can practically sidestep solving an interacting dynamic\ninversion problem that would have been much more computationally demanding.",
          "arxiv_id": "2105.01820v1"
        },
        {
          "title": "TrafficGPT: Towards Multi-Scale Traffic Analysis and Generation with Spatial-Temporal Agent Framework",
          "year": "2024-05",
          "abstract": "The precise prediction of multi-scale traffic is a ubiquitous challenge in\nthe urbanization process for car owners, road administrators, and governments.\nIn the case of complex road networks, current and past traffic information from\nboth upstream and downstream roads are crucial since various road networks have\ndifferent semantic information about traffic. Rationalizing the utilization of\nsemantic information can realize short-term, long-term, and unseen road traffic\nprediction. As the demands of multi-scale traffic analysis increase, on-demand\ninteractions and visualizations are expected to be available for transportation\nparticipants. We have designed a multi-scale traffic generation system, namely\nTrafficGPT, using three AI agents to process multi-scale traffic data, conduct\nmulti-scale traffic analysis, and present multi-scale visualization results.\nTrafficGPT consists of three essential AI agents: 1) a text-to-demand agent\nthat is employed with Question & Answer AI to interact with users and extract\nprediction tasks through texts; 2) a traffic prediction agent that leverages\nmulti-scale traffic data to generate temporal features and similarity, and fuse\nthem with limited spatial features and similarity, to achieve accurate\nprediction of three tasks; and 3) a suggestion and visualization agent that\nuses the prediction results to generate suggestions and visualizations,\nproviding users with a comprehensive understanding of traffic conditions. Our\nTrafficGPT system focuses on addressing concerns about traffic prediction from\ntransportation participants, and conducted extensive experiments on five\nreal-world road datasets to demonstrate its superior predictive and interactive\nperformance",
          "arxiv_id": "2405.05985v1"
        },
        {
          "title": "Transferable and Adaptable Driving Behavior Prediction",
          "year": "2022-02",
          "abstract": "While autonomous vehicles still struggle to solve challenging situations\nduring on-road driving, humans have long mastered the essence of driving with\nefficient, transferable, and adaptable driving capability. By mimicking humans'\ncognition model and semantic understanding during driving, we propose HATN, a\nhierarchical framework to generate high-quality, transferable, and adaptable\npredictions for driving behaviors in multi-agent dense-traffic environments.\nOur hierarchical method consists of a high-level intention identification\npolicy and a low-level trajectory generation policy. We introduce a novel\nsemantic sub-task definition and generic state representation for each\nsub-task. With these techniques, the hierarchical framework is transferable\nacross different driving scenarios. Besides, our model is able to capture\nvariations of driving behaviors among individuals and scenarios by an online\nadaptation module. We demonstrate our algorithms in the task of trajectory\nprediction for real traffic data at intersections and roundabouts from the\nINTERACTION dataset. Through extensive numerical studies, it is evident that\nour method significantly outperformed other methods in terms of prediction\naccuracy, transferability, and adaptability. Pushing the state-of-the-art\nperformance by a considerable margin, we also provide a cognitive view of\nunderstanding the driving behavior behind such improvement. We highlight that\nin the future, more research attention and effort are deserved for\ntransferability and adaptability. It is not only due to the promising\nperformance elevation of prediction and planning algorithms, but more\nfundamentally, they are crucial for the scalable and general deployment of\nautonomous vehicles.",
          "arxiv_id": "2202.05140v2"
        }
      ],
      "6": [
        {
          "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
          "year": "2025-01",
          "abstract": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
          "arxiv_id": "2501.09686v3"
        },
        {
          "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
          "year": "2024-10",
          "abstract": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
          "arxiv_id": "2410.17635v2"
        },
        {
          "title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
          "year": "2023-10",
          "abstract": "Recent advances in large language models (LLMs) have revolutionized the\nlandscape of reasoning tasks. To enhance the capabilities of LLMs to emulate\nhuman reasoning, prior studies have focused on modeling reasoning steps using\nvarious thought structures like chains, trees, or graphs. However, LLM-based\nreasoning still encounters the following challenges: (1) Limited adaptability\nof preset structures to diverse tasks; (2) Insufficient precision in exploiting\nknown conditions to derive new ones; and (3) Inadequate consideration of\nhistorical reasoning experiences for subsequent reasoning steps. To this end,\nwe propose DetermLR, a novel perspective that rethinks the reasoning process as\nan evolution from indeterminacy to determinacy. First, we categorize known\nconditions into two types: determinate and indeterminate premises This provides\nan oveall direction for the reasoning process and guides LLMs in converting\nindeterminate data into progressively determinate insights. Subsequently, we\nleverage quantitative measurements to prioritize more relevant premises to\nexplore new insights. Furthermore, we automate the storage and extraction of\navailable premises and reasoning paths with reasoning memory, preserving\nhistorical reasoning details for subsequent reasoning steps. Comprehensive\nexperimental results demonstrate that DetermLR surpasses all baselines on\nvarious logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and\nLogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR\nachieves higher accuracy with fewer reasoning steps, highlighting its superior\nefficiency and effectiveness in solving logical reasoning tasks.",
          "arxiv_id": "2310.18659v2"
        }
      ],
      "7": [
        {
          "title": "Make Heterophily Graphs Better Fit GNN: A Graph Rewiring Approach",
          "year": "2022-09",
          "abstract": "Graph Neural Networks (GNNs) are popular machine learning methods for\nmodeling graph data. A lot of GNNs perform well on homophily graphs while\nhaving unsatisfactory performance on heterophily graphs. Recently, some\nresearchers turn their attention to designing GNNs for heterophily graphs by\nadjusting the message passing mechanism or enlarging the receptive field of the\nmessage passing. Different from existing works that mitigate the issues of\nheterophily from model design perspective, we propose to study heterophily\ngraphs from an orthogonal perspective by rewiring the graph structure to reduce\nheterophily and making the traditional GNNs perform better. Through\ncomprehensive empirical studies and analysis, we verify the potential of the\nrewiring methods. To fully exploit its potential, we propose a method named\nDeep Heterophily Graph Rewiring (DHGR) to rewire graphs by adding homophilic\nedges and pruning heterophilic edges. The detailed way of rewiring is\ndetermined by comparing the similarity of label/feature-distribution of node\nneighbors. Besides, we design a scalable implementation for DHGR to guarantee\nhigh efficiency. DHRG can be easily used as a plug-in module, i.e., a graph\npre-processing step, for any GNNs, including both GNN for homophily and\nheterophily, to boost their performance on the node classification task. To the\nbest of our knowledge, it is the first work studying graph rewiring for\nheterophily graphs. Extensive experiments on 11 public graph datasets\ndemonstrate the superiority of our proposed methods.",
          "arxiv_id": "2209.08264v1"
        },
        {
          "title": "GDM: Dual Mixup for Graph Classification with Limited Supervision",
          "year": "2023-09",
          "abstract": "Graph Neural Networks (GNNs) require a large number of labeled graph samples\nto obtain good performance on the graph classification task. The performance of\nGNNs degrades significantly as the number of labeled graph samples decreases.\nTo reduce the annotation cost, it is therefore important to develop graph\naugmentation methods that can generate new graph instances to increase the size\nand diversity of the limited set of available labeled graph samples. In this\nwork, we propose a novel mixup-based graph augmentation method, Graph Dual\nMixup (GDM), that leverages both functional and structural information of the\ngraph instances to generate new labeled graph samples. GDM employs a graph\nstructural auto-encoder to learn structural embeddings of the graph samples,\nand then applies mixup to the structural information of the graphs in the\nlearned structural embedding space and generates new graph structures from the\nmixup structural embeddings. As for the functional information, GDM applies\nmixup directly to the input node features of the graph samples to generate\nfunctional node feature information for new mixup graph instances. Jointly, the\ngenerated input node features and graph structures yield new graph samples\nwhich can supplement the set of original labeled graphs. Furthermore, we\npropose two novel Balanced Graph Sampling methods to enhance the balanced\ndifficulty and diversity for the generated graph samples. Experimental results\non the benchmark datasets demonstrate that our proposed method substantially\noutperforms the state-of-the-art graph augmentation methods when the labeled\ngraphs are scarce.",
          "arxiv_id": "2309.10134v1"
        },
        {
          "title": "Graph Anomaly Detection with Graph Neural Networks: Current Status and Challenges",
          "year": "2022-09",
          "abstract": "Graphs are used widely to model complex systems, and detecting anomalies in a\ngraph is an important task in the analysis of complex systems. Graph anomalies\nare patterns in a graph that do not conform to normal patterns expected of the\nattributes and/or structures of the graph. In recent years, graph neural\nnetworks (GNNs) have been studied extensively and have successfully performed\ndifficult machine learning tasks in node classification, link prediction, and\ngraph classification thanks to the highly expressive capability via message\npassing in effectively learning graph representations. To solve the graph\nanomaly detection problem, GNN-based methods leverage information about the\ngraph attributes (or features) and/or structures to learn to score anomalies\nappropriately. In this survey, we review the recent advances made in detecting\ngraph anomalies using GNN models. Specifically, we summarize GNN-based methods\naccording to the graph type (i.e., static and dynamic), the anomaly type (i.e.,\nnode, edge, subgraph, and whole graph), and the network architecture (e.g.,\ngraph autoencoder, graph convolutional network). To the best of our knowledge,\nthis survey is the first comprehensive review of graph anomaly detection\nmethods based on GNNs.",
          "arxiv_id": "2209.14930v2"
        }
      ],
      "8": [
        {
          "title": "Responsible Artificial Intelligence Systems: A Roadmap to Society's Trust through Trustworthy AI, Auditability, Accountability, and Governance",
          "year": "2025-02",
          "abstract": "Artificial intelligence (AI) has matured as a technology, necessitating the\ndevelopment of responsibility frameworks that are fair, inclusive, trustworthy,\nsafe and secure, transparent, and accountable. By establishing such frameworks,\nwe can harness the full potential of AI while mitigating its risks,\nparticularly in high-risk scenarios. This requires the design of responsible AI\nsystems based on trustworthy AI technologies and ethical principles, with the\naim of ensuring auditability and accountability throughout their design,\ndevelopment, and deployment, adhering to domain-specific regulations and\nstandards.\n  This paper explores the concept of a responsible AI system from a holistic\nperspective, which encompasses four key dimensions: 1) regulatory context; 2)\ntrustworthy AI technology along with standardization and assessments; 3)\nauditability and accountability; and 4) AI governance. The aim of this paper is\ndouble. First, we analyze and understand these four dimensions and their\ninterconnections in the form of an analysis and overview. Second, the final\ngoal of the paper is to propose a roadmap in the design of responsible AI\nsystems, ensuring that they can gain society's trust. To achieve this\ntrustworthiness, this paper also fosters interdisciplinary discussions on the\nethical, legal, social, economic, and cultural aspects of AI from a global\ngovernance perspective. Last but not least, we also reflect on the current\nstate and those aspects that need to be developed in the near future, as ten\nlessons learned.",
          "arxiv_id": "2503.04739v1"
        },
        {
          "title": "Building Symbiotic AI: Reviewing the AI Act for a Human-Centred, Principle-Based Framework",
          "year": "2025-01",
          "abstract": "Artificial Intelligence (AI) spreads quickly as new technologies and services\ntake over modern society. The need to regulate AI design, development, and use\nis strictly necessary to avoid unethical and potentially dangerous consequences\nto humans. The European Union (EU) has released a new legal framework, the AI\nAct, to regulate AI by undertaking a risk-based approach to safeguard humans\nduring interaction. At the same time, researchers offer a new perspective on AI\nsystems, commonly known as Human-Centred AI (HCAI), highlighting the need for a\nhuman-centred approach to their design. In this context, Symbiotic AI (a\nsubtype of HCAI) promises to enhance human capabilities through a deeper and\ncontinuous collaboration between human intelligence and AI. This article\npresents the results of a Systematic Literature Review (SLR) that aims to\nidentify principles that characterise the design and development of Symbiotic\nAI systems while considering humans as the core of the process. Through content\nanalysis, four principles emerged from the review that must be applied to\ncreate Human-Centred AI systems that can establish a symbiotic relationship\nwith humans. In addition, current trends and challenges were defined to\nindicate open questions that may guide future research for the development of\nSAI systems that comply with the AI Act.",
          "arxiv_id": "2501.08046v3"
        },
        {
          "title": "Public Constitutional AI",
          "year": "2024-06",
          "abstract": "We are increasingly subjected to the power of AI authorities. As AI decisions\nbecome inescapable, entering domains such as healthcare, education, and law, we\nmust confront a vital question: how can we ensure AI systems have the\nlegitimacy necessary for effective governance? This essay argues that to secure\nAI legitimacy, we need methods that engage the public in designing and\nconstraining AI systems, ensuring these technologies reflect the community's\nshared values. Constitutional AI, proposed by Anthropic, represents a step\ntowards this goal, offering a model for democratic control of AI. However,\nwhile Constitutional AI's commitment to hardcoding explicit principles into AI\nmodels enhances transparency and accountability, it falls short in two crucial\naspects: addressing the opacity of individual AI decisions and fostering\ngenuine democratic legitimacy. To overcome these limitations, this essay\nproposes \"Public Constitutional AI.\" This approach envisions a participatory\nprocess where diverse stakeholders, including ordinary citizens, deliberate on\nthe principles guiding AI development. The resulting \"AI Constitution\" would\ncarry the legitimacy of popular authorship, grounding AI governance in the\npublic will. Furthermore, the essay proposes \"AI Courts\" to develop \"AI case\nlaw,\" providing concrete examples for operationalizing constitutional\nprinciples in AI training. This evolving combination of constitutional\nprinciples and case law aims to make AI governance more responsive to public\nvalues. By grounding AI governance in deliberative democratic processes, Public\nConstitutional AI offers a path to imbue automated authorities with genuine\ndemocratic legitimacy, addressing the unique challenges posed by increasingly\npowerful AI systems while ensuring their alignment with the public interest.",
          "arxiv_id": "2406.16696v2"
        }
      ],
      "9": [
        {
          "title": "Large Language Model Driven Recommendation",
          "year": "2024-08",
          "abstract": "While previous chapters focused on recommendation systems (RSs) based on\nstandardized, non-verbal user feedback such as purchases, views, and clicks --\nthe advent of LLMs has unlocked the use of natural language (NL) interactions\nfor recommendation. This chapter discusses how LLMs' abilities for general NL\nreasoning present novel opportunities to build highly personalized RSs -- which\ncan effectively connect nuanced and diverse user preferences to items,\npotentially via interactive dialogues. To begin this discussion, we first\npresent a taxonomy of the key data sources for language-driven recommendation,\ncovering item descriptions, user-system interactions, and user profiles. We\nthen proceed to fundamental techniques for LLM recommendation, reviewing the\nuse of encoder-only and autoregressive LLM recommendation in both tuned and\nuntuned settings. Afterwards, we move to multi-module recommendation\narchitectures in which LLMs interact with components such as retrievers and RSs\nin multi-stage pipelines. This brings us to architectures for conversational\nrecommender systems (CRSs), in which LLMs facilitate multi-turn dialogues where\neach turn presents an opportunity not only to make recommendations, but also to\nengage with the user in interactive preference elicitation, critiquing, and\nquestion-answering.",
          "arxiv_id": "2408.10946v2"
        },
        {
          "title": "DEKGCI: A double-sided recommendation model for integrating knowledge graph and user-item interaction graph",
          "year": "2023-06",
          "abstract": "Both knowledge graphs and user-item interaction graphs are frequently used in\nrecommender systems due to their ability to provide rich information for\nmodeling users and items. However, existing studies often focused on one of\nthese sources (either the knowledge graph or the user-item interaction graph),\nresulting in underutilization of the benefits that can be obtained by\nintegrating both sources of information. In this paper, we propose DEKGCI, a\nnovel double-sided recommendation model. In DEKGCI, we use the high-order\ncollaborative signals from the user-item interaction graph to enrich the user\nrepresentations on the user side. Additionally, we utilize the high-order\nstructural and semantic information from the knowledge graph to enrich the item\nrepresentations on the item side. DEKGCI simultaneously learns the user and\nitem representations to effectively capture the joint interactions between\nusers and items. Three real-world datasets are adopted in the experiments to\nevaluate DEKGCI's performance, and experimental results demonstrate its high\neffectiveness compared to seven state-of-the-art baselines in terms of AUC and\nACC.",
          "arxiv_id": "2306.13837v1"
        },
        {
          "title": "Multi-Perspective Attention Mechanism for Bias-Aware Sequential Recommendation",
          "year": "2025-02",
          "abstract": "In the era of advancing information technology, recommender systems have\nemerged as crucial tools for dealing with information overload. However,\ntraditional recommender systems still have limitations in capturing the dynamic\nevolution of user behavior. To better understand and predict user behavior,\nespecially taking into account the complexity of temporal evolution, sequential\nrecommender systems have gradually become the focus of research. Currently,\nmany sequential recommendation algorithms ignore the amplification effects of\nprevalent biases, which leads to recommendation results being susceptible to\nthe Matthew Effect. Additionally, it will impose limitations on the recommender\nsystem's ability to deeply perceive and capture the dynamic shifts in user\npreferences, thereby diminishing the extent of its recommendation reach. To\naddress this issue effectively, we propose a recommendation system based on\nsequential information and attention mechanism called Multi-Perspective\nAttention Bias Sequential Recommendation (MABSRec). Firstly, we reconstruct\nuser sequences into three short types and utilize graph neural networks for\nitem weighting. Subsequently, an adaptive multi-bias perspective attention\nmodule is proposed to enhance the accuracy of recommendations. Experimental\nresults show that the MABSRec model exhibits significant advantages in all\nevaluation metrics, demonstrating its excellent performance in the sequence\nrecommendation task.",
          "arxiv_id": "2504.05323v1"
        }
      ],
      "10": [
        {
          "title": "Most General Explanations of Tree Ensembles (Extended Version)",
          "year": "2025-05",
          "abstract": "Explainable Artificial Intelligence (XAI) is critical for attaining trust in\nthe operation of AI systems. A key question of an AI system is ``why was this\ndecision made this way''. Formal approaches to XAI use a formal model of the AI\nsystem to identify abductive explanations. While abductive explanations may be\napplicable to a large number of inputs sharing the same concrete values, more\ngeneral explanations may be preferred for numeric inputs. So-called inflated\nabductive explanations give intervals for each feature ensuring that any input\nwhose values fall withing these intervals is still guaranteed to make the same\nprediction. Inflated explanations cover a larger portion of the input space,\nand hence are deemed more general explanations. But there can be many\n(inflated) abductive explanations for an instance. Which is the best? In this\npaper, we show how to find a most general abductive explanation for an AI\ndecision. This explanation covers as much of the input space as possible, while\nstill being a correct formal explanation of the model's behaviour. Given that\nwe only want to give a human one explanation for a decision, the most general\nexplanation gives us the explanation with the broadest applicability, and hence\nthe one most likely to seem sensible. (The paper has been accepted at IJCAI2025\nconference.)",
          "arxiv_id": "2505.10991v3"
        },
        {
          "title": "XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development",
          "year": "2024-03",
          "abstract": "In this study, we propose the early adoption of Explainable AI (XAI) with a\nfocus on three properties: Quality of explanation, the explanation summaries\nshould be consistent across multiple XAI methods; Architectural Compatibility,\nfor effective integration in XAI, the architecture styles of both the XAI\nmethods and the models to be explained must be compatible with the framework;\nConfigurable operations, XAI explanations are operable, akin to machine\nlearning operations. Thus, an explanation for AI models should be reproducible\nand tractable to be trustworthy. We present XAIport, a framework of XAI\nmicroservices encapsulated into Open APIs to deliver early explanations as\nobservation for learning model quality assurance. XAIport enables configurable\nXAI operations along with machine learning development. We quantify the\noperational costs of incorporating XAI with three cloud computer vision\nservices on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and\nAmazon Rekognition. Our findings show comparable operational costs between XAI\nand traditional machine learning, with XAIport significantly improving both\ncloud AI model performance and explanation stability.",
          "arxiv_id": "2403.16858v1"
        },
        {
          "title": "Disagreement amongst counterfactual explanations: How transparency can be deceptive",
          "year": "2023-04",
          "abstract": "Counterfactual explanations are increasingly used as an Explainable\nArtificial Intelligence (XAI) technique to provide stakeholders of complex\nmachine learning algorithms with explanations for data-driven decisions. The\npopularity of counterfactual explanations resulted in a boom in the algorithms\ngenerating them. However, not every algorithm creates uniform explanations for\nthe same instance. Even though in some contexts multiple possible explanations\nare beneficial, there are circumstances where diversity amongst counterfactual\nexplanations results in a potential disagreement problem among stakeholders.\nEthical issues arise when for example, malicious agents use this diversity to\nfairwash an unfair machine learning model by hiding sensitive features. As\nlegislators worldwide tend to start including the right to explanations for\ndata-driven, high-stakes decisions in their policies, these ethical issues\nshould be understood and addressed. Our literature review on the disagreement\nproblem in XAI reveals that this problem has never been empirically assessed\nfor counterfactual explanations. Therefore, in this work, we conduct a\nlarge-scale empirical analysis, on 40 datasets, using 12 explanation-generating\nmethods, for two black-box models, yielding over 192.0000 explanations. Our\nstudy finds alarmingly high disagreement levels between the methods tested. A\nmalicious user is able to both exclude and include desired features when\nmultiple counterfactual explanations are available. This disagreement seems to\nbe driven mainly by the dataset characteristics and the type of counterfactual\nalgorithm. XAI centers on the transparency of algorithmic decision-making, but\nour analysis advocates for transparency about this self-proclaimed transparency",
          "arxiv_id": "2304.12667v1"
        }
      ],
      "11": [
        {
          "title": "A Survey on Large Language Models for Code Generation",
          "year": "2024-06",
          "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across\ndiverse code-related tasks, known as Code LLMs, particularly in code generation\nthat generates source code with LLM from natural language descriptions. This\nburgeoning field has captured significant interest from both academic\nresearchers and industry professionals due to its practical significance in\nsoftware development, e.g., GitHub Copilot. Despite the active exploration of\nLLMs for a variety of code tasks, either from the perspective of natural\nlanguage processing (NLP) or software engineering (SE) or both, there is a\nnoticeable absence of a comprehensive and up-to-date literature review\ndedicated to LLM for code generation. In this survey, we aim to bridge this gap\nby providing a systematic literature review that serves as a valuable reference\nfor researchers investigating the cutting-edge progress in LLMs for code\ngeneration. We introduce a taxonomy to categorize and discuss the recent\ndevelopments in LLMs for code generation, covering aspects such as data\ncuration, latest advances, performance evaluation, ethical implications,\nenvironmental impact, and real-world applications. In addition, we present a\nhistorical overview of the evolution of LLMs for code generation and offer an\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks\nacross various levels of difficulty and types of programming tasks to highlight\nthe progressive enhancements in LLM capabilities for code generation. We\nidentify critical challenges and promising opportunities regarding the gap\nbetween academia and practical development. Furthermore, we have established a\ndedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey)\nto continuously document and disseminate the most recent advances in the field.",
          "arxiv_id": "2406.00515v2"
        },
        {
          "title": "Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis",
          "year": "2024-12",
          "abstract": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools.",
          "arxiv_id": "2412.14841v2"
        },
        {
          "title": "How Accurately Do Large Language Models Understand Code?",
          "year": "2025-04",
          "abstract": "Large Language Models (LLMs) are increasingly used in post-development tasks\nsuch as code repair and testing. A key factor in these tasks' success is the\nmodel's deep understanding of code. However, the extent to which LLMs truly\nunderstand code remains largely unevaluated. Quantifying code comprehension is\nchallenging due to its abstract nature and the lack of a standardized metric.\nPreviously, this was assessed through developer surveys, which are not feasible\nfor evaluating LLMs. Existing LLM benchmarks focus primarily on code\ngeneration, fundamentally different from code comprehension. Additionally,\nfixed benchmarks quickly become obsolete as they become part of the training\ndata. This paper presents the first large-scale empirical investigation into\nLLMs' ability to understand code. Inspired by mutation testing, we use an LLM's\nfault-finding ability as a proxy for its deep code understanding. This approach\nis based on the insight that a model capable of identifying subtle functional\ndiscrepancies must understand the code well. We inject faults in real-world\nprograms and ask the LLM to localize them, ensuring the specifications suffice\nfor fault localization. Next, we apply semantic-preserving code mutations\n(SPMs) to the faulty programs and test whether the LLMs still locate the\nfaults, verifying their confidence in code understanding. We evaluate nine\npopular LLMs on 600,010 debugging tasks from 670 Java and 637 Python programs.\nWe find that LLMs lose the ability to debug the same bug in 78% of faulty\nprograms when SPMs are applied, indicating a shallow understanding of code and\nreliance on features irrelevant to semantics. We also find that LLMs understand\ncode earlier in the program better than later. This suggests that LLMs' code\ncomprehension remains tied to lexical and syntactic features due to\ntokenization designed for natural languages, which overlooks code semantics.",
          "arxiv_id": "2504.04372v2"
        }
      ],
      "12": [
        {
          "title": "DiffMorph: Text-less Image Morphing with Diffusion Models",
          "year": "2024-01",
          "abstract": "Text-conditioned image generation models are a prevalent use of AI image\nsynthesis, yet intuitively controlling output guided by an artist remains\nchallenging. Current methods require multiple images and textual prompts for\neach object to specify them as concepts to generate a single customized image.\n  On the other hand, our work, \\verb|DiffMorph|, introduces a novel approach\nthat synthesizes images that mix concepts without the use of textual prompts.\nOur work integrates a sketch-to-image module to incorporate user sketches as\ninput. \\verb|DiffMorph| takes an initial image with conditioning artist-drawn\nsketches to generate a morphed image.\n  We employ a pre-trained text-to-image diffusion model and fine-tune it to\nreconstruct each image faithfully. We seamlessly merge images and concepts from\nsketches into a cohesive composition. The image generation capability of our\nwork is demonstrated through our results and a comparison of these with\nprompt-based image generation.",
          "arxiv_id": "2401.00739v1"
        },
        {
          "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
          "year": "2023-10",
          "abstract": "Text-to-image diffusion models have recently emerged at the forefront of\nimage generation, powered by very large-scale unsupervised or weakly supervised\ntext-to-image training datasets. Due to their unsupervised training,\ncontrolling their behavior in downstream tasks, such as maximizing\nhuman-perceived image quality, image-text alignment, or ethical image\ngeneration, is difficult. Recent works finetune diffusion models to downstream\nreward functions using vanilla reinforcement learning, notorious for the high\nvariance of the gradient estimators. In this paper, we propose AlignProp, a\nmethod that aligns diffusion models to downstream reward functions using\nend-to-end backpropagation of the reward gradient through the denoising\nprocess. While naive implementation of such backpropagation would require\nprohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test\nAlignProp in finetuning diffusion models to various objectives, such as\nimage-text semantic alignment, aesthetics, compressibility and controllability\nof the number of objects present, as well as their combinations. We show\nAlignProp achieves higher rewards in fewer training steps than alternatives,\nwhile being conceptually simpler, making it a straightforward choice for\noptimizing diffusion models for differentiable reward functions of interest.\nCode and Visualization results are available at https://align-prop.github.io/.",
          "arxiv_id": "2310.03739v5"
        },
        {
          "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
          "year": "2024-01",
          "abstract": "Diffusion models have exhibit exceptional performance in text-to-image\ngeneration and editing. However, existing methods often face challenges when\nhandling complex text prompts that involve multiple objects with multiple\nattributes and relationships. In this paper, we propose a brand new\ntraining-free text-to-image generation/editing framework, namely Recaption,\nPlan and Generate (RPG), harnessing the powerful chain-of-thought reasoning\nability of multimodal LLMs to enhance the compositionality of text-to-image\ndiffusion models. Our approach employs the MLLM as a global planner to\ndecompose the process of generating complex images into multiple simpler\ngeneration tasks within subregions. We propose complementary regional diffusion\nto enable region-wise compositional generation. Furthermore, we integrate\ntext-guided image generation and editing within the proposed RPG in a\nclosed-loop fashion, thereby enhancing generalization ability. Extensive\nexperiments demonstrate our RPG outperforms state-of-the-art text-to-image\ndiffusion models, including DALL-E 3 and SDXL, particularly in multi-category\nobject composition and text-image semantic alignment. Notably, our RPG\nframework exhibits wide compatibility with various MLLM architectures (e.g.,\nMiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available\nat: https://github.com/YangLing0818/RPG-DiffusionMaster",
          "arxiv_id": "2401.11708v3"
        }
      ],
      "13": [
        {
          "title": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs",
          "year": "2025-09",
          "abstract": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack.",
          "arxiv_id": "2509.05367v2"
        },
        {
          "title": "QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language",
          "year": "2025-02",
          "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to $64\\%$ on GPT-4-1106. Our code is\navailable at https://github.com/horizonsinzqs/QueryAttack.",
          "arxiv_id": "2502.09723v3"
        },
        {
          "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on Large Language Models",
          "year": "2024-07",
          "abstract": "The rapid development of Large Language Models (LLMs) has brought impressive\nadvancements across various tasks. However, despite these achievements, LLMs\nstill pose inherent safety risks, especially in the context of jailbreak\nattacks. Most existing jailbreak methods follow an input-level manipulation\nparadigm to bypass safety mechanisms. Yet, as alignment techniques improve,\nsuch attacks are becoming increasingly detectable. In this work, we identify an\nunderexplored threat vector: the model's internal reasoning process, which can\nbe manipulated to elicit harmful outputs in a more stealthy way. To explore\nthis overlooked attack surface, we propose a novel black-box jailbreak attack\nmethod, Analyzing-based Jailbreak (ABJ). ABJ comprises two independent attack\npaths: textual and visual reasoning attacks, which exploit the model's\nmultimodal reasoning capabilities to bypass safety mechanisms, comprehensively\nexposing vulnerabilities in its reasoning chain. We conduct extensive\nexperiments on ABJ across various open-source and closed-source LLMs, VLMs, and\nRLMs. In particular, ABJ achieves high attack success rate (ASR) (82.1% on\nGPT-4o-2024-11-20) with exceptional attack efficiency (AE) among all target\nmodels, showcasing its remarkable attack effectiveness, transferability, and\nefficiency. Our work reveals a new type of safety risk and highlights the\nurgent need to mitigate implicit vulnerabilities in the model's reasoning\nprocess.",
          "arxiv_id": "2407.16205v6"
        }
      ],
      "14": [
        {
          "title": "Devolutionary genetic algorithms with application to the minimum labeling Steiner tree problem",
          "year": "2020-04",
          "abstract": "This paper characterizes and discusses devolutionary genetic algorithms and\nevaluates their performances in solving the minimum labeling Steiner tree\n(MLST) problem. We define devolutionary algorithms as the process of reaching a\nfeasible solution by devolving a population of super-optimal unfeasible\nsolutions over time. We claim that distinguishing them from the widely used\nevolutionary algorithms is relevant. The most important distinction lies in the\nfact that in the former type of processes, the value function decreases over\nsuccessive generation of solutions, thus providing a natural stopping condition\nfor the computation process. We show how classical evolutionary concepts, such\nas crossing, mutation and fitness can be adapted to aim at reaching an optimal\nor close-to-optimal solution among the first generations of feasible solutions.\nWe additionally introduce a novel integer linear programming formulation of the\nMLST problem and a valid constraint used for speeding up the devolutionary\nprocess. Finally, we conduct an experiment comparing the performances of\ndevolutionary algorithms to those of state of the art approaches used for\nsolving randomly generated instances of the MLST problem. Results of this\nexperiment support the use of devolutionary algorithms for the MLST problem and\ntheir development for other NP-hard combinatorial optimization problems.",
          "arxiv_id": "2004.10048v1"
        },
        {
          "title": "An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling Problems Based on Constraint Programming",
          "year": "2023-06",
          "abstract": "Constraint Programming (CP) is a declarative programming paradigm that allows\nfor modeling and solving combinatorial optimization problems, such as the\nJob-Shop Scheduling Problem (JSSP). While CP solvers manage to find optimal or\nnear-optimal solutions for small instances, they do not scale well to large\nones, i.e., they require long computation times or yield low-quality solutions.\nTherefore, real-world scheduling applications often resort to fast,\nhandcrafted, priority-based dispatching heuristics to find a good initial\nsolution and then refine it using optimization methods.\n  This paper proposes a novel end-to-end approach to solving scheduling\nproblems by means of CP and Reinforcement Learning (RL). In contrast to\nprevious RL methods, tailored for a given problem by including procedural\nsimulation algorithms, complex feature engineering, or handcrafted reward\nfunctions, our neural-network architecture and training algorithm merely\nrequire a generic CP encoding of some scheduling problem along with a set of\nsmall instances. Our approach leverages existing CP solvers to train an agent\nlearning a Priority Dispatching Rule (PDR) that generalizes well to large\ninstances, even from separate datasets. We evaluate our method on seven JSSP\ndatasets from the literature, showing its ability to find higher-quality\nsolutions for very large instances than obtained by static PDRs and by a CP\nsolver within the same time limit.",
          "arxiv_id": "2306.05747v1"
        },
        {
          "title": "Multi-Space Evolutionary Search for Large-Scale Optimization",
          "year": "2021-02",
          "abstract": "In recent years, to improve the evolutionary algorithms used to solve\noptimization problems involving a large number of decision variables, many\nattempts have been made to simplify the problem solution space of a given\nproblem for the evolutionary search. In the literature, the existing approaches\ncan generally be categorized as decomposition-based methods and\ndimension-reduction-based methods. The former decomposes a large-scale problem\ninto several smaller subproblems, while the latter transforms the original\nhigh-dimensional solution space into a low-dimensional space. However, it is\nworth noting that a given large-scale optimization problem may not always be\ndecomposable, and it is also difficult to guarantee that the global optimum of\nthe original problem is preserved in the reduced low-dimensional problem space.\nThis paper thus proposes a new search paradigm, namely the multi-space\nevolutionary search, to enhance the existing evolutionary search methods for\nsolving large-scale optimization problems. In contrast to existing approaches\nthat perform an evolutionary search in a single search space, the proposed\nparadigm is designed to conduct a search in multiple solution spaces that are\nderived from the given problem, each possessing a unique landscape. The\nproposed paradigm makes no assumptions about the large-scale optimization\nproblem of interest, such as that the problem is decomposable or that a certain\nrelationship exists among the decision variables. To verify the efficacy of the\nproposed paradigm, comprehensive empirical studies in comparison to four\nstate-of-the-art algorithms were conducted using the CEC2013 large-scale\nbenchmark problems.",
          "arxiv_id": "2102.11693v2"
        }
      ],
      "15": [
        {
          "title": "TurboAttention: Efficient Attention Approximation For High Throughputs LLMs",
          "year": "2024-12",
          "abstract": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
          "arxiv_id": "2412.08585v3"
        },
        {
          "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
          "year": "2024-07",
          "abstract": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
          "arxiv_id": "2407.21018v3"
        },
        {
          "title": "Low-Rank Quantization-Aware Training for LLMs",
          "year": "2024-06",
          "abstract": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT",
          "arxiv_id": "2406.06385v3"
        }
      ],
      "16": [
        {
          "title": "ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model",
          "year": "2024-08",
          "abstract": "Advancements in 3D scene reconstruction have transformed 2D images from the\nreal world into 3D models, producing realistic 3D results from hundreds of\ninput photos. Despite great success in dense-view reconstruction scenarios,\nrendering a detailed scene from insufficient captured views is still an\nill-posed optimization problem, often resulting in artifacts and distortions in\nunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction\nparadigm that reframes the ambiguous reconstruction challenge as a temporal\ngeneration task. The key insight is to unleash the strong generative prior of\nlarge pre-trained video diffusion models for sparse-view reconstruction.\nHowever, 3D view consistency struggles to be accurately preserved in directly\ngenerated video frames from pre-trained models. To address this, given limited\ninput views, the proposed ReconX first constructs a global point cloud and\nencodes it into a contextual space as the 3D structure condition. Guided by the\ncondition, the video diffusion model then synthesizes video frames that are\nboth detail-preserved and exhibit a high degree of 3D consistency, ensuring the\ncoherence of the scene from various perspectives. Finally, we recover the 3D\nscene from the generated video through a confidence-aware 3D Gaussian Splatting\noptimization scheme. Extensive experiments on various real-world datasets show\nthe superiority of our ReconX over state-of-the-art methods in terms of quality\nand generalizability.",
          "arxiv_id": "2408.16767v4"
        },
        {
          "title": "3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation",
          "year": "2022-12",
          "abstract": "Text-guided 3D object generation aims to generate 3D objects described by\nuser-defined captions, which paves a flexible way to visualize what we\nimagined. Although some works have been devoted to solving this challenging\ntask, these works either utilize some explicit 3D representations (e.g., mesh),\nwhich lack texture and require post-processing for rendering photo-realistic\nviews; or require individual time-consuming optimization for every single case.\nHere, we make the first attempt to achieve generic text-guided cross-category\n3D object generation via a new 3D-TOGO model, which integrates a text-to-views\ngeneration module and a views-to-3D generation module. The text-to-views\ngeneration module is designed to generate different views of the target 3D\nobject given an input caption. prior-guidance, caption-guidance and view\ncontrastive learning are proposed for achieving better view-consistency and\ncaption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D\ngeneration module to obtain the implicit 3D neural representation from the\npreviously-generated views. Our 3D-TOGO model generates 3D objects in the form\nof the neural radiance field with good texture and requires no time-cost\noptimization for every single caption. Besides, 3D-TOGO can control the\ncategory, color and shape of generated 3D objects with the input caption.\nExtensive experiments on the largest 3D object dataset (i.e., ABO) are\nconducted to verify that 3D-TOGO can better generate high-quality 3D objects\naccording to the input captions across 98 different categories, in terms of\nPSNR, SSIM, LPIPS and CLIP-score, compared with text-NeRF and Dreamfields.",
          "arxiv_id": "2212.01103v2"
        },
        {
          "title": "Point Cloud Self-supervised Learning via 3D to Multi-view Masked Learner",
          "year": "2023-11",
          "abstract": "Recently, multi-modal masked autoencoders (MAE) has been introduced in 3D\nself-supervised learning, offering enhanced feature learning by leveraging both\n2D and 3D data to capture richer cross-modal representations. However, these\napproaches have two limitations: (1) they inefficiently require both 2D and 3D\nmodalities as inputs, even though the inherent multi-view properties of 3D\npoint clouds already contain 2D modality. (2) input 2D modality causes the\nreconstruction learning to unnecessarily rely on visible 2D information,\nhindering 3D geometric representation learning. To address these challenges, we\npropose a 3D to Multi-View Learner (Multi-View ML) that only utilizes 3D\nmodalities as inputs and effectively capture rich spatial information in 3D\npoint clouds. Specifically, we first project 3D point clouds to multi-view 2D\nimages at the feature level based on 3D-based pose. Then, we introduce two\ncomponents: (1) a 3D to multi-view autoencoder that reconstructs point clouds\nand multi-view images from 3D and projected 2D features; (2) a multi-scale\nmulti-head (MSMH) attention mechanism that facilitates local-global information\ninteractions in each decoder transformer block through attention heads at\nvarious scales. Additionally, a novel two-stage self-training strategy is\nproposed to align 2D and 3D representations. Our method outperforms\nstate-of-the-art counterparts across various downstream tasks, including 3D\nclassification, part segmentation, and object detection.",
          "arxiv_id": "2311.10887v2"
        }
      ],
      "17": [
        {
          "title": "GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation",
          "year": "2024-09",
          "abstract": "The ability to form, retrieve, and reason about memories in response to\nstimuli serves as the cornerstone for general intelligence - shaping entities\ncapable of learning, adaptation, and intuitive insight. Large Language Models\n(LLMs) have proven their ability, given the proper memories or context, to\nreason and respond meaningfully to stimuli. However, they are still unable to\noptimally encode, store, and retrieve memories - the ability to do this would\nunlock their full ability to operate as AI agents, and to specialize to niche\ndomains. To remedy this, one promising area of research is Retrieval Augmented\nGeneration (RAG), which aims to augment LLMs by providing them with rich\nin-context examples and information. In question-answering (QA) applications,\nRAG methods embed the text of interest in chunks, and retrieve the most\nrelevant chunks for a prompt using text embeddings. Motivated by human memory\nencoding and retrieval, we aim to improve over standard RAG methods by\ngenerating and encoding higher-level information and tagging the chunks by\ntheir utility to answer questions. We introduce Graphical Eigen Memories For\nRetrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk\nof text in a given text corpus with LLM generated ``utility'' questions,\nconnecting chunks in a graph based on the similarity of both their text and\nutility questions, and then using the eigendecomposition of the memory graph to\nbuild higher level summary nodes that capture the main themes of the text. We\nevaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with\nSBERT, and OpenAI's text encoders on two standard QA tasks, showing that\nGEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also\ndiscuss the implications of having a robust RAG system and future directions.",
          "arxiv_id": "2409.15566v1"
        },
        {
          "title": "RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models for Question Answering",
          "year": "2024-10",
          "abstract": "Retrieval-augmented generation (RAG) has shown promising potential in\nknowledge intensive question answering (QA). However, existing approaches only\nconsider the query itself, neither specifying the retrieval preferences for the\nretrievers nor informing the generators of how to refer to the retrieved\ndocuments for the answers, which poses a significant challenge to the QA\nperformance. To address these issues, we propose Rule-guided\nRetrieval-Augmented Generation with LMs, which explicitly introduces rules for\nin-context learning (RuleRAG-ICL) to guide retrievers to recall related\ndocuments in the directions of rules and uniformly guide generators to reason\nattributed by the same rules. Moreover, most existing RAG datasets were\nconstructed without considering rules and Knowledge Graphs (KGs) are recognized\nas providing high-quality rules. Therefore, we construct five rule-aware RAG\nbenchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval\nand reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL\nimproves the retrieval quality of +89.2% in Recall@10 and answer accuracy of\n+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,\nexperiments on four existing RAG datasets show RuleRAG is also effective by\noffering rules in RuleQA to them, further proving the generalization of rule\nguidance in RuleRAG.",
          "arxiv_id": "2410.22353v3"
        },
        {
          "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering",
          "year": "2025-05",
          "abstract": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.",
          "arxiv_id": "2505.12662v1"
        }
      ],
      "18": [
        {
          "title": "Times2D: Multi-Period Decomposition and Derivative Mapping for General Time Series Forecasting",
          "year": "2025-03",
          "abstract": "Time series forecasting is an important application in various domains such\nas energy management, traffic planning, financial markets, meteorology, and\nmedicine. However, real-time series data often present intricate temporal\nvariability and sharp fluctuations, which pose significant challenges for time\nseries forecasting. Previous models that rely on 1D time series representations\nusually struggle with complex temporal variations. To address the limitations\nof 1D time series, this study introduces the Times2D method that transforms the\n1D time series into 2D space. Times2D consists of three main parts: first, a\nPeriodic Decomposition Block (PDB) that captures temporal variations within a\nperiod and between the same periods by converting the time series into a 2D\ntensor in the frequency domain. Second, the First and Second Derivative\nHeatmaps (FSDH) capture sharp changes and turning points, respectively.\nFinally, an Aggregation Forecasting Block (AFB) integrates the output tensors\nfrom PDB and FSDH for accurate forecasting. This 2D transformation enables the\nutilization of 2D convolutional operations to effectively capture long and\nshort characteristics of the time series. Comprehensive experimental results\nacross large-scale data in the literature demonstrate that the proposed Times2D\nmodel achieves state-of-the-art performance in both short-term and long-term\nforecasting. The code is available in this repository:\nhttps://github.com/Tims2D/Times2D.",
          "arxiv_id": "2504.00118v1"
        },
        {
          "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
          "year": "2023-10",
          "abstract": "Time series forecasting holds significant importance in many real-world\ndynamic systems and has been extensively studied. Unlike natural language\nprocess (NLP) and computer vision (CV), where a single large model can tackle\nmultiple tasks, models for time series forecasting are often specialized,\nnecessitating distinct designs for different tasks and applications. While\npre-trained foundation models have made impressive strides in NLP and CV, their\ndevelopment in time series domains has been constrained by data sparsity.\nRecent studies have revealed that large language models (LLMs) possess robust\npattern recognition and reasoning abilities over complex sequences of tokens.\nHowever, the challenge remains in effectively aligning the modalities of time\nseries data and natural language to leverage these capabilities. In this work,\nwe present Time-LLM, a reprogramming framework to repurpose LLMs for general\ntime series forecasting with the backbone language models kept intact. We begin\nby reprogramming the input time series with text prototypes before feeding it\ninto the frozen LLM to align the two modalities. To augment the LLM's ability\nto reason with time series data, we propose Prompt-as-Prefix (PaP), which\nenriches the input context and directs the transformation of reprogrammed input\npatches. The transformed time series patches from the LLM are finally projected\nto obtain the forecasts. Our comprehensive evaluations demonstrate that\nTime-LLM is a powerful time series learner that outperforms state-of-the-art,\nspecialized forecasting models. Moreover, Time-LLM excels in both few-shot and\nzero-shot learning scenarios.",
          "arxiv_id": "2310.01728v2"
        },
        {
          "title": "Respecting Time Series Properties Makes Deep Time Series Forecasting Perfect",
          "year": "2022-07",
          "abstract": "How to handle time features shall be the core question of any time series\nforecasting model. Ironically, it is often ignored or misunderstood by\ndeep-learning based models, even those baselines which are state-of-the-art.\nThis behavior makes their inefficient, untenable and unstable. In this paper,\nwe rigorously analyze three prevalent but deficient/unfounded deep time series\nforecasting mechanisms or methods from the view of time series properties,\nincluding normalization methods, multivariate forecasting and input sequence\nlength. Corresponding corollaries and solutions are given on both empirical and\ntheoretical basis. We thereby propose a novel time series forecasting network,\ni.e. RTNet, on the basis of aforementioned analysis. It is general enough to be\ncombined with both supervised and self-supervised forecasting format. Thanks to\nthe core idea of respecting time series properties, no matter in which\nforecasting format, RTNet shows obviously superior forecasting performances\ncompared with dozens of other SOTA time series forecasting baselines in three\nreal-world benchmark datasets. By and large, it even occupies less time\ncomplexity and memory usage while acquiring better forecasting accuracy. The\nsource code is available at https://github.com/OrigamiSL/RTNet.",
          "arxiv_id": "2207.10941v1"
        }
      ],
      "19": [
        {
          "title": "FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning",
          "year": "2024-03",
          "abstract": "Federated learning (FL) is a promising approach for learning a model from\ndata distributed on massive clients without exposing data privacy. It works\neffectively in the ideal federation where clients share homogeneous data\ndistribution and learning behavior. However, FL may fail to function\nappropriately when the federation is not ideal, amid an unhealthy state called\nNegative Federated Learning (NFL), in which most clients gain no benefit from\nparticipating in FL. Many studies have tried to address NFL. However, their\nsolutions either (1) predetermine to prevent NFL in the entire learning\nlife-cycle or (2) tackle NFL in the aftermath of numerous learning rounds.\nThus, they either (1) indiscriminately incur extra costs even if FL can perform\nwell without such costs or (2) waste numerous learning rounds. Additionally,\nnone of the previous work takes into account the clients who may be\nunwilling/unable to follow the proposed NFL solutions when using those\nsolutions to upgrade an FL system in use. This paper introduces FL-GUARD, a\nholistic framework that can be employed on any FL system for tackling NFL in a\nrun-time paradigm. That is, to dynamically detect NFL at the early stage (tens\nof rounds) of learning and then to activate recovery measures when necessary.\nSpecifically, we devise a cost-effective NFL detection mechanism, which relies\non an estimation of performance gain on clients. Only when NFL is detected, we\nactivate the NFL recovery process, in which each client learns in parallel an\nadapted model when training the global model. Extensive experiment results\nconfirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL\nto a healthy learning state. We also show that FL-GUARD is compatible with\nprevious NFL solutions and robust against clients unwilling/unable to take any\nrecovery measures.",
          "arxiv_id": "2403.04146v1"
        },
        {
          "title": "FedDefender: Backdoor Attack Defense in Federated Learning",
          "year": "2023-07",
          "abstract": "Federated Learning (FL) is a privacy-preserving distributed machine learning\ntechnique that enables individual clients (e.g., user participants, edge\ndevices, or organizations) to train a model on their local data in a secure\nenvironment and then share the trained model with an aggregator to build a\nglobal model collaboratively. In this work, we propose FedDefender, a defense\nmechanism against targeted poisoning attacks in FL by leveraging differential\ntesting. Our proposed method fingerprints the neuron activations of clients'\nmodels on the same input and uses differential testing to identify a\npotentially malicious client containing a backdoor. We evaluate FedDefender\nusing MNIST and FashionMNIST datasets with 20 and 30 clients, and our results\ndemonstrate that FedDefender effectively mitigates such attacks, reducing the\nattack success rate (ASR) to 10\\% without deteriorating the global model\nperformance.",
          "arxiv_id": "2307.08672v2"
        },
        {
          "title": "Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning",
          "year": "2024-02",
          "abstract": "Federated Learning (FL) is an emerging machine learning paradigm that enables\nthe collaborative training of a shared global model across distributed clients\nwhile keeping the data decentralized. Recent works on designing systems for\nefficient FL have shown that utilizing serverless computing technologies,\nparticularly Function-as-a-Service (FaaS) for FL, can enhance resource\nefficiency, reduce training costs, and alleviate the complex infrastructure\nmanagement burden on data holders. However, existing serverless FL systems\nimplicitly assume a uniform global model architecture across all participating\nclients during training. This assumption fails to address fundamental\nchallenges in practical FL due to the resource and statistical data\nheterogeneity among FL clients. To address these challenges and enable\nheterogeneous client models in serverless FL, we utilize Knowledge Distillation\n(KD) in this paper. Towards this, we propose novel optimized serverless\nworkflows for two popular conventional federated KD techniques, i.e., FedMD and\nFedDF. We implement these workflows by introducing several extensions to an\nopen-source serverless FL system called FedLess. Moreover, we comprehensively\nevaluate the two strategies on multiple datasets across varying levels of\nclient data heterogeneity using heterogeneous client models with respect to\naccuracy, fine-grained training times, and costs. Results from our experiments\ndemonstrate that serverless FedDF is more robust to extreme non-IID data\ndistributions, is faster, and leads to lower costs than serverless FedMD. In\naddition, compared to the original implementation, our optimizations for\nparticular steps in FedMD and FedDF lead to an average speedup of 3.5x and\n1.76x across all datasets.",
          "arxiv_id": "2402.07295v1"
        }
      ],
      "20": [
        {
          "title": "A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems",
          "year": "2024-04",
          "abstract": "Recently, deep learning (DL) has been emerging as a promising approach for\nchannel estimation and signal detection in wireless communications. The\nmajority of the existing studies investigating the use of DL techniques in this\ndomain focus on analysing channel impulse responses that are generated from\nonly one channel distribution such as additive white Gaussian channel noise and\nRayleigh channels. In practice, to cope with the dynamic nature of the wireless\nchannel, DL methods must be re-trained on newly non-aged collected data which\nis costly, inefficient, and impractical. To tackle this challenge, this paper\nproposes a novel universal deep neural network (Uni-DNN) that can achieve high\ndetection performance in various wireless environments without retraining the\nmodel. In particular, our proposed Uni-DNN model consists of a wireless channel\nclassifier and a signal detector which are constructed by using DNNs. The\nwireless channel classifier enables the signal detector to generalise and\nperform optimally for multiple wireless channel distributions. In addition, to\nfurther improve the signal detection performance of the proposed model,\nconvolutional neural network is employed. Extensive simulations using the\northogonal frequency division multiplexing scheme demonstrate that the bit\nerror rate performance of our proposed solution can outperform conventional\nDL-based approaches as well as least square and minimum mean square error\nchannel estimators in practical low pilot density scenarios.",
          "arxiv_id": "2404.02648v1"
        },
        {
          "title": "Multi Objective Resource Optimization of Wireless Network Based on Cross Domain Virtual Network Embedding",
          "year": "2022-02",
          "abstract": "The rapid development of virtual network architecture makes it possible for\nwireless network to be widely used. With the popularity of artificial\nintelligence (AI) industry in daily life, efficient resource allocation of\nwireless network has become a problem. Especially when network users request\nwireless network resources from different management domains, they still face\nmany practical problems. From the perspective of virtual network embedding\n(VNE), this paper designs and implements a multi-objective optimization VNE\nalgorithm for wireless network resource allocation. Resource allocation in\nvirtual network is essentially a problem of allocating underlying resources for\nvirtual network requests (VNRs). According to the proposed objective formula,\nwe consider the optimization mapping cost, network delay and VNR acceptance\nrate. VNE is completed by node mapping and link mapping. In the experiment and\nsimulation stage, it is compared with other VNE algorithms, the cross domain\nVNE algorithm proposed in this paper is optimal in the above three indicators.\nThis shows the effectiveness of the algorithm in wireless network resource\nallocation.",
          "arxiv_id": "2202.02139v1"
        },
        {
          "title": "Leveraging AI and Intelligent Reflecting Surface for Energy-Efficient Communication in 6G IoT",
          "year": "2020-12",
          "abstract": "The ever-increasing data traffic, various delay-sensitive services, and the\nmassive deployment of energy-limited Internet of Things (IoT) devices have\nbrought huge challenges to the current communication networks, motivating\nacademia and industry to move to the sixth-generation (6G) network. With the\npowerful capability of data transmission and processing, 6G is considered as an\nenabler for IoT communication with low latency and energy cost. In this paper,\nwe propose an artificial intelligence (AI) and intelligent reflecting surface\n(IRS) empowered energy-efficiency communication system for 6G IoT. First, we\ndesign a smart and efficient communication architecture including the IRS-aided\ndata transmission and the AI-driven network resource management mechanisms.\nSecond, an energy efficiency-maximizing model under given transmission latency\nfor 6G IoT system is formulated, which jointly optimizes the settings of all\ncommunication participants, i.e. IoT transmission power, IRS-reflection phase\nshift, and BS detection matrix. Third, a deep reinforcement learning (DRL)\nempowered network resource control and allocation scheme is proposed to solve\nthe formulated optimization model. Based on the network and channel status, the\nDRL-enabled scheme facilities the energy-efficiency and low-latency\ncommunication. Finally, experimental results verified the effectiveness of our\nproposed communication system for 6G IoT.",
          "arxiv_id": "2012.14716v1"
        }
      ],
      "21": [
        {
          "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China",
          "year": "2025-05",
          "abstract": "With the rapid advancement of generative artificial intelligence(AI), its\npotential applications in higher education have attracted significant\nattention. This study investigated how 148 students from diverse engineering\ndisciplines and regions across China used generative AI, focusing on its impact\non their learning experience and the opportunities and challenges it poses in\nengineering education. Based on the surveyed data, we explored four key areas:\nthe frequency and application scenarios of AI use among engineering students,\nits impact on students' learning and performance, commonly encountered\nchallenges in using generative AI, and future prospects for its adoption in\nengineering education. The results showed that more than half of the\nparticipants reported a positive impact of generative AI on their learning\nefficiency, initiative, and creativity, with nearly half believing it also\nenhanced their independent thinking. However, despite acknowledging improved\nstudy efficiency, many felt their actual academic performance remained largely\nunchanged and expressed concerns about the accuracy and domain-specific\nreliability of generative AI. Our findings provide a first-hand insight into\nthe current benefits and challenges generative AI brings to students,\nparticularly Chinese engineering students, while offering several\nrecommendations, especially from the students' perspective, for effectively\nintegrating generative AI into engineering education.",
          "arxiv_id": "2505.09208v1"
        },
        {
          "title": "Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography",
          "year": "2024-03",
          "abstract": "In recent years, the rapid development of artificial intelligence technology,\nespecially the emergence of large language models (LLMs) such as ChatGPT, has\npresented significant prospects for application in the field of education. LLMs\npossess the capability to interpret knowledge, answer questions, and consider\ncontext, thus providing support for dialogic teaching to students. Therefore,\nan examination of the capacity of LLMs to effectively fulfill instructional\nroles, thereby facilitating student learning akin to human educators within\ndialogic teaching scenarios, is an exceptionally valuable research topic. This\nresearch recruited 34 undergraduate students as participants, who were randomly\ndivided into two groups. The experimental group engaged in dialogic teaching\nusing ChatGPT, while the control group interacted with human teachers. Both\ngroups learned the histogram equalization unit in the information-related\ncourse \"Digital Image Processing\". The research findings show comparable scores\nbetween the two groups on the retention test. However, students who engaged in\ndialogue with ChatGPT exhibited lower performance on the transfer test.\nElectroencephalography data revealed that students who interacted with ChatGPT\nexhibited higher levels of cognitive activity, suggesting that ChatGPT could\nhelp students establish a knowledge foundation and stimulate cognitive\nactivity. However, its strengths on promoting students. knowledge application\nand creativity were insignificant. Based upon the research findings, it is\nevident that ChatGPT cannot fully excel in fulfilling teaching tasks in the\ndialogue teaching in information related courses. Combining ChatGPT with\ntraditional human teachers might be a more ideal approach. The synergistic use\nof both can provide students with more comprehensive learning support, thus\ncontributing to enhancing the quality of teaching.",
          "arxiv_id": "2403.16687v5"
        },
        {
          "title": "Learning-by-teaching with ChatGPT: The effect of teachable ChatGPT agent on programming education",
          "year": "2024-12",
          "abstract": "This study investigates the potential of using ChatGPT as a teachable agent\nto support students' learning by teaching process, specifically in programming\neducation. While learning by teaching is an effective pedagogical strategy for\npromoting active learning, traditional teachable agents have limitations,\nparticularly in facilitating natural language dialogue. Our research explored\nwhether ChatGPT, with its ability to engage learners in natural conversations,\ncan support this process. The findings reveal that interacting with ChatGPT\nimproves students' knowledge gains and programming abilities, particularly in\nwriting readable and logically sound code. However, it had limited impact on\ndeveloping learners' error-correction skills, likely because ChatGPT tends to\ngenerate correct code, reducing opportunities for students to practice\ndebugging. Additionally, students' self-regulated learning (SRL) abilities\nimproved, suggesting that teaching ChatGPT fosters learners' higher\nself-efficacy and better implementation of SRL strategies. This study discussed\nthe role of natural dialogue in fostering socialized learning by teaching, and\nexplored ChatGPT's specific contributions in supporting students' SRL through\nthe learning by teaching process. Overall, the study highlights ChatGPT's\npotential as a teachable agent, offering insights for future research on\nChatGPT-supported education.",
          "arxiv_id": "2412.15226v1"
        }
      ],
      "22": [
        {
          "title": "Treewidth-aware Reductions of Normal ASP to SAT -- Is Normal ASP Harder than SAT after All?",
          "year": "2022-10",
          "abstract": "Answer Set Programming (ASP) is a paradigm for modeling and solving problems\nfor knowledge representation and reasoning. There are plenty of results\ndedicated to studying the hardness of (fragments of) ASP. So far, these studies\nresulted in characterizations in terms of computational complexity as well as\nin fine-grained insights presented in form of dichotomy-style results, lower\nbounds when translating to other formalisms like propositional satisfiability\n(SAT), and even detailed parameterized complexity landscapes. A generic\nparameter in parameterized complexity originating from graph theory is the\nso-called treewidth, which in a sense captures structural density of a program.\nRecently, there was an increase in the number of treewidth-based solvers\nrelated to SAT. While there are translations from (normal) ASP to SAT, no\nreduction that preserves treewidth or at least keeps track of the treewidth\nincrease is known. In this paper we propose a novel reduction from normal ASP\nto SAT that is aware of the treewidth, and guarantees that a slight increase of\ntreewidth is indeed sufficient. Further, we show a new result establishing\nthat, when considering treewidth, already the fragment of normal ASP is\nslightly harder than SAT (under reasonable assumptions in computational\ncomplexity). This also confirms that our reduction probably cannot be\nsignificantly improved and that the slight increase of treewidth is\nunavoidable. Finally, we present an empirical study of our novel reduction from\nnormal ASP to SAT, where we compare treewidth upper bounds that are obtained\nvia known decomposition heuristics. Overall, our reduction works better with\nthese heuristics than existing translations.",
          "arxiv_id": "2210.03553v1"
        },
        {
          "title": "Relating Answer Set Programming and Many-sorted Logics for Formal Verification",
          "year": "2025-02",
          "abstract": "Answer Set Programming (ASP) is an important logic programming paradigm\nwithin the field of Knowledge Representation and Reasoning. As a concise,\nhuman-readable, declarative language, ASP is an excellent tool for developing\ntrustworthy (especially, artificially intelligent) software systems. However,\nformally verifying ASP programs offers some unique challenges, such as\n  1. a lack of modularity (the meanings of rules are difficult to define in\nisolation from the enclosing program),\n  2. the ground-and-solve semantics (the meanings of rules are dependent on the\ninput data with which the program is grounded), and\n  3. limitations of existing tools.\n  My research agenda has been focused on addressing these three issues with the\nintention of making ASP verification an accessible, routine task that is\nregularly performed alongside program development. In this vein, I have\ninvestigated alternative semantics for ASP based on translations into the logic\nof here-and-there and many-sorted first-order logic. These semantics promote a\nmodular understanding of logic programs, bypass grounding, and enable us to use\nautomated theorem provers to automatically verify properties of programs.",
          "arxiv_id": "2502.09230v1"
        },
        {
          "title": "Implementing Dynamic Answer Set Programming",
          "year": "2020-02",
          "abstract": "We introduce an implementation of an extension of Answer Set Programming\n(ASP) with language constructs from dynamic (and temporal) logic that provides\nan expressive computational framework for modeling dynamic applications.\nStarting from logical foundations, provided by dynamic and temporal equilibrium\nlogics over finite linear traces, we develop a translation of dynamic formulas\ninto temporal logic programs. This provides us with a normal form result\nestablishing the strong equivalence of formulas in different logics. Our\ntranslation relies on the introduction of auxiliary atoms to guarantee\npolynomial space complexity and to provide an embedding that is doomed to be\nimpossible over the same language. Finally, the reduction of dynamic formulas\nto temporal logic programs allows us to extend ASP with both approaches in a\nuniform way and to implement both extensions via temporal ASP solvers such as\ntelingo",
          "arxiv_id": "2002.06916v2"
        }
      ],
      "23": [
        {
          "title": "Fake News Detection and Manipulation Reasoning via Large Vision-Language Models",
          "year": "2024-07",
          "abstract": "Fake news becomes a growing threat to information security and public opinion\nwith the rapid sprawl of media manipulation. Therefore, fake news detection\nattracts widespread attention from academic community. Traditional fake news\ndetection models demonstrate remarkable performance on authenticity binary\nclassification but their ability to reason detailed faked traces based on the\nnews content remains under-explored. Furthermore, due to the lack of external\nknowledge, the performance of existing methods on fact-related news is\nquestionable, leaving their practical implementation unclear. In this paper, we\npropose a new multi-media research topic, namely manipulation reasoning.\nManipulation reasoning aims to reason manipulations based on news content. To\nsupport the research, we introduce a benchmark for fake news detection and\nmanipulation reasoning, referred to as Human-centric and Fact-related Fake News\n(HFFN). The benchmark highlights the centrality of human and the high factual\nrelevance, with detailed manual annotations. HFFN encompasses four realistic\ndomains with fake news samples generated through three manipulation approaches.\nMoreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is\npresented not only to judge on the authenticity of multi-modal news, but also\nraise analytical reasoning about potential manipulations. On the feature\nextraction level, a cross-attention mechanism is employed to extract\nfine-grained fusion features from multi-modal inputs. On the reasoning level, a\nlarge vision-language model (LVLM) serves as the backbone to facilitate\nfact-related reasoning. A two-stage training framework is deployed to better\nactivate the capacity of identification and reasoning. Comprehensive\nexperiments demonstrate that our model outperforms state-of-the-art (SOTA) fake\nnews detection models and powerful LVLMs like GPT-4 and LLaVA.",
          "arxiv_id": "2407.02042v1"
        },
        {
          "title": "It's All in the Embedding! Fake News Detection Using Document Embeddings",
          "year": "2023-04",
          "abstract": "With the current shift in the mass media landscape from journalistic rigor to\nsocial media, personalized social media is becoming the new norm. Although the\ndigitalization progress of the media brings many advantages, it also increases\nthe risk of spreading disinformation, misinformation, and malformation through\nthe use of fake news. The emergence of this harmful phenomenon has managed to\npolarize society and manipulate public opinion on particular topics, e.g.,\nelections, vaccinations, etc. Such information propagated on social media can\ndistort public perceptions and generate social unrest while lacking the rigor\nof traditional journalism. Natural Language Processing and Machine Learning\ntechniques are essential for developing efficient tools that can detect fake\nnews. Models that use the context of textual data are essential for resolving\nthe fake news detection problem, as they manage to encode linguistic features\nwithin the vector representation of words. In this paper, we propose a new\napproach that uses document embeddings to build multiple models that accurately\nlabel news articles as reliable or fake. We also present a benchmark on\ndifferent architectures that detect fake news using binary or multi-labeled\nclassification. We evaluated the models on five large news corpora using\naccuracy, precision, and recall. We obtained better results than more complex\nstate-of-the-art Deep Neural Network models. We observe that the most important\nfactor for obtaining high accuracy is the document encoding, not the\nclassification model's complexity.",
          "arxiv_id": "2304.07781v1"
        },
        {
          "title": "CrediRAG: Network-Augmented Credibility-Based Retrieval for Misinformation Detection in Reddit",
          "year": "2024-10",
          "abstract": "Fake news threatens democracy and exacerbates the polarization and divisions\nin society; therefore, accurately detecting online misinformation is the\nfoundation of addressing this issue. We present CrediRAG, the first fake news\ndetection model that combines language models with access to a rich external\npolitical knowledge base with a dense social network to detect fake news across\nsocial media at scale. CrediRAG uses a news retriever to initially assign a\nmisinformation score to each post based on the source credibility of similar\nnews articles to the post title content. CrediRAG then improves the initial\nretrieval estimations through a novel weighted post-to-post network connected\nbased on shared commenters and weighted by the average stance of all shared\ncommenters across every pair of posts. We achieve 11% increase in the F1-score\nin detecting misinformative posts over state-of-the-art methods. Extensive\nexperiments conducted on curated real-world Reddit data of over 200,000 posts\ndemonstrate the superior performance of CrediRAG on existing baselines. Thus,\nour approach offers a more accurate and scalable solution to combat the spread\nof fake news across social media platforms.",
          "arxiv_id": "2410.12061v2"
        }
      ],
      "24": [
        {
          "title": "Non-stationary BERT: Exploring Augmented IMU Data For Robust Human Activity Recognition",
          "year": "2024-09",
          "abstract": "Human Activity Recognition (HAR) has gained great attention from researchers\ndue to the popularity of mobile devices and the need to observe users' daily\nactivity data for better human-computer interaction. In this work, we collect a\nhuman activity recognition dataset called OPPOHAR consisting of phone IMU data.\nTo facilitate the employment of HAR system in mobile phone and to achieve\nuser-specific activity recognition, we propose a novel light-weight network\ncalled Non-stationary BERT with a two-stage training method. We also propose a\nsimple yet effective data augmentation method to explore the deeper\nrelationship between the accelerator and gyroscope data from the IMU. The\nnetwork achieves the state-of-the-art performance testing on various activity\nrecognition datasets and the data augmentation method demonstrates its wide\napplicability.",
          "arxiv_id": "2409.16730v1"
        },
        {
          "title": "Human Activity Recognition Using Cascaded Dual Attention CNN and Bi-Directional GRU Framework",
          "year": "2022-08",
          "abstract": "Vision-based human activity recognition has emerged as one of the essential\nresearch areas in video analytics domain. Over the last decade, numerous\nadvanced deep learning algorithms have been introduced to recognize complex\nhuman actions from video streams. These deep learning algorithms have shown\nimpressive performance for the human activity recognition task. However, these\nnewly introduced methods either exclusively focus on model performance or the\neffectiveness of these models in terms of computational efficiency and\nrobustness, resulting in a biased tradeoff in their proposals to deal with\nchallenging human activity recognition problem. To overcome the limitations of\ncontemporary deep learning models for human activity recognition, this paper\npresents a computationally efficient yet generic spatial-temporal cascaded\nframework that exploits the deep discriminative spatial and temporal features\nfor human activity recognition. For efficient representation of human actions,\nwe have proposed an efficient dual attentional convolutional neural network\n(CNN) architecture that leverages a unified channel-spatial attention mechanism\nto extract human-centric salient features in video frames. The dual\nchannel-spatial attention layers together with the convolutional layers learn\nto be more attentive in the spatial receptive fields having objects over the\nnumber of feature maps. The extracted discriminative salient features are then\nforwarded to stacked bi-directional gated recurrent unit (Bi-GRU) for long-term\ntemporal modeling and recognition of human actions using both forward and\nbackward pass gradient learning. Extensive experiments are conducted, where the\nobtained results show that the proposed framework attains an improvement in\nexecution time up to 167 times in terms of frames per second as compared to\nmost of the contemporary action recognition methods.",
          "arxiv_id": "2208.05034v1"
        },
        {
          "title": "ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos",
          "year": "2024-04",
          "abstract": "Human action or activity recognition in videos is a fundamental task in\ncomputer vision with applications in surveillance and monitoring, self-driving\ncars, sports analytics, human-robot interaction and many more. Traditional\nsupervised methods require large annotated datasets for training, which are\nexpensive and time-consuming to acquire. This work proposes a novel approach\nusing Cross-Architecture Pseudo-Labeling with contrastive learning for\nsemi-supervised action recognition. Our framework leverages both labeled and\nunlabelled data to robustly learn action representations in videos, combining\npseudo-labeling with contrastive learning for effective learning from both\ntypes of samples. We introduce a novel cross-architecture approach where 3D\nConvolutional Neural Networks (3D CNNs) and video transformers (VIT) are\nutilised to capture different aspects of action representations; hence we call\nit ActNetFormer. The 3D CNNs excel at capturing spatial features and local\ndependencies in the temporal domain, while VIT excels at capturing long-range\ndependencies across frames. By integrating these complementary architectures\nwithin the ActNetFormer framework, our approach can effectively capture both\nlocal and global contextual information of an action. This comprehensive\nrepresentation learning enables the model to achieve better performance in\nsemi-supervised action recognition tasks by leveraging the strengths of each of\nthese architectures. Experimental results on standard action recognition\ndatasets demonstrate that our approach performs better than the existing\nmethods, achieving state-of-the-art performance with only a fraction of labeled\ndata. The official website of this work is available at:\nhttps://github.com/rana2149/ActNetFormer.",
          "arxiv_id": "2404.06243v1"
        }
      ],
      "25": [
        {
          "title": "Detect Faces Efficiently: A Survey and Evaluations",
          "year": "2021-12",
          "abstract": "Face detection is to search all the possible regions for faces in images and\nlocate the faces if there are any. Many applications including face\nrecognition, facial expression recognition, face tracking and head-pose\nestimation assume that both the location and the size of faces are known in the\nimage. In recent decades, researchers have created many typical and efficient\nface detectors from the Viola-Jones face detector to current CNN-based ones.\nHowever, with the tremendous increase in images and videos with variations in\nface scale, appearance, expression, occlusion and pose, traditional face\ndetectors are challenged to detect various \"in the wild\" faces. The emergence\nof deep learning techniques brought remarkable breakthroughs to face detection\nalong with the price of a considerable increase in computation. This paper\nintroduces representative deep learning-based methods and presents a deep and\nthorough analysis in terms of accuracy and efficiency. We further compare and\ndiscuss the popular and challenging datasets and their evaluation metrics. A\ncomprehensive comparison of several successful deep learning-based face\ndetectors is conducted to uncover their efficiency using two metrics: FLOPs and\nlatency. The paper can guide to choose appropriate face detectors for different\napplications and also to develop more efficient and accurate detectors.",
          "arxiv_id": "2112.01787v1"
        },
        {
          "title": "FlowFace: Semantic Flow-guided Shape-aware Face Swapping",
          "year": "2022-12",
          "abstract": "In this work, we propose a semantic flow-guided two-stage framework for\nshape-aware face swapping, namely FlowFace. Unlike most previous methods that\nfocus on transferring the source inner facial features but neglect facial\ncontours, our FlowFace can transfer both of them to a target face, thus leading\nto more realistic face swapping. Concretely, our FlowFace consists of a face\nreshaping network and a face swapping network. The face reshaping network\naddresses the shape outline differences between the source and target faces. It\nfirst estimates a semantic flow (i.e., face shape differences) between the\nsource and the target face, and then explicitly warps the target face shape\nwith the estimated semantic flow. After reshaping, the face swapping network\ngenerates inner facial features that exhibit the identity of the source face.\nWe employ a pre-trained face masked autoencoder (MAE) to extract facial\nfeatures from both the source face and the target face. In contrast to previous\nmethods that use identity embedding to preserve identity information, the\nfeatures extracted by our encoder can better capture facial appearances and\nidentity information. Then, we develop a cross-attention fusion module to\nadaptively fuse inner facial features from the source face with the target\nfacial attributes, thus leading to better identity preservation. Extensive\nquantitative and qualitative experiments on in-the-wild faces demonstrate that\nour FlowFace outperforms the state-of-the-art significantly.",
          "arxiv_id": "2212.02797v1"
        },
        {
          "title": "Face Encryption via Frequency-Restricted Identity-Agnostic Attacks",
          "year": "2023-08",
          "abstract": "Billions of people are sharing their daily live images on social media\neveryday. However, malicious collectors use deep face recognition systems to\neasily steal their biometric information (e.g., faces) from these images. Some\nstudies are being conducted to generate encrypted face photos using adversarial\nattacks by introducing imperceptible perturbations to reduce face information\nleakage. However, existing studies need stronger black-box scenario feasibility\nand more natural visual appearances, which challenge the feasibility of privacy\nprotection. To address these problems, we propose a frequency-restricted\nidentity-agnostic (FRIA) framework to encrypt face images from unauthorized\nface recognition without access to personal information. As for the weak\nblack-box scenario feasibility, we obverse that representations of the average\nfeature in multiple face recognition models are similar, thus we propose to\nutilize the average feature via the crawled dataset from the Internet as the\ntarget to guide the generation, which is also agnostic to identities of unknown\nface recognition systems; in nature, the low-frequency perturbations are more\nvisually perceptible by the human vision system. Inspired by this, we restrict\nthe perturbation in the low-frequency facial regions by discrete cosine\ntransform to achieve the visual naturalness guarantee. Extensive experiments on\nseveral face recognition models demonstrate that our FRIA outperforms other\nstate-of-the-art methods in generating more natural encrypted faces while\nattaining high black-box attack success rates of 96%. In addition, we validate\nthe efficacy of FRIA using real-world black-box commercial API, which reveals\nthe potential of FRIA in practice. Our codes can be found in\nhttps://github.com/XinDong10/FRIA.",
          "arxiv_id": "2308.05983v3"
        }
      ],
      "26": [
        {
          "title": "Fusing task-oriented and open-domain dialogues in conversational agents",
          "year": "2021-09",
          "abstract": "The goal of building intelligent dialogue systems has largely been separately\npursued under two paradigms: task-oriented dialogue (TOD) systems, which\nperform goal-oriented functions, and open-domain dialogue (ODD) systems, which\nfocus on non-goal-oriented chitchat. The two dialogue modes can potentially be\nintertwined together seamlessly in the same conversation, as easily done by a\nfriendly human assistant. Such ability is desirable in conversational agents,\nas the integration makes them more accessible and useful. Our paper addresses\nthis problem of fusing TODs and ODDs in multi-turn dialogues. Based on the\npopular TOD dataset MultiWOZ, we build a new dataset FusedChat, by rewriting\nthe existing TOD turns and adding new ODD turns. This procedure constructs\nconversation sessions containing exchanges from both dialogue modes. It\nfeatures inter-mode contextual dependency, i.e., the dialogue turns from the\ntwo modes depend on each other. Rich dependency patterns including co-reference\nand ellipsis are features. The new dataset, with 60k new human-written ODD\nturns and 5k re-written TOD turns, offers a benchmark to test a dialogue\nmodel's ability to perform inter-mode conversations. This is a more challenging\ntask since the model has to determine the appropriate dialogue mode and\ngenerate the response based on the inter-mode context. But such models would\nbetter mimic human-level conversation capabilities. We evaluate baseline models\non this task, including classification-based two-stage models and two-in-one\nfused models. We publicly release FusedChat and the baselines to propel future\nwork on inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.",
          "arxiv_id": "2109.04137v3"
        },
        {
          "title": "DSBERT:Unsupervised Dialogue Structure learning with BERT",
          "year": "2021-11",
          "abstract": "Unsupervised dialogue structure learning is an important and meaningful task\nin natural language processing. The extracted dialogue structure and process\ncan help analyze human dialogue, and play a vital role in the design and\nevaluation of dialogue systems. The traditional dialogue system requires\nexperts to manually design the dialogue structure, which is very costly. But\nthrough unsupervised dialogue structure learning, dialogue structure can be\nautomatically obtained, reducing the cost of developers constructing dialogue\nprocess. The learned dialogue structure can be used to promote the dialogue\ngeneration of the downstream task system, and improve the logic and consistency\nof the dialogue robot's reply.In this paper, we propose a Bert-based\nunsupervised dialogue structure learning algorithm DSBERT (Dialogue Structure\nBERT). Different from the previous SOTA models VRNN and SVRNN, we combine BERT\nand AutoEncoder, which can effectively combine context information. In order to\nbetter prevent the model from falling into the local optimal solution and make\nthe dialogue state distribution more uniform and reasonable, we also propose\nthree balanced loss functions that can be used for dialogue structure learning.\nExperimental results show that DSBERT can generate a dialogue structure closer\nto the real structure, can distinguish sentences with different semantics and\nmap them to different hidden states.",
          "arxiv_id": "2111.04933v1"
        },
        {
          "title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking",
          "year": "2022-08",
          "abstract": "As an essential component in task-oriented dialogue systems, dialogue state\ntracking (DST) aims to track human-machine interactions and generate state\nrepresentations for managing the dialogue. Representations of dialogue states\nare dependent on the domain ontology and the user's goals. In several\ntask-oriented dialogues with a limited scope of objectives, dialogue states can\nbe represented as a set of slot-value pairs. As the capabilities of dialogue\nsystems expand to support increasing naturalness in communication,\nincorporating dialogue act processing into dialogue model design becomes\nessential. The lack of such consideration limits the scalability of dialogue\nstate tracking models for dialogues having specific objectives and ontology. To\naddress this issue, we formulate and incorporate dialogue acts, and leverage\nrecent advances in machine reading comprehension to predict both categorical\nand non-categorical types of slots for multi-domain dialogue state tracking.\nExperimental results show that our models can improve the overall accuracy of\ndialogue state tracking on the MultiWOZ 2.1 dataset, and demonstrate that\nincorporating dialogue acts can guide dialogue state design for future\ntask-oriented dialogue systems.",
          "arxiv_id": "2208.02462v1"
        }
      ],
      "27": [
        {
          "title": "Binary structured physics-informed neural networks for solving equations with rapidly changing solutions",
          "year": "2024-01",
          "abstract": "Physics-informed neural networks (PINNs), rooted in deep learning, have\nemerged as a promising approach for solving partial differential equations\n(PDEs). By embedding the physical information described by PDEs into\nfeedforward neural networks, PINNs are trained as surrogate models to\napproximate solutions without the need for label data. Nevertheless, even\nthough PINNs have shown remarkable performance, they can face difficulties,\nespecially when dealing with equations featuring rapidly changing solutions.\nThese difficulties encompass slow convergence, susceptibility to becoming\ntrapped in local minima, and reduced solution accuracy. To address these\nissues, we propose a binary structured physics-informed neural network (BsPINN)\nframework, which employs binary structured neural network (BsNN) as the neural\nnetwork component. By leveraging a binary structure that reduces inter-neuron\nconnections compared to fully connected neural networks, BsPINNs excel in\ncapturing the local features of solutions more effectively and efficiently.\nThese features are particularly crucial for learning the rapidly changing in\nthe nature of solutions. In a series of numerical experiments solving Burgers\nequation, Euler equation, Helmholtz equation, and high-dimension Poisson\nequation, BsPINNs exhibit superior convergence speed and heightened accuracy\ncompared to PINNs. From these experiments, we discover that BsPINNs resolve the\nissues caused by increased hidden layers in PINNs resulting in over-smoothing,\nand prevent the decline in accuracy due to non-smoothness of PDEs solutions.",
          "arxiv_id": "2401.12806v2"
        },
        {
          "title": "Learning in Sinusoidal Spaces with Physics-Informed Neural Networks",
          "year": "2021-09",
          "abstract": "A physics-informed neural network (PINN) uses physics-augmented loss\nfunctions, e.g., incorporating the residual term from governing partial\ndifferential equations (PDEs), to ensure its output is consistent with\nfundamental physics laws. However, it turns out to be difficult to train an\naccurate PINN model for many problems in practice. In this paper, we present a\nnovel perspective of the merits of learning in sinusoidal spaces with PINNs. By\nanalyzing behavior at model initialization, we first show that a PINN of\nincreasing expressiveness induces an initial bias around flat output functions.\nNotably, this initial solution can be very close to satisfying many physics\nPDEs, i.e., falling into a local minimum of the PINN loss that only minimizes\nPDE residuals, while still being far from the true solution that jointly\nminimizes PDE residuals and the initial and/or boundary conditions. It is\ndifficult for gradient descent optimization to escape from such a local minimum\ntrap, often causing the training to stall. We then prove that the sinusoidal\nmapping of inputs, in an architecture we label as sf-PINN, is effective to\nincrease input gradient variability, thus avoiding being trapped in such\ndeceptive local minimum. The level of variability can be effectively modulated\nto match high-frequency patterns in the problem at hand. A key facet of this\npaper is the comprehensive empirical study that demonstrates the efficacy of\nlearning in sinusoidal spaces with PINNs for a wide range of forward and\ninverse modelling problems spanning multiple physics domains.",
          "arxiv_id": "2109.09338v2"
        },
        {
          "title": "Are Two Hidden Layers Still Enough for the Physics-Informed Neural Networks?",
          "year": "2024-12",
          "abstract": "The article discusses the development of various methods and techniques for\ninitializing and training neural networks with a single hidden layer, as well\nas training a separable physics-informed neural network consisting of neural\nnetworks with a single hidden layer to solve physical problems described by\nordinary differential equations (ODEs) and partial differential equations\n(PDEs). A method for strictly deterministic initialization of a neural network\nwith one hidden layer for solving physical problems described by an ODE is\nproposed. Modifications to existing methods for weighting the loss function are\ngiven, as well as new methods developed for training strictly\ndeterministic-initialized neural networks to solve ODEs (detaching, additional\nweighting based on the second derivative, predicted solution-based weighting,\nrelative residuals). An algorithm for physics-informed data-driven\ninitialization of a neural network with one hidden layer is proposed. A neural\nnetwork with pronounced generalizing properties is presented, whose\ngeneralizing abilities of which can be precisely controlled by adjusting\nnetwork parameters. A metric for measuring the generalization of such neural\nnetwork has been introduced. A gradient-free neuron-by-neuron fitting method\nhas been developed for adjusting the parameters of a single-hidden-layer neural\nnetwork, which does not require the use of an optimizer or solver for its\nimplementation. The proposed methods have been extended to 2D problems using\nthe separable physics-informed neural networks approach. Numerous experiments\nhave been carried out to develop the above methods and approaches. Experiments\non physical problems, such as solving various ODEs and PDEs, have demonstrated\nthat these methods for initializing and training neural networks with one or\ntwo hidden layers (SPINN) achieve competitive accuracy and, in some cases,\nstate-of-the-art results.",
          "arxiv_id": "2412.19235v1"
        }
      ],
      "28": [
        {
          "title": "Topos Causal Models",
          "year": "2025-08",
          "abstract": "We propose topos causal models (TCMs), a novel class of causal models that\nexploit the key properties of a topos category: they are (co)complete, meaning\nall (co)limits exist, they admit a subobject classifier, and allow exponential\nobjects. The main goal of this paper is to show that these properties are\ncentral to many applications in causal inference. For example, subobject\nclassifiers allow a categorical formulation of causal intervention, which\ncreates sub-models. Limits and colimits allow causal diagrams of arbitrary\ncomplexity to be ``solved\", using a novel interpretation of causal\napproximation. Exponential objects enable reasoning about equivalence classes\nof operations on causal models, such as covered edge reversal and causal\nhomotopy. Analogous to structural causal models (SCMs), TCMs are defined by a\ncollection of functions, each defining a ``local autonomous\" causal mechanism\nthat assemble to induce a unique global function from exogenous to endogenous\nvariables. Since the category of TCMs is (co)complete, which we prove in this\npaper, every causal diagram has a ``solution\" in the form of a (co)limit: this\nimplies that any arbitrary causal model can be ``approximated\" by some global\nfunction with respect to the morphisms going into or out of the diagram.\nNatural transformations are crucial in measuring the quality of approximation.\nIn addition, we show that causal interventions are modeled by subobject\nclassifiers: any sub-model is defined by a monic arrow into its parent model.\nExponential objects permit reasoning about entire classes of causal\nequivalences and interventions. Finally, as TCMs form a topos, they admit an\ninternal logic defined as a Mitchell-Benabou language with an associated\nKripke-Joyal semantics. We show how to reason about causal models in TCMs using\nthis internal logic.",
          "arxiv_id": "2508.08295v1"
        },
        {
          "title": "Causal Bias Quantification for Continuous Treatments",
          "year": "2021-06",
          "abstract": "We extend the definition of the marginal causal effect to the continuous\ntreatment setting and develop a novel characterization of causal bias in the\nframework of structural causal models. We prove that our derived bias\nexpression is zero if, and only if, the causal effect is identifiable via\ncovariate adjustment. We show that under some restrictions on the structural\nequations, the causal bias can be estimated efficiently and allows for causal\nregularization of predictive probabilistic models. We demonstrate the\neffectiveness of our method for causal bias quantification in various settings\nwhere (not) controlling for certain covariates would introduce causal bias.",
          "arxiv_id": "2106.09762v2"
        },
        {
          "title": "Active Bayesian Causal Inference",
          "year": "2022-06",
          "abstract": "Causal discovery and causal reasoning are classically treated as separate and\nconsecutive tasks: one first infers the causal graph, and then uses it to\nestimate causal effects of interventions. However, such a two-stage approach is\nuneconomical, especially in terms of actively collected interventional data,\nsince the causal query of interest may not require a fully-specified causal\nmodel. From a Bayesian perspective, it is also unnatural, since a causal query\n(e.g., the causal graph or some causal effect) can be viewed as a latent\nquantity subject to posterior inference -- other unobserved quantities that are\nnot of direct interest (e.g., the full causal model) ought to be marginalized\nout in this process and contribute to our epistemic uncertainty. In this work,\nwe propose Active Bayesian Causal Inference (ABCI), a fully-Bayesian active\nlearning framework for integrated causal discovery and reasoning, which jointly\ninfers a posterior over causal models and queries of interest. In our approach\nto ABCI, we focus on the class of causally-sufficient, nonlinear additive noise\nmodels, which we model using Gaussian processes. We sequentially design\nexperiments that are maximally informative about our target causal query,\ncollect the corresponding interventional data, and update our beliefs to choose\nthe next experiment. Through simulations, we demonstrate that our approach is\nmore data-efficient than several baselines that only focus on learning the full\ncausal graph. This allows us to accurately learn downstream causal queries from\nfewer samples while providing well-calibrated uncertainty estimates for the\nquantities of interest.",
          "arxiv_id": "2206.02063v2"
        }
      ],
      "29": [
        {
          "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
          "year": "2023-12",
          "abstract": "Diffusion models have achieved state-of-the-art results on many modalities\nincluding images, speech, and video. However, existing models are not tailored\nto support remote sensing data, which is widely used in important applications\nincluding environmental monitoring and crop-yield prediction. Satellite images\nare significantly different from natural images -- they can be multi-spectral,\nirregularly sampled across time -- and existing diffusion models trained on\nimages from the Web do not support them. Furthermore, remote sensing data is\ninherently spatio-temporal, requiring conditional generation tasks not\nsupported by traditional methods based on captions or images. In this paper, we\npresent DiffusionSat, to date the largest generative foundation model trained\non a collection of publicly available large, high-resolution remote sensing\ndatasets. As text-based captions are sparsely available for satellite images,\nwe incorporate the associated metadata such as geolocation as conditioning\ninformation. Our method produces realistic samples and can be used to solve\nmultiple generative tasks including temporal generation, superresolution given\nmulti-spectral inputs and in-painting. Our method outperforms previous\nstate-of-the-art methods for satellite image generation and is the first\nlarge-scale generative foundation model for satellite imagery. The project\nwebsite can be found here: https://samar-khanna.github.io/DiffusionSat/",
          "arxiv_id": "2312.03606v2"
        },
        {
          "title": "DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation",
          "year": "2023-05",
          "abstract": "Image segmentation aims to partition an image according to the objects in the\nscene and is a fundamental step in analysing very high spatial-resolution (VHR)\nremote sensing imagery. Current methods struggle to effectively consider land\nobjects with diverse shapes and sizes. Additionally, the determination of\nsegmentation scale parameters frequently adheres to a static and empirical\ndoctrine, posing limitations on the segmentation of large-scale remote sensing\nimages and yielding algorithms with limited interpretability. To address the\nabove challenges, we propose a deep-learning-based region merging method dubbed\nDeepMerge to handle the segmentation of complete objects in large VHR images by\nintegrating deep learning and region adjacency graph (RAG). This is the first\nmethod to use deep learning to learn the similarity and merge similar adjacent\nsuper-pixels in RAG. We propose a modified binary tree sampling method to\ngenerate shift-scale data, serving as inputs for transformer-based deep\nlearning networks, a shift-scale attention with 3-Dimension relative position\nembedding to learn features across scales, and an embedding to fuse learned\nfeatures with hand-crafted features. DeepMerge can achieve high segmentation\naccuracy in a supervised manner from large-scale remotely sensed images and\nprovides an interpretable optimal scale parameter, which is validated using a\nremote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The\nexperimental results show that DeepMerge achieves the highest F value (0.9550)\nand the lowest total error TE (0.0895), correctly segmenting objects of\ndifferent sizes and outperforming all competing segmentation methods.",
          "arxiv_id": "2305.19787v2"
        },
        {
          "title": "Remote Sensing Image Super-resolution and Object Detection: Benchmark and State of the Art",
          "year": "2021-11",
          "abstract": "For the past two decades, there have been significant efforts to develop\nmethods for object detection in Remote Sensing (RS) images. In most cases, the\ndatasets for small object detection in remote sensing images are inadequate.\nMany researchers used scene classification datasets for object detection, which\nhas its limitations; for example, the large-sized objects outnumber the small\nobjects in object categories. Thus, they lack diversity; this further affects\nthe detection performance of small object detectors in RS images. This paper\nreviews current datasets and object detection methods (deep learning-based) for\nremote sensing images. We also propose a large-scale, publicly available\nbenchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The\nRSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of\nvery high resolution (VHR) images with a spatial resolution of ~0.05 m. There\nare five classes with varying frequencies of labels per class. The image\npatches are extracted from satellite images, including real image distortions\nsuch as tangential scale distortion and skew distortion. We also propose a\nnovel Multi-class Cyclic super-resolution Generative adversarial network with\nResidual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark\nimage super-resolution-based object detection and compare with the existing\nstate-of-the-art methods based on image super-resolution (SR). The proposed\nMCGR achieved state-of-the-art performance for image SR with an improvement of\n1.2dB PSNR compared to the current state-of-the-art NLSN method. MCGR achieved\nbest object detection mAPs of 0.758, 0.881, 0.841, and 0.983, respectively, for\nfive-class, four-class, two-class, and single classes, respectively surpassing\nthe performance of the state-of-the-art object detectors YOLOv5, EfficientDet,\nFaster RCNN, SSD, and RetinaNet.",
          "arxiv_id": "2111.03260v1"
        }
      ],
      "30": [
        {
          "title": "Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding",
          "year": "2024-10",
          "abstract": "An adversarial example is a modified input image designed to cause a Machine\nLearning (ML) model to make a mistake; these perturbations are often invisible\nor subtle to human observers and highlight vulnerabilities in a model's ability\nto generalize from its training data. Several adversarial attacks can create\nsuch examples, each with a different perspective, effectiveness, and\nperceptibility of changes. Conversely, defending against such adversarial\nattacks improves the robustness of ML models in image processing and other\ndomains of deep learning. Most defence mechanisms require either a level of\nmodel awareness, changes to the model, or access to a comprehensive set of\nadversarial examples during training, which is impractical. Another option is\nto use an auxiliary model in a preprocessing manner without changing the\nprimary model. This study presents a practical and effective solution -- using\npredictive coding networks (PCnets) as an auxiliary step for adversarial\ndefence. By seamlessly integrating PCnets into feed-forward networks as a\npreprocessing step, we substantially bolster resilience to adversarial\nperturbations. Our experiments on MNIST and CIFAR10 demonstrate the remarkable\neffectiveness of PCnets in mitigating adversarial examples with about 82% and\n65% improvements in robustness, respectively. The PCnet, trained on a small\nsubset of the dataset, leverages its generative nature to effectively counter\nadversarial efforts, reverting perturbed images closer to their original forms.\nThis innovative approach holds promise for enhancing the security and\nreliability of neural network classifiers in the face of the escalating threat\nof adversarial attacks.",
          "arxiv_id": "2411.00222v1"
        },
        {
          "title": "Guidance Through Surrogate: Towards a Generic Diagnostic Attack",
          "year": "2022-12",
          "abstract": "Adversarial training is an effective approach to make deep neural networks\nrobust against adversarial attacks. Recently, different adversarial training\ndefenses are proposed that not only maintain a high clean accuracy but also\nshow significant robustness against popular and well studied adversarial\nattacks such as PGD. High adversarial robustness can also arise if an attack\nfails to find adversarial gradient directions, a phenomenon known as `gradient\nmasking'. In this work, we analyse the effect of label smoothing on adversarial\ntraining as one of the potential causes of gradient masking. We then develop a\nguided mechanism to avoid local minima during attack optimization, leading to a\nnovel attack dubbed Guided Projected Gradient Attack (G-PGA). Our attack\napproach is based on a `match and deceive' loss that finds optimal adversarial\ndirections through guidance from a surrogate model. Our modified attack does\nnot require random restarts, large number of attack iterations or search for an\noptimal step-size. Furthermore, our proposed G-PGA is generic, thus it can be\ncombined with an ensemble attack strategy as we demonstrate for the case of\nAuto-Attack, leading to efficiency and convergence speed improvements. More\nthan an effective attack, G-PGA can be used as a diagnostic tool to reveal\nelusive robustness due to gradient masking in adversarial defenses.",
          "arxiv_id": "2212.14875v1"
        },
        {
          "title": "Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing",
          "year": "2023-03",
          "abstract": "Deep neural networks can be easily fooled into making incorrect predictions\nthrough corruption of the input by adversarial perturbations:\nhuman-imperceptible artificial noise. So far adversarial training has been the\nmost successful defense against such adversarial attacks. This work focuses on\nimproving adversarial training to boost adversarial robustness. We first\nanalyze, from an instance-wise perspective, how adversarial vulnerability\nevolves during adversarial training. We find that during training an overall\nreduction of adversarial loss is achieved by sacrificing a considerable\nproportion of training samples to be more vulnerable to adversarial attack,\nwhich results in an uneven distribution of adversarial vulnerability among\ndata. Such \"uneven vulnerability\", is prevalent across several popular robust\ntraining methods and, more importantly, relates to overfitting in adversarial\ntraining. Motivated by this observation, we propose a new adversarial training\nmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It\njointly smooths both input and weight loss landscapes in an adaptive,\ninstance-specific, way to enhance robustness more for those samples with higher\nadversarial vulnerability. Extensive experiments demonstrate the superiority of\nour method over existing defense methods. Noticeably, our method, when combined\nwith the latest data augmentation and semi-supervised learning techniques,\nachieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrained\nattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and\n61.55% for Wide ResNet28-10 with extra data. Code is available at\nhttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.",
          "arxiv_id": "2303.14077v2"
        }
      ],
      "31": [
        {
          "title": "Back-translation for Large-Scale Multilingual Machine Translation",
          "year": "2021-09",
          "abstract": "This paper illustrates our approach to the shared task on large-scale\nmultilingual machine translation in the sixth conference on machine translation\n(WMT-21). This work aims to build a single multilingual translation system with\na hypothesis that a universal cross-language representation leads to better\nmultilingual translation performance. We extend the exploration of different\nback-translation methods from bilingual translation to multilingual\ntranslation. Better performance is obtained by the constrained sampling method,\nwhich is different from the finding of the bilingual translation. Besides, we\nalso explore the effect of vocabularies and the amount of synthetic data.\nSurprisingly, the smaller size of vocabularies perform better, and the\nextensive monolingual English data offers a modest improvement. We submitted to\nboth the small tasks and achieved the second place.",
          "arxiv_id": "2109.08712v1"
        },
        {
          "title": "HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation",
          "year": "2022-07",
          "abstract": "Multilingual neural machine translation (MNMT) trained in multiple language\npairs has attracted considerable attention due to fewer model parameters and\nlower training costs by sharing knowledge among multiple languages.\nNonetheless, multilingual training is plagued by language interference\ndegeneration in shared parameters because of the negative interference among\ndifferent translation directions, especially on high-resource languages. In\nthis paper, we propose the multilingual translation model with the\nhigh-resource language-specific training (HLT-MT) to alleviate the negative\ninterference, which adopts the two-stage training with the language-specific\nselection mechanism. Specifically, we first train the multilingual model only\nwith the high-resource pairs and select the language-specific modules at the\ntop of the decoder to enhance the translation quality of high-resource\ndirections. Next, the model is further trained on all available corpora to\ntransfer knowledge from high-resource languages (HRLs) to low-resource\nlanguages (LRLs). Experimental results show that HLT-MT outperforms various\nstrong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic\nexperiments validate the effectiveness of our method in mitigating the negative\ninterference in multilingual training.",
          "arxiv_id": "2207.04906v2"
        },
        {
          "title": "Extended Parallel Corpus for Amharic-English Machine Translation",
          "year": "2021-04",
          "abstract": "This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be helpful for machine\ntranslation of a low-resource language, Amharic. We freely released the corpus\nfor research purposes. Furthermore, we developed baseline statistical and\nneural machine translation systems; we trained statistical and neural machine\ntranslation models using the corpus. In the experiments, we also used a large\nmonolingual corpus for the language model of statistical machine translation\nand back-translation of neural machine translation. In the automatic\nevaluation, neural machine translation models outperform statistical machine\ntranslation models by approximately six to seven Bilingual Evaluation\nUnderstudy (BLEU) points. Besides, among the neural machine translation models,\nthe subword models outperform the word-based models by three to four BLEU\npoints. Moreover, two other relevant automatic evaluation metrics, Translation\nEdit Rate on Character Level and Better Evaluation as Ranking, reflect\ncorresponding differences among the trained models.",
          "arxiv_id": "2104.03543v3"
        }
      ],
      "32": [
        {
          "title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System",
          "year": "2024-02",
          "abstract": "The booming success of LLMs initiates rapid development in LLM agents. Though\nthe foundation of an LLM agent is the generative model, it is critical to\ndevise the optimal reasoning strategies and agent architectures. Accordingly,\nLLM agent research advances from the simple chain-of-thought prompting to more\ncomplex ReAct and Reflection reasoning strategy; agent architecture also\nevolves from single agent generation to multi-agent conversation, as well as\nmulti-LLM multi-agent group chat. However, with the existing intricate\nframeworks and libraries, creating and evaluating new reasoning strategies and\nagent architectures has become a complex challenge, which hinders research\ninvestigation into LLM agents. Thus, we open-source a new AI agent library,\nAgentLite, which simplifies this process by offering a lightweight,\nuser-friendly platform for innovating LLM agent reasoning, architectures, and\napplications with ease. AgentLite is a task-oriented framework designed to\nenhance the ability of agents to break down tasks and facilitate the\ndevelopment of multi-agent systems. Furthermore, we introduce multiple\npractical applications developed with AgentLite to demonstrate its convenience\nand flexibility. Get started now at:\n\\url{https://github.com/SalesforceAIResearch/AgentLite}.",
          "arxiv_id": "2402.15538v1"
        },
        {
          "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
          "year": "2023-09",
          "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.",
          "arxiv_id": "2309.07864v3"
        },
        {
          "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
          "year": "2024-10",
          "abstract": "Autonomy via agents using large language models (LLMs) for personalized,\nstandardized tasks boosts human efficiency. Automating web tasks (like booking\nhotels within a budget) is increasingly sought after. Fulfilling practical\nneeds, the web agent also serves as an important proof-of-concept example for\nvarious agent grounding scenarios, with its success promising advancements in\nmany future applications. Prior research often handcrafts web agent strategies\n(e.g., prompting templates, multi-agent systems, search methods, etc.) and the\ncorresponding in-context examples, which may not generalize well across all\nreal-world scenarios. On the other hand, there has been limited study on the\nmisalignment between a web agent's observation/action representation and the\npre-training data of the LLM it's based on. This discrepancy is especially\nnotable when LLMs are primarily trained for language completion rather than\ntasks involving embodied navigation actions and symbolic web elements. Our\nstudy enhances an LLM-based web agent by simply refining its observation and\naction space to better align with the LLM's capabilities. This approach enables\nour base agent to significantly outperform previous methods on a wide variety\nof web tasks. Specifically, on WebArena, a benchmark featuring general-purpose\nweb interaction tasks, our agent AgentOccam surpasses the previous\nstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute\npoints respectively, and boosts the success rate by 26.6 points (+161%) over\nsimilar plain web agents with its observation and action space alignment. We\nachieve this without using in-context examples, new agent roles, online\nfeedback or search strategies. AgentOccam's simple design highlights LLMs'\nimpressive zero-shot performance on web tasks, and underlines the critical role\nof carefully tuning observation and action spaces for LLM-based agents.",
          "arxiv_id": "2410.13825v2"
        }
      ],
      "33": [
        {
          "title": "PROB: Probabilistic Objectness for Open World Object Detection",
          "year": "2022-12",
          "abstract": "Open World Object Detection (OWOD) is a new and challenging computer vision\ntask that bridges the gap between classic object detection (OD) benchmarks and\nobject detection in the real world. In addition to detecting and classifying\nseen/labeled objects, OWOD algorithms are expected to detect novel/unknown\nobjects - which can be classified and incrementally learned. In standard OD,\nobject proposals not overlapping with a labeled object are automatically\nclassified as background. Therefore, simply applying OD methods to OWOD fails\nas unknown objects would be predicted as background. The challenge of detecting\nunknown objects stems from the lack of supervision in distinguishing unknown\nobjects and background object proposals. Previous OWOD methods have attempted\nto overcome this issue by generating supervision using pseudo-labeling -\nhowever, unknown object detection has remained low. Probabilistic/generative\nmodels may provide a solution for this challenge. Herein, we introduce a novel\nprobabilistic framework for objectness estimation, where we alternate between\nprobability distribution estimation and objectness likelihood maximization of\nknown objects in the embedded feature space - ultimately allowing us to\nestimate the objectness probability of different proposals. The resulting\nProbabilistic Objectness transformer-based open-world detector, PROB,\nintegrates our framework into traditional object detection models, adapting\nthem for the open-world setting. Comprehensive experiments on OWOD benchmarks\nshow that PROB outperforms all existing OWOD methods in both unknown object\ndetection ($\\sim 2\\times$ unknown recall) and known object detection ($\\sim\n10\\%$ mAP). Our code will be made available upon publication at\nhttps://github.com/orrzohar/PROB.",
          "arxiv_id": "2212.01424v1"
        },
        {
          "title": "TransFGU: A Top-down Approach to Fine-Grained Unsupervised Semantic Segmentation",
          "year": "2021-12",
          "abstract": "Unsupervised semantic segmentation aims to obtain high-level semantic\nrepresentation on low-level visual features without manual annotations. Most\nexisting methods are bottom-up approaches that try to group pixels into regions\nbased on their visual cues or certain predefined rules. As a result, it is\ndifficult for these bottom-up approaches to generate fine-grained semantic\nsegmentation when coming to complicated scenes with multiple objects and some\nobjects sharing similar visual appearance. In contrast, we propose the first\ntop-down unsupervised semantic segmentation framework for fine-grained\nsegmentation in extremely complicated scenarios. Specifically, we first obtain\nrich high-level structured semantic concept information from large-scale vision\ndata in a self-supervised learning manner, and use such information as a prior\nto discover potential semantic categories presented in target datasets.\nSecondly, the discovered high-level semantic categories are mapped to low-level\npixel features by calculating the class activate map (CAM) with respect to\ncertain discovered semantic representation. Lastly, the obtained CAMs serve as\npseudo labels to train the segmentation module and produce the final semantic\nsegmentation. Experimental results on multiple semantic segmentation benchmarks\nshow that our top-down unsupervised segmentation is robust to both\nobject-centric and scene-centric datasets under different semantic granularity\nlevels, and outperforms all the current state-of-the-art bottom-up methods. Our\ncode is available at \\url{https://github.com/damo-cv/TransFGU}.",
          "arxiv_id": "2112.01515v2"
        },
        {
          "title": "LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation via Category-wise Attentive Classifier",
          "year": "2025-01",
          "abstract": "Scaling up the vocabulary of semantic segmentation models is extremely\nchallenging because annotating large-scale mask labels is labour-intensive and\ntime-consuming. Recently, language-guided segmentation models have been\nproposed to address this challenge. However, their performance drops\nsignificantly when applied to out-of-distribution categories. In this paper, we\npropose a new large vocabulary semantic segmentation framework, called LarvSeg.\nDifferent from previous works, LarvSeg leverages image classification data to\nscale the vocabulary of semantic segmentation models as large-vocabulary\nclassification datasets usually contain balanced categories and are much easier\nto obtain. However, for classification tasks, the category is image-level,\nwhile for segmentation we need to predict the label at pixel level. To address\nthis issue, we first propose a general baseline framework to incorporate\nimage-level supervision into the training process of a pixel-level segmentation\nmodel, making the trained network perform semantic segmentation on newly\nintroduced categories in the classification data. We then observe that a model\ntrained on segmentation data can group pixel features of categories beyond the\ntraining vocabulary. Inspired by this finding, we design a category-wise\nattentive classifier to apply supervision to the precise regions of\ncorresponding categories to improve the model performance. Extensive\nexperiments demonstrate that LarvSeg significantly improves the large\nvocabulary semantic segmentation performance, especially in the categories\nwithout mask labels. For the first time, we provide a 21K-category semantic\nsegmentation model with the help of ImageNet21K. The code is available at\nhttps://github.com/HaojunYu1998/large_voc_seg.",
          "arxiv_id": "2501.06862v1"
        }
      ],
      "34": [
        {
          "title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction",
          "year": "2020-04",
          "abstract": "The task of link prediction for knowledge graphs is to predict missing\nrelationships between entities. Knowledge graph embedding, which aims to\nrepresent entities and relations of a knowledge graph as low dimensional\nvectors in a continuous vector space, has achieved promising predictive\nperformance. If an embedding model can cover different types of connectivity\npatterns and mapping properties of relations as many as possible, it will\npotentially bring more benefits for link prediction tasks. In this paper, we\npropose a novel embedding model, namely LineaRE, which is capable of modeling\nfour connectivity patterns (i.e., symmetry, antisymmetry, inversion, and\ncomposition) and four mapping properties (i.e., one-to-one, one-to-many,\nmany-to-one, and many-to-many) of relations. Specifically, we regard knowledge\ngraph embedding as a simple linear regression task, where a relation is modeled\nas a linear function of two low-dimensional vector-presented entities with two\nweight vectors and a bias vector. Since the vectors are defined in a real\nnumber space and the scoring function of the model is linear, our model is\nsimple and scalable to large knowledge graphs. Experimental results on multiple\nwidely used real-world datasets show that the proposed LineaRE model\nsignificantly outperforms existing state-of-the-art models for link prediction\ntasks.",
          "arxiv_id": "2004.10037v2"
        },
        {
          "title": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs",
          "year": "2023-05",
          "abstract": "Inductive knowledge graph completion has been considered as the task of\npredicting missing triplets between new entities that are not observed during\ntraining. While most inductive knowledge graph completion methods assume that\nall entities can be new, they do not allow new relations to appear at inference\ntime. This restriction prohibits the existing methods from appropriately\nhandling real-world knowledge graphs where new entities accompany new\nrelations. In this paper, we propose an INductive knowledge GRAph eMbedding\nmethod, InGram, that can generate embeddings of new relations as well as new\nentities at inference time. Given a knowledge graph, we define a relation graph\nas a weighted graph consisting of relations and the affinity weights between\nthem. Based on the relation graph and the original knowledge graph, InGram\nlearns how to aggregate neighboring embeddings to generate relation and entity\nembeddings using an attention mechanism. Experimental results show that InGram\noutperforms 14 different state-of-the-art methods on varied inductive learning\nscenarios.",
          "arxiv_id": "2305.19987v3"
        },
        {
          "title": "Universal Knowledge Graph Embeddings",
          "year": "2023-10",
          "abstract": "A variety of knowledge graph embedding approaches have been developed. Most\nof them obtain embeddings by learning the structure of the knowledge graph\nwithin a link prediction setting. As a result, the embeddings reflect only the\nstructure of a single knowledge graph, and embeddings for different knowledge\ngraphs are not aligned, e.g., they cannot be used to find similar entities\nacross knowledge graphs via nearest neighbor search. However, knowledge graph\nembedding applications such as entity disambiguation require a more global\nrepresentation, i.e., a representation that is valid across multiple sources.\nWe propose to learn universal knowledge graph embeddings from large-scale\ninterlinked knowledge sources. To this end, we fuse large knowledge graphs\nbased on the owl:sameAs relation such that every entity is represented by a\nunique identity. We instantiate our idea by computing universal embeddings\nbased on DBpedia and Wikidata yielding embeddings for about 180 million\nentities, 15 thousand relations, and 1.2 billion triples. We believe our\ncomputed embeddings will support the emerging field of graph foundation models.\nMoreover, we develop a convenient API to provide embeddings as a service.\nExperiments on link prediction suggest that universal knowledge graph\nembeddings encode better semantics compared to embeddings computed on a single\nknowledge graph. For reproducibility purposes, we provide our source code and\ndatasets open access.",
          "arxiv_id": "2310.14899v2"
        }
      ],
      "35": [
        {
          "title": "Classical surrogates for quantum learning models",
          "year": "2022-06",
          "abstract": "The advent of noisy intermediate-scale quantum computers has put the search\nfor possible applications to the forefront of quantum information science. One\narea where hopes for an advantage through near-term quantum computers are high\nis quantum machine learning, where variational quantum learning models based on\nparametrized quantum circuits are discussed. In this work, we introduce the\nconcept of a classical surrogate, a classical model which can be efficiently\nobtained from a trained quantum learning model and reproduces its input-output\nrelations. As inference can be performed classically, the existence of a\nclassical surrogate greatly enhances the applicability of a quantum learning\nstrategy. However, the classical surrogate also challenges possible advantages\nof quantum schemes. As it is possible to directly optimize the ansatz of the\nclassical surrogate, they create a natural benchmark the quantum model has to\noutperform. We show that large classes of well-analyzed re-uploading models\nhave a classical surrogate. We conducted numerical experiments and found that\nthese quantum models show no advantage in performance or trainability in the\nproblems we analyze. This leaves only generalization capability as possible\npoint of quantum advantage and emphasizes the dire need for a better\nunderstanding of inductive biases of quantum learning models.",
          "arxiv_id": "2206.11740v1"
        },
        {
          "title": "Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC Middleware: Applications in Quantum Simulations",
          "year": "2024-03",
          "abstract": "Achieving high-performance computation on quantum systems presents a\nformidable challenge that necessitates bridging the capabilities between\nquantum hardware and classical computing resources. This study introduces an\ninnovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,\nwhich integrates cutting-edge quantum software framework works with\nhigh-performance classical computing resources to address challenges in quantum\nsimulation for materials and condensed matter physics. At the heart of this\narchitecture is the seamless integration of VQE algorithms running on QPUs for\nefficient quantum state preparation, Tensor Network states, and QCNNs for\nclassifying quantum states on classical hardware.\n  For benchmarking quantum simulators, the QCQ architecture utilizes the\ncuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's\nLightning plugin, demonstrating up to tenfold increases in computational speed\nfor complex phase transition classification tasks compared to traditional\nCPU-based methods. This significant acceleration enables models such as the\ntransverse field Ising and XXZ systems to accurately predict phase transitions\nwith a 99.5% accuracy. The architecture's ability to distribute computation\nbetween QPUs and classical resources addresses critical bottlenecks in\nQuantum-HPC, paving the way for scalable quantum simulation.\n  The QCQ framework embodies a synergistic combination of quantum algorithms,\nmachine learning, and Quantum-HPC capabilities, enhancing its potential to\nprovide transformative insights into the behavior of quantum systems across\ndifferent scales. As quantum hardware continues to improve, this hybrid\ndistribution-aware framework will play a crucial role in realizing the full\npotential of quantum computing by seamlessly integrating distributed quantum\nresources with the state-of-the-art classical computing infrastructure.",
          "arxiv_id": "2403.05828v2"
        },
        {
          "title": "Scalable Quantum Neural Networks for Classification",
          "year": "2022-08",
          "abstract": "Many recent machine learning tasks resort to quantum computing to improve\nclassification accuracy and training efficiency by taking advantage of quantum\nmechanics, known as quantum machine learning (QML). The variational quantum\ncircuit (VQC) is frequently utilized to build a quantum neural network (QNN),\nwhich is a counterpart to the conventional neural network. Due to hardware\nlimitations, however, current quantum devices only allow one to use few qubits\nto represent data and perform simple quantum computations. The limited quantum\nresource on a single quantum device degrades the data usage and limits the\nscale of the quantum circuits, preventing quantum advantage to some extent. To\nalleviate this constraint, we propose an approach to implementing a scalable\nquantum neural network (SQNN) by utilizing the quantum resource of multiple\nsmall-size quantum devices cooperatively. In an SQNN system, several quantum\ndevices are used as quantum feature extractors, extracting local features from\nan input instance in parallel, and a quantum device works as a quantum\npredictor, performing prediction over the local features collected through\nclassical communication channels. The quantum feature extractors in the SQNN\nsystem are independent of each other, so one can flexibly use quantum devices\nof varying sizes, with larger quantum devices extracting more local features.\nEspecially, the SQNN can be performed on a single quantum device in a modular\nfashion. Our work is exploratory and carried out on a quantum system simulator\nusing the TensorFlow Quantum library. The evaluation conducts a binary\nclassification on the MNIST dataset. It shows that the SQNN model achieves a\ncomparable classification accuracy to a regular QNN model of the same scale.\nFurthermore, it demonstrates that the SQNN model with more quantum resources\ncan significantly improve classification accuracy.",
          "arxiv_id": "2208.07719v1"
        }
      ],
      "36": [
        {
          "title": "Shortcomings of Counterfactual Fairness and a Proposed Modification",
          "year": "2020-11",
          "abstract": "In this paper, I argue that counterfactual fairness does not constitute a\nnecessary condition for an algorithm to be fair, and subsequently suggest how\nthe constraint can be modified in order to remedy this shortcoming. To this\nend, I discuss a hypothetical scenario in which counterfactual fairness and an\nintuitive judgment of fairness come apart. Then, I turn to the question how the\nconcept of discrimination can be explicated in order to examine the\nshortcomings of counterfactual fairness as a necessary condition of algorithmic\nfairness in more detail. I then incorporate the insights of this analysis into\na novel fairness constraint, causal relevance fairness, which is a modification\nof the counterfactual fairness constraint that seems to circumvent its\nshortcomings.",
          "arxiv_id": "2011.07312v1"
        },
        {
          "title": "Metrics and methods for a systematic comparison of fairness-aware machine learning algorithms",
          "year": "2020-10",
          "abstract": "Understanding and removing bias from the decisions made by machine learning\nmodels is essential to avoid discrimination against unprivileged groups.\nDespite recent progress in algorithmic fairness, there is still no clear answer\nas to which bias-mitigation approaches are most effective. Evaluation\nstrategies are typically use-case specific, rely on data with unclear bias, and\nemploy a fixed policy to convert model outputs to decision outcomes. To address\nthese problems, we performed a systematic comparison of a number of popular\nfairness algorithms applicable to supervised classification. Our study is the\nmost comprehensive of its kind. It utilizes three real and four synthetic\ndatasets, and two different ways of converting model outputs to decisions. It\nconsiders fairness, predictive-performance, calibration quality, and speed of\n28 different modelling pipelines, corresponding to both fairness-unaware and\nfairness-aware algorithms. We found that fairness-unaware algorithms typically\nfail to produce adequately fair models and that the simplest algorithms are not\nnecessarily the fairest ones. We also found that fairness-aware algorithms can\ninduce fairness without material drops in predictive power. Finally, we found\nthat dataset idiosyncracies (e.g., degree of intrinsic unfairness, nature of\ncorrelations) do affect the performance of fairness-aware approaches. Our\nresults allow the practitioner to narrow down the approach(es) they would like\nto adopt without having to know in advance their fairness requirements.",
          "arxiv_id": "2010.03986v1"
        },
        {
          "title": "Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness",
          "year": "2023-10",
          "abstract": "Counterfactual fairness requires that a person would have been classified in\nthe same way by an AI or other algorithmic system if they had a different\nprotected class, such as a different race or gender. This is an intuitive\nstandard, as reflected in the U.S. legal system, but its use is limited because\ncounterfactuals cannot be directly observed in real-world data. On the other\nhand, group fairness metrics (e.g., demographic parity or equalized odds) are\nless intuitive but more readily observed. In this paper, we use $\\textit{causal\ncontext}$ to bridge the gaps between counterfactual fairness, robust\nprediction, and group fairness. First, we motivate counterfactual fairness by\nshowing that there is not necessarily a fundamental trade-off between fairness\nand accuracy because, under plausible conditions, the counterfactually fair\npredictor is in fact accuracy-optimal in an unbiased target distribution.\nSecond, we develop a correspondence between the causal graph of the\ndata-generating process and which, if any, group fairness metrics are\nequivalent to counterfactual fairness. Third, we show that in three common\nfairness contexts$\\unicode{x2013}$measurement error, selection on label, and\nselection on predictors$\\unicode{x2013}$counterfactual fairness is equivalent\nto demographic parity, equalized odds, and calibration, respectively.\nCounterfactual fairness can sometimes be tested by measuring relatively simple\ngroup fairness metrics.",
          "arxiv_id": "2310.19691v1"
        }
      ],
      "37": [
        {
          "title": "Applications of blockchain and artificial intelligence technologies for enabling prosumers in smart grids: A review",
          "year": "2022-02",
          "abstract": "Governments' net zero emission target aims at increasing the share of\nrenewable energy sources as well as influencing the behaviours of consumers to\nsupport the cost-effective balancing of energy supply and demand. These will be\nachieved by the advanced information and control infrastructures of smart grids\nwhich allow the interoperability among various stakeholders. Under this\ncircumstance, increasing number of consumers produce, store, and consume\nenergy, giving them a new role of prosumers. The integration of prosumers and\naccommodation of incurred bidirectional flows of energy and information rely on\ntwo key factors: flexible structures of energy markets and intelligent\noperations of power systems. The blockchain and artificial intelligence (AI)\nare innovative technologies to fulfil these two factors, by which the\nblockchain provides decentralised trading platforms for energy markets and the\nAI supports the optimal operational control of power systems. This paper\nattempts to address how to incorporate the blockchain and AI in the smart grids\nfor facilitating prosumers to participate in energy markets. To achieve this\nobjective, first, this paper reviews how policy designs price carbon emissions\ncaused by the fossil-fuel based generation so as to facilitate the integration\nof prosumers with renewable energy sources. Second, the potential structures of\nenergy markets with the support of the blockchain technologies are discussed.\nLast, how to apply the AI for enhancing the state monitoring and decision\nmaking during the operations of power systems is introduced.",
          "arxiv_id": "2202.10098v1"
        },
        {
          "title": "Load and Renewable Energy Forecasting Using Deep Learning for Grid Stability",
          "year": "2025-01",
          "abstract": "As the energy landscape changes quickly, grid operators face several\nchallenges, especially when integrating renewable energy sources with the grid.\nThe most important challenge is to balance supply and demand because the solar\nand wind energy are highly unpredictable. When dealing with such uncertainty,\ntrustworthy short-term load and renewable energy forecasting can help stabilize\nthe grid, maximize energy storage, and guarantee the effective use of renewable\nresources. Physical models and statistical techniques were the previous\napproaches employed for this kind of forecasting tasks. In forecasting\nrenewable energy, machine learning and deep learning techniques have recently\ndemonstrated encouraging results. More specifically, the deep learning\ntechniques like CNN and LSTM and the conventional machine learning techniques\nlike regression that are mostly utilized for load and renewable energy\nforecasting tasks. In this article, we will focus mainly on CNN and LSTM-based\nforecasting methods.",
          "arxiv_id": "2501.13412v1"
        },
        {
          "title": "Combating Uncertainties in Wind and Distributed PV Energy Sources Using Integrated Reinforcement Learning and Time-Series Forecasting",
          "year": "2023-02",
          "abstract": "Renewable energy sources, such as wind and solar power, are increasingly\nbeing integrated into smart grid systems. However, when compared to traditional\nenergy resources, the unpredictability of renewable energy generation poses\nsignificant challenges for both electricity providers and utility companies.\nFurthermore, the large-scale integration of distributed energy resources (such\nas PV systems) creates new challenges for energy management in microgrids. To\ntackle these issues, we propose a novel framework with two objectives: (i)\ncombating uncertainty of renewable energy in smart grid by leveraging\ntime-series forecasting with Long-Short Term Memory (LSTM) solutions, and (ii)\nestablishing distributed and dynamic decision-making framework with multi-agent\nreinforcement learning using Deep Deterministic Policy Gradient (DDPG)\nalgorithm. The proposed framework considers both objectives concurrently to\nfully integrate them, while considering both wholesale and retail markets,\nthereby enabling efficient energy management in the presence of uncertain and\ndistributed renewable energy sources. Through extensive numerical simulations,\nwe demonstrate that the proposed solution significantly improves the profit of\nload serving entities (LSE) by providing a more accurate wind generation\nforecast. Furthermore, our results demonstrate that households with PV and\nbattery installations can increase their profits by using intelligent battery\ncharge/discharge actions determined by the DDPG agents.",
          "arxiv_id": "2302.14094v1"
        }
      ],
      "38": [
        {
          "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
          "year": "2024-06",
          "abstract": "The use of Large Language Models (LLMs) in hiring has led to legislative\nactions to protect vulnerable demographic groups. This paper presents a novel\nframework for benchmarking hierarchical gender hiring bias in Large Language\nModels (LLMs) for resume scoring, revealing significant issues of reverse\ngender hiring bias and overdebiasing. Our contributions are fourfold: Firstly,\nwe introduce a new construct grounded in labour economics, legal principles,\nand critiques of current bias benchmarks: hiring bias can be categorized into\ntwo types: Level bias (difference in the average outcomes between demographic\ncounterfactual groups) and Spread bias (difference in the variance of outcomes\nbetween demographic counterfactual groups); Level bias can be further\nsubdivided into statistical bias (i.e. changing with non-demographic content)\nand taste-based bias (i.e. consistent regardless of non-demographic content).\nSecondly, the framework includes rigorous statistical and computational hiring\nbias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio,\nPermutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring\nbiases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant\nbiases against males in at least one industry. An industry-effect regression\nreveals that the healthcare industry is the most biased against males.\nMoreover, we found that the bias performance remains invariant with resume\ncontent for eight out of ten LLMs. This indicates that the bias performance\nmeasured in this paper might apply to other resume datasets with different\nresume qualities. Fourthly, we provide a user-friendly demo and resume dataset\nto support the adoption and practical use of the framework, which can be\ngeneralized to other social traits and tasks.",
          "arxiv_id": "2406.15484v2"
        },
        {
          "title": "Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs",
          "year": "2024-10",
          "abstract": "Large Language Models (LLMs) are being adopted across a wide range of tasks,\nincluding decision-making processes in industries where bias in AI systems is a\nsignificant concern. Recent research indicates that LLMs can harbor implicit\nbiases even when they pass explicit bias evaluations. Building upon the\nframeworks of the LLM Implicit Association Test (IAT) Bias and LLM Decision\nBias, this study highlights that newer or larger language models do not\nautomatically exhibit reduced bias; in some cases, they displayed higher bias\nscores than their predecessors, such as in Meta's Llama series and OpenAI's GPT\nmodels. This suggests that increasing model complexity without deliberate bias\nmitigation strategies can unintentionally amplify existing biases. The\nvariability in bias scores within and across providers underscores the need for\nstandardized evaluation metrics and benchmarks for bias assessment. The lack of\nconsistency indicates that bias mitigation is not yet a universally prioritized\ngoal in model development, which can lead to unfair or discriminatory outcomes.\nBy broadening the detection of implicit bias, this research provides a more\ncomprehensive understanding of the biases present in advanced models and\nunderscores the critical importance of addressing these issues to ensure the\ndevelopment of fair and responsible AI systems.",
          "arxiv_id": "2410.12864v1"
        },
        {
          "title": "Public Perceptions of Gender Bias in Large Language Models: Cases of ChatGPT and Ernie",
          "year": "2023-09",
          "abstract": "Large language models are quickly gaining momentum, yet are found to\ndemonstrate gender bias in their responses. In this paper, we conducted a\ncontent analysis of social media discussions to gauge public perceptions of\ngender bias in LLMs which are trained in different cultural contexts, i.e.,\nChatGPT, a US-based LLM, or Ernie, a China-based LLM. People shared both\nobservations of gender bias in their personal use and scientific findings about\ngender bias in LLMs. A difference between the two LLMs was seen -- ChatGPT was\nmore often found to carry implicit gender bias, e.g., associating men and women\nwith different profession titles, while explicit gender bias was found in\nErnie's responses, e.g., overly promoting women's pursuit of marriage over\ncareer. Based on the findings, we reflect on the impact of culture on gender\nbias and propose governance recommendations to regulate gender bias in LLMs.",
          "arxiv_id": "2309.09120v1"
        }
      ],
      "39": [
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "year": "2023-05",
          "abstract": "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.",
          "arxiv_id": "2305.18290v3"
        },
        {
          "title": "TSO: Self-Training with Scaled Preference Optimization",
          "year": "2024-08",
          "abstract": "Enhancing the conformity of large language models (LLMs) to human preferences\nremains an ongoing research challenge. Recently, offline approaches such as\nDirect Preference Optimization (DPO) have gained prominence as attractive\noptions due to offering effective improvement in simple, efficient, and stable\nwithout interactions with reward models. However, these offline preference\noptimization methods highly rely on the quality of pairwise preference samples.\nMeanwhile, numerous iterative methods require additional training of reward\nmodels to select positive and negative samples from the model's own generated\nresponses for preference learning. Furthermore, as LLMs' capabilities advance,\nit is quite challenging to continuously construct high-quality positive and\nnegative preference instances from the model's outputs due to the lack of\ndiversity. To tackle these challenges, we propose TSO, or Self-Training with\nScaled Preference Optimization, a framework for preference optimization that\nconducts self-training preference learning without training an additional\nreward model. TSO enhances the diversity of responses by constructing a model\nmatrix and incorporating human preference responses. Furthermore, TSO\nintroduces corrections for model preference errors through human and AI\nfeedback. Finally, TSO adopts iterative and dual clip reward strategies to\nupdate the reference model and its responses, adaptively adjusting preference\ndata and balancing the optimization process. Experimental results demonstrate\nthat TSO outperforms existing mainstream methods on various alignment\nevaluation benchmarks, providing practical insight into preference data\nconstruction and model training strategies in the alignment domain.",
          "arxiv_id": "2409.02118v1"
        },
        {
          "title": "Accelerated Preference Optimization for Large Language Model Alignment",
          "year": "2024-10",
          "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntool for aligning large language models (LLMs) with human preferences. Direct\nPreference Optimization (DPO), one of the most popular approaches, formulates\nRLHF as a policy optimization problem without explicitly estimating the reward\nfunction. It overcomes the stability and efficiency issues of two-step\napproaches, which typically involve first estimating the reward function and\nthen optimizing the policy via proximal policy optimization (PPO). Since RLHF\nis essentially an optimization problem, and it is well-known that momentum\ntechniques can accelerate optimization both theoretically and empirically, a\nnatural question arises: Can RLHF be accelerated by momentum? This paper\nanswers this question in the affirmative. In detail, we first show that the\niterative preference optimization method can be viewed as a proximal point\nmethod. Based on this observation, we propose a general Accelerated Preference\nOptimization (APO) framework, which unifies many existing preference\noptimization algorithms and employs Nesterov's momentum technique to speed up\nthe alignment of LLMs. Theoretically, we demonstrate that APO can achieve a\nfaster convergence rate than the standard iterative preference optimization\nmethods, including DPO and Self-Play Preference Optimization (SPPO).\nEmpirically, we show the superiority of APO over DPO, iterative DPO, and other\nstrong baselines for RLHF on the AlpacaEval 2.0 benchmark.",
          "arxiv_id": "2410.06293v1"
        }
      ],
      "40": [
        {
          "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
          "year": "2024-04",
          "abstract": "Scientific publishing lays the foundation of science by disseminating\nresearch findings, fostering collaboration, encouraging reproducibility, and\nensuring that scientific knowledge is accessible, verifiable, and built upon\nover time. Recently, there has been immense speculation about how many people\nare using large language models (LLMs) like ChatGPT in their academic writing,\nand to what extent this tool might have an effect on global scientific\npractices. However, we lack a precise measure of the proportion of academic\nwriting substantially modified or produced by LLMs. To address this gap, we\nconduct the first systematic, large-scale analysis across 950,965 papers\npublished between January 2020 and February 2024 on the arXiv, bioRxiv, and\nNature portfolio journals, using a population-level statistical framework to\nmeasure the prevalence of LLM-modified content over time. Our statistical\nestimation operates on the corpus level and is more robust than inference on\nindividual instances. Our findings reveal a steady increase in LLM usage, with\nthe largest and fastest growth observed in Computer Science papers (up to\n17.5%). In comparison, Mathematics papers and the Nature portfolio showed the\nleast LLM modification (up to 6.3%). Moreover, at an aggregate level, our\nanalysis reveals that higher levels of LLM-modification are associated with\npapers whose first authors post preprints more frequently, papers in more\ncrowded research areas, and papers of shorter lengths. Our findings suggests\nthat LLMs are being broadly used in scientific writings.",
          "arxiv_id": "2404.01268v1"
        },
        {
          "title": "Quantifying the Benefit of Artificial Intelligence for Scientific Research",
          "year": "2023-04",
          "abstract": "The ongoing artificial intelligence (AI) revolution has the potential to\nchange almost every line of work. As AI capabilities continue to improve in\naccuracy, robustness, and reach, AI may outperform and even replace human\nexperts across many valuable tasks. Despite enormous effort devoted to\nunderstanding the impact of AI on labor and the economy and AI's recent\nsuccesses in accelerating scientific discovery and progress, we lack a\nsystematic understanding of how AI advances may benefit scientific research\nacross disciplines and fields. Here, drawing from the literature on the future\nof work and the science of science, we develop a measurement framework to\nestimate both the direct use of AI and the potential benefit of AI in\nscientific research, applying natural language processing techniques to 74.6\nmillion publications and 7.1 million patents. We find that the use of AI in\nresearch is widespread throughout the sciences, growing especially rapidly\nsince 2015, and papers that use AI exhibit a citation premium, more likely to\nbe highly cited both within and outside their disciplines. Moreover, our\nanalysis reveals considerable potential for AI to benefit numerous scientific\nfields, yet a notable disconnect exists between AI education and its research\napplications, highlighting a mismatch between the supply of AI expertise and\nits demand in research. Lastly, we examine demographic disparities in AI's\nbenefits across scientific disciplines and find that disciplines with a higher\nproportion of women or Black scientists tend to be associated with less\nbenefit, suggesting that AI's growing impact on research may further exacerbate\nexisting inequalities in science. As the connection between AI and scientific\nresearch deepens, our findings may become increasingly important, with\nimplications for the equity and sustainability of the research enterprise.",
          "arxiv_id": "2304.10578v2"
        },
        {
          "title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
          "year": "2025-04",
          "abstract": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work.",
          "arxiv_id": "2504.02767v1"
        }
      ],
      "41": [
        {
          "title": "A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges",
          "year": "2025-06",
          "abstract": "IDS aims to protect computer networks from security threats by detecting,\nnotifying, and taking appropriate action to prevent illegal access and protect\nconfidential information. As the globe becomes increasingly dependent on\ntechnology and automated processes, ensuring secured systems, applications, and\nnetworks has become one of the most significant problems of this era. The\nglobal web and digital technology have significantly accelerated the evolution\nof the modern world, necessitating the use of telecommunications and data\ntransfer platforms. Researchers are enhancing the effectiveness of IDS by\nincorporating popular datasets into machine learning algorithms. IDS, equipped\nwith machine learning classifiers, enhances security attack detection accuracy\nby identifying normal or abnormal network traffic. This paper explores the\nmethods of capturing and reviewing intrusion detection systems (IDS) and\nevaluates the challenges existing datasets face. A deluge of research on\nmachine learning (ML) and deep learning (DL) architecture-based intrusion\ndetection techniques has been conducted in the past ten years on various\ncybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017,\nand CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth\nanalysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB,\nRF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique,\nexplaining the role of the classifiers and algorithms used. A detailed tabular\nanalysis highlights the datasets used, classifiers employed, attacks detected,\nevaluation metrics, and conclusions drawn. This article offers a thorough\nreview for future IDS research.",
          "arxiv_id": "2506.02438v1"
        },
        {
          "title": "Towards Learning-automation IoT Attack Detection through Reinforcement Learning",
          "year": "2020-06",
          "abstract": "As a massive number of the Internet of Things (IoT) devices are deployed, the\nsecurity and privacy issues in IoT arouse more and more attention. The IoT\nattacks are causing tremendous loss to the IoT networks and even threatening\nhuman safety. Compared to traditional networks, IoT networks have unique\ncharacteristics, which make the attack detection more challenging. First, the\nheterogeneity of platforms, protocols, software, and hardware exposes various\nvulnerabilities. Second, in addition to the traditional high-rate attacks, the\nlow-rate attacks are also extensively used by IoT attackers to obfuscate the\nlegitimate and malicious traffic. These low-rate attacks are challenging to\ndetect and can persist in the networks. Last, the attackers are evolving to be\nmore intelligent and can dynamically change their attack strategies based on\nthe environment feedback to avoid being detected, making it more challenging\nfor the defender to discover a consistent pattern to identify the attack.\n  In order to adapt to the new characteristics in IoT attacks, we propose a\nreinforcement learning-based attack detection model that can automatically\nlearn and recognize the transformation of the attack pattern. Therefore, we can\ncontinuously detect IoT attacks with less human intervention. In this paper, we\nexplore the crucial features of IoT traffics and utilize the entropy-based\nmetrics to detect both the high-rate and low-rate IoT attacks. Afterward, we\nleverage the reinforcement learning technique to continuously adjust the attack\ndetection threshold based on the detection feedback, which optimizes the\ndetection and the false alarm rate. We conduct extensive experiments over a\nreal IoT attack dataset and demonstrate the effectiveness of our IoT attack\ndetection framework.",
          "arxiv_id": "2006.15826v1"
        },
        {
          "title": "Using EBGAN for Anomaly Intrusion Detection",
          "year": "2022-06",
          "abstract": "As an active network security protection scheme, intrusion detection system\n(IDS) undertakes the important responsibility of detecting network attacks in\nthe form of malicious network traffic. Intrusion detection technology is an\nimportant part of IDS. At present, many scholars have carried out extensive\nresearch on intrusion detection technology. However, developing an efficient\nintrusion detection method for massive network traffic data is still difficult.\nSince Generative Adversarial Networks (GANs) have powerful modeling\ncapabilities for complex high-dimensional data, they provide new ideas for\naddressing this problem. In this paper, we put forward an EBGAN-based intrusion\ndetection method, IDS-EBGAN, that classifies network records as normal traffic\nor malicious traffic. The generator in IDS-EBGAN is responsible for converting\nthe original malicious network traffic in the training set into adversarial\nmalicious examples. This is because we want to use adversarial learning to\nimprove the ability of discriminator to detect malicious traffic. At the same\ntime, the discriminator adopts Autoencoder model. During testing, IDS-EBGAN\nuses reconstruction error of discriminator to classify traffic records.",
          "arxiv_id": "2206.10400v1"
        }
      ],
      "42": [
        {
          "title": "Personalized DP-SGD using Sampling Mechanisms",
          "year": "2023-05",
          "abstract": "Personalized privacy becomes critical in deep learning for Trustworthy AI.\nWhile Differentially Private Stochastic Gradient Descent (DP-SGD) is widely\nused in deep learning methods supporting privacy, it provides the same level of\nprivacy to all individuals, which may lead to overprotection and low utility.\nIn practice, different users may require different privacy levels, and the\nmodel can be improved by using more information about the users with lower\nprivacy requirements. There are also recent works on differential privacy of\nindividuals when using DP-SGD, but they are mostly about individual privacy\naccounting and do not focus on satisfying different privacy levels. We thus\nextend DP-SGD to support a recent privacy notion called\n($\\Phi$,$\\Delta$)-Personalized Differential Privacy (($\\Phi$,$\\Delta$)-PDP),\nwhich extends an existing PDP concept called $\\Phi$-PDP. Our algorithm uses a\nmulti-round personalized sampling mechanism and embeds it within the DP-SGD\niterations. Experiments on real datasets show that our algorithm outperforms\nDP-SGD and simple combinations of DP-SGD with existing PDP mechanisms in terms\nof model performance and efficiency due to its embedded sampling mechanism.",
          "arxiv_id": "2305.15165v1"
        },
        {
          "title": "Have it your way: Individualized Privacy Assignment for DP-SGD",
          "year": "2023-03",
          "abstract": "When training a machine learning model with differential privacy, one sets a\nprivacy budget. This budget represents a maximal privacy violation that any\nuser is willing to face by contributing their data to the training set. We\nargue that this approach is limited because different users may have different\nprivacy expectations. Thus, setting a uniform privacy budget across all points\nmay be overly conservative for some users or, conversely, not sufficiently\nprotective for others. In this paper, we capture these preferences through\nindividualized privacy budgets. To demonstrate their practicality, we introduce\na variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which\nsupports such individualized budgets. DP-SGD is the canonical approach to\ntraining models with differential privacy. We modify its data sampling and\ngradient noising mechanisms to arrive at our approach, which we call\nIndividualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees\ntailored to the preferences of individual users and their data points, we find\nit empirically improves privacy-utility trade-offs.",
          "arxiv_id": "2303.17046v2"
        },
        {
          "title": "Attack-Aware Noise Calibration for Differential Privacy",
          "year": "2024-07",
          "abstract": "Differential privacy (DP) is a widely used approach for mitigating privacy\nrisks when training machine learning models on sensitive data. DP mechanisms\nadd noise during training to limit the risk of information leakage. The scale\nof the added noise is critical, as it determines the trade-off between privacy\nand utility. The standard practice is to select the noise scale to satisfy a\ngiven privacy budget $\\varepsilon$. This privacy budget is in turn interpreted\nin terms of operational attack risks, such as accuracy, sensitivity, and\nspecificity of inference attacks aimed to recover information about the\ntraining data records. We show that first calibrating the noise scale to a\nprivacy budget $\\varepsilon$, and then translating {\\epsilon} to attack risk\nleads to overly conservative risk assessments and unnecessarily low utility.\nInstead, we propose methods to directly calibrate the noise scale to a desired\nattack risk level, bypassing the step of choosing $\\varepsilon$. For a given\nnotion of attack risk, our approach significantly decreases noise scale,\nleading to increased utility at the same level of privacy. We empirically\ndemonstrate that calibrating noise to attack sensitivity/specificity, rather\nthan $\\varepsilon$, when training privacy-preserving ML models substantially\nimproves model accuracy for the same risk level. Our work provides a principled\nand practical way to improve the utility of privacy-preserving ML without\ncompromising on privacy. The code is available at\nhttps://github.com/Felipe-Gomez/riskcal",
          "arxiv_id": "2407.02191v2"
        }
      ],
      "43": [
        {
          "title": "Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees",
          "year": "2022-09",
          "abstract": "Current abstractive summarization models either suffer from a lack of clear\ninterpretability or provide incomplete rationales by only highlighting parts of\nthe source document. To this end, we propose the Summarization Program (SP), an\ninterpretable modular framework consisting of an (ordered) list of binary\ntrees, each encoding the step-by-step generative process of an abstractive\nsummary sentence from the source document. A Summarization Program contains one\nroot node per summary sentence, and a distinct tree connects each summary\nsentence (root node) to the document sentences (leaf nodes) from which it is\nderived, with the connecting nodes containing intermediate generated sentences.\nEdges represent different modular operations involved in summarization such as\nsentence fusion, compression, and paraphrasing. We first propose an efficient\nbest-first search method over neural modules, SP-Search that identifies SPs for\nhuman summaries by directly optimizing for ROUGE scores. Next, using these\nprograms as automatic supervision, we propose seq2seq models that generate\nSummarization Programs, which are then executed to obtain final summaries. We\ndemonstrate that SP-Search effectively represents the generative process behind\nhuman summaries using modules that are typically faithful to their intended\nbehavior. We also conduct a simulation study to show that Summarization\nPrograms improve the interpretability of summarization models by allowing\nhumans to better simulate model reasoning. Summarization Programs constitute a\npromising step toward interpretable and modular abstractive summarization, a\ncomplex task previously addressed primarily through blackbox end-to-end neural\nsystems. Supporting code available at\nhttps://github.com/swarnaHub/SummarizationPrograms",
          "arxiv_id": "2209.10492v2"
        },
        {
          "title": "Bengali Abstractive News Summarization(BANS): A Neural Attention Approach",
          "year": "2020-12",
          "abstract": "Abstractive summarization is the process of generating novel sentences based\non the information extracted from the original text document while retaining\nthe context. Due to abstractive summarization's underlying complexities, most\nof the past research work has been done on the extractive summarization\napproach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)\nmodel, abstractive summarization becomes more viable. Although a significant\nnumber of notable research has been done in the English language based on\nabstractive summarization, only a couple of works have been done on Bengali\nabstractive news summarization (BANS). In this article, we presented a seq2seq\nbased Long Short-Term Memory (LSTM) network model with attention at\nencoder-decoder. Our proposed system deploys a local attention-based model that\nproduces a long sequence of words with lucid and human-like generated sentences\nwith noteworthy information of the original document. We also prepared a\ndataset of more than 19k articles and corresponding human-written summaries\ncollected from bangla.bdnews24.com1 which is till now the most extensive\ndataset for Bengali news document summarization and publicly published in\nKaggle2. We evaluated our model qualitatively and quantitatively and compared\nit with other published results. It showed significant improvement in terms of\nhuman evaluation scores with state-of-the-art approaches for BANS.",
          "arxiv_id": "2012.01747v1"
        },
        {
          "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization",
          "year": "2023-02",
          "abstract": "Text summarization has been a crucial problem in natural language processing\n(NLP) for several decades. It aims to condense lengthy documents into shorter\nversions while retaining the most critical information. Various methods have\nbeen proposed for text summarization, including extractive and abstractive\nsummarization. The emergence of large language models (LLMs) like GPT3 and\nChatGPT has recently created significant interest in using these models for\ntext summarization tasks. Recent studies \\cite{goyal2022news,\nzhang2023benchmarking} have shown that LLMs-generated news summaries are\nalready on par with humans. However, the performance of LLMs for more practical\napplications like aspect or query-based summaries is underexplored. To fill\nthis gap, we conducted an evaluation of ChatGPT's performance on four widely\nused benchmark datasets, encompassing diverse summaries from Reddit posts, news\narticles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's\nperformance is comparable to traditional fine-tuning methods in terms of Rouge\nscores. Moreover, we highlight some unique differences between\nChatGPT-generated summaries and human references, providing valuable insights\ninto the superpower of ChatGPT for diverse text summarization tasks. Our\nfindings call for new directions in this area, and we plan to conduct further\nresearch to systematically examine the characteristics of ChatGPT-generated\nsummaries through extensive human evaluation.",
          "arxiv_id": "2302.08081v1"
        }
      ],
      "44": [
        {
          "title": "CSPO: Cross-Market Synergistic Stock Price Movement Forecasting with Pseudo-volatility Optimization",
          "year": "2025-03",
          "abstract": "The stock market, as a cornerstone of the financial markets, places\nforecasting stock price movements at the forefront of challenges in\nquantitative finance. Emerging learning-based approaches have made significant\nprogress in capturing the intricate and ever-evolving data patterns of modern\nmarkets. With the rapid expansion of the stock market, it presents two\ncharacteristics, i.e., stock exogeneity and volatility heterogeneity, that\nheighten the complexity of price forecasting. Specifically, while stock\nexogeneity reflects the influence of external market factors on price\nmovements, volatility heterogeneity showcases the varying difficulty in\nmovement forecasting against price fluctuations. In this work, we introduce the\nframework of Cross-market Synergy with Pseudo-volatility Optimization (CSPO).\nSpecifically, CSPO implements an effective deep neural architecture to leverage\nexternal futures knowledge. This enriches stock embeddings with cross-market\ninsights and thus enhances the CSPO's predictive capability. Furthermore, CSPO\nincorporates pseudo-volatility to model stock-specific forecasting confidence,\nenabling a dynamic adaptation of its optimization process to improve accuracy\nand robustness. Our extensive experiments, encompassing industrial evaluation\nand public benchmarking, highlight CSPO's superior performance over existing\nmethods and effectiveness of all proposed modules contained therein.",
          "arxiv_id": "2503.22740v1"
        },
        {
          "title": "DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities",
          "year": "2021-12",
          "abstract": "Reinforcement learning (RL) techniques have shown great success in many\nchallenging quantitative trading tasks, such as portfolio management and\nalgorithmic trading. Especially, intraday trading is one of the most profitable\nand risky tasks because of the intraday behaviors of the financial market that\nreflect billions of rapidly fluctuating capitals. However, a vast majority of\nexisting RL methods focus on the relatively low frequency trading scenarios\n(e.g., day-level) and fail to capture the fleeting intraday investment\nopportunities due to two major challenges: 1) how to effectively train\nprofitable RL agents for intraday investment decision-making, which involves\nhigh-dimensional fine-grained action space; 2) how to learn meaningful\nmulti-modality market representation to understand the intraday behaviors of\nthe financial market at tick-level. Motivated by the efficient workflow of\nprofessional human intraday traders, we propose DeepScalper, a deep\nreinforcement learning framework for intraday trading to tackle the above\nchallenges. Specifically, DeepScalper includes four components: 1) a dueling\nQ-network with action branching to deal with the large action space of intraday\ntrading for efficient RL optimization; 2) a novel reward function with a\nhindsight bonus to encourage RL agents making trading decisions with a\nlong-term horizon of the entire trading day; 3) an encoder-decoder architecture\nto learn multi-modality temporal market embedding, which incorporates both\nmacro-level and micro-level market information; 4) a risk-aware auxiliary task\nto maintain a striking balance between maximizing profit and minimizing risk.\nThrough extensive experiments on real-world market data spanning over three\nyears on six financial futures, we demonstrate that DeepScalper significantly\noutperforms many state-of-the-art baselines in terms of four financial\ncriteria.",
          "arxiv_id": "2201.09058v3"
        },
        {
          "title": "Predicting Stock Prices with FinBERT-LSTM: Integrating News Sentiment Analysis",
          "year": "2024-07",
          "abstract": "The stock market's ascent typically mirrors the flourishing state of the\neconomy, whereas its decline is often an indicator of an economic downturn.\nTherefore, for a long time, significant correlation elements for predicting\ntrends in financial stock markets have been widely discussed, and people are\nbecoming increasingly interested in the task of financial text mining. The\ninherent instability of stock prices makes them acutely responsive to\nfluctuations within the financial markets. In this article, we use deep\nlearning networks, based on the history of stock prices and articles of\nfinancial, business, technical news that introduce market information to\npredict stock prices. We illustrate the enhancement of predictive precision by\nintegrating weighted news categories into the forecasting model. We developed a\npre-trained NLP model known as FinBERT, designed to discern the sentiments\nwithin financial texts. Subsequently, we advanced this model by incorporating\nthe sophisticated Long Short Term Memory (LSTM) architecture, thus constructing\nthe innovative FinBERT-LSTM model. This model utilizes news categories related\nto the stock market structure hierarchy, namely market, industry, and stock\nrelated news categories, combined with the stock market's stock price situation\nin the previous week for prediction. We selected NASDAQ-100 index stock data\nand trained the model on Benzinga news articles, and utilized Mean Absolute\nError (MAE), Mean Absolute Percentage Error (MAPE), and Accuracy as the key\nmetrics for the assessment and comparative analysis of the model's performance.\nThe results indicate that FinBERT-LSTM performs the best, followed by LSTM, and\nDNN model ranks third in terms of effectiveness.",
          "arxiv_id": "2407.16150v1"
        }
      ],
      "45": [
        {
          "title": "EFO: the Emotion Frame Ontology",
          "year": "2024-01",
          "abstract": "Emotions are a subject of intense debate in various disciplines. Despite the\nproliferation of theories and definitions, there is still no consensus on what\nemotions are, and how to model the different concepts involved when we talk\nabout - or categorize - them. In this paper, we propose an OWL frame-based\nontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as\nsemantic frames, with a set of semantic roles that capture the different\naspects of emotional experience. EFO follows pattern-based ontology design, and\nis aligned to the DOLCE foundational ontology. EFO is used to model multiple\nemotion theories, which can be cross-linked as modules in an Emotion Ontology\nNetwork. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE)\nTheory as an EFO-BE module, and demonstrate how to perform automated inferences\non the representation of emotion situations. EFO-BE has been evaluated by\nlexicalizing the BE emotion frames from within the Framester knowledge graph,\nand implementing a graph-based emotion detector from text. In addition, an EFO\nintegration of multimodal datasets, including emotional speech and emotional\nface expressions, has been performed to enable further inquiry into crossmodal\nemotion semantics.",
          "arxiv_id": "2401.10751v1"
        },
        {
          "title": "Leveraging Retrieval Augment Approach for Multimodal Emotion Recognition Under Missing Modalities",
          "year": "2024-09",
          "abstract": "Multimodal emotion recognition utilizes complete multimodal information and\nrobust multimodal joint representation to gain high performance. However, the\nideal condition of full modality integrity is often not applicable in reality\nand there always appears the situation that some modalities are missing. For\nexample, video, audio, or text data is missing due to sensor failure or network\nbandwidth problems, which presents a great challenge to MER research.\nTraditional methods extract useful information from the complete modalities and\nreconstruct the missing modalities to learn robust multimodal joint\nrepresentation. These methods have laid a solid foundation for research in this\nfield, and to a certain extent, alleviated the difficulty of multimodal emotion\nrecognition under missing modalities. However, relying solely on internal\nreconstruction and multimodal joint learning has its limitations, especially\nwhen the missing information is critical for emotion recognition. To address\nthis challenge, we propose a novel framework of Retrieval Augment for Missing\nModality Multimodal Emotion Recognition (RAMER), which introduces similar\nmultimodal emotion data to enhance the performance of emotion recognition under\nmissing modalities. By leveraging databases, that contain related multimodal\nemotion data, we can retrieve similar multimodal emotion information to fill in\nthe gaps left by missing modalities. Various experimental results demonstrate\nthat our framework is superior to existing state-of-the-art approaches in\nmissing modality MER tasks. Our whole project is publicly available on\nhttps://github.com/WooyoohL/Retrieval_Augment_MER.",
          "arxiv_id": "2410.02804v1"
        },
        {
          "title": "FAF: A novel multimodal emotion recognition approach integrating face, body and text",
          "year": "2022-11",
          "abstract": "Multimodal emotion analysis performed better in emotion recognition depending\non more comprehensive emotional clues and multimodal emotion dataset. In this\npaper, we developed a large multimodal emotion dataset, named \"HED\" dataset, to\nfacilitate the emotion recognition task, and accordingly propose a multimodal\nemotion recognition method. To promote recognition accuracy, \"Feature After\nFeature\" framework was used to explore crucial emotional information from the\naligned face, body and text samples. We employ various benchmarks to evaluate\nthe \"HED\" dataset and compare the performance with our method. The results show\nthat the five classification accuracy of the proposed multimodal fusion method\nis about 83.75%, and the performance is improved by 1.83%, 9.38%, and 21.62%\nrespectively compared with that of individual modalities. The complementarity\nbetween each channel is effectively used to improve the performance of emotion\nrecognition. We had also established a multimodal online emotion prediction\nplatform, aiming to provide free emotion prediction to more users.",
          "arxiv_id": "2211.15425v1"
        }
      ],
      "46": [
        {
          "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
          "year": "2023-09",
          "abstract": "Large language models (LLMs) have demonstrated strong capabilities in various\naspects. However, when applying them to the highly specialized, safe-critical\nlegal domain, it is unclear how much legal knowledge they possess and whether\nthey can reliably perform legal-related tasks. To address this gap, we propose\na comprehensive evaluation benchmark LawBench. LawBench has been meticulously\ncrafted to have precise assessment of the LLMs' legal capabilities from three\ncognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize\nneeded legal concepts, articles and facts; (2) Legal knowledge understanding:\nwhether LLMs can comprehend entities, events and relationships within legal\ntext; (3) Legal knowledge applying: whether LLMs can properly utilize their\nlegal knowledge and make necessary reasoning steps to solve realistic legal\ntasks. LawBench contains 20 diverse tasks covering 5 task types: single-label\nclassification (SLC), multi-label classification (MLC), regression, extraction\nand generation. We perform extensive evaluations of 51 LLMs on LawBench,\nincluding 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific\nLLMs. The results show that GPT-4 remains the best-performing LLM in the legal\ndomain, surpassing the others by a significant margin. While fine-tuning LLMs\non legal specific text brings certain improvements, we are still a long way\nfrom obtaining usable and reliable LLMs in legal tasks. All data, model\npredictions and evaluation code are released in\nhttps://github.com/open-compass/LawBench/. We hope this benchmark provides\nin-depth understanding of the LLMs' domain-specified capabilities and speed up\nthe development of LLMs in the legal domain.",
          "arxiv_id": "2309.16289v1"
        },
        {
          "title": "Legal Evalutions and Challenges of Large Language Models",
          "year": "2024-11",
          "abstract": "In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field.",
          "arxiv_id": "2411.10137v1"
        },
        {
          "title": "LeKUBE: A Legal Knowledge Update BEnchmark",
          "year": "2024-07",
          "abstract": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
          "arxiv_id": "2407.14192v2"
        }
      ],
      "47": [
        {
          "title": "FengWu-4DVar: Coupling the Data-driven Weather Forecasting Model with 4D Variational Assimilation",
          "year": "2023-12",
          "abstract": "Weather forecasting is a crucial yet highly challenging task. With the\nmaturity of Artificial Intelligence (AI), the emergence of data-driven weather\nforecasting models has opened up a new paradigm for the development of weather\nforecasting systems. Despite the significant successes that have been achieved\n(e.g., surpassing advanced traditional physical models for global medium-range\nforecasting), existing data-driven weather forecasting models still rely on the\nanalysis fields generated by the traditional assimilation and forecasting\nsystem, which hampers the significance of data-driven weather forecasting\nmodels regarding both computational cost and forecasting accuracy. In this\nwork, we explore the possibility of coupling the data-driven weather\nforecasting model with data assimilation by integrating the global AI weather\nforecasting model, FengWu, with one of the most popular assimilation\nalgorithms, Four-Dimensional Variational (4DVar) assimilation, and develop an\nAI-based cyclic weather forecasting system, FengWu-4DVar. FengWu-4DVar can\nincorporate observational data into the data-driven weather forecasting model\nand consider the temporal evolution of atmospheric dynamics to obtain accurate\nanalysis fields for making predictions in a cycling manner without the help of\nphysical models. Owning to the auto-differentiation ability of deep learning\nmodels, FengWu-4DVar eliminates the need of developing the cumbersome adjoint\nmodel, which is usually required in the traditional implementation of the 4DVar\nalgorithm. Experiments on the simulated observational dataset demonstrate that\nFengWu-4DVar is capable of generating reasonable analysis fields for making\naccurate and efficient iterative predictions.",
          "arxiv_id": "2312.12455v2"
        },
        {
          "title": "FengWu-W2S: A deep learning model for seamless weather-to-subseasonal forecast of global atmosphere",
          "year": "2024-11",
          "abstract": "Seamless forecasting that produces warning information at continuum\ntimescales based on only one system is a long-standing pursuit for\nweather-climate service. While the rapid advancement of deep learning has\ninduced revolutionary changes in classical forecasting field, current efforts\nare still focused on building separate AI models for weather and climate\nforecasts. To explore the seamless forecasting ability based on one AI model,\nwe propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the\nFengWu global weather forecast model and incorporates an ocean-atmosphere-land\ncoupling structure along with a diverse perturbation strategy. FengWu-W2S can\ngenerate 6-hourly atmosphere forecasts extending up to 42 days through an\nautoregressive and seamless manner. Our hindcast results demonstrate that\nFengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead,\nenhancing predictive capabilities for global surface air temperature,\nprecipitation, geopotential height and intraseasonal signals such as the\nMadden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover,\nour ablation experiments on forecast error growth from daily to seasonal\ntimescales reveal potential pathways for developing AI-based integrated system\nfor seamless weather-climate forecasting in the future.",
          "arxiv_id": "2411.10191v2"
        },
        {
          "title": "FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting",
          "year": "2024-01",
          "abstract": "Kilometer-scale modeling of global atmosphere dynamics enables fine-grained\nweather forecasting and decreases the risk of disastrous weather and climate\nactivity. Therefore, building a kilometer-scale global forecast model is a\npersistent pursuit in the meteorology domain. Active international efforts have\nbeen made in past decades to improve the spatial resolution of numerical\nweather models. Nonetheless, developing the higher resolution numerical model\nremains a long-standing challenge due to the substantial consumption of\ncomputational resources. Recent advances in data-driven global weather\nforecasting models utilize reanalysis data for model training and have\ndemonstrated comparable or even higher forecasting skills than numerical\nmodels. However, they are all limited by the resolution of reanalysis data and\nincapable of generating higher-resolution forecasts. This work presents\nFengWu-GHR, the first data-driven global weather forecasting model running at\nthe 0.09$^{\\circ}$ horizontal resolution. FengWu-GHR introduces a novel\napproach that opens the door for operating ML-based high-resolution forecasts\nby inheriting prior knowledge from a pretrained low-resolution model. The\nhindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to\nthe IFS-HRES. Furthermore, evaluations on station observations and case studies\nof extreme events support the competitive operational forecasting skill of\nFengWu-GHR at the high resolution.",
          "arxiv_id": "2402.00059v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T18:45:35Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}