{
  "topics": {
    "data": {
      "0": {
        "name": "0_medical_segmentation_images_clinical",
        "keywords": [
          [
            "medical",
            0.013662655084021995
          ],
          [
            "segmentation",
            0.013374949287524966
          ],
          [
            "images",
            0.009322482317196897
          ],
          [
            "clinical",
            0.008654790357911724
          ],
          [
            "learning",
            0.008368988440320099
          ],
          [
            "imaging",
            0.008367548818047802
          ],
          [
            "MRI",
            0.008187383297329015
          ],
          [
            "CT",
            0.008063429673647687
          ],
          [
            "data",
            0.007557744057247228
          ],
          [
            "image",
            0.007551868261629062
          ]
        ],
        "count": 19471
      },
      "1": {
        "name": "1_image_resolution_quality_images",
        "keywords": [
          [
            "image",
            0.013790441749165874
          ],
          [
            "resolution",
            0.010218999714611843
          ],
          [
            "quality",
            0.009681117773418225
          ],
          [
            "images",
            0.009377075969575363
          ],
          [
            "Image",
            0.00869414442864091
          ],
          [
            "low",
            0.008176957808025687
          ],
          [
            "restoration",
            0.007930935686387586
          ],
          [
            "compression",
            0.00780510186107486
          ],
          [
            "super",
            0.007465675563354887
          ],
          [
            "methods",
            0.007196898762522086
          ]
        ],
        "count": 7753
      },
      "2": {
        "name": "2_visual_language_reasoning_Language",
        "keywords": [
          [
            "visual",
            0.016922986920230573
          ],
          [
            "language",
            0.016644057269087638
          ],
          [
            "reasoning",
            0.012657337447323547
          ],
          [
            "Language",
            0.012472750516377062
          ],
          [
            "text",
            0.012035443503828645
          ],
          [
            "models",
            0.010840362273125508
          ],
          [
            "vision",
            0.010537303512040465
          ],
          [
            "multimodal",
            0.010127629602656278
          ],
          [
            "tasks",
            0.009865754998544038
          ],
          [
            "vision language",
            0.009419174799075804
          ]
        ],
        "count": 5601
      },
      "3": {
        "name": "3_remote sensing_remote_sensing_imagery",
        "keywords": [
          [
            "remote sensing",
            0.014366036632444389
          ],
          [
            "remote",
            0.014195474501117803
          ],
          [
            "sensing",
            0.01419003380454792
          ],
          [
            "imagery",
            0.010684928163256639
          ],
          [
            "data",
            0.010153921194740873
          ],
          [
            "satellite",
            0.009960785839124013
          ],
          [
            "SAR",
            0.009454630448314898
          ],
          [
            "change",
            0.009249933311182554
          ],
          [
            "images",
            0.00842652234773302
          ],
          [
            "learning",
            0.007677837740721875
          ]
        ],
        "count": 4015
      },
      "4": {
        "name": "4_robot_navigation_manipulation_object",
        "keywords": [
          [
            "robot",
            0.017182283939838835
          ],
          [
            "navigation",
            0.013958709665483803
          ],
          [
            "manipulation",
            0.01274442002534452
          ],
          [
            "object",
            0.01135970058714699
          ],
          [
            "environments",
            0.01134898961367074
          ],
          [
            "agent",
            0.011156872072069862
          ],
          [
            "tasks",
            0.011010618800125008
          ],
          [
            "world",
            0.010903219293599713
          ],
          [
            "3D",
            0.0107187176184433
          ],
          [
            "objects",
            0.010682494268427923
          ]
        ],
        "count": 3144
      },
      "5": {
        "name": "5_diffusion_text_generation_Diffusion",
        "keywords": [
          [
            "diffusion",
            0.023173539462771817
          ],
          [
            "text",
            0.01991051795907338
          ],
          [
            "generation",
            0.019501562506993458
          ],
          [
            "Diffusion",
            0.017856656859540118
          ],
          [
            "image",
            0.016952028390709875
          ],
          [
            "diffusion models",
            0.015641311815497185
          ],
          [
            "models",
            0.01496787530632696
          ],
          [
            "text image",
            0.014236879087673316
          ],
          [
            "image generation",
            0.013446639168784367
          ],
          [
            "style",
            0.01335701494962438
          ]
        ],
        "count": 3099
      },
      "6": {
        "name": "6_action_video_action recognition_Action",
        "keywords": [
          [
            "action",
            0.04213810146194571
          ],
          [
            "video",
            0.020444900005151725
          ],
          [
            "action recognition",
            0.020296575553104
          ],
          [
            "Action",
            0.018902488700426105
          ],
          [
            "temporal",
            0.018489633191134367
          ],
          [
            "recognition",
            0.017800692682238876
          ],
          [
            "actions",
            0.013866924024641777
          ],
          [
            "videos",
            0.010878281105234281
          ],
          [
            "skeleton",
            0.01035721054410523
          ],
          [
            "Recognition",
            0.00917258551546768
          ]
        ],
        "count": 2508
      },
      "7": {
        "name": "7_adversarial_attacks_attack_robustness",
        "keywords": [
          [
            "adversarial",
            0.052293029013864796
          ],
          [
            "attacks",
            0.03823784613312528
          ],
          [
            "attack",
            0.029464824641969208
          ],
          [
            "robustness",
            0.020683432182187017
          ],
          [
            "Adversarial",
            0.02031980055583396
          ],
          [
            "examples",
            0.015612456245223953
          ],
          [
            "adversarial examples",
            0.015600663989343929
          ],
          [
            "adversarial attacks",
            0.015454085063231246
          ],
          [
            "perturbations",
            0.014664779535421245
          ],
          [
            "defense",
            0.012989137179015637
          ]
        ],
        "count": 2443
      },
      "8": {
        "name": "8_rendering_NeRF_Gaussian_view",
        "keywords": [
          [
            "rendering",
            0.030305505482637232
          ],
          [
            "NeRF",
            0.028759846223641663
          ],
          [
            "Gaussian",
            0.02729227306500446
          ],
          [
            "view",
            0.021309183426988024
          ],
          [
            "scene",
            0.019472944936936607
          ],
          [
            "Splatting",
            0.018229956548037544
          ],
          [
            "3D",
            0.017094895361066252
          ],
          [
            "view synthesis",
            0.016264067499391327
          ],
          [
            "scenes",
            0.015957095279873282
          ],
          [
            "3DGS",
            0.014844967996954986
          ]
        ],
        "count": 1993
      },
      "9": {
        "name": "9_3D_LiDAR_3D object_3D object detection",
        "keywords": [
          [
            "3D",
            0.028309761517927804
          ],
          [
            "LiDAR",
            0.026741258788692816
          ],
          [
            "3D object",
            0.021124774885996164
          ],
          [
            "3D object detection",
            0.01909098363593564
          ],
          [
            "object",
            0.01862075554611894
          ],
          [
            "detection",
            0.018415321234214948
          ],
          [
            "object detection",
            0.017458959686441997
          ],
          [
            "point",
            0.015352920710753027
          ],
          [
            "3D Object",
            0.012094836787867672
          ],
          [
            "autonomous",
            0.011974362802682409
          ]
        ],
        "count": 1905
      },
      "10": {
        "name": "10_video_Video_videos_temporal",
        "keywords": [
          [
            "video",
            0.047267001582477354
          ],
          [
            "Video",
            0.02708190576115809
          ],
          [
            "videos",
            0.016251466489998593
          ],
          [
            "temporal",
            0.01557200461695398
          ],
          [
            "understanding",
            0.014585688141710032
          ],
          [
            "language",
            0.01299750215336621
          ],
          [
            "video understanding",
            0.01240810707750546
          ],
          [
            "text",
            0.0115025879951143
          ],
          [
            "reasoning",
            0.010968057323850151
          ],
          [
            "visual",
            0.01064192567825511
          ]
        ],
        "count": 1828
      },
      "11": {
        "name": "11_depth_depth estimation_estimation_Depth",
        "keywords": [
          [
            "depth",
            0.05420044043127854
          ],
          [
            "depth estimation",
            0.023466479451132546
          ],
          [
            "estimation",
            0.023461656552496706
          ],
          [
            "Depth",
            0.02163302421052815
          ],
          [
            "flow",
            0.017942679867435654
          ],
          [
            "stereo",
            0.017830586986766647
          ],
          [
            "monocular",
            0.012778319580292715
          ],
          [
            "optical flow",
            0.010505682674652291
          ],
          [
            "monocular depth",
            0.010322542245435056
          ],
          [
            "Estimation",
            0.009633405139675586
          ]
        ],
        "count": 1768
      },
      "12": {
        "name": "12_driving_trajectory_traffic_prediction",
        "keywords": [
          [
            "driving",
            0.030755150148010755
          ],
          [
            "trajectory",
            0.02339479426897483
          ],
          [
            "traffic",
            0.020420598841844457
          ],
          [
            "prediction",
            0.018178824754753353
          ],
          [
            "autonomous",
            0.01765654582230112
          ],
          [
            "trajectories",
            0.014900953114004219
          ],
          [
            "vehicle",
            0.014698957689558112
          ],
          [
            "autonomous driving",
            0.014365695295213147
          ],
          [
            "trajectory prediction",
            0.012739454549749223
          ],
          [
            "Trajectory",
            0.010705452166186208
          ]
        ],
        "count": 1742
      },
      "13": {
        "name": "13_text_document_recognition_OCR",
        "keywords": [
          [
            "text",
            0.028564010749016735
          ],
          [
            "document",
            0.02324164735890249
          ],
          [
            "recognition",
            0.018204944582229498
          ],
          [
            "OCR",
            0.01781751280755962
          ],
          [
            "documents",
            0.015447534915855614
          ],
          [
            "handwritten",
            0.014428955640355547
          ],
          [
            "character",
            0.013249976654497281
          ],
          [
            "Text",
            0.01220694464051622
          ],
          [
            "text recognition",
            0.011167863800334731
          ],
          [
            "characters",
            0.011153616343307797
          ]
        ],
        "count": 1718
      },
      "14": {
        "name": "14_anomaly_detection_defect_Anomaly",
        "keywords": [
          [
            "anomaly",
            0.031320429316653034
          ],
          [
            "detection",
            0.021817998952759987
          ],
          [
            "defect",
            0.01958335206819801
          ],
          [
            "Anomaly",
            0.016828812973027373
          ],
          [
            "anomalies",
            0.013123606668303846
          ],
          [
            "industrial",
            0.012604058676922528
          ],
          [
            "inspection",
            0.01224494100464506
          ],
          [
            "defects",
            0.012096026407787733
          ],
          [
            "Detection",
            0.011242294932210295
          ],
          [
            "normal",
            0.011195749165368356
          ]
        ],
        "count": 1674
      },
      "15": {
        "name": "15_object_object detection_detection_Object",
        "keywords": [
          [
            "object",
            0.02899436328633545
          ],
          [
            "object detection",
            0.025134801483779955
          ],
          [
            "detection",
            0.024691704249785604
          ],
          [
            "Object",
            0.017715625631278736
          ],
          [
            "detectors",
            0.014848930268264773
          ],
          [
            "Detection",
            0.014387730262237247
          ],
          [
            "COCO",
            0.012407316235470004
          ],
          [
            "objects",
            0.011338106408000106
          ],
          [
            "detector",
            0.010613154133940062
          ],
          [
            "AP",
            0.009171683410506629
          ]
        ],
        "count": 1555
      },
      "16": {
        "name": "16_pruning_quantization_NAS_search",
        "keywords": [
          [
            "pruning",
            0.0332435786772623
          ],
          [
            "quantization",
            0.020466192759639814
          ],
          [
            "NAS",
            0.019820344649079433
          ],
          [
            "search",
            0.01981345956888334
          ],
          [
            "accuracy",
            0.014370644294560052
          ],
          [
            "Neural",
            0.013984059817775125
          ],
          [
            "neural",
            0.013896942870170126
          ],
          [
            "networks",
            0.013809153782337165
          ],
          [
            "network",
            0.013659290044376098
          ],
          [
            "architectures",
            0.011628600190562215
          ]
        ],
        "count": 1439
      },
      "17": {
        "name": "17_supervised_learning_SSL_label",
        "keywords": [
          [
            "supervised",
            0.019547915742332916
          ],
          [
            "learning",
            0.017115015839405064
          ],
          [
            "SSL",
            0.015926120975445192
          ],
          [
            "label",
            0.015589163815748248
          ],
          [
            "labels",
            0.015553184462438094
          ],
          [
            "supervised learning",
            0.013441234179534016
          ],
          [
            "contrastive",
            0.01209898610478285
          ],
          [
            "samples",
            0.01181398731633771
          ],
          [
            "class",
            0.011476446313118512
          ],
          [
            "Learning",
            0.011469062135016942
          ]
        ],
        "count": 1425
      },
      "18": {
        "name": "18_pose_human_pose estimation_3D",
        "keywords": [
          [
            "pose",
            0.03975371899350635
          ],
          [
            "human",
            0.026158991934798436
          ],
          [
            "pose estimation",
            0.025791418256483775
          ],
          [
            "3D",
            0.02277914588630342
          ],
          [
            "estimation",
            0.02267648691454118
          ],
          [
            "hand",
            0.020179322698464053
          ],
          [
            "human pose",
            0.01955401704807065
          ],
          [
            "Pose",
            0.019058326074371957
          ],
          [
            "body",
            0.018197832408180417
          ],
          [
            "Human",
            0.01573288264108957
          ]
        ],
        "count": 1399
      },
      "19": {
        "name": "19_GANs_GAN_latent_image",
        "keywords": [
          [
            "GANs",
            0.0270118257939287
          ],
          [
            "GAN",
            0.026316399367983728
          ],
          [
            "latent",
            0.01754534705901571
          ],
          [
            "image",
            0.015952969031190138
          ],
          [
            "Generative",
            0.01508265539668566
          ],
          [
            "generative",
            0.01433705653944883
          ],
          [
            "translation",
            0.01339269662596724
          ],
          [
            "generator",
            0.012751114356437294
          ],
          [
            "images",
            0.010879027661206601
          ],
          [
            "Adversarial",
            0.010488796442756644
          ]
        ],
        "count": 1226
      },
      "20": {
        "name": "20_deepfake_detection_fake_Deepfake",
        "keywords": [
          [
            "deepfake",
            0.03051000322369986
          ],
          [
            "detection",
            0.022637507421733608
          ],
          [
            "fake",
            0.018455544713305943
          ],
          [
            "Deepfake",
            0.017873435868178147
          ],
          [
            "deepfake detection",
            0.014666686083261363
          ],
          [
            "Detection",
            0.012615016519746183
          ],
          [
            "images",
            0.012304038418290026
          ],
          [
            "face",
            0.011109462266306476
          ],
          [
            "AI",
            0.010754321352922208
          ],
          [
            "deepfakes",
            0.01064668425146878
          ]
        ],
        "count": 1183
      },
      "21": {
        "name": "21_facial_emotion_expression_Facial",
        "keywords": [
          [
            "facial",
            0.03603887328403626
          ],
          [
            "emotion",
            0.03196314991313223
          ],
          [
            "expression",
            0.02629505005806221
          ],
          [
            "Facial",
            0.021547114228632813
          ],
          [
            "recognition",
            0.019661720475137694
          ],
          [
            "AU",
            0.01677435864473091
          ],
          [
            "emotions",
            0.016538559291704138
          ],
          [
            "expressions",
            0.016420063938043978
          ],
          [
            "emotion recognition",
            0.015952335425591917
          ],
          [
            "Emotion",
            0.015585706536877568
          ]
        ],
        "count": 1144
      },
      "22": {
        "name": "22_segmentation_semantic segmentation_semantic_Segmentation",
        "keywords": [
          [
            "segmentation",
            0.032520195206856985
          ],
          [
            "semantic segmentation",
            0.021800106623218986
          ],
          [
            "semantic",
            0.02057471238909823
          ],
          [
            "Segmentation",
            0.01652979186376778
          ],
          [
            "Semantic",
            0.014306904819584512
          ],
          [
            "pixel",
            0.01014575827051538
          ],
          [
            "level",
            0.00917113098753229
          ],
          [
            "Cityscapes",
            0.00905369107116237
          ],
          [
            "supervised",
            0.00871418905048858
          ],
          [
            "labels",
            0.00853700149031937
          ]
        ],
        "count": 1141
      },
      "23": {
        "name": "23_SLAM_localization_odometry_Visual",
        "keywords": [
          [
            "SLAM",
            0.051254868217909304
          ],
          [
            "localization",
            0.015314071614267722
          ],
          [
            "odometry",
            0.013909596141786475
          ],
          [
            "Visual",
            0.012342490576530037
          ],
          [
            "LiDAR",
            0.011256453911156266
          ],
          [
            "map",
            0.010776851897295957
          ],
          [
            "camera",
            0.010523659928699466
          ],
          [
            "mapping",
            0.01035564752069851
          ],
          [
            "visual",
            0.009768784119210562
          ],
          [
            "pose",
            0.009140748432799429
          ]
        ],
        "count": 1129
      },
      "24": {
        "name": "24_domain_target_source_Domain",
        "keywords": [
          [
            "domain",
            0.05322323449258233
          ],
          [
            "target",
            0.030417055801832234
          ],
          [
            "source",
            0.02993601050064711
          ],
          [
            "Domain",
            0.027845403241389094
          ],
          [
            "adaptation",
            0.027681638967371254
          ],
          [
            "domains",
            0.0217091179551966
          ],
          [
            "Adaptation",
            0.02157094114807548
          ],
          [
            "domain adaptation",
            0.020275351496812078
          ],
          [
            "target domain",
            0.019332661903597104
          ],
          [
            "Domain Adaptation",
            0.01843455487269893
          ]
        ],
        "count": 1126
      },
      "25": {
        "name": "25_video_video generation_generation_Video",
        "keywords": [
          [
            "video",
            0.04976456975963443
          ],
          [
            "video generation",
            0.031207116972929178
          ],
          [
            "generation",
            0.026405414898897455
          ],
          [
            "Video",
            0.022688487010893408
          ],
          [
            "videos",
            0.022086285922245968
          ],
          [
            "motion",
            0.01877607493472651
          ],
          [
            "editing",
            0.0148757447761153
          ],
          [
            "diffusion",
            0.014310899304840016
          ],
          [
            "text",
            0.01383410530657038
          ],
          [
            "temporal",
            0.013316248907991386
          ]
        ],
        "count": 1126
      },
      "26": {
        "name": "26_3D_generation_view_diffusion",
        "keywords": [
          [
            "3D",
            0.046354503436414375
          ],
          [
            "generation",
            0.02329937116102164
          ],
          [
            "view",
            0.021832201676970298
          ],
          [
            "diffusion",
            0.019404735111816965
          ],
          [
            "text",
            0.01405493222996963
          ],
          [
            "scene",
            0.012399181696309103
          ],
          [
            "2D",
            0.012154796252807265
          ],
          [
            "quality",
            0.011024783349021565
          ],
          [
            "editing",
            0.010410487635321484
          ],
          [
            "texture",
            0.010171289197774993
          ]
        ],
        "count": 1107
      },
      "27": {
        "name": "27_Re_person_ReID_identification",
        "keywords": [
          [
            "Re",
            0.03981129155057688
          ],
          [
            "person",
            0.03971240605950642
          ],
          [
            "ReID",
            0.03806809714006855
          ],
          [
            "identification",
            0.029031589111601374
          ],
          [
            "Person",
            0.02854222319693549
          ],
          [
            "ID",
            0.026069726448899273
          ],
          [
            "person identification",
            0.018223101777684454
          ],
          [
            "Identification",
            0.014220511353206641
          ],
          [
            "features",
            0.011610705365877216
          ],
          [
            "modality",
            0.01066711995518748
          ]
        ],
        "count": 1041
      },
      "28": {
        "name": "28_explanations_explanation_XAI_concepts",
        "keywords": [
          [
            "explanations",
            0.02901204174338215
          ],
          [
            "explanation",
            0.018502122955122522
          ],
          [
            "XAI",
            0.013617233829783955
          ],
          [
            "concepts",
            0.012603544334021914
          ],
          [
            "interpretability",
            0.012353117062993565
          ],
          [
            "attribution",
            0.011919377886815546
          ],
          [
            "CAM",
            0.011680161776052578
          ],
          [
            "AI",
            0.010733584395315533
          ],
          [
            "saliency",
            0.010689728497877913
          ],
          [
            "concept",
            0.010276881308306573
          ]
        ],
        "count": 992
      },
      "29": {
        "name": "29_tracking_Tracking_MOT_object tracking",
        "keywords": [
          [
            "tracking",
            0.06322139953659438
          ],
          [
            "Tracking",
            0.03037207161858389
          ],
          [
            "MOT",
            0.024675809052989408
          ],
          [
            "object tracking",
            0.02283136972959994
          ],
          [
            "trackers",
            0.02060442115898474
          ],
          [
            "object",
            0.020482695797755086
          ],
          [
            "Object Tracking",
            0.019812704555864865
          ],
          [
            "tracker",
            0.017206599470640148
          ],
          [
            "Object",
            0.013940748949626575
          ],
          [
            "association",
            0.01366041683617078
          ]
        ],
        "count": 986
      },
      "30": {
        "name": "30_adversarial_attacks_attack_models",
        "keywords": [
          [
            "adversarial",
            0.018022051173401134
          ],
          [
            "attacks",
            0.017248363020378934
          ],
          [
            "attack",
            0.014504663356168773
          ],
          [
            "models",
            0.01428214344101989
          ],
          [
            "Models",
            0.011879157075477906
          ],
          [
            "image",
            0.011854692056086222
          ],
          [
            "text",
            0.011832347447680533
          ],
          [
            "content",
            0.01167079246988356
          ],
          [
            "robustness",
            0.011497836025872217
          ],
          [
            "diffusion",
            0.011131252515448954
          ]
        ],
        "count": 948
      },
      "31": {
        "name": "31_attention_ViT_Vision_Transformer",
        "keywords": [
          [
            "attention",
            0.0319051940842583
          ],
          [
            "ViT",
            0.021245980457888877
          ],
          [
            "Vision",
            0.018391600732773034
          ],
          [
            "Transformer",
            0.017970583697173213
          ],
          [
            "Transformers",
            0.01756069995156174
          ],
          [
            "vision",
            0.016276072525794574
          ],
          [
            "token",
            0.015322282727572247
          ],
          [
            "tokens",
            0.014907275890097603
          ],
          [
            "Attention",
            0.012422864861573561
          ],
          [
            "self",
            0.012249930068097886
          ]
        ],
        "count": 910
      },
      "32": {
        "name": "32_shot_shot learning_Few_Shot",
        "keywords": [
          [
            "shot",
            0.04195596577200076
          ],
          [
            "shot learning",
            0.026977932367814636
          ],
          [
            "Few",
            0.021539871418161403
          ],
          [
            "Shot",
            0.020207203480867564
          ],
          [
            "classes",
            0.018556569189130824
          ],
          [
            "learning",
            0.01781128384475761
          ],
          [
            "meta",
            0.01733896994472599
          ],
          [
            "class",
            0.01541459092013141
          ],
          [
            "FSL",
            0.015163722063806473
          ],
          [
            "Learning",
            0.014323472619417212
          ]
        ],
        "count": 905
      },
      "33": {
        "name": "33_registration_point_matching_correspondences",
        "keywords": [
          [
            "registration",
            0.03024640563679781
          ],
          [
            "point",
            0.027547078800568486
          ],
          [
            "matching",
            0.02608528858182617
          ],
          [
            "correspondences",
            0.019702866837980275
          ],
          [
            "cloud",
            0.015456024732097498
          ],
          [
            "point cloud",
            0.014714422281455386
          ],
          [
            "Registration",
            0.012559568187466228
          ],
          [
            "Point",
            0.011217125029221918
          ],
          [
            "problem",
            0.010383474981507847
          ],
          [
            "feature",
            0.010137270070726982
          ]
        ],
        "count": 878
      },
      "34": {
        "name": "34_point_point cloud_cloud_Point",
        "keywords": [
          [
            "point",
            0.04966393554864799
          ],
          [
            "point cloud",
            0.03499075647450766
          ],
          [
            "cloud",
            0.0347998950786139
          ],
          [
            "Point",
            0.024774480926626888
          ],
          [
            "3D",
            0.022939730418947885
          ],
          [
            "clouds",
            0.017898761239150095
          ],
          [
            "point clouds",
            0.017599236913992743
          ],
          [
            "segmentation",
            0.01514421603215712
          ],
          [
            "3D point",
            0.013893222690095295
          ],
          [
            "Cloud",
            0.012891106063750435
          ]
        ],
        "count": 831
      },
      "35": {
        "name": "35_human_body_clothing_3D",
        "keywords": [
          [
            "human",
            0.03087160604325655
          ],
          [
            "body",
            0.02519965645806604
          ],
          [
            "clothing",
            0.023264584742187892
          ],
          [
            "3D",
            0.020550485394752467
          ],
          [
            "pose",
            0.015509055426567428
          ],
          [
            "rendering",
            0.013985035020700656
          ],
          [
            "avatars",
            0.012903553826371861
          ],
          [
            "Human",
            0.012330183439587078
          ],
          [
            "view",
            0.011999111318276221
          ],
          [
            "geometry",
            0.010866357882955832
          ]
        ],
        "count": 774
      },
      "36": {
        "name": "36_spectral_hyperspectral_spatial_classification",
        "keywords": [
          [
            "spectral",
            0.04924184564846842
          ],
          [
            "hyperspectral",
            0.04531408446678222
          ],
          [
            "spatial",
            0.02041236230610378
          ],
          [
            "classification",
            0.013539588631160338
          ],
          [
            "Spectral",
            0.01244777522943597
          ],
          [
            "bands",
            0.010357294362472878
          ],
          [
            "information",
            0.00962517537418558
          ],
          [
            "resolution",
            0.008798052905299293
          ],
          [
            "spectral information",
            0.007969952361639429
          ],
          [
            "data",
            0.007795279532598346
          ]
        ],
        "count": 752
      },
      "37": {
        "name": "37_forgetting_continual_continual learning_Continual",
        "keywords": [
          [
            "forgetting",
            0.03427933792045686
          ],
          [
            "continual",
            0.03136423510612468
          ],
          [
            "continual learning",
            0.029264402473941138
          ],
          [
            "Continual",
            0.024124242877737557
          ],
          [
            "learning",
            0.024011136892258214
          ],
          [
            "incremental",
            0.02325824693925205
          ],
          [
            "catastrophic forgetting",
            0.021222933562615676
          ],
          [
            "catastrophic",
            0.02101783137501968
          ],
          [
            "Learning",
            0.018037334556593013
          ],
          [
            "new",
            0.01779424991772077
          ]
        ],
        "count": 751
      },
      "38": {
        "name": "38_event_SNNs_SNN_Spiking",
        "keywords": [
          [
            "event",
            0.048555248357793686
          ],
          [
            "SNNs",
            0.033152628929780124
          ],
          [
            "SNN",
            0.02878223548144761
          ],
          [
            "Spiking",
            0.027775897579643574
          ],
          [
            "Event",
            0.02169832893076135
          ],
          [
            "spike",
            0.02054500283422779
          ],
          [
            "neuromorphic",
            0.01919274688325466
          ],
          [
            "cameras",
            0.01668885633051113
          ],
          [
            "temporal",
            0.013954341039978715
          ],
          [
            "events",
            0.01392855199880779
          ]
        ],
        "count": 748
      },
      "39": {
        "name": "39_face_facial_3D_identity",
        "keywords": [
          [
            "face",
            0.035820639917904615
          ],
          [
            "facial",
            0.030638449658302525
          ],
          [
            "3D",
            0.018301023046778997
          ],
          [
            "identity",
            0.01728578696863058
          ],
          [
            "head",
            0.013624348698516179
          ],
          [
            "Face",
            0.012985377113247144
          ],
          [
            "faces",
            0.012474860731562211
          ],
          [
            "expression",
            0.011746183530971621
          ],
          [
            "makeup",
            0.011363374462116275
          ],
          [
            "expressions",
            0.011186662677218484
          ]
        ],
        "count": 669
      },
      "40": {
        "name": "40_face_face recognition_recognition_Face",
        "keywords": [
          [
            "face",
            0.0643597519836832
          ],
          [
            "face recognition",
            0.03788330373521473
          ],
          [
            "recognition",
            0.030531012103323412
          ],
          [
            "Face",
            0.029823687582051985
          ],
          [
            "facial",
            0.016237401351063992
          ],
          [
            "faces",
            0.013757156014051555
          ],
          [
            "Recognition",
            0.012923841061017198
          ],
          [
            "gender",
            0.011212920485681784
          ],
          [
            "face detection",
            0.010371289261318384
          ],
          [
            "bias",
            0.010265089188694641
          ]
        ],
        "count": 647
      },
      "41": {
        "name": "41_underwater_species_animal_fish",
        "keywords": [
          [
            "underwater",
            0.02792439342555059
          ],
          [
            "species",
            0.020021583552948728
          ],
          [
            "animal",
            0.01967498264678523
          ],
          [
            "fish",
            0.015983114042507556
          ],
          [
            "detection",
            0.014232128679651714
          ],
          [
            "monitoring",
            0.012269243298459484
          ],
          [
            "wildlife",
            0.011147682962939904
          ],
          [
            "animals",
            0.010326891291776856
          ],
          [
            "dataset",
            0.010116412988182097
          ],
          [
            "identification",
            0.009802980793790722
          ]
        ],
        "count": 642
      },
      "42": {
        "name": "42_pose_6D_estimation_object",
        "keywords": [
          [
            "pose",
            0.05914681253667477
          ],
          [
            "6D",
            0.045047108221554936
          ],
          [
            "estimation",
            0.032941892829383375
          ],
          [
            "object",
            0.02869928731183787
          ],
          [
            "object pose",
            0.02770508477470893
          ],
          [
            "Pose",
            0.025737614098349636
          ],
          [
            "Estimation",
            0.016821405995901286
          ],
          [
            "objects",
            0.016736185773624325
          ],
          [
            "RGB",
            0.014116639449881466
          ],
          [
            "Object",
            0.01324253921316319
          ]
        ],
        "count": 541
      },
      "43": {
        "name": "43_UAV_aerial_drone_detection",
        "keywords": [
          [
            "UAV",
            0.032836010592559334
          ],
          [
            "aerial",
            0.022840037722255763
          ],
          [
            "drone",
            0.021306253217085628
          ],
          [
            "detection",
            0.02099433188495138
          ],
          [
            "UAVs",
            0.017217277818188343
          ],
          [
            "Aerial",
            0.015200072218280432
          ],
          [
            "object",
            0.014323228702270733
          ],
          [
            "object detection",
            0.013661155816497947
          ],
          [
            "drones",
            0.012049704343019189
          ],
          [
            "Unmanned",
            0.009850102930726523
          ]
        ],
        "count": 537
      },
      "44": {
        "name": "44_face_Face_attacks_spoofing",
        "keywords": [
          [
            "face",
            0.05045080314955662
          ],
          [
            "Face",
            0.03439307390066558
          ],
          [
            "attacks",
            0.02931259346302832
          ],
          [
            "spoofing",
            0.028078085449508804
          ],
          [
            "attack",
            0.027332708111292656
          ],
          [
            "face recognition",
            0.021496654955989673
          ],
          [
            "recognition",
            0.019003533955043565
          ],
          [
            "systems",
            0.017541275720190652
          ],
          [
            "anti",
            0.01638795221234054
          ],
          [
            "Attack",
            0.015385244572929505
          ]
        ],
        "count": 527
      },
      "45": {
        "name": "45_audio_sound_visual_Audio",
        "keywords": [
          [
            "audio",
            0.08414580455432763
          ],
          [
            "sound",
            0.03835643264286662
          ],
          [
            "visual",
            0.027423512443904268
          ],
          [
            "Audio",
            0.02688041848630239
          ],
          [
            "music",
            0.025621814516086926
          ],
          [
            "video",
            0.01475041888365278
          ],
          [
            "sounds",
            0.013234449464483211
          ],
          [
            "Sound",
            0.012982889918405726
          ],
          [
            "Visual",
            0.01111797090901722
          ],
          [
            "generation",
            0.009730896294893
          ]
        ],
        "count": 513
      },
      "46": {
        "name": "46_video_segmentation_Video_VIS",
        "keywords": [
          [
            "video",
            0.02800555528497197
          ],
          [
            "segmentation",
            0.025234940691741106
          ],
          [
            "Video",
            0.023403477381377196
          ],
          [
            "VIS",
            0.021033454059611774
          ],
          [
            "Segmentation",
            0.02055645188150714
          ],
          [
            "object",
            0.02019159555758617
          ],
          [
            "Object Segmentation",
            0.019388466240646933
          ],
          [
            "object segmentation",
            0.01830786275528083
          ],
          [
            "frame",
            0.01799869212009351
          ],
          [
            "video object",
            0.01679728963199471
          ]
        ],
        "count": 511
      },
      "47": {
        "name": "47_motion_motions_motion generation_Motion",
        "keywords": [
          [
            "motion",
            0.08473446846251122
          ],
          [
            "motions",
            0.03201704145699238
          ],
          [
            "motion generation",
            0.028793987953222855
          ],
          [
            "Motion",
            0.02847377941506682
          ],
          [
            "human",
            0.027635368344572184
          ],
          [
            "human motion",
            0.02684724510584282
          ],
          [
            "generation",
            0.02109055880006847
          ],
          [
            "text",
            0.016861952376724575
          ],
          [
            "Human",
            0.013705934051569996
          ],
          [
            "motion synthesis",
            0.013071613650587136
          ]
        ],
        "count": 489
      },
      "48": {
        "name": "48_surface_shape_mesh_3D",
        "keywords": [
          [
            "surface",
            0.03401316053788646
          ],
          [
            "shape",
            0.028447712928492
          ],
          [
            "mesh",
            0.026557665320550792
          ],
          [
            "3D",
            0.02217853560787734
          ],
          [
            "point",
            0.021492248311664876
          ],
          [
            "implicit",
            0.01897170392102226
          ],
          [
            "surfaces",
            0.018876244061526614
          ],
          [
            "shapes",
            0.01833171950352753
          ],
          [
            "meshes",
            0.01743552841203414
          ],
          [
            "reconstruction",
            0.016975062046320877
          ]
        ],
        "count": 459
      },
      "49": {
        "name": "49_talking_audio_lip_facial",
        "keywords": [
          [
            "talking",
            0.05416324851922666
          ],
          [
            "audio",
            0.04378068094797651
          ],
          [
            "lip",
            0.04065541328092192
          ],
          [
            "facial",
            0.038005953375456425
          ],
          [
            "head",
            0.03235862485084215
          ],
          [
            "speech",
            0.025612857655607054
          ],
          [
            "face",
            0.02229738942579773
          ],
          [
            "generation",
            0.01752388830464964
          ],
          [
            "animation",
            0.01715943824426347
          ],
          [
            "motion",
            0.017032810290127598
          ]
        ],
        "count": 458
      },
      "50": {
        "name": "50_activation_networks_neural_activation functions",
        "keywords": [
          [
            "activation",
            0.026372992306469185
          ],
          [
            "networks",
            0.01856281001874763
          ],
          [
            "neural",
            0.01728855065554435
          ],
          [
            "activation functions",
            0.016241363943811937
          ],
          [
            "functions",
            0.015764239915404866
          ],
          [
            "network",
            0.014987786512769209
          ],
          [
            "layers",
            0.014793827167643877
          ],
          [
            "layer",
            0.013597064611433101
          ],
          [
            "neural networks",
            0.01345148428636277
          ],
          [
            "normalization",
            0.013247453321084544
          ]
        ],
        "count": 444
      },
      "51": {
        "name": "51_OOD_distribution_detection_Out",
        "keywords": [
          [
            "OOD",
            0.10318311881643459
          ],
          [
            "distribution",
            0.031103886756428046
          ],
          [
            "detection",
            0.02968558476435497
          ],
          [
            "Out",
            0.028496768722075873
          ],
          [
            "ID",
            0.02471194157407779
          ],
          [
            "samples",
            0.02090706298733762
          ],
          [
            "Distribution",
            0.020104757081082284
          ],
          [
            "OOD samples",
            0.016099198423791625
          ],
          [
            "Detection",
            0.012381950932071362
          ],
          [
            "class",
            0.012084358365382832
          ]
        ],
        "count": 425
      },
      "52": {
        "name": "52_anomaly_Anomaly_detection_video",
        "keywords": [
          [
            "anomaly",
            0.04329351550436364
          ],
          [
            "Anomaly",
            0.02681758190071402
          ],
          [
            "detection",
            0.0233481568757994
          ],
          [
            "video",
            0.021643901752138442
          ],
          [
            "anomalies",
            0.019378906287110743
          ],
          [
            "Video",
            0.01837757397571969
          ],
          [
            "surveillance",
            0.01756194956486612
          ],
          [
            "Detection",
            0.01741239854290876
          ],
          [
            "abnormal",
            0.016203694080918075
          ],
          [
            "normal",
            0.015522900841896383
          ]
        ],
        "count": 408
      },
      "53": {
        "name": "53_inpainting_Inpainting_image inpainting_image",
        "keywords": [
          [
            "inpainting",
            0.07509547753908052
          ],
          [
            "Inpainting",
            0.02962705600589146
          ],
          [
            "image inpainting",
            0.029208893381043658
          ],
          [
            "image",
            0.018516534306364876
          ],
          [
            "missing",
            0.013058778852431898
          ],
          [
            "regions",
            0.011950353040096992
          ],
          [
            "Image",
            0.011442120138045676
          ],
          [
            "diffusion",
            0.009875679368404815
          ],
          [
            "mask",
            0.009261133006040808
          ],
          [
            "images",
            0.008705932292435247
          ]
        ],
        "count": 392
      },
      "54": {
        "name": "54_sign_Sign_language_Language",
        "keywords": [
          [
            "sign",
            0.10118796802052579
          ],
          [
            "Sign",
            0.07308011336521421
          ],
          [
            "language",
            0.04614770694047224
          ],
          [
            "Language",
            0.03549398490139454
          ],
          [
            "SLR",
            0.01865445085377679
          ],
          [
            "signs",
            0.018184682074541308
          ],
          [
            "recognition",
            0.017455986210798455
          ],
          [
            "translation",
            0.016417175148179076
          ],
          [
            "Recognition",
            0.013257414241941344
          ],
          [
            "hand",
            0.011864427883487648
          ]
        ],
        "count": 392
      },
      "55": {
        "name": "55_domain_adaptation_target_source",
        "keywords": [
          [
            "domain",
            0.050258038655702615
          ],
          [
            "adaptation",
            0.02813712185775891
          ],
          [
            "target",
            0.0279395636890454
          ],
          [
            "source",
            0.02552825670530728
          ],
          [
            "semantic segmentation",
            0.024050538137930868
          ],
          [
            "UDA",
            0.023463407839253814
          ],
          [
            "Domain",
            0.02306759618311435
          ],
          [
            "target domain",
            0.022022958819315264
          ],
          [
            "domain adaptation",
            0.021489969049883064
          ],
          [
            "semantic",
            0.020822372869100613
          ]
        ],
        "count": 386
      },
      "56": {
        "name": "56_student_teacher_distillation_KD",
        "keywords": [
          [
            "student",
            0.0627586998553276
          ],
          [
            "teacher",
            0.06088656778621932
          ],
          [
            "distillation",
            0.0556229464904038
          ],
          [
            "KD",
            0.040903549792080614
          ],
          [
            "knowledge",
            0.04032324356387974
          ],
          [
            "Knowledge",
            0.03652346066018259
          ],
          [
            "Distillation",
            0.03332237768777429
          ],
          [
            "knowledge distillation",
            0.027122460480245372
          ],
          [
            "student model",
            0.015921326922336144
          ],
          [
            "Knowledge distillation",
            0.012704448030192399
          ]
        ],
        "count": 385
      },
      "57": {
        "name": "57_speech_audio_speaker_lip",
        "keywords": [
          [
            "speech",
            0.06936503474520488
          ],
          [
            "audio",
            0.043273205323196604
          ],
          [
            "speaker",
            0.03644053056764348
          ],
          [
            "lip",
            0.030029203365166762
          ],
          [
            "Speech",
            0.027689883102575558
          ],
          [
            "visual",
            0.0254613200260642
          ],
          [
            "Audio",
            0.016895685790189928
          ],
          [
            "speech recognition",
            0.015965126266822012
          ],
          [
            "reading",
            0.013599160734260674
          ],
          [
            "Visual",
            0.013554267050728885
          ]
        ],
        "count": 377
      }
    },
    "correlations": [
      [
        1.0,
        -0.7059456197652112,
        -0.7333313114182927,
        -0.7375878813891297,
        -0.7455171264329936,
        -0.7014675372705286,
        -0.7562069527237774,
        -0.7329196770243589,
        -0.7542197492567255,
        -0.7298269967855925,
        -0.7533249676488143,
        -0.7407395616627621,
        -0.7398834762850844,
        -0.7455891660739546,
        -0.7277270930635604,
        -0.7257037919975087,
        -0.7284086049251244,
        -0.661891334455791,
        -0.7280807649011296,
        -0.7000239440405198,
        -0.734104642298473,
        -0.7542146788269368,
        -0.41124838664096264,
        -0.7604104757492498,
        -0.6746971268456889,
        -0.7459835016019907,
        -0.6931172733257553,
        -0.7406008439209335,
        -0.7186489772233099,
        -0.7504409757688186,
        -0.7066665254330016,
        -0.6902449540988747,
        -0.7157862813198166,
        -0.7142060892461828,
        -0.7306827130274007,
        -0.7224779650761434,
        -0.7485611969401174,
        -0.7329086648058951,
        -0.7582596368153061,
        -0.7447636578042048,
        -0.7419786342906675,
        -0.7376035879668454,
        -0.7436611980306264,
        -0.7374799475040946,
        -0.7465830030858247,
        -0.7561549151900839,
        -0.5480335660196388,
        -0.746968120007266,
        -0.7215734103084468,
        -0.7499332143950592,
        -0.662365468180026,
        -0.7272157493589542,
        -0.7225027004798764,
        -0.7338107729505481,
        -0.7564497127826798,
        -0.5994146543911834,
        -0.7208277939188312,
        -0.7580682721130148
      ],
      [
        -0.7059456197652112,
        1.0,
        -0.7280281892129185,
        -0.7330500788818474,
        -0.7289517933072522,
        -0.47881970362829496,
        -0.7547621048187687,
        -0.7280276183176324,
        -0.735465856089508,
        -0.7376075279116063,
        -0.7324351644497415,
        -0.7256282310085619,
        -0.7481056607947547,
        -0.7197805295597126,
        -0.7486678609950186,
        -0.7288575697084335,
        -0.728396083359442,
        -0.7161610429412191,
        -0.7323485016685183,
        -0.4322213860625823,
        -0.7399511892457652,
        -0.747013221916657,
        -0.7209819775390003,
        -0.7589747466573142,
        -0.7151210897545734,
        -0.7176157731133319,
        -0.7192628950914279,
        -0.7441073508339093,
        -0.7398620057333174,
        -0.7532957847010526,
        -0.7009982784202937,
        -0.6874077180616394,
        -0.7309979324244698,
        -0.7384964598645419,
        -0.7422517087498841,
        -0.7387876144525325,
        -0.7244782228858333,
        -0.7499571167446453,
        -0.7565703615717801,
        -0.73484117744863,
        -0.7341334736498855,
        -0.743754011624237,
        -0.7373658290542413,
        -0.7408069460019967,
        -0.7354777199429179,
        -0.7532509295168348,
        -0.728329725612427,
        -0.7362999594508975,
        -0.7363164542695304,
        -0.7511234323246878,
        -0.6873864557421561,
        -0.74474616726636,
        -0.746347308731522,
        -0.3336389846282717,
        -0.7499063719825448,
        -0.7123174600852862,
        -0.7314712108376791,
        -0.753893219430569
      ],
      [
        -0.7333313114182927,
        -0.7280281892129185,
        1.0,
        -0.7429484890162054,
        -0.6888469309396246,
        -0.6294693263663098,
        -0.7169136236335709,
        -0.7362179600858084,
        -0.7516085818876012,
        -0.7350594773130873,
        -0.6753345854447133,
        -0.7386388839869322,
        -0.7282500131847991,
        -0.6122977739110143,
        -0.7520506322330547,
        -0.7282623310637745,
        -0.73696176939683,
        -0.7336677102225464,
        -0.7398058812809272,
        -0.7289833889953495,
        -0.7443020074011127,
        -0.7404116666430065,
        -0.7407088736967752,
        -0.7601503666430722,
        -0.6969141307438266,
        -0.6968621983105154,
        -0.7177545131816401,
        -0.7436748525383651,
        -0.7328438818186347,
        -0.7531518223142243,
        -0.4243793660871683,
        -0.6953955051914966,
        -0.647014493272925,
        -0.7471227545695205,
        -0.7488693347435227,
        -0.7275910508039309,
        -0.7580027410517125,
        -0.7381150882324379,
        -0.7534206836615727,
        -0.7454525321986973,
        -0.7301382869638624,
        -0.7507211236625944,
        -0.7473731384077247,
        -0.7491281799787923,
        -0.7352891129611172,
        -0.6685725868454493,
        -0.7360017697220607,
        -0.7424937728079684,
        -0.7505888127615062,
        -0.7434982249626086,
        -0.7491400233451497,
        -0.7416948087808899,
        -0.7455021833949196,
        -0.7306697429367099,
        -0.5604831259604142,
        -0.7069860222978848,
        -0.7160077160090756,
        -0.6686164856855561
      ],
      [
        -0.7375878813891297,
        -0.7330500788818474,
        -0.7429484890162054,
        1.0,
        -0.7531613124461637,
        -0.739992235933497,
        -0.7580906833485646,
        -0.7551647656020715,
        -0.7538002673744141,
        -0.7305067954219367,
        -0.757855908144405,
        -0.7491313469005427,
        -0.7488960899863453,
        -0.7574263066064455,
        -0.745104219648219,
        -0.7210050950242034,
        -0.7502496043835354,
        -0.72757970311153,
        -0.7493165589350301,
        -0.7436000633330739,
        -0.7390279127252009,
        -0.7575038189918987,
        -0.7083663679212611,
        -0.7630089077669115,
        -0.7274357631245734,
        -0.7558002319005905,
        -0.7495383726238924,
        -0.7556322732288405,
        -0.7538258977695553,
        -0.7536775654033998,
        -0.7424328419059234,
        -0.7331567376745505,
        -0.7440945411693385,
        -0.7460980640583805,
        -0.7334747562374107,
        -0.7525709951978193,
        -0.6733787934810282,
        -0.7551489943023368,
        -0.7548200298290278,
        -0.7567064139252444,
        -0.7500530316744484,
        -0.729392917160048,
        -0.7502795032957603,
        -0.6909388945431532,
        -0.7553811414838631,
        -0.7604480377108604,
        -0.7446676981862822,
        -0.7579081430126241,
        -0.7390796954761909,
        -0.7591110572719076,
        -0.7287910199107215,
        -0.7471916358125668,
        -0.744738962311157,
        -0.7493133576850282,
        -0.7585424939304897,
        -0.7128367503177999,
        -0.74861005420395,
        -0.7614927697630127
      ],
      [
        -0.7455171264329936,
        -0.7289517933072522,
        -0.6888469309396246,
        -0.7531613124461637,
        1.0,
        -0.726828156133914,
        -0.6848956890478379,
        -0.7409449055337631,
        -0.7247111537527786,
        -0.7110576064144343,
        -0.7333382026947963,
        -0.7211384294133785,
        -0.6995512792923675,
        -0.7457713687594616,
        -0.7492213191140412,
        -0.7213897270911123,
        -0.7465454789580985,
        -0.7205131156487644,
        -0.7043373048046949,
        -0.7373342764762348,
        -0.7479027539889749,
        -0.7524672179714182,
        -0.7390750699197814,
        -0.7321722805910099,
        -0.7225489802116771,
        -0.7240872009871284,
        -0.7096665505807476,
        -0.7457619294887338,
        -0.7506245985469232,
        -0.7346817867809521,
        -0.7278537960227123,
        -0.7339448518622715,
        -0.7221433032495745,
        -0.7318031485423491,
        -0.7250575708496904,
        -0.7160564631727122,
        -0.7575418276668412,
        -0.7391121863627598,
        -0.7536155740011998,
        -0.7475205215625687,
        -0.7418479415212764,
        -0.7458363598455506,
        -0.7043453142677385,
        -0.7367320075113821,
        -0.745752820831016,
        -0.7441951974245321,
        -0.7326935052472711,
        -0.7176963048966563,
        -0.7260127069050842,
        -0.7516798819091594,
        -0.7365629385795772,
        -0.7430190114287198,
        -0.7446542573845802,
        -0.7485015346625921,
        -0.7340070391448087,
        -0.7240580814707774,
        -0.7404863609359921,
        -0.7455808891135249
      ],
      [
        -0.7014675372705286,
        -0.47881970362829496,
        -0.6294693263663098,
        -0.739992235933497,
        -0.726828156133914,
        1.0,
        -0.7401959908957498,
        -0.7026742822726832,
        -0.7192642580808839,
        -0.7180870905628038,
        -0.705846577492121,
        -0.7199180791174259,
        -0.7248770811594809,
        -0.4908700130127134,
        -0.7397005015245238,
        -0.7138378758490838,
        -0.7259878784656086,
        -0.7072388381819759,
        -0.7106551082044912,
        -0.4682917436275533,
        -0.718775612848669,
        -0.7331902466916795,
        -0.707489587324175,
        -0.759969088711981,
        -0.686255884059513,
        -0.5392675789119884,
        -0.4649752455109226,
        -0.7334834488182029,
        -0.7322584884632172,
        -0.7485490225096763,
        -0.09612756245395114,
        -0.6828503717549611,
        -0.6878583783067851,
        -0.7286014735707131,
        -0.7296330250232095,
        -0.7093381559068686,
        -0.7423496554321839,
        -0.739068882150156,
        -0.7564021500055089,
        -0.7043073973210341,
        -0.7147784762805992,
        -0.7404019316941245,
        -0.7224045398057344,
        -0.7414483655025196,
        -0.7136492395741698,
        -0.7334283683175202,
        -0.7163990943177341,
        -0.6960282494906767,
        -0.7177295032498907,
        -0.7296351763597466,
        -0.7055716666248986,
        -0.7268953527425486,
        -0.7368742685000047,
        -0.4901461611819724,
        -0.7225931274166895,
        -0.6849281891019946,
        -0.7029069996919922,
        -0.7340085125033291
      ],
      [
        -0.7562069527237774,
        -0.7547621048187687,
        -0.7169136236335709,
        -0.7580906833485646,
        -0.6848956890478379,
        -0.7401959908957498,
        1.0,
        -0.7530609783669153,
        -0.7536847576878116,
        -0.7393663523149441,
        -0.5328019954073921,
        -0.7416775205768225,
        -0.723402401303129,
        -0.706129866117486,
        -0.7509221484172793,
        -0.7375809270276591,
        -0.7490586958937095,
        -0.7320283080565737,
        -0.7134251370135589,
        -0.7516033136127365,
        -0.7500500685996063,
        -0.6823765182802437,
        -0.748401434313474,
        -0.7598454255663042,
        -0.736130542869643,
        -0.576558224169545,
        -0.7410110199632967,
        -0.7370649553731059,
        -0.7516186378517583,
        -0.741731613509051,
        -0.7364573520935207,
        -0.7266760245611776,
        -0.7221865200288684,
        -0.7492147571511499,
        -0.747371427062222,
        -0.7169498158762456,
        -0.7542672864854538,
        -0.749477045189141,
        -0.7369497579356763,
        -0.7371616645757588,
        -0.6830822001746355,
        -0.7499092957315621,
        -0.729156596206286,
        -0.7444799629707175,
        -0.7174333433738086,
        -0.7389031051034174,
        -0.6682639997232868,
        -0.6838772292208024,
        -0.7528204317782314,
        -0.7330462556474111,
        -0.7409765111412625,
        -0.7540801635774268,
        -0.6914570821633808,
        -0.7564615179535383,
        -0.7379064590259695,
        -0.7371665389107405,
        -0.7395029061021525,
        -0.7420409507374789
      ],
      [
        -0.7329196770243589,
        -0.7280276183176324,
        -0.7362179600858084,
        -0.7551647656020715,
        -0.7409449055337631,
        -0.7026742822726832,
        -0.7530609783669153,
        1.0,
        -0.7509200377982455,
        -0.7316620481113701,
        -0.7497231610800752,
        -0.7455253786681078,
        -0.7338866442347649,
        -0.7454884490857212,
        -0.7453910083673585,
        -0.7266823512029534,
        -0.7193345714229287,
        -0.728136161636649,
        -0.7415340211515982,
        -0.6798902888778227,
        -0.7222986091052518,
        -0.7456439354116258,
        -0.7376164286280411,
        -0.7598153094974034,
        -0.708201052142825,
        -0.7411424284278545,
        -0.7390064674186849,
        -0.7457784034747597,
        -0.7148920879369709,
        -0.7525821625161375,
        -0.3642190739549391,
        -0.7298836731411591,
        -0.7396096807813127,
        -0.7438288719841623,
        -0.7401051017857341,
        -0.7449304141808231,
        -0.7531454520188251,
        -0.74873321250752,
        -0.7548219675360046,
        -0.7281024346583452,
        -0.7143242415076532,
        -0.7506488859569256,
        -0.7453641325623677,
        -0.7437410215221119,
        -0.32565365043001443,
        -0.7523248698319066,
        -0.7476205168956689,
        -0.7491399376590808,
        -0.7434982394103571,
        -0.7473123955689953,
        -0.6934855964339321,
        -0.7321409649255064,
        -0.7438446776360697,
        -0.734250335283045,
        -0.7406662213120991,
        -0.709621252297393,
        -0.7328561100862572,
        -0.7526551981623972
      ],
      [
        -0.7542197492567255,
        -0.735465856089508,
        -0.7516085818876012,
        -0.7538002673744141,
        -0.7247111537527786,
        -0.7192642580808839,
        -0.7536847576878116,
        -0.7509200377982455,
        1.0,
        -0.6968050720206284,
        -0.7368191286122767,
        -0.675330665971777,
        -0.7254409053009775,
        -0.7501301303368898,
        -0.7591150317652466,
        -0.7388616464125309,
        -0.7399613048689866,
        -0.7427198100732455,
        -0.6928098621428962,
        -0.7337763655679381,
        -0.759400621787876,
        -0.7521553465602978,
        -0.7454986065511022,
        -0.7290773186627062,
        -0.7417221621164982,
        -0.7180986452791014,
        -0.47140105302822083,
        -0.7523459771403729,
        -0.7568753068549767,
        -0.7369319371840306,
        -0.7400685010080641,
        -0.7474726696272367,
        -0.7418080497862495,
        -0.7154812427119913,
        -0.6960029780584738,
        -0.6760961966665503,
        -0.7520231070534242,
        -0.7550044925726183,
        -0.7527577673123496,
        -0.7194445846378956,
        -0.7444542316860614,
        -0.7500534731426984,
        -0.6922812130744979,
        -0.7457834220716074,
        -0.7472062475424153,
        -0.7554811675203795,
        -0.7348047781168203,
        -0.7074347967765592,
        -0.6378706683502697,
        -0.74006876950014,
        -0.7383587434402401,
        -0.7543787363834433,
        -0.7565791562839375,
        -0.7314009067874033,
        -0.7576907761993493,
        -0.7397039239576504,
        -0.7430915001571432,
        -0.7569600609961148
      ],
      [
        -0.7298269967855925,
        -0.7376075279116063,
        -0.7350594773130873,
        -0.7305067954219367,
        -0.7110576064144343,
        -0.7180870905628038,
        -0.7393663523149441,
        -0.7316620481113701,
        -0.6968050720206284,
        1.0,
        -0.7360789658715051,
        -0.6605675862820295,
        -0.4641408423806446,
        -0.7438589883506456,
        -0.6201043928864223,
        -0.073097715350393,
        -0.735985961355093,
        -0.7220500168114851,
        -0.65409116898642,
        -0.7337594460652093,
        -0.6092212101673578,
        -0.7506396955313819,
        -0.674238948357716,
        -0.7182542860918824,
        -0.7071922252864404,
        -0.7247217248095325,
        -0.5233268151675468,
        -0.7416927545932126,
        -0.7431706108944299,
        -0.6567432607448764,
        -0.721140920081923,
        -0.7121986362147932,
        -0.7269675222393528,
        -0.5669580941408343,
        -0.42586503532215236,
        -0.6741269835027586,
        -0.7459380421444366,
        -0.7443368689358538,
        -0.7464797911338232,
        -0.6882622778610363,
        -0.7375649447835646,
        -0.5554447712636725,
        -0.6383349036133594,
        -0.3752308705665488,
        -0.7353436451876205,
        -0.7541998645688579,
        -0.6416340779709826,
        -0.7064170520300633,
        -0.5530197329490019,
        -0.742664521296247,
        -0.7213107121924832,
        -0.6645975344552708,
        -0.6308931515605991,
        -0.7355610687050749,
        -0.7476278665595395,
        -0.6783829835601936,
        -0.7173963221973987,
        -0.7558969874742214
      ],
      [
        -0.7533249676488143,
        -0.7324351644497415,
        -0.6753345854447133,
        -0.757855908144405,
        -0.7333382026947963,
        -0.705846577492121,
        -0.5328019954073921,
        -0.7497231610800752,
        -0.7368191286122767,
        -0.7360789658715051,
        1.0,
        -0.7245372043946445,
        -0.7265600991765131,
        -0.7152603450083317,
        -0.7416523841207766,
        -0.7288378488457057,
        -0.7430626066089441,
        -0.7311904773684316,
        -0.7228905813519855,
        -0.7395973714044803,
        -0.7339013327066628,
        -0.7280467734509353,
        -0.7419411587496583,
        -0.7591179313951583,
        -0.728776680578769,
        0.23295973087059058,
        -0.7188404901650064,
        -0.7299434216526735,
        -0.7530010107143112,
        -0.7116090250046894,
        -0.7119255218236025,
        -0.7158690664439507,
        -0.7273707559294675,
        -0.7442066046803057,
        -0.7458733702799365,
        -0.7251161326757697,
        -0.7503971912741978,
        -0.7523537470036651,
        -0.712309145898315,
        -0.724892883322021,
        -0.7271212101042701,
        -0.745818589468547,
        -0.7298793601910629,
        -0.7414562166982059,
        -0.7368025886711597,
        -0.68273763469571,
        -0.47182051263956737,
        -0.6530740861969808,
        -0.748250773971704,
        -0.6899615644049701,
        -0.7403395082586735,
        -0.7519133018334325,
        -0.6055280803239926,
        -0.740044981344046,
        -0.7161801544560131,
        -0.729570795966249,
        -0.7365193026096397,
        -0.6946422012852305
      ],
      [
        -0.7407395616627621,
        -0.7256282310085619,
        -0.7386388839869322,
        -0.7491313469005427,
        -0.7211384294133785,
        -0.7199180791174259,
        -0.7416775205768225,
        -0.7455253786681078,
        -0.675330665971777,
        -0.6605675862820295,
        -0.7245372043946445,
        1.0,
        -0.7077903834125654,
        -0.750627926887555,
        -0.7509220888249685,
        -0.7106919975143327,
        -0.735609928161298,
        -0.7251518964756062,
        -0.6443909835797286,
        -0.7359985241806963,
        -0.7508201726671907,
        -0.7520353199996519,
        -0.7203475934440466,
        -0.7245719220982113,
        -0.725842085242451,
        -0.7166534818192809,
        -0.6702753588010766,
        -0.7497995116773778,
        -0.7497841666777179,
        -0.7277450665573697,
        -0.7293779481447211,
        -0.724646186438985,
        -0.738604503589517,
        -0.6971884828774098,
        -0.7004123050311916,
        -0.7250643139342929,
        -0.7489553335624095,
        -0.7535996648806245,
        -0.736128735398724,
        -0.7421470214202408,
        -0.7438366790260666,
        -0.737070021710782,
        -0.6238424314484808,
        -0.7321252632705891,
        -0.7441168162931913,
        -0.7530485985476365,
        -0.7187321847571846,
        -0.6852133581718873,
        -0.688986813299922,
        -0.7490488843886742,
        -0.7167465282418154,
        -0.7524412863466616,
        -0.7465070270072105,
        -0.731922994531836,
        -0.7559216223934047,
        -0.7155025064205223,
        -0.7350086583233152,
        -0.7557837424414817
      ],
      [
        -0.7398834762850844,
        -0.7481056607947547,
        -0.7282500131847991,
        -0.7488960899863453,
        -0.6995512792923675,
        -0.7248770811594809,
        -0.723402401303129,
        -0.7338866442347649,
        -0.7254409053009775,
        -0.4641408423806446,
        -0.7265600991765131,
        -0.7077903834125654,
        1.0,
        -0.7524050569419073,
        -0.7387984888341581,
        -0.6784828911788857,
        -0.7427640585516824,
        -0.732427689523642,
        -0.7207282488182618,
        -0.7379383773276169,
        -0.7446862828450294,
        -0.7497020217541839,
        -0.7152851953087753,
        -0.7362590948224452,
        -0.7244864432197756,
        -0.7127999185993746,
        -0.7057213020474529,
        -0.739200938466575,
        -0.7386773686379002,
        -0.7068495690692294,
        -0.7190602885970119,
        -0.7276961294694342,
        -0.7456385961617795,
        -0.7235252530837295,
        -0.702357005586852,
        -0.7345813842906215,
        -0.7539197104887609,
        -0.7495267116446227,
        -0.7439363743618068,
        -0.7455400071840439,
        -0.7408031223300812,
        -0.7381631056547214,
        -0.724550071650925,
        -0.7075472842459241,
        -0.7411961108071229,
        -0.7552920296985102,
        -0.7294079235036754,
        -0.6667726419800237,
        -0.7239100154193556,
        -0.747627517074157,
        -0.7259089098690582,
        -0.7408407306857667,
        -0.7323759636905501,
        -0.750310216574253,
        -0.7433957127503699,
        -0.7095508331769722,
        -0.7313706624028791,
        -0.7564601214932738
      ],
      [
        -0.7455891660739546,
        -0.7197805295597126,
        -0.6122977739110143,
        -0.7574263066064455,
        -0.7457713687594616,
        -0.4908700130127134,
        -0.706129866117486,
        -0.7454884490857212,
        -0.7501301303368898,
        -0.7438589883506456,
        -0.7152603450083317,
        -0.750627926887555,
        -0.7524050569419073,
        1.0,
        -0.7484153701903513,
        -0.7301124000752244,
        -0.747923076675193,
        -0.7358042675942738,
        -0.7424977054266912,
        -0.7173551832310038,
        -0.7345589514088775,
        -0.6896268120791635,
        -0.7398879080518476,
        -0.7602511706368225,
        -0.7226390109041286,
        -0.7008680888927747,
        -0.7145124458740042,
        -0.736529397316597,
        -0.7502703615197308,
        -0.7555481579892716,
        -0.6272165284264535,
        -0.7174176872301159,
        -0.7138983980593081,
        -0.7480900780655033,
        -0.751743951604904,
        -0.7396094594442535,
        -0.7597929135205022,
        -0.7529948273350697,
        -0.7591489164362097,
        -0.7377465739566418,
        -0.6605829422812866,
        -0.7519211820630012,
        -0.7503776385009924,
        -0.7485930383199134,
        -0.7055918368784744,
        -0.7388897840715423,
        -0.7400761245350627,
        -0.7407871817528443,
        -0.7429850116278431,
        -0.7438592840611911,
        -0.7387698239562678,
        -0.7507027835441562,
        -0.7478130411222694,
        -0.7214809730865643,
        -0.7046037488794017,
        -0.7236048024425539,
        -0.733070311241038,
        -0.7335467619151981
      ],
      [
        -0.7277270930635604,
        -0.7486678609950186,
        -0.7520506322330547,
        -0.745104219648219,
        -0.7492213191140412,
        -0.7397005015245238,
        -0.7509221484172793,
        -0.7453910083673585,
        -0.7591150317652466,
        -0.6201043928864223,
        -0.7416523841207766,
        -0.7509220888249685,
        -0.7387984888341581,
        -0.7484153701903513,
        1.0,
        -0.5683184469820229,
        -0.7526112131891948,
        -0.7376081709558744,
        -0.745350265470029,
        -0.7401648897508694,
        -0.46560594102100705,
        -0.7552142134105014,
        -0.7298868923576862,
        -0.7609122930945015,
        -0.7384342661347821,
        -0.7412040568005391,
        -0.7443387195707101,
        -0.7453615436169121,
        -0.7495329930233265,
        -0.7454911959240889,
        -0.7405988294983572,
        -0.7409746275529341,
        -0.7370184977128119,
        -0.749676043961621,
        -0.7426341050827076,
        -0.7463332193009327,
        -0.7503235100684493,
        -0.7514734165182388,
        -0.752955372758328,
        -0.7481199635918157,
        -0.7444964524771267,
        -0.5815271304460121,
        -0.7510287546468095,
        -0.5434183073245734,
        -0.7430448744890389,
        -0.7562350194847463,
        -0.731663676975614,
        -0.7482842178655507,
        -0.7311814524206489,
        -0.7502015989675146,
        -0.7345341245616162,
        -0.6051068809124757,
        0.2537772202666506,
        -0.7457883827850463,
        -0.759934661228392,
        -0.7325004036538055,
        -0.7446938102973999,
        -0.7565530790533396
      ],
      [
        -0.7257037919975087,
        -0.7288575697084335,
        -0.7282623310637745,
        -0.7210050950242034,
        -0.7213897270911123,
        -0.7138378758490838,
        -0.7375809270276591,
        -0.7266823512029534,
        -0.7388616464125309,
        -0.073097715350393,
        -0.7288378488457057,
        -0.7106919975143327,
        -0.6784828911788857,
        -0.7301124000752244,
        -0.5683184469820229,
        1.0,
        -0.7296373460589574,
        -0.7113257672976203,
        -0.7142043324808136,
        -0.7289027867122294,
        -0.5434161997226372,
        -0.7493374305303515,
        -0.6890827969058693,
        -0.7528035831167654,
        -0.7049746256423585,
        -0.7250524174006937,
        -0.7081540886602985,
        -0.7348827779796251,
        -0.7391068846857234,
        -0.6539562834151202,
        -0.7132091265723844,
        -0.6888305266800409,
        -0.7081580744200322,
        -0.7171923236759827,
        -0.6964126066390572,
        -0.7310686093887686,
        -0.7487997352950984,
        -0.7372907037485901,
        -0.7418774949969624,
        -0.7448187303860458,
        -0.7352330224330894,
        -0.5012800922137085,
        -0.6538060171245388,
        -0.23920873607093762,
        -0.7319571406215798,
        -0.7501114845071084,
        -0.6236486028231452,
        -0.7317563266757645,
        -0.7129317609479906,
        -0.7351077357168041,
        -0.7048922129558286,
        -0.6455943149354156,
        -0.5352068211636087,
        -0.7274395958302461,
        -0.7461242782785251,
        -0.6902614914698022,
        -0.7140072103310794,
        -0.7519763137923064
      ],
      [
        -0.7284086049251244,
        -0.728396083359442,
        -0.73696176939683,
        -0.7502496043835354,
        -0.7465454789580985,
        -0.7259878784656086,
        -0.7490586958937095,
        -0.7193345714229287,
        -0.7399613048689866,
        -0.735985961355093,
        -0.7430626066089441,
        -0.735609928161298,
        -0.7427640585516824,
        -0.747923076675193,
        -0.7526112131891948,
        -0.7296373460589574,
        1.0,
        -0.7292902407127415,
        -0.7392617642883568,
        -0.7348459446421443,
        -0.7520110452473803,
        -0.7543066616747256,
        -0.733607968729618,
        -0.7597491638555729,
        -0.7289236193915666,
        -0.7400033247276596,
        -0.739524162067336,
        -0.7471893112490335,
        -0.5435169245117137,
        -0.7520785680247177,
        -0.7127466777168266,
        -0.7049563567317374,
        -0.7301981435276641,
        -0.7371967415773814,
        -0.7333408709931389,
        -0.7454756586414111,
        -0.7510910241693403,
        -0.7411239937829269,
        -0.7446258322257753,
        -0.7508861828365792,
        -0.7447090917556027,
        -0.7494478317222228,
        -0.7457575424279638,
        -0.7415823310948646,
        -0.7375969200091501,
        -0.7549293617904864,
        -0.7414194748040643,
        -0.7508550488808174,
        -0.7381033364422581,
        -0.7525805051257968,
        -0.38884622131911656,
        -0.7449242610922964,
        -0.7506130596565803,
        -0.7450746535866997,
        -0.7527545619440505,
        -0.7264683519391855,
        -0.7231292448280093,
        -0.7535799532600379
      ],
      [
        -0.661891334455791,
        -0.7161610429412191,
        -0.7336677102225464,
        -0.72757970311153,
        -0.7205131156487644,
        -0.7072388381819759,
        -0.7320283080565737,
        -0.728136161636649,
        -0.7427198100732455,
        -0.7220500168114851,
        -0.7311904773684316,
        -0.7251518964756062,
        -0.732427689523642,
        -0.7358042675942738,
        -0.7376081709558744,
        -0.7113257672976203,
        -0.7292902407127415,
        1.0,
        -0.7282215777993403,
        -0.70786254587194,
        -0.7447485397974691,
        -0.7398623510732549,
        -0.6797385134711669,
        -0.7583942661262664,
        -0.6893829008116199,
        -0.7306780757569739,
        -0.7188065344765653,
        -0.7365061855675901,
        -0.7369403671558763,
        -0.748385255311119,
        -0.7184045229045086,
        -0.702238766038367,
        -0.6358021239295284,
        -0.7280563136271936,
        -0.7283051819188664,
        -0.7361269279644129,
        -0.7456189476116188,
        -0.6307220565589959,
        -0.7546957098177708,
        -0.7421678358044297,
        -0.7361748826684765,
        -0.7369814076048549,
        -0.7351444704681346,
        -0.7371919739658392,
        -0.7424139209712985,
        -0.7452118052351817,
        -0.7096251526616142,
        -0.7393787541964969,
        -0.7330586877078407,
        -0.7360563960776909,
        -0.7006397464219303,
        -0.7222793951450844,
        -0.7339044139496452,
        -0.728099620610072,
        -0.7512444497165293,
        -0.6800286342426569,
        -0.6977428465753586,
        -0.7472709578230737
      ],
      [
        -0.7280807649011296,
        -0.7323485016685183,
        -0.7398058812809272,
        -0.7493165589350301,
        -0.7043373048046949,
        -0.7106551082044912,
        -0.7134251370135589,
        -0.7415340211515982,
        -0.6928098621428962,
        -0.65409116898642,
        -0.7228905813519855,
        -0.6443909835797286,
        -0.7207282488182618,
        -0.7424977054266912,
        -0.745350265470029,
        -0.7142043324808136,
        -0.7392617642883568,
        -0.7282215777993403,
        1.0,
        -0.7248371195521477,
        -0.7454849289901462,
        -0.7303699751494859,
        -0.7248793023617437,
        -0.7193765491136027,
        -0.7236034569383851,
        -0.7073286774620431,
        -0.600363354437775,
        -0.694641919818038,
        -0.7457963868466484,
        -0.7069036214450889,
        -0.7261535440945506,
        -0.7255578811265451,
        -0.7353448955580717,
        -0.6919882704190256,
        -0.6783743987309455,
        -0.22129103758389138,
        -0.7542850003119907,
        -0.7500482242193727,
        -0.749488822923633,
        -0.6679157749389972,
        -0.7175355152087378,
        -0.7338557280806457,
        0.2039664618564338,
        -0.732467961070656,
        -0.7269116788406107,
        -0.7457341911414271,
        -0.7138966360365187,
        -0.5836555151692137,
        -0.631724460577924,
        -0.720341655449076,
        -0.7194197765033808,
        -0.7450179247822624,
        -0.7370033297435262,
        -0.7348579108577352,
        -0.7406538218961156,
        -0.7189861515292149,
        -0.7365547156030496,
        -0.7467284363453922
      ],
      [
        -0.7000239440405198,
        -0.4322213860625823,
        -0.7289833889953495,
        -0.7436000633330739,
        -0.7373342764762348,
        -0.4682917436275533,
        -0.7516033136127365,
        -0.6798902888778227,
        -0.7337763655679381,
        -0.7337594460652093,
        -0.7395973714044803,
        -0.7359985241806963,
        -0.7379383773276169,
        -0.7173551832310038,
        -0.7401648897508694,
        -0.7289027867122294,
        -0.7348459446421443,
        -0.70786254587194,
        -0.7248371195521477,
        1.0,
        -0.7078128164425316,
        -0.7326550417075395,
        -0.7167729623220067,
        -0.760224802448616,
        -0.687414134899881,
        -0.7048723637977816,
        -0.6847376001748748,
        -0.7404855842577447,
        -0.7340687726669123,
        -0.7544014460079487,
        -0.6773604846382684,
        -0.718334503903111,
        -0.7190471890547914,
        -0.733307971572631,
        -0.7390377716059819,
        -0.7274838319459644,
        -0.7386853238298814,
        -0.7432827773114241,
        -0.7573675118871325,
        -0.693882629816521,
        -0.713619258187854,
        -0.7441063354130775,
        -0.7300563170922335,
        -0.7452535819809698,
        -0.7121570862275192,
        -0.7485052876739946,
        -0.7313422004207009,
        -0.7331798283422362,
        -0.7246455058253325,
        -0.7354336254502545,
        -0.7134947472268982,
        -0.7325072033451501,
        -0.7387202834098343,
        -0.45369742349805453,
        -0.7499151986300636,
        -0.6871331009047825,
        -0.7280608767721534,
        -0.7491865678163924
      ],
      [
        -0.734104642298473,
        -0.7399511892457652,
        -0.7443020074011127,
        -0.7390279127252009,
        -0.7479027539889749,
        -0.718775612848669,
        -0.7500500685996063,
        -0.7222986091052518,
        -0.759400621787876,
        -0.6092212101673578,
        -0.7339013327066628,
        -0.7508201726671907,
        -0.7446862828450294,
        -0.7345589514088775,
        -0.46560594102100705,
        -0.5434161997226372,
        -0.7520110452473803,
        -0.7447485397974691,
        -0.7454849289901462,
        -0.7078128164425316,
        1.0,
        -0.731551064883186,
        -0.739263583522397,
        -0.7602702725484685,
        -0.7354600729853826,
        -0.7251535907106051,
        -0.7456787735747623,
        -0.7421089217057542,
        -0.7442440358107358,
        -0.7430677357097166,
        -0.7229385928941188,
        -0.7383586125394672,
        -0.7468255792084009,
        -0.7534800357523452,
        -0.7489940367754088,
        -0.7442304016233598,
        -0.7485273464516327,
        -0.7541352071737492,
        -0.7580288395964487,
        -0.6929730473668915,
        -0.6954472698835885,
        -0.5604851880254809,
        -0.7494792301384006,
        -0.5251532983690992,
        -0.6862179316342537,
        -0.7352369219795543,
        -0.7335835228035184,
        -0.7501057804888724,
        -0.7499666120140251,
        -0.7182639377114258,
        -0.7356875025078291,
        -0.5955395862683109,
        -0.5065001170215695,
        -0.7400233367342903,
        -0.7571550169017217,
        -0.7342195072310472,
        -0.7482607133141055,
        -0.7408399582242554
      ],
      [
        -0.7542146788269368,
        -0.747013221916657,
        -0.7404116666430065,
        -0.7575038189918987,
        -0.7524672179714182,
        -0.7331902466916795,
        -0.6823765182802437,
        -0.7456439354116258,
        -0.7521553465602978,
        -0.7506396955313819,
        -0.7280467734509353,
        -0.7520353199996519,
        -0.7497020217541839,
        -0.6896268120791635,
        -0.7552142134105014,
        -0.7493374305303515,
        -0.7543066616747256,
        -0.7398623510732549,
        -0.7303699751494859,
        -0.7326550417075395,
        -0.731551064883186,
        1.0,
        -0.7524258104717895,
        -0.7608571954159996,
        -0.7375246211473339,
        -0.7241459647126429,
        -0.739103292741107,
        -0.7312498595215713,
        -0.7525024096130982,
        -0.7517699394363475,
        -0.7363359570823745,
        -0.7356377106624479,
        -0.746113900397136,
        -0.7543090277998463,
        -0.752099592765581,
        -0.7243973858039567,
        -0.755841414353449,
        -0.756111736138172,
        -0.7555560194446028,
        -0.2112182674586627,
        -0.5113252556097008,
        -0.7523993546180073,
        -0.737652694203708,
        -0.7545031120310524,
        -0.5799389935890111,
        -0.7028255349578479,
        -0.7430657864149813,
        -0.7338534891019698,
        -0.7468866578435126,
        -0.36315479533615347,
        -0.7399087239417399,
        -0.7531345556506468,
        -0.7490801556454807,
        -0.7466585769561884,
        -0.7356177424966693,
        -0.7387273297929351,
        -0.7428125282243667,
        -0.6899623593566162
      ],
      [
        -0.41124838664096264,
        -0.7209819775390003,
        -0.7407088736967752,
        -0.7083663679212611,
        -0.7390750699197814,
        -0.707489587324175,
        -0.748401434313474,
        -0.7376164286280411,
        -0.7454986065511022,
        -0.674238948357716,
        -0.7419411587496583,
        -0.7203475934440466,
        -0.7152851953087753,
        -0.7398879080518476,
        -0.7298868923576862,
        -0.6890827969058693,
        -0.733607968729618,
        -0.6797385134711669,
        -0.7248793023617437,
        -0.7167729623220067,
        -0.739263583522397,
        -0.7524258104717895,
        1.0,
        -0.756351081128771,
        -0.6783642670420508,
        -0.736676225860962,
        -0.6982377757727178,
        -0.7427357662963141,
        -0.7413736839080671,
        -0.7385287085023549,
        -0.7196630721244957,
        -0.6815312895088965,
        -0.7012643509092136,
        -0.7097833106443937,
        -0.6545326021837723,
        -0.7231906711341409,
        -0.7449872790010007,
        -0.7357089627962434,
        -0.7558877053217912,
        -0.7454875618856484,
        -0.7424674863137195,
        -0.7302207170299824,
        -0.7354167969069743,
        -0.7076384602769364,
        -0.7489276188533218,
        -0.7565441446424155,
        -0.20745876528658513,
        -0.7427425353003286,
        -0.69598748228706,
        -0.7436468644859682,
        -0.6883123085484184,
        -0.7355184942768087,
        -0.7327084852768811,
        -0.7256648089078943,
        -0.7533904680679768,
        -0.2663350535025509,
        -0.7162572754421579,
        -0.7575522223224951
      ],
      [
        -0.7604104757492498,
        -0.7589747466573142,
        -0.7601503666430722,
        -0.7630089077669115,
        -0.7321722805910099,
        -0.759969088711981,
        -0.7598454255663042,
        -0.7598153094974034,
        -0.7290773186627062,
        -0.7182542860918824,
        -0.7591179313951583,
        -0.7245719220982113,
        -0.7362590948224452,
        -0.7602511706368225,
        -0.7609122930945015,
        -0.7528035831167654,
        -0.7597491638555729,
        -0.7583942661262664,
        -0.7193765491136027,
        -0.760224802448616,
        -0.7602702725484685,
        -0.7608571954159996,
        -0.756351081128771,
        1.0,
        -0.7566050488844482,
        -0.7581135049181815,
        -0.7474212740220558,
        -0.7603262018790197,
        -0.7617123451953306,
        -0.7205093312150798,
        -0.7601520563392415,
        -0.7556839649302738,
        -0.7611599146991017,
        -0.7118609036304642,
        -0.7246728743707667,
        -0.7581865478745424,
        -0.7639156356781656,
        -0.7599936596157923,
        -0.7499069165067841,
        -0.7617197704276596,
        -0.7533612876765272,
        -0.7533302254562672,
        -0.7023597652030511,
        -0.7483900613144648,
        -0.7607500204577101,
        -0.758912918508353,
        -0.7568151678644395,
        -0.7357038868278489,
        -0.7420738272830452,
        -0.761138944205831,
        -0.7544628168639176,
        -0.7619841682974848,
        -0.7612488037469861,
        -0.7576772063944075,
        -0.7635136096922109,
        -0.7544965604510918,
        -0.7565354287754582,
        -0.7601393055867935
      ],
      [
        -0.6746971268456889,
        -0.7151210897545734,
        -0.6969141307438266,
        -0.7274357631245734,
        -0.7225489802116771,
        -0.686255884059513,
        -0.736130542869643,
        -0.708201052142825,
        -0.7417221621164982,
        -0.7071922252864404,
        -0.728776680578769,
        -0.725842085242451,
        -0.7244864432197756,
        -0.7226390109041286,
        -0.7384342661347821,
        -0.7049746256423585,
        -0.7289236193915666,
        -0.6893829008116199,
        -0.7236034569383851,
        -0.687414134899881,
        -0.7354600729853826,
        -0.7375246211473339,
        -0.6783642670420508,
        -0.7566050488844482,
        1.0,
        -0.7206320338723784,
        -0.7183835898692015,
        -0.7221803300764984,
        -0.7344774567436904,
        -0.7356381565392243,
        -0.694011691547714,
        -0.7163861413642402,
        -0.676634065400882,
        -0.7265429540179212,
        -0.7261821244988396,
        -0.7335210621608239,
        -0.7407735408589613,
        -0.7195071371351021,
        -0.7538998713139684,
        -0.7271892627155272,
        -0.7197511331189534,
        -0.7332031152852874,
        -0.7288721501173838,
        -0.7262160282575597,
        -0.7197461155943385,
        -0.7445899178110797,
        -0.7128978627992184,
        -0.7348473925126826,
        -0.7261935626870964,
        -0.7385697936081504,
        -0.7095121482244705,
        -0.7136227162997696,
        -0.7346184682874626,
        -0.7311230839114119,
        -0.7418146228087161,
        0.36285794257309684,
        -0.7049990106851363,
        -0.744799601291267
      ],
      [
        -0.7459835016019907,
        -0.7176157731133319,
        -0.6968621983105154,
        -0.7558002319005905,
        -0.7240872009871284,
        -0.5392675789119884,
        -0.576558224169545,
        -0.7411424284278545,
        -0.7180986452791014,
        -0.7247217248095325,
        0.23295973087059058,
        -0.7166534818192809,
        -0.7127999185993746,
        -0.7008680888927747,
        -0.7412040568005391,
        -0.7250524174006937,
        -0.7400033247276596,
        -0.7306780757569739,
        -0.7073286774620431,
        -0.7048723637977816,
        -0.7251535907106051,
        -0.7241459647126429,
        -0.736676225860962,
        -0.7581135049181815,
        -0.7206320338723784,
        1.0,
        -0.5743708725440692,
        -0.7311819342687043,
        -0.7512090238249653,
        -0.7112149174455115,
        -0.645334942180305,
        -0.7128342835547619,
        -0.7209746466722018,
        -0.7380368471331289,
        -0.7377955123016379,
        -0.7079921482001791,
        -0.7483790061462733,
        -0.7511404711823282,
        -0.7279269178179903,
        -0.7034217564236405,
        -0.718659573077415,
        -0.7443190347922355,
        -0.7183829663936313,
        -0.7400633725159408,
        -0.7268949875830546,
        -0.6886264001681307,
        -0.4970915894483761,
        -0.5410728673422391,
        -0.7351725225879857,
        -0.6796049577328966,
        -0.7367221357169392,
        -0.7482624724349042,
        -0.6219712112105011,
        -0.7225296195900304,
        -0.7277933737820277,
        -0.721952332250041,
        -0.7296541360935869,
        -0.6987485507208271
      ],
      [
        -0.6931172733257553,
        -0.7192628950914279,
        -0.7177545131816401,
        -0.7495383726238924,
        -0.7096665505807476,
        -0.4649752455109226,
        -0.7410110199632967,
        -0.7390064674186849,
        -0.47140105302822083,
        -0.5233268151675468,
        -0.7188404901650064,
        -0.6702753588010766,
        -0.7057213020474529,
        -0.7145124458740042,
        -0.7443387195707101,
        -0.7081540886602985,
        -0.739524162067336,
        -0.7188065344765653,
        -0.600363354437775,
        -0.6847376001748748,
        -0.7456787735747623,
        -0.739103292741107,
        -0.6982377757727178,
        -0.7474212740220558,
        -0.7183835898692015,
        -0.5743708725440692,
        1.0,
        -0.7422592561250108,
        -0.7509811097395539,
        -0.7286572912492375,
        -0.6275697415659468,
        -0.7204225673386166,
        -0.7245540732360164,
        -0.6609373410637818,
        -0.6169207596854978,
        -0.5935709188676844,
        -0.7456318323670201,
        -0.7498214400682124,
        -0.7556374865626387,
        -0.6143425335657715,
        -0.7220784721452508,
        -0.7450569815610438,
        -0.6680702563600511,
        -0.7362091612962829,
        -0.7275081779557495,
        -0.7481546040037769,
        -0.701860257370208,
        -0.665433011375516,
        -0.5211308077984275,
        -0.7258316200890962,
        -0.7211706864735672,
        -0.7429801463188409,
        -0.7422754108919781,
        -0.7137050728231855,
        -0.7454359667090891,
        -0.7083048216294666,
        -0.7204247458242958,
        -0.7499113795282457
      ],
      [
        -0.7406008439209335,
        -0.7441073508339093,
        -0.7436748525383651,
        -0.7556322732288405,
        -0.7457619294887338,
        -0.7334834488182029,
        -0.7370649553731059,
        -0.7457784034747597,
        -0.7523459771403729,
        -0.7416927545932126,
        -0.7299434216526735,
        -0.7497995116773778,
        -0.739200938466575,
        -0.736529397316597,
        -0.7453615436169121,
        -0.7348827779796251,
        -0.7471893112490335,
        -0.7365061855675901,
        -0.694641919818038,
        -0.7404855842577447,
        -0.7421089217057542,
        -0.7312498595215713,
        -0.7427357662963141,
        -0.7603262018790197,
        -0.7221803300764984,
        -0.7311819342687043,
        -0.7422592561250108,
        1.0,
        -0.7518443107367175,
        -0.7239939907709592,
        -0.7401494270934323,
        -0.735826565058076,
        -0.7441653474982909,
        -0.7445922451695046,
        -0.7505923138073614,
        -0.6799236354925862,
        -0.752409526519316,
        -0.7502770838211106,
        -0.7585504361903064,
        -0.7037565768672858,
        -0.7104024624501056,
        -0.7249178493624878,
        -0.7110959614006965,
        -0.7376572465285309,
        -0.7170671374484088,
        -0.7497355734215677,
        -0.7376393632025325,
        -0.735307953283453,
        -0.7454392471239657,
        -0.7344809634697316,
        -0.7379414625322902,
        -0.7213177388100382,
        -0.7370175898471663,
        -0.7442047129466713,
        -0.7575737715412187,
        -0.7223443661238718,
        -0.7422818394644197,
        -0.7491205375602257
      ],
      [
        -0.7186489772233099,
        -0.7398620057333174,
        -0.7328438818186347,
        -0.7538258977695553,
        -0.7506245985469232,
        -0.7322584884632172,
        -0.7516186378517583,
        -0.7148920879369709,
        -0.7568753068549767,
        -0.7431706108944299,
        -0.7530010107143112,
        -0.7497841666777179,
        -0.7386773686379002,
        -0.7502703615197308,
        -0.7495329930233265,
        -0.7391068846857234,
        -0.5435169245117137,
        -0.7369403671558763,
        -0.7457963868466484,
        -0.7340687726669123,
        -0.7442440358107358,
        -0.7525024096130982,
        -0.7413736839080671,
        -0.7617123451953306,
        -0.7344774567436904,
        -0.7512090238249653,
        -0.7509811097395539,
        -0.7518443107367175,
        1.0,
        -0.7569529572982283,
        -0.7170248267502244,
        -0.7292738547217089,
        -0.7439806713780067,
        -0.7511174536683791,
        -0.7480935136510751,
        -0.739009107548428,
        -0.755203105809076,
        -0.748044091590613,
        -0.7524705493293533,
        -0.7519865845630729,
        -0.746226033642172,
        -0.7482219101008042,
        -0.7529979624180881,
        -0.7498378444359048,
        -0.7364432536929959,
        -0.7528569688172801,
        -0.7482233048457199,
        -0.7548684701792079,
        -0.7492955104202539,
        -0.7548386962877665,
        -0.5023766194098981,
        -0.7443858456985217,
        -0.7475086473366213,
        -0.7439741729572984,
        -0.7517617363341847,
        -0.7347644044407613,
        -0.7452006975762886,
        -0.7543136932799991
      ],
      [
        -0.7504409757688186,
        -0.7532957847010526,
        -0.7531518223142243,
        -0.7536775654033998,
        -0.7346817867809521,
        -0.7485490225096763,
        -0.741731613509051,
        -0.7525821625161375,
        -0.7369319371840306,
        -0.6567432607448764,
        -0.7116090250046894,
        -0.7277450665573697,
        -0.7068495690692294,
        -0.7555481579892716,
        -0.7454911959240889,
        -0.6539562834151202,
        -0.7520785680247177,
        -0.748385255311119,
        -0.7069036214450889,
        -0.7544014460079487,
        -0.7430677357097166,
        -0.7517699394363475,
        -0.7385287085023549,
        -0.7205093312150798,
        -0.7356381565392243,
        -0.7112149174455115,
        -0.7286572912492375,
        -0.7239939907709592,
        -0.7569529572982283,
        1.0,
        -0.7495800541711799,
        -0.7428663326762266,
        -0.7494271196078774,
        -0.7235051789776145,
        -0.7254466597810607,
        -0.7314067699379913,
        -0.757101406119808,
        -0.7568634039535918,
        -0.7267921074468625,
        -0.746276189936331,
        -0.7473558869012791,
        -0.7197126382286985,
        -0.6546119835838619,
        -0.6824514277794811,
        -0.7503436932606274,
        -0.754115852183213,
        -0.6426943924623831,
        -0.66857819842608,
        -0.7289604569056196,
        -0.7458205536610407,
        -0.7459464488658276,
        -0.7526298138231393,
        -0.7368712885251247,
        -0.7533764169623104,
        -0.7564823696874189,
        -0.7354687106424074,
        -0.7489301423036041,
        -0.7552391635454785
      ],
      [
        -0.7066665254330016,
        -0.7009982784202937,
        -0.4243793660871683,
        -0.7424328419059234,
        -0.7278537960227123,
        -0.09612756245395114,
        -0.7364573520935207,
        -0.3642190739549391,
        -0.7400685010080641,
        -0.721140920081923,
        -0.7119255218236025,
        -0.7293779481447211,
        -0.7190602885970119,
        -0.6272165284264535,
        -0.7405988294983572,
        -0.7132091265723844,
        -0.7127466777168266,
        -0.7184045229045086,
        -0.7261535440945506,
        -0.6773604846382684,
        -0.7229385928941188,
        -0.7363359570823745,
        -0.7196630721244957,
        -0.7601520563392415,
        -0.694011691547714,
        -0.645334942180305,
        -0.6275697415659468,
        -0.7401494270934323,
        -0.7170248267502244,
        -0.7495800541711799,
        1.0,
        -0.693893212153914,
        -0.6871613168166255,
        -0.7399192510002728,
        -0.7345419354732108,
        -0.7266647024796008,
        -0.7474857084309415,
        -0.7341696033193155,
        -0.7543653958266723,
        -0.7238166465494262,
        -0.7162720323486914,
        -0.7409371619656581,
        -0.7357034319205533,
        -0.73930698971864,
        -0.5241188732615566,
        -0.738048129787704,
        -0.7263211170306498,
        -0.7323564713822812,
        -0.7320254982669868,
        -0.7379765067764996,
        -0.6941055084131358,
        -0.7244912319356556,
        -0.7362648634727156,
        -0.7107070586587925,
        -0.7127732253169722,
        -0.6956911035671217,
        -0.708347452129384,
        -0.7380747732765534
      ],
      [
        -0.6902449540988747,
        -0.6874077180616394,
        -0.6953955051914966,
        -0.7331567376745505,
        -0.7339448518622715,
        -0.6828503717549611,
        -0.7266760245611776,
        -0.7298836731411591,
        -0.7474726696272367,
        -0.7121986362147932,
        -0.7158690664439507,
        -0.724646186438985,
        -0.7276961294694342,
        -0.7174176872301159,
        -0.7409746275529341,
        -0.6888305266800409,
        -0.7049563567317374,
        -0.702238766038367,
        -0.7255578811265451,
        -0.718334503903111,
        -0.7383586125394672,
        -0.7356377106624479,
        -0.6815312895088965,
        -0.7556839649302738,
        -0.7163861413642402,
        -0.7128342835547619,
        -0.7204225673386166,
        -0.735826565058076,
        -0.7292738547217089,
        -0.7428663326762266,
        -0.693893212153914,
        1.0,
        -0.7220819360455129,
        -0.7305817471769491,
        -0.7317521349448308,
        -0.7323609072700464,
        -0.7358254577592398,
        -0.7417769037145835,
        -0.748628662621014,
        -0.7394606219920218,
        -0.729602390968176,
        -0.7354662777625374,
        -0.7363548590522049,
        -0.7260925785787864,
        -0.7339501100095465,
        -0.7452058136455675,
        -0.7079889417803049,
        -0.7372017610382542,
        -0.733120604936145,
        -0.7325892817625299,
        -0.6568835434767868,
        -0.7432417778491198,
        -0.7355755489373552,
        -0.7241444500715466,
        -0.7342082366659769,
        -0.6991513495344854,
        -0.7200909055988464,
        -0.7444192801586147
      ],
      [
        -0.7157862813198166,
        -0.7309979324244698,
        -0.647014493272925,
        -0.7440945411693385,
        -0.7221433032495745,
        -0.6878583783067851,
        -0.7221865200288684,
        -0.7396096807813127,
        -0.7418080497862495,
        -0.7269675222393528,
        -0.7273707559294675,
        -0.738604503589517,
        -0.7456385961617795,
        -0.7138983980593081,
        -0.7370184977128119,
        -0.7081580744200322,
        -0.7301981435276641,
        -0.6358021239295284,
        -0.7353448955580717,
        -0.7190471890547914,
        -0.7468255792084009,
        -0.746113900397136,
        -0.7012643509092136,
        -0.7611599146991017,
        -0.676634065400882,
        -0.7209746466722018,
        -0.7245540732360164,
        -0.7441653474982909,
        -0.7439806713780067,
        -0.7494271196078774,
        -0.6871613168166255,
        -0.7220819360455129,
        1.0,
        -0.7350260445582477,
        -0.7355702684634653,
        -0.739885379397804,
        -0.7533410692828746,
        -0.6215065929297847,
        -0.7585481063170767,
        -0.742833122674613,
        -0.7342284892665698,
        -0.7408144209496703,
        -0.7389717631112787,
        -0.740430433866361,
        -0.7411142119531773,
        -0.7451570267683287,
        -0.7193706824989637,
        -0.7464427789770148,
        -0.7373909839794279,
        -0.7422845447833561,
        -0.7267564234272819,
        -0.7282286117865461,
        -0.7333051252271173,
        -0.7357036444039802,
        -0.7295358667292523,
        -0.6724990310622327,
        -0.7137263157434992,
        -0.7463111447014816
      ],
      [
        -0.7142060892461828,
        -0.7384964598645419,
        -0.7471227545695205,
        -0.7460980640583805,
        -0.7318031485423491,
        -0.7286014735707131,
        -0.7492147571511499,
        -0.7438288719841623,
        -0.7154812427119913,
        -0.5669580941408343,
        -0.7442066046803057,
        -0.6971884828774098,
        -0.7235252530837295,
        -0.7480900780655033,
        -0.749676043961621,
        -0.7171923236759827,
        -0.7371967415773814,
        -0.7280563136271936,
        -0.6919882704190256,
        -0.733307971572631,
        -0.7534800357523452,
        -0.7543090277998463,
        -0.7097833106443937,
        -0.7118609036304642,
        -0.7265429540179212,
        -0.7380368471331289,
        -0.6609373410637818,
        -0.7445922451695046,
        -0.7511174536683791,
        -0.7235051789776145,
        -0.7399192510002728,
        -0.7305817471769491,
        -0.7350260445582477,
        1.0,
        -0.12947923943358305,
        -0.7267786306911059,
        -0.7453499382270373,
        -0.7511971143349966,
        -0.7575036568592681,
        -0.7377430219165778,
        -0.7444687635226728,
        -0.7466900131248306,
        -0.6837574144966478,
        -0.7289451976690451,
        -0.7453937956508337,
        -0.7558057463401805,
        -0.7192515745843995,
        -0.7173201627215775,
        -0.4819474704669271,
        -0.7517616559241329,
        -0.7283176203142181,
        -0.7492092767608032,
        -0.7485998060387085,
        -0.742387954020105,
        -0.7560640307724429,
        -0.7140915638228402,
        -0.7395863361237283,
        -0.7574956908005781
      ],
      [
        -0.7306827130274007,
        -0.7422517087498841,
        -0.7488693347435227,
        -0.7334747562374107,
        -0.7250575708496904,
        -0.7296330250232095,
        -0.747371427062222,
        -0.7401051017857341,
        -0.6960029780584738,
        -0.42586503532215236,
        -0.7458733702799365,
        -0.7004123050311916,
        -0.702357005586852,
        -0.751743951604904,
        -0.7426341050827076,
        -0.6964126066390572,
        -0.7333408709931389,
        -0.7283051819188664,
        -0.6783743987309455,
        -0.7390377716059819,
        -0.7489940367754088,
        -0.752099592765581,
        -0.6545326021837723,
        -0.7246728743707667,
        -0.7261821244988396,
        -0.7377955123016379,
        -0.6169207596854978,
        -0.7505923138073614,
        -0.7480935136510751,
        -0.7254466597810607,
        -0.7345419354732108,
        -0.7317521349448308,
        -0.7355702684634653,
        -0.12947923943358305,
        1.0,
        -0.6965436917261396,
        -0.7430699491554196,
        -0.7495253747739299,
        -0.7524570421311821,
        -0.7084089316136432,
        -0.7436843938917015,
        -0.742916650324392,
        -0.6989799830996126,
        -0.7249611961229914,
        -0.7433121019968303,
        -0.7564637201775741,
        -0.7050716674657875,
        -0.7221736336325968,
        -0.3647106436859413,
        -0.7513788979270912,
        -0.7224286896751377,
        -0.7466359000120671,
        -0.7432069046724381,
        -0.7440512093127363,
        -0.7554213704621204,
        -0.6770878318135769,
        -0.7380594016260819,
        -0.7575282455361336
      ],
      [
        -0.7224779650761434,
        -0.7387876144525325,
        -0.7275910508039309,
        -0.7525709951978193,
        -0.7160564631727122,
        -0.7093381559068686,
        -0.7169498158762456,
        -0.7449304141808231,
        -0.6760961966665503,
        -0.6741269835027586,
        -0.7251161326757697,
        -0.7250643139342929,
        -0.7345813842906215,
        -0.7396094594442535,
        -0.7463332193009327,
        -0.7310686093887686,
        -0.7454756586414111,
        -0.7361269279644129,
        -0.22129103758389138,
        -0.7274838319459644,
        -0.7442304016233598,
        -0.7243973858039567,
        -0.7231906711341409,
        -0.7581865478745424,
        -0.7335210621608239,
        -0.7079921482001791,
        -0.5935709188676844,
        -0.6799236354925862,
        -0.739009107548428,
        -0.7314067699379913,
        -0.7266647024796008,
        -0.7323609072700464,
        -0.739885379397804,
        -0.7267786306911059,
        -0.6965436917261396,
        1.0,
        -0.7567778971982473,
        -0.7524511801057017,
        -0.7567535162966346,
        -0.6505717163968676,
        -0.7207495789482746,
        -0.7431753427072839,
        -0.6002319845222518,
        -0.7436842272316857,
        -0.730440127684431,
        -0.7442659695514324,
        -0.7237018834120215,
        -0.5801106258289469,
        -0.6247936364352583,
        -0.7151330855304645,
        -0.7239845421252982,
        -0.7489474575203825,
        -0.7402101445061633,
        -0.7356957100840215,
        -0.7468129091308935,
        -0.7284084648225191,
        -0.7402262827720372,
        -0.7453429672622707
      ],
      [
        -0.7485611969401174,
        -0.7244782228858333,
        -0.7580027410517125,
        -0.6733787934810282,
        -0.7575418276668412,
        -0.7423496554321839,
        -0.7542672864854538,
        -0.7531454520188251,
        -0.7520231070534242,
        -0.7459380421444366,
        -0.7503971912741978,
        -0.7489553335624095,
        -0.7539197104887609,
        -0.7597929135205022,
        -0.7503235100684493,
        -0.7487997352950984,
        -0.7510910241693403,
        -0.7456189476116188,
        -0.7542850003119907,
        -0.7386853238298814,
        -0.7485273464516327,
        -0.755841414353449,
        -0.7449872790010007,
        -0.7639156356781656,
        -0.7407735408589613,
        -0.7483790061462733,
        -0.7456318323670201,
        -0.752409526519316,
        -0.755203105809076,
        -0.757101406119808,
        -0.7474857084309415,
        -0.7358254577592398,
        -0.7533410692828746,
        -0.7453499382270373,
        -0.7430699491554196,
        -0.7567778971982473,
        1.0,
        -0.7588209517988848,
        -0.7585972912615009,
        -0.754669379832192,
        -0.7503430540453854,
        -0.7479049095585726,
        -0.7552244724779131,
        -0.7453400919461799,
        -0.7532966211300751,
        -0.7586484417393198,
        -0.7491633466497958,
        -0.7532154308919139,
        -0.7399260616084409,
        -0.7578805826068059,
        -0.7307157136664267,
        -0.7549260391538277,
        -0.7486835240638441,
        -0.7457379281418439,
        -0.7615302646706784,
        -0.7381851759968482,
        -0.7536487401929526,
        -0.7589011080650856
      ],
      [
        -0.7329086648058951,
        -0.7499571167446453,
        -0.7381150882324379,
        -0.7551489943023368,
        -0.7391121863627598,
        -0.739068882150156,
        -0.749477045189141,
        -0.74873321250752,
        -0.7550044925726183,
        -0.7443368689358538,
        -0.7523537470036651,
        -0.7535996648806245,
        -0.7495267116446227,
        -0.7529948273350697,
        -0.7514734165182388,
        -0.7372907037485901,
        -0.7411239937829269,
        -0.6307220565589959,
        -0.7500482242193727,
        -0.7432827773114241,
        -0.7541352071737492,
        -0.756111736138172,
        -0.7357089627962434,
        -0.7599936596157923,
        -0.7195071371351021,
        -0.7511404711823282,
        -0.7498214400682124,
        -0.7502770838211106,
        -0.748044091590613,
        -0.7568634039535918,
        -0.7341696033193155,
        -0.7417769037145835,
        -0.6215065929297847,
        -0.7511971143349966,
        -0.7495253747739299,
        -0.7524511801057017,
        -0.7588209517988848,
        1.0,
        -0.759059965996519,
        -0.7531365553757456,
        -0.7462238321276269,
        -0.7551036472924653,
        -0.7537461365471504,
        -0.7525229359046466,
        -0.7490787506924839,
        -0.7553985012815143,
        -0.7483715767701464,
        -0.7564460514294218,
        -0.751752504754009,
        -0.7546603101545186,
        -0.7296428322865893,
        -0.7429665851264614,
        -0.750478558722026,
        -0.7542010767589028,
        -0.7567897558630955,
        -0.7170745204794721,
        -0.6929897661801314,
        -0.7569305600834475
      ],
      [
        -0.7582596368153061,
        -0.7565703615717801,
        -0.7534206836615727,
        -0.7548200298290278,
        -0.7536155740011998,
        -0.7564021500055089,
        -0.7369497579356763,
        -0.7548219675360046,
        -0.7527577673123496,
        -0.7464797911338232,
        -0.712309145898315,
        -0.736128735398724,
        -0.7439363743618068,
        -0.7591489164362097,
        -0.752955372758328,
        -0.7418774949969624,
        -0.7446258322257753,
        -0.7546957098177708,
        -0.749488822923633,
        -0.7573675118871325,
        -0.7580288395964487,
        -0.7555560194446028,
        -0.7558877053217912,
        -0.7499069165067841,
        -0.7538998713139684,
        -0.7279269178179903,
        -0.7556374865626387,
        -0.7585504361903064,
        -0.7524705493293533,
        -0.7267921074468625,
        -0.7543653958266723,
        -0.748628662621014,
        -0.7585481063170767,
        -0.7575036568592681,
        -0.7524570421311821,
        -0.7567535162966346,
        -0.7585972912615009,
        -0.759059965996519,
        1.0,
        -0.7593971677767832,
        -0.7550654707230278,
        -0.75528900765274,
        -0.750851068263794,
        -0.7433340867787646,
        -0.7559185033452176,
        -0.7454228336221047,
        -0.7460746953957931,
        -0.6977390787640199,
        -0.7567097273793935,
        -0.7528305945156821,
        -0.7145682245419108,
        -0.7596811599942142,
        -0.7413133406810423,
        -0.7597339133209068,
        -0.7569916934602181,
        -0.7530703327670338,
        -0.7507881824430498,
        -0.7470219834346901
      ],
      [
        -0.7447636578042048,
        -0.73484117744863,
        -0.7454525321986973,
        -0.7567064139252444,
        -0.7475205215625687,
        -0.7043073973210341,
        -0.7371616645757588,
        -0.7281024346583452,
        -0.7194445846378956,
        -0.6882622778610363,
        -0.724892883322021,
        -0.7421470214202408,
        -0.7455400071840439,
        -0.7377465739566418,
        -0.7481199635918157,
        -0.7448187303860458,
        -0.7508861828365792,
        -0.7421678358044297,
        -0.6679157749389972,
        -0.693882629816521,
        -0.6929730473668915,
        -0.2112182674586627,
        -0.7454875618856484,
        -0.7617197704276596,
        -0.7271892627155272,
        -0.7034217564236405,
        -0.6143425335657715,
        -0.7037565768672858,
        -0.7519865845630729,
        -0.746276189936331,
        -0.7238166465494262,
        -0.7394606219920218,
        -0.742833122674613,
        -0.7377430219165778,
        -0.7084089316136432,
        -0.6505717163968676,
        -0.754669379832192,
        -0.7531365553757456,
        -0.7593971677767832,
        1.0,
        0.0356738815846464,
        -0.7494704064606286,
        -0.7137936468507395,
        -0.7505688924130509,
        -0.0765540247987758,
        -0.7199389897607551,
        -0.7391710502935576,
        -0.7224041451479446,
        -0.6836896683313625,
        -0.2879933058704772,
        -0.7391896470702084,
        -0.7485809427170109,
        -0.7414098855640554,
        -0.7331758412914127,
        -0.7464578428459525,
        -0.7295412885761172,
        -0.7407605267852451,
        -0.7010456081254577
      ],
      [
        -0.7419786342906675,
        -0.7341334736498855,
        -0.7301382869638624,
        -0.7500530316744484,
        -0.7418479415212764,
        -0.7147784762805992,
        -0.6830822001746355,
        -0.7143242415076532,
        -0.7444542316860614,
        -0.7375649447835646,
        -0.7271212101042701,
        -0.7438366790260666,
        -0.7408031223300812,
        -0.6605829422812866,
        -0.7444964524771267,
        -0.7352330224330894,
        -0.7447090917556027,
        -0.7361748826684765,
        -0.7175355152087378,
        -0.713619258187854,
        -0.6954472698835885,
        -0.5113252556097008,
        -0.7424674863137195,
        -0.7533612876765272,
        -0.7197511331189534,
        -0.718659573077415,
        -0.7220784721452508,
        -0.7104024624501056,
        -0.746226033642172,
        -0.7473558869012791,
        -0.7162720323486914,
        -0.729602390968176,
        -0.7342284892665698,
        -0.7444687635226728,
        -0.7436843938917015,
        -0.7207495789482746,
        -0.7503430540453854,
        -0.7462238321276269,
        -0.7550654707230278,
        0.0356738815846464,
        1.0,
        -0.7396114656315617,
        -0.7251834482666555,
        -0.7431365754929533,
        0.2559216971321554,
        -0.7343953479922694,
        -0.7394026513492342,
        -0.7362132167526345,
        -0.7335719473655089,
        -0.5745490129735006,
        -0.7264666856828244,
        -0.7449041411327768,
        -0.7373371508234751,
        -0.7367529682737106,
        -0.7436361097616828,
        -0.7238932021707991,
        -0.7339582128612718,
        -0.7226259437688252
      ],
      [
        -0.7376035879668454,
        -0.743754011624237,
        -0.7507211236625944,
        -0.729392917160048,
        -0.7458363598455506,
        -0.7404019316941245,
        -0.7499092957315621,
        -0.7506488859569256,
        -0.7500534731426984,
        -0.5554447712636725,
        -0.745818589468547,
        -0.737070021710782,
        -0.7381631056547214,
        -0.7519211820630012,
        -0.5815271304460121,
        -0.5012800922137085,
        -0.7494478317222228,
        -0.7369814076048549,
        -0.7338557280806457,
        -0.7441063354130775,
        -0.5604851880254809,
        -0.7523993546180073,
        -0.7302207170299824,
        -0.7533302254562672,
        -0.7332031152852874,
        -0.7443190347922355,
        -0.7450569815610438,
        -0.7249178493624878,
        -0.7482219101008042,
        -0.7197126382286985,
        -0.7409371619656581,
        -0.7354662777625374,
        -0.7408144209496703,
        -0.7466900131248306,
        -0.742916650324392,
        -0.7431753427072839,
        -0.7479049095585726,
        -0.7551036472924653,
        -0.75528900765274,
        -0.7494704064606286,
        -0.7396114656315617,
        1.0,
        -0.7318398218335348,
        -0.4814302495422347,
        -0.7444633103831624,
        -0.753806446580294,
        -0.7293095932965842,
        -0.7445946593142958,
        -0.7429734502780483,
        -0.7495813521793502,
        -0.7318132582036205,
        -0.6388068609843851,
        -0.600278985303002,
        -0.7482961309461562,
        -0.7579159698806013,
        -0.7281952176734854,
        -0.7446629440002707,
        -0.7561531216179564
      ],
      [
        -0.7436611980306264,
        -0.7373658290542413,
        -0.7473731384077247,
        -0.7502795032957603,
        -0.7043453142677385,
        -0.7224045398057344,
        -0.729156596206286,
        -0.7453641325623677,
        -0.6922812130744979,
        -0.6383349036133594,
        -0.7298793601910629,
        -0.6238424314484808,
        -0.724550071650925,
        -0.7503776385009924,
        -0.7510287546468095,
        -0.6538060171245388,
        -0.7457575424279638,
        -0.7351444704681346,
        0.2039664618564338,
        -0.7300563170922335,
        -0.7494792301384006,
        -0.737652694203708,
        -0.7354167969069743,
        -0.7023597652030511,
        -0.7288721501173838,
        -0.7183829663936313,
        -0.6680702563600511,
        -0.7110959614006965,
        -0.7529979624180881,
        -0.6546119835838619,
        -0.7357034319205533,
        -0.7363548590522049,
        -0.7389717631112787,
        -0.6837574144966478,
        -0.6989799830996126,
        -0.6002319845222518,
        -0.7552244724779131,
        -0.7537461365471504,
        -0.750851068263794,
        -0.7137936468507395,
        -0.7251834482666555,
        -0.7318398218335348,
        1.0,
        -0.7259109333275066,
        -0.7306477445328314,
        -0.7494012740471818,
        -0.66648547186918,
        -0.6706263834713216,
        -0.6754714652723328,
        -0.7267285165651116,
        -0.7334699754201142,
        -0.7496577618067679,
        -0.7426750371865511,
        -0.7366019666032264,
        -0.7466192375717816,
        -0.7276583689559231,
        -0.7429571219790605,
        -0.7521938895126222
      ],
      [
        -0.7374799475040946,
        -0.7408069460019967,
        -0.7491281799787923,
        -0.6909388945431532,
        -0.7367320075113821,
        -0.7414483655025196,
        -0.7444799629707175,
        -0.7437410215221119,
        -0.7457834220716074,
        -0.3752308705665488,
        -0.7414562166982059,
        -0.7321252632705891,
        -0.7075472842459241,
        -0.7485930383199134,
        -0.5434183073245734,
        -0.23920873607093762,
        -0.7415823310948646,
        -0.7371919739658392,
        -0.732467961070656,
        -0.7452535819809698,
        -0.5251532983690992,
        -0.7545031120310524,
        -0.7076384602769364,
        -0.7483900613144648,
        -0.7262160282575597,
        -0.7400633725159408,
        -0.7362091612962829,
        -0.7376572465285309,
        -0.7498378444359048,
        -0.6824514277794811,
        -0.73930698971864,
        -0.7260925785787864,
        -0.740430433866361,
        -0.7289451976690451,
        -0.7249611961229914,
        -0.7436842272316857,
        -0.7453400919461799,
        -0.7525229359046466,
        -0.7433340867787646,
        -0.7505688924130509,
        -0.7431365754929533,
        -0.4814302495422347,
        -0.7259109333275066,
        1.0,
        -0.7419945839627073,
        -0.7535654864811985,
        -0.7146605873657395,
        -0.7386728669828813,
        -0.7363915237446077,
        -0.7473027928802268,
        -0.725821116315289,
        -0.6322077637102115,
        -0.5726287072322112,
        -0.7459376032416735,
        -0.7568075041243327,
        -0.7114879904679652,
        -0.7371524288361178,
        -0.7559788778172629
      ],
      [
        -0.7465830030858247,
        -0.7354777199429179,
        -0.7352891129611172,
        -0.7553811414838631,
        -0.745752820831016,
        -0.7136492395741698,
        -0.7174333433738086,
        -0.32565365043001443,
        -0.7472062475424153,
        -0.7353436451876205,
        -0.7368025886711597,
        -0.7441168162931913,
        -0.7411961108071229,
        -0.7055918368784744,
        -0.7430448744890389,
        -0.7319571406215798,
        -0.7375969200091501,
        -0.7424139209712985,
        -0.7269116788406107,
        -0.7121570862275192,
        -0.6862179316342537,
        -0.5799389935890111,
        -0.7489276188533218,
        -0.7607500204577101,
        -0.7197461155943385,
        -0.7268949875830546,
        -0.7275081779557495,
        -0.7170671374484088,
        -0.7364432536929959,
        -0.7503436932606274,
        -0.5241188732615566,
        -0.7339501100095465,
        -0.7411142119531773,
        -0.7453937956508337,
        -0.7433121019968303,
        -0.730440127684431,
        -0.7532966211300751,
        -0.7490787506924839,
        -0.7559185033452176,
        -0.0765540247987758,
        0.2559216971321554,
        -0.7444633103831624,
        -0.7306477445328314,
        -0.7419945839627073,
        1.0,
        -0.7373232452281018,
        -0.7466517960938228,
        -0.7424985014905351,
        -0.7371663218117956,
        -0.6031625971003324,
        -0.7206930756470202,
        -0.7441777427020773,
        -0.7370769195166871,
        -0.7366219020914866,
        -0.743613472639779,
        -0.7251005880039729,
        -0.7364845249483202,
        -0.7280592760327782
      ],
      [
        -0.7561549151900839,
        -0.7532509295168348,
        -0.6685725868454493,
        -0.7604480377108604,
        -0.7441951974245321,
        -0.7334283683175202,
        -0.7389031051034174,
        -0.7523248698319066,
        -0.7554811675203795,
        -0.7541998645688579,
        -0.68273763469571,
        -0.7530485985476365,
        -0.7552920296985102,
        -0.7388897840715423,
        -0.7562350194847463,
        -0.7501114845071084,
        -0.7549293617904864,
        -0.7452118052351817,
        -0.7457341911414271,
        -0.7485052876739946,
        -0.7352369219795543,
        -0.7028255349578479,
        -0.7565441446424155,
        -0.758912918508353,
        -0.7445899178110797,
        -0.6886264001681307,
        -0.7481546040037769,
        -0.7497355734215677,
        -0.7528569688172801,
        -0.754115852183213,
        -0.738048129787704,
        -0.7452058136455675,
        -0.7451570267683287,
        -0.7558057463401805,
        -0.7564637201775741,
        -0.7442659695514324,
        -0.7586484417393198,
        -0.7553985012815143,
        -0.7454228336221047,
        -0.7199389897607551,
        -0.7343953479922694,
        -0.753806446580294,
        -0.7494012740471818,
        -0.7535654864811985,
        -0.7373232452281018,
        1.0,
        -0.7391383113255499,
        -0.727629175676322,
        -0.7549710722903656,
        -0.27033273914426215,
        -0.7486279349272837,
        -0.75765511969656,
        -0.7453338366006418,
        -0.7526307575707876,
        -0.7426602128574744,
        -0.7465109605897942,
        -0.7494755549743104,
        0.053109763886126864
      ],
      [
        -0.5480335660196388,
        -0.728329725612427,
        -0.7360017697220607,
        -0.7446676981862822,
        -0.7326935052472711,
        -0.7163990943177341,
        -0.6682639997232868,
        -0.7476205168956689,
        -0.7348047781168203,
        -0.6416340779709826,
        -0.47182051263956737,
        -0.7187321847571846,
        -0.7294079235036754,
        -0.7400761245350627,
        -0.731663676975614,
        -0.6236486028231452,
        -0.7414194748040643,
        -0.7096251526616142,
        -0.7138966360365187,
        -0.7313422004207009,
        -0.7335835228035184,
        -0.7430657864149813,
        -0.20745876528658513,
        -0.7568151678644395,
        -0.7128978627992184,
        -0.4970915894483761,
        -0.701860257370208,
        -0.7376393632025325,
        -0.7482233048457199,
        -0.6426943924623831,
        -0.7263211170306498,
        -0.7079889417803049,
        -0.7193706824989637,
        -0.7192515745843995,
        -0.7050716674657875,
        -0.7237018834120215,
        -0.7491633466497958,
        -0.7483715767701464,
        -0.7460746953957931,
        -0.7391710502935576,
        -0.7394026513492342,
        -0.7293095932965842,
        -0.66648547186918,
        -0.7146605873657395,
        -0.7466517960938228,
        -0.7391383113255499,
        1.0,
        -0.6881572330789493,
        -0.7071818104754083,
        -0.7297856353812318,
        -0.7173087187770482,
        -0.7427688287043908,
        -0.6524087504225775,
        -0.7297649907128241,
        -0.7474043960116408,
        -0.5790370720980876,
        -0.7328930676896985,
        -0.7425921665320439
      ],
      [
        -0.746968120007266,
        -0.7362999594508975,
        -0.7424937728079684,
        -0.7579081430126241,
        -0.7176963048966563,
        -0.6960282494906767,
        -0.6838772292208024,
        -0.7491399376590808,
        -0.7074347967765592,
        -0.7064170520300633,
        -0.6530740861969808,
        -0.6852133581718873,
        -0.6667726419800237,
        -0.7407871817528443,
        -0.7482842178655507,
        -0.7317563266757645,
        -0.7508550488808174,
        -0.7393787541964969,
        -0.5836555151692137,
        -0.7331798283422362,
        -0.7501057804888724,
        -0.7338534891019698,
        -0.7427425353003286,
        -0.7357038868278489,
        -0.7348473925126826,
        -0.5410728673422391,
        -0.665433011375516,
        -0.735307953283453,
        -0.7548684701792079,
        -0.66857819842608,
        -0.7323564713822812,
        -0.7372017610382542,
        -0.7464427789770148,
        -0.7173201627215775,
        -0.7221736336325968,
        -0.5801106258289469,
        -0.7532154308919139,
        -0.7564460514294218,
        -0.6977390787640199,
        -0.7224041451479446,
        -0.7362132167526345,
        -0.7445946593142958,
        -0.6706263834713216,
        -0.7386728669828813,
        -0.7424985014905351,
        -0.727629175676322,
        -0.6881572330789493,
        1.0,
        -0.7139137195496204,
        -0.7051638824523325,
        -0.7414466730152216,
        -0.7541216706474307,
        -0.7312266053925549,
        -0.7448192932275279,
        -0.7503828819192377,
        -0.7343038585520367,
        -0.7461463682924531,
        -0.7276080445233364
      ],
      [
        -0.7215734103084468,
        -0.7363164542695304,
        -0.7505888127615062,
        -0.7390796954761909,
        -0.7260127069050842,
        -0.7177295032498907,
        -0.7528204317782314,
        -0.7434982394103571,
        -0.6378706683502697,
        -0.5530197329490019,
        -0.748250773971704,
        -0.688986813299922,
        -0.7239100154193556,
        -0.7429850116278431,
        -0.7311814524206489,
        -0.7129317609479906,
        -0.7381033364422581,
        -0.7330586877078407,
        -0.631724460577924,
        -0.7246455058253325,
        -0.7499666120140251,
        -0.7468866578435126,
        -0.69598748228706,
        -0.7420738272830452,
        -0.7261935626870964,
        -0.7351725225879857,
        -0.5211308077984275,
        -0.7454392471239657,
        -0.7492955104202539,
        -0.7289604569056196,
        -0.7320254982669868,
        -0.733120604936145,
        -0.7373909839794279,
        -0.4819474704669271,
        -0.3647106436859413,
        -0.6247936364352583,
        -0.7399260616084409,
        -0.751752504754009,
        -0.7567097273793935,
        -0.6836896683313625,
        -0.7335719473655089,
        -0.7429734502780483,
        -0.6754714652723328,
        -0.7363915237446077,
        -0.7371663218117956,
        -0.7549710722903656,
        -0.7071818104754083,
        -0.7139137195496204,
        1.0,
        -0.7411403391166083,
        -0.7110768292981686,
        -0.7479686773947454,
        -0.7435877413056069,
        -0.7379510217451484,
        -0.7560949328228916,
        -0.7123220692913674,
        -0.7428063390743993,
        -0.757778557567029
      ],
      [
        -0.7499332143950592,
        -0.7511234323246878,
        -0.7434982249626086,
        -0.7591110572719076,
        -0.7516798819091594,
        -0.7296351763597466,
        -0.7330462556474111,
        -0.7473123955689953,
        -0.74006876950014,
        -0.742664521296247,
        -0.6899615644049701,
        -0.7490488843886742,
        -0.747627517074157,
        -0.7438592840611911,
        -0.7502015989675146,
        -0.7351077357168041,
        -0.7525805051257968,
        -0.7360563960776909,
        -0.720341655449076,
        -0.7354336254502545,
        -0.7182639377114258,
        -0.36315479533615347,
        -0.7436468644859682,
        -0.761138944205831,
        -0.7385697936081504,
        -0.6796049577328966,
        -0.7258316200890962,
        -0.7344809634697316,
        -0.7548386962877665,
        -0.7458205536610407,
        -0.7379765067764996,
        -0.7325892817625299,
        -0.7422845447833561,
        -0.7517616559241329,
        -0.7513788979270912,
        -0.7151330855304645,
        -0.7578805826068059,
        -0.7546603101545186,
        -0.7528305945156821,
        -0.2879933058704772,
        -0.5745490129735006,
        -0.7495813521793502,
        -0.7267285165651116,
        -0.7473027928802268,
        -0.6031625971003324,
        -0.27033273914426215,
        -0.7297856353812318,
        -0.7051638824523325,
        -0.7411403391166083,
        1.0,
        -0.7429198400582168,
        -0.7493657455941487,
        -0.7394591737356526,
        -0.748117497417109,
        -0.7431391098819469,
        -0.7382234787971967,
        -0.7436942902296951,
        -0.23375975290240383
      ],
      [
        -0.662365468180026,
        -0.6873864557421561,
        -0.7491400233451497,
        -0.7287910199107215,
        -0.7365629385795772,
        -0.7055716666248986,
        -0.7409765111412625,
        -0.6934855964339321,
        -0.7383587434402401,
        -0.7213107121924832,
        -0.7403395082586735,
        -0.7167465282418154,
        -0.7259089098690582,
        -0.7387698239562678,
        -0.7345341245616162,
        -0.7048922129558286,
        -0.38884622131911656,
        -0.7006397464219303,
        -0.7194197765033808,
        -0.7134947472268982,
        -0.7356875025078291,
        -0.7399087239417399,
        -0.6883123085484184,
        -0.7544628168639176,
        -0.7095121482244705,
        -0.7367221357169392,
        -0.7211706864735672,
        -0.7379414625322902,
        -0.5023766194098981,
        -0.7459464488658276,
        -0.6941055084131358,
        -0.6568835434767868,
        -0.7267564234272819,
        -0.7283176203142181,
        -0.7224286896751377,
        -0.7239845421252982,
        -0.7307157136664267,
        -0.7296428322865893,
        -0.7145682245419108,
        -0.7391896470702084,
        -0.7264666856828244,
        -0.7318132582036205,
        -0.7334699754201142,
        -0.725821116315289,
        -0.7206930756470202,
        -0.7486279349272837,
        -0.7173087187770482,
        -0.7414466730152216,
        -0.7110768292981686,
        -0.7429198400582168,
        1.0,
        -0.7276614943822095,
        -0.7294106197606144,
        -0.7262914589525894,
        -0.7488900917401695,
        -0.6980376612895905,
        -0.7217746235713737,
        -0.7483519245314751
      ],
      [
        -0.7272157493589542,
        -0.74474616726636,
        -0.7416948087808899,
        -0.7471916358125668,
        -0.7430190114287198,
        -0.7268953527425486,
        -0.7540801635774268,
        -0.7321409649255064,
        -0.7543787363834433,
        -0.6645975344552708,
        -0.7519133018334325,
        -0.7524412863466616,
        -0.7408407306857667,
        -0.7507027835441562,
        -0.6051068809124757,
        -0.6455943149354156,
        -0.7449242610922964,
        -0.7222793951450844,
        -0.7450179247822624,
        -0.7325072033451501,
        -0.5955395862683109,
        -0.7531345556506468,
        -0.7355184942768087,
        -0.7619841682974848,
        -0.7136227162997696,
        -0.7482624724349042,
        -0.7429801463188409,
        -0.7213177388100382,
        -0.7443858456985217,
        -0.7526298138231393,
        -0.7244912319356556,
        -0.7432417778491198,
        -0.7282286117865461,
        -0.7492092767608032,
        -0.7466359000120671,
        -0.7489474575203825,
        -0.7549260391538277,
        -0.7429665851264614,
        -0.7596811599942142,
        -0.7485809427170109,
        -0.7449041411327768,
        -0.6388068609843851,
        -0.7496577618067679,
        -0.6322077637102115,
        -0.7441777427020773,
        -0.75765511969656,
        -0.7427688287043908,
        -0.7541216706474307,
        -0.7479686773947454,
        -0.7493657455941487,
        -0.7276614943822095,
        1.0,
        -0.6197495429108197,
        -0.7469743797947219,
        -0.7553878235735506,
        -0.7158736454767374,
        -0.7458791760284109,
        -0.7567313088435212
      ],
      [
        -0.7225027004798764,
        -0.746347308731522,
        -0.7455021833949196,
        -0.744738962311157,
        -0.7446542573845802,
        -0.7368742685000047,
        -0.6914570821633808,
        -0.7438446776360697,
        -0.7565791562839375,
        -0.6308931515605991,
        -0.6055280803239926,
        -0.7465070270072105,
        -0.7323759636905501,
        -0.7478130411222694,
        0.2537772202666506,
        -0.5352068211636087,
        -0.7506130596565803,
        -0.7339044139496452,
        -0.7370033297435262,
        -0.7387202834098343,
        -0.5065001170215695,
        -0.7490801556454807,
        -0.7327084852768811,
        -0.7612488037469861,
        -0.7346184682874626,
        -0.6219712112105011,
        -0.7422754108919781,
        -0.7370175898471663,
        -0.7475086473366213,
        -0.7368712885251247,
        -0.7362648634727156,
        -0.7355755489373552,
        -0.7333051252271173,
        -0.7485998060387085,
        -0.7432069046724381,
        -0.7402101445061633,
        -0.7486835240638441,
        -0.750478558722026,
        -0.7413133406810423,
        -0.7414098855640554,
        -0.7373371508234751,
        -0.600278985303002,
        -0.7426750371865511,
        -0.5726287072322112,
        -0.7370769195166871,
        -0.7453338366006418,
        -0.6524087504225775,
        -0.7312266053925549,
        -0.7435877413056069,
        -0.7394591737356526,
        -0.7294106197606144,
        -0.6197495429108197,
        1.0,
        -0.7438712775297849,
        -0.7552905109114113,
        -0.7301956541374195,
        -0.7403616926621808,
        -0.7471511444359682
      ],
      [
        -0.7338107729505481,
        -0.3336389846282717,
        -0.7306697429367099,
        -0.7493133576850282,
        -0.7485015346625921,
        -0.4901461611819724,
        -0.7564615179535383,
        -0.734250335283045,
        -0.7314009067874033,
        -0.7355610687050749,
        -0.740044981344046,
        -0.731922994531836,
        -0.750310216574253,
        -0.7214809730865643,
        -0.7457883827850463,
        -0.7274395958302461,
        -0.7450746535866997,
        -0.728099620610072,
        -0.7348579108577352,
        -0.45369742349805453,
        -0.7400233367342903,
        -0.7466585769561884,
        -0.7256648089078943,
        -0.7576772063944075,
        -0.7311230839114119,
        -0.7225296195900304,
        -0.7137050728231855,
        -0.7442047129466713,
        -0.7439741729572984,
        -0.7533764169623104,
        -0.7107070586587925,
        -0.7241444500715466,
        -0.7357036444039802,
        -0.742387954020105,
        -0.7440512093127363,
        -0.7356957100840215,
        -0.7457379281418439,
        -0.7542010767589028,
        -0.7597339133209068,
        -0.7331758412914127,
        -0.7367529682737106,
        -0.7482961309461562,
        -0.7366019666032264,
        -0.7459376032416735,
        -0.7366219020914866,
        -0.7526307575707876,
        -0.7297649907128241,
        -0.7448192932275279,
        -0.7379510217451484,
        -0.748117497417109,
        -0.7262914589525894,
        -0.7469743797947219,
        -0.7438712775297849,
        1.0,
        -0.7496435523234806,
        -0.7250450001178346,
        -0.7442540769471191,
        -0.7533616074507483
      ],
      [
        -0.7564497127826798,
        -0.7499063719825448,
        -0.5604831259604142,
        -0.7585424939304897,
        -0.7340070391448087,
        -0.7225931274166895,
        -0.7379064590259695,
        -0.7406662213120991,
        -0.7576907761993493,
        -0.7476278665595395,
        -0.7161801544560131,
        -0.7559216223934047,
        -0.7433957127503699,
        -0.7046037488794017,
        -0.759934661228392,
        -0.7461242782785251,
        -0.7527545619440505,
        -0.7512444497165293,
        -0.7406538218961156,
        -0.7499151986300636,
        -0.7571550169017217,
        -0.7356177424966693,
        -0.7533904680679768,
        -0.7635136096922109,
        -0.7418146228087161,
        -0.7277933737820277,
        -0.7454359667090891,
        -0.7575737715412187,
        -0.7517617363341847,
        -0.7564823696874189,
        -0.7127732253169722,
        -0.7342082366659769,
        -0.7295358667292523,
        -0.7560640307724429,
        -0.7554213704621204,
        -0.7468129091308935,
        -0.7615302646706784,
        -0.7567897558630955,
        -0.7569916934602181,
        -0.7464578428459525,
        -0.7436361097616828,
        -0.7579159698806013,
        -0.7466192375717816,
        -0.7568075041243327,
        -0.743613472639779,
        -0.7426602128574744,
        -0.7474043960116408,
        -0.7503828819192377,
        -0.7560949328228916,
        -0.7431391098819469,
        -0.7488900917401695,
        -0.7553878235735506,
        -0.7552905109114113,
        -0.7496435523234806,
        1.0,
        -0.7432119322528503,
        -0.7503820058240609,
        -0.7284213600408476
      ],
      [
        -0.5994146543911834,
        -0.7123174600852862,
        -0.7069860222978848,
        -0.7128367503177999,
        -0.7240580814707774,
        -0.6849281891019946,
        -0.7371665389107405,
        -0.709621252297393,
        -0.7397039239576504,
        -0.6783829835601936,
        -0.729570795966249,
        -0.7155025064205223,
        -0.7095508331769722,
        -0.7236048024425539,
        -0.7325004036538055,
        -0.6902614914698022,
        -0.7264683519391855,
        -0.6800286342426569,
        -0.7189861515292149,
        -0.6871331009047825,
        -0.7342195072310472,
        -0.7387273297929351,
        -0.2663350535025509,
        -0.7544965604510918,
        0.36285794257309684,
        -0.721952332250041,
        -0.7083048216294666,
        -0.7223443661238718,
        -0.7347644044407613,
        -0.7354687106424074,
        -0.6956911035671217,
        -0.6991513495344854,
        -0.6724990310622327,
        -0.7140915638228402,
        -0.6770878318135769,
        -0.7284084648225191,
        -0.7381851759968482,
        -0.7170745204794721,
        -0.7530703327670338,
        -0.7295412885761172,
        -0.7238932021707991,
        -0.7281952176734854,
        -0.7276583689559231,
        -0.7114879904679652,
        -0.7251005880039729,
        -0.7465109605897942,
        -0.5790370720980876,
        -0.7343038585520367,
        -0.7123220692913674,
        -0.7382234787971967,
        -0.6980376612895905,
        -0.7158736454767374,
        -0.7301956541374195,
        -0.7250450001178346,
        -0.7432119322528503,
        1.0,
        -0.698186771422386,
        -0.7472040803391578
      ],
      [
        -0.7208277939188312,
        -0.7314712108376791,
        -0.7160077160090756,
        -0.74861005420395,
        -0.7404863609359921,
        -0.7029069996919922,
        -0.7395029061021525,
        -0.7328561100862572,
        -0.7430915001571432,
        -0.7173963221973987,
        -0.7365193026096397,
        -0.7350086583233152,
        -0.7313706624028791,
        -0.733070311241038,
        -0.7446938102973999,
        -0.7140072103310794,
        -0.7231292448280093,
        -0.6977428465753586,
        -0.7365547156030496,
        -0.7280608767721534,
        -0.7482607133141055,
        -0.7428125282243667,
        -0.7162572754421579,
        -0.7565354287754582,
        -0.7049990106851363,
        -0.7296541360935869,
        -0.7204247458242958,
        -0.7422818394644197,
        -0.7452006975762886,
        -0.7489301423036041,
        -0.708347452129384,
        -0.7200909055988464,
        -0.7137263157434992,
        -0.7395863361237283,
        -0.7380594016260819,
        -0.7402262827720372,
        -0.7536487401929526,
        -0.6929897661801314,
        -0.7507881824430498,
        -0.7407605267852451,
        -0.7339582128612718,
        -0.7446629440002707,
        -0.7429571219790605,
        -0.7371524288361178,
        -0.7364845249483202,
        -0.7494755549743104,
        -0.7328930676896985,
        -0.7461463682924531,
        -0.7428063390743993,
        -0.7436942902296951,
        -0.7217746235713737,
        -0.7458791760284109,
        -0.7403616926621808,
        -0.7442540769471191,
        -0.7503820058240609,
        -0.698186771422386,
        1.0,
        -0.7502173694011879
      ],
      [
        -0.7580682721130148,
        -0.753893219430569,
        -0.6686164856855561,
        -0.7614927697630127,
        -0.7455808891135249,
        -0.7340085125033291,
        -0.7420409507374789,
        -0.7526551981623972,
        -0.7569600609961148,
        -0.7558969874742214,
        -0.6946422012852305,
        -0.7557837424414817,
        -0.7564601214932738,
        -0.7335467619151981,
        -0.7565530790533396,
        -0.7519763137923064,
        -0.7535799532600379,
        -0.7472709578230737,
        -0.7467284363453922,
        -0.7491865678163924,
        -0.7408399582242554,
        -0.6899623593566162,
        -0.7575522223224951,
        -0.7601393055867935,
        -0.744799601291267,
        -0.6987485507208271,
        -0.7499113795282457,
        -0.7491205375602257,
        -0.7543136932799991,
        -0.7552391635454785,
        -0.7380747732765534,
        -0.7444192801586147,
        -0.7463111447014816,
        -0.7574956908005781,
        -0.7575282455361336,
        -0.7453429672622707,
        -0.7589011080650856,
        -0.7569305600834475,
        -0.7470219834346901,
        -0.7010456081254577,
        -0.7226259437688252,
        -0.7561531216179564,
        -0.7521938895126222,
        -0.7559788778172629,
        -0.7280592760327782,
        0.053109763886126864,
        -0.7425921665320439,
        -0.7276080445233364,
        -0.757778557567029,
        -0.23375975290240383,
        -0.7483519245314751,
        -0.7567313088435212,
        -0.7471511444359682,
        -0.7533616074507483,
        -0.7284213600408476,
        -0.7472040803391578,
        -0.7502173694011879,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        52,
        5,
        3,
        8,
        5,
        20,
        15,
        26,
        3,
        3,
        19,
        31,
        17,
        14,
        3,
        29,
        24,
        10,
        7,
        29,
        3,
        13,
        47,
        8,
        38,
        5,
        9,
        19,
        7,
        20,
        0,
        6,
        17,
        21,
        17,
        8,
        13,
        6,
        7,
        2,
        17,
        1,
        24,
        6,
        1,
        9,
        12,
        18,
        5,
        3,
        66,
        1,
        5,
        5,
        2,
        5,
        10,
        2
      ],
      "2020-02": [
        66,
        4,
        2,
        6,
        4,
        24,
        25,
        62,
        3,
        8,
        16,
        25,
        23,
        11,
        1,
        41,
        25,
        16,
        9,
        31,
        5,
        17,
        59,
        5,
        38,
        9,
        12,
        24,
        1,
        18,
        0,
        13,
        19,
        12,
        27,
        8,
        13,
        6,
        4,
        3,
        43,
        4,
        19,
        6,
        6,
        13,
        6,
        22,
        17,
        3,
        73,
        5,
        11,
        7,
        3,
        2,
        14,
        5
      ],
      "2020-03": [
        67,
        16,
        5,
        15,
        12,
        25,
        37,
        58,
        7,
        12,
        22,
        61,
        36,
        22,
        4,
        68,
        37,
        21,
        23,
        40,
        7,
        14,
        64,
        7,
        56,
        11,
        20,
        28,
        9,
        40,
        0,
        17,
        46,
        27,
        49,
        11,
        15,
        16,
        12,
        7,
        49,
        5,
        34,
        13,
        8,
        5,
        16,
        28,
        34,
        2,
        76,
        9,
        9,
        6,
        11,
        13,
        28,
        10
      ],
      "2020-04": [
        75,
        19,
        10,
        16,
        12,
        21,
        26,
        45,
        5,
        15,
        18,
        46,
        31,
        20,
        1,
        47,
        27,
        16,
        26,
        36,
        2,
        24,
        72,
        8,
        56,
        6,
        9,
        38,
        13,
        32,
        0,
        25,
        35,
        14,
        52,
        17,
        22,
        16,
        10,
        4,
        52,
        11,
        35,
        17,
        11,
        9,
        9,
        31,
        25,
        6,
        92,
        5,
        13,
        8,
        9,
        13,
        21,
        8
      ],
      "2020-05": [
        84,
        11,
        2,
        8,
        8,
        13,
        22,
        38,
        3,
        6,
        15,
        31,
        29,
        20,
        1,
        54,
        29,
        17,
        12,
        36,
        3,
        21,
        50,
        7,
        38,
        12,
        12,
        28,
        6,
        21,
        1,
        12,
        27,
        13,
        35,
        11,
        16,
        6,
        13,
        6,
        27,
        10,
        25,
        11,
        6,
        18,
        7,
        27,
        18,
        5,
        71,
        4,
        10,
        6,
        3,
        4,
        11,
        11
      ],
      "2020-06": [
        91,
        13,
        3,
        16,
        6,
        32,
        40,
        60,
        7,
        13,
        20,
        45,
        22,
        15,
        1,
        58,
        35,
        32,
        13,
        50,
        5,
        18,
        65,
        5,
        41,
        13,
        19,
        24,
        10,
        41,
        0,
        17,
        29,
        20,
        49,
        13,
        19,
        22,
        9,
        3,
        50,
        7,
        23,
        9,
        12,
        16,
        3,
        26,
        32,
        2,
        110,
        9,
        21,
        10,
        7,
        7,
        19,
        9
      ],
      "2020-07": [
        89,
        10,
        5,
        9,
        12,
        40,
        49,
        71,
        7,
        12,
        31,
        57,
        46,
        27,
        3,
        80,
        36,
        37,
        26,
        47,
        4,
        24,
        94,
        9,
        60,
        13,
        15,
        36,
        8,
        43,
        0,
        15,
        42,
        21,
        61,
        18,
        24,
        25,
        12,
        7,
        49,
        3,
        36,
        8,
        10,
        25,
        18,
        47,
        34,
        2,
        111,
        8,
        14,
        11,
        11,
        10,
        36,
        12
      ],
      "2020-08": [
        66,
        19,
        4,
        9,
        10,
        29,
        52,
        40,
        13,
        11,
        30,
        43,
        29,
        20,
        2,
        64,
        23,
        33,
        25,
        37,
        4,
        25,
        59,
        8,
        64,
        8,
        16,
        36,
        6,
        39,
        0,
        11,
        40,
        22,
        57,
        24,
        14,
        14,
        8,
        4,
        38,
        3,
        20,
        15,
        6,
        8,
        11,
        45,
        26,
        3,
        75,
        1,
        10,
        6,
        10,
        4,
        22,
        6
      ],
      "2020-09": [
        86,
        19,
        3,
        16,
        7,
        23,
        24,
        49,
        4,
        12,
        20,
        30,
        27,
        25,
        2,
        52,
        34,
        28,
        17,
        27,
        4,
        24,
        47,
        13,
        35,
        8,
        6,
        32,
        10,
        25,
        1,
        12,
        33,
        13,
        41,
        15,
        18,
        14,
        10,
        4,
        32,
        6,
        11,
        9,
        7,
        7,
        6,
        24,
        23,
        4,
        73,
        7,
        10,
        9,
        6,
        7,
        23,
        4
      ],
      "2020-10": [
        103,
        10,
        7,
        10,
        10,
        27,
        32,
        75,
        6,
        4,
        23,
        37,
        35,
        22,
        4,
        46,
        34,
        45,
        23,
        38,
        6,
        28,
        62,
        12,
        56,
        7,
        5,
        22,
        5,
        31,
        0,
        26,
        29,
        17,
        52,
        18,
        16,
        12,
        8,
        5,
        47,
        9,
        31,
        9,
        8,
        12,
        11,
        30,
        25,
        5,
        100,
        4,
        14,
        8,
        17,
        12,
        19,
        12
      ],
      "2020-11": [
        70,
        15,
        4,
        16,
        14,
        30,
        40,
        52,
        13,
        12,
        28,
        54,
        29,
        19,
        3,
        65,
        35,
        40,
        18,
        54,
        2,
        22,
        65,
        15,
        35,
        8,
        19,
        31,
        10,
        40,
        0,
        21,
        30,
        27,
        55,
        16,
        14,
        14,
        6,
        9,
        33,
        6,
        35,
        10,
        7,
        12,
        12,
        47,
        32,
        6,
        92,
        4,
        23,
        11,
        8,
        5,
        21,
        10
      ],
      "2020-12": [
        75,
        13,
        4,
        16,
        12,
        22,
        36,
        68,
        22,
        23,
        23,
        49,
        35,
        26,
        8,
        68,
        21,
        42,
        26,
        42,
        8,
        23,
        81,
        3,
        60,
        14,
        16,
        36,
        11,
        34,
        0,
        10,
        29,
        25,
        79,
        11,
        28,
        15,
        9,
        3,
        35,
        6,
        37,
        8,
        7,
        16,
        14,
        30,
        36,
        4,
        85,
        9,
        13,
        11,
        10,
        9,
        34,
        10
      ],
      "2021-01": [
        60,
        15,
        2,
        13,
        9,
        22,
        23,
        48,
        6,
        4,
        23,
        27,
        25,
        12,
        6,
        50,
        26,
        22,
        5,
        28,
        4,
        15,
        62,
        6,
        28,
        14,
        10,
        24,
        8,
        27,
        0,
        18,
        36,
        12,
        29,
        18,
        13,
        8,
        7,
        4,
        41,
        5,
        15,
        7,
        4,
        6,
        8,
        26,
        15,
        0,
        62,
        8,
        11,
        11,
        5,
        9,
        18,
        4
      ],
      "2021-02": [
        58,
        11,
        3,
        6,
        7,
        21,
        20,
        43,
        5,
        5,
        15,
        28,
        23,
        14,
        2,
        45,
        24,
        26,
        10,
        30,
        5,
        10,
        65,
        9,
        37,
        8,
        9,
        18,
        10,
        15,
        0,
        20,
        26,
        9,
        24,
        10,
        11,
        9,
        9,
        3,
        30,
        7,
        21,
        4,
        3,
        11,
        4,
        20,
        18,
        3,
        62,
        4,
        4,
        4,
        3,
        5,
        22,
        5
      ],
      "2021-03": [
        97,
        21,
        7,
        16,
        10,
        28,
        36,
        83,
        23,
        22,
        45,
        76,
        31,
        22,
        2,
        68,
        41,
        57,
        25,
        59,
        7,
        32,
        97,
        22,
        80,
        15,
        13,
        44,
        11,
        48,
        0,
        37,
        54,
        30,
        67,
        12,
        34,
        22,
        13,
        4,
        65,
        11,
        47,
        12,
        11,
        13,
        14,
        46,
        49,
        5,
        75,
        10,
        19,
        10,
        12,
        15,
        45,
        4
      ],
      "2021-04": [
        76,
        13,
        9,
        16,
        11,
        24,
        44,
        43,
        14,
        14,
        40,
        58,
        37,
        27,
        7,
        72,
        23,
        41,
        28,
        49,
        8,
        21,
        68,
        9,
        72,
        13,
        27,
        46,
        12,
        40,
        0,
        34,
        42,
        18,
        59,
        16,
        15,
        19,
        11,
        7,
        54,
        5,
        44,
        13,
        5,
        25,
        18,
        54,
        31,
        13,
        79,
        11,
        30,
        12,
        9,
        9,
        40,
        7
      ],
      "2021-05": [
        74,
        14,
        2,
        16,
        14,
        30,
        34,
        40,
        10,
        9,
        27,
        32,
        32,
        41,
        1,
        51,
        33,
        39,
        18,
        41,
        12,
        19,
        65,
        9,
        49,
        11,
        22,
        31,
        8,
        33,
        0,
        39,
        31,
        16,
        46,
        14,
        18,
        13,
        21,
        4,
        47,
        7,
        23,
        11,
        8,
        9,
        9,
        37,
        19,
        1,
        73,
        7,
        15,
        4,
        11,
        9,
        24,
        2
      ],
      "2021-06": [
        87,
        20,
        5,
        13,
        10,
        31,
        37,
        60,
        16,
        15,
        32,
        35,
        39,
        39,
        4,
        76,
        23,
        48,
        31,
        60,
        5,
        18,
        77,
        9,
        68,
        7,
        14,
        26,
        7,
        28,
        0,
        59,
        39,
        26,
        45,
        15,
        23,
        26,
        13,
        4,
        54,
        13,
        17,
        12,
        7,
        15,
        17,
        28,
        30,
        4,
        95,
        13,
        17,
        12,
        5,
        11,
        34,
        15
      ],
      "2021-07": [
        81,
        12,
        3,
        15,
        10,
        24,
        36,
        44,
        7,
        9,
        25,
        52,
        22,
        24,
        4,
        54,
        18,
        35,
        25,
        46,
        3,
        30,
        81,
        4,
        71,
        12,
        13,
        27,
        11,
        26,
        0,
        48,
        34,
        16,
        64,
        12,
        14,
        15,
        6,
        2,
        42,
        4,
        24,
        10,
        5,
        13,
        11,
        22,
        18,
        10,
        71,
        13,
        15,
        3,
        15,
        18,
        36,
        6
      ],
      "2021-08": [
        85,
        14,
        4,
        10,
        15,
        26,
        26,
        52,
        11,
        14,
        27,
        53,
        22,
        23,
        6,
        73,
        31,
        31,
        25,
        41,
        4,
        13,
        87,
        9,
        73,
        8,
        26,
        35,
        9,
        33,
        0,
        43,
        39,
        16,
        60,
        18,
        20,
        13,
        10,
        6,
        36,
        12,
        28,
        17,
        7,
        12,
        12,
        35,
        33,
        7,
        57,
        10,
        14,
        9,
        3,
        16,
        35,
        8
      ],
      "2021-09": [
        70,
        8,
        11,
        10,
        14,
        31,
        30,
        40,
        7,
        10,
        34,
        32,
        29,
        21,
        7,
        49,
        21,
        32,
        22,
        39,
        8,
        20,
        71,
        13,
        64,
        14,
        19,
        32,
        9,
        16,
        0,
        37,
        28,
        25,
        35,
        9,
        15,
        11,
        22,
        6,
        39,
        10,
        30,
        8,
        5,
        14,
        6,
        25,
        22,
        3,
        66,
        12,
        9,
        9,
        6,
        9,
        16,
        10
      ],
      "2021-10": [
        79,
        15,
        3,
        10,
        12,
        37,
        41,
        61,
        11,
        14,
        39,
        46,
        36,
        22,
        4,
        80,
        32,
        53,
        24,
        41,
        5,
        21,
        75,
        5,
        71,
        10,
        27,
        35,
        12,
        32,
        0,
        40,
        45,
        24,
        55,
        15,
        17,
        17,
        17,
        7,
        52,
        6,
        26,
        12,
        12,
        17,
        11,
        27,
        22,
        4,
        93,
        12,
        12,
        12,
        14,
        15,
        33,
        12
      ],
      "2021-11": [
        68,
        14,
        7,
        18,
        16,
        26,
        31,
        60,
        21,
        9,
        30,
        37,
        34,
        33,
        2,
        42,
        27,
        43,
        22,
        37,
        3,
        26,
        69,
        6,
        52,
        11,
        14,
        34,
        11,
        36,
        0,
        59,
        36,
        23,
        62,
        16,
        29,
        12,
        15,
        9,
        34,
        13,
        27,
        13,
        11,
        19,
        13,
        38,
        30,
        4,
        59,
        11,
        15,
        11,
        7,
        16,
        41,
        7
      ],
      "2021-12": [
        70,
        15,
        9,
        11,
        9,
        40,
        41,
        55,
        28,
        18,
        40,
        54,
        20,
        24,
        3,
        81,
        22,
        51,
        25,
        50,
        4,
        22,
        83,
        7,
        48,
        10,
        19,
        33,
        7,
        35,
        0,
        48,
        54,
        23,
        65,
        16,
        16,
        20,
        13,
        9,
        40,
        7,
        38,
        8,
        6,
        19,
        13,
        42,
        26,
        4,
        69,
        9,
        17,
        10,
        9,
        10,
        26,
        6
      ],
      "2022-01": [
        76,
        15,
        7,
        18,
        3,
        23,
        14,
        47,
        9,
        7,
        35,
        26,
        18,
        20,
        2,
        53,
        17,
        35,
        14,
        28,
        3,
        27,
        54,
        7,
        33,
        10,
        10,
        25,
        6,
        23,
        0,
        48,
        22,
        13,
        41,
        11,
        15,
        14,
        9,
        4,
        38,
        5,
        15,
        12,
        3,
        10,
        1,
        28,
        30,
        7,
        72,
        5,
        7,
        7,
        14,
        15,
        27,
        8
      ],
      "2022-02": [
        69,
        14,
        7,
        12,
        17,
        27,
        24,
        56,
        12,
        11,
        19,
        35,
        21,
        15,
        1,
        45,
        23,
        41,
        7,
        27,
        9,
        10,
        64,
        13,
        38,
        8,
        9,
        16,
        7,
        33,
        0,
        23,
        20,
        23,
        46,
        10,
        11,
        12,
        11,
        3,
        36,
        5,
        23,
        9,
        7,
        19,
        10,
        34,
        16,
        2,
        63,
        9,
        14,
        14,
        9,
        7,
        28,
        9
      ],
      "2022-03": [
        123,
        23,
        16,
        15,
        20,
        48,
        46,
        65,
        27,
        31,
        43,
        85,
        46,
        40,
        6,
        99,
        34,
        70,
        29,
        45,
        14,
        43,
        119,
        13,
        85,
        19,
        18,
        37,
        12,
        42,
        0,
        80,
        60,
        38,
        103,
        13,
        29,
        26,
        27,
        7,
        74,
        7,
        59,
        11,
        9,
        28,
        22,
        72,
        47,
        9,
        87,
        22,
        28,
        7,
        10,
        14,
        58,
        9
      ],
      "2022-04": [
        81,
        21,
        13,
        16,
        11,
        38,
        42,
        56,
        19,
        19,
        34,
        52,
        35,
        24,
        8,
        77,
        29,
        30,
        27,
        38,
        4,
        24,
        73,
        8,
        64,
        17,
        17,
        31,
        6,
        34,
        0,
        54,
        48,
        17,
        51,
        20,
        22,
        25,
        6,
        4,
        47,
        7,
        36,
        7,
        5,
        24,
        8,
        37,
        34,
        6,
        69,
        14,
        11,
        9,
        15,
        21,
        26,
        12
      ],
      "2022-05": [
        95,
        17,
        12,
        20,
        7,
        33,
        30,
        52,
        30,
        18,
        30,
        47,
        37,
        25,
        6,
        54,
        15,
        46,
        17,
        42,
        11,
        19,
        62,
        5,
        45,
        11,
        23,
        24,
        6,
        34,
        1,
        56,
        28,
        15,
        63,
        20,
        23,
        16,
        13,
        8,
        33,
        7,
        30,
        10,
        8,
        17,
        9,
        31,
        29,
        5,
        67,
        14,
        10,
        14,
        13,
        12,
        35,
        16
      ],
      "2022-06": [
        109,
        15,
        10,
        14,
        19,
        42,
        42,
        66,
        20,
        19,
        37,
        47,
        44,
        22,
        5,
        73,
        29,
        63,
        15,
        29,
        10,
        12,
        75,
        6,
        61,
        14,
        26,
        24,
        19,
        28,
        0,
        55,
        44,
        18,
        42,
        15,
        12,
        21,
        21,
        2,
        38,
        12,
        29,
        8,
        4,
        11,
        9,
        44,
        43,
        9,
        75,
        21,
        20,
        8,
        4,
        11,
        30,
        10
      ],
      "2022-07": [
        98,
        12,
        16,
        16,
        11,
        34,
        40,
        59,
        28,
        23,
        53,
        67,
        36,
        36,
        8,
        82,
        25,
        75,
        24,
        53,
        10,
        30,
        91,
        13,
        84,
        13,
        29,
        44,
        14,
        36,
        0,
        72,
        54,
        22,
        83,
        24,
        21,
        27,
        25,
        8,
        44,
        8,
        46,
        13,
        7,
        27,
        17,
        55,
        34,
        7,
        82,
        23,
        25,
        9,
        11,
        21,
        45,
        9
      ],
      "2022-08": [
        78,
        13,
        10,
        15,
        12,
        40,
        36,
        59,
        21,
        15,
        34,
        47,
        27,
        29,
        8,
        56,
        11,
        54,
        17,
        30,
        7,
        24,
        63,
        8,
        62,
        14,
        19,
        23,
        5,
        28,
        0,
        48,
        32,
        25,
        50,
        18,
        7,
        14,
        11,
        5,
        40,
        8,
        33,
        12,
        14,
        12,
        15,
        35,
        20,
        9,
        63,
        23,
        14,
        11,
        9,
        17,
        35,
        7
      ],
      "2022-09": [
        78,
        18,
        14,
        13,
        12,
        47,
        46,
        51,
        24,
        28,
        22,
        45,
        31,
        26,
        3,
        60,
        24,
        36,
        13,
        34,
        6,
        22,
        76,
        14,
        60,
        12,
        27,
        38,
        13,
        29,
        0,
        41,
        34,
        15,
        72,
        18,
        15,
        23,
        20,
        5,
        31,
        11,
        24,
        13,
        7,
        9,
        11,
        43,
        33,
        7,
        59,
        18,
        23,
        9,
        12,
        10,
        40,
        9
      ],
      "2022-10": [
        96,
        18,
        24,
        22,
        20,
        72,
        27,
        76,
        33,
        13,
        47,
        69,
        34,
        30,
        5,
        66,
        21,
        66,
        31,
        41,
        8,
        28,
        103,
        21,
        69,
        18,
        29,
        25,
        10,
        34,
        0,
        75,
        72,
        16,
        94,
        22,
        28,
        29,
        18,
        6,
        43,
        4,
        46,
        14,
        7,
        19,
        13,
        47,
        44,
        6,
        72,
        24,
        21,
        12,
        13,
        13,
        51,
        12
      ],
      "2022-11": [
        88,
        16,
        22,
        14,
        19,
        93,
        36,
        64,
        45,
        31,
        42,
        51,
        42,
        40,
        6,
        78,
        19,
        57,
        24,
        49,
        8,
        20,
        90,
        10,
        77,
        15,
        27,
        37,
        17,
        44,
        0,
        62,
        64,
        31,
        72,
        22,
        23,
        31,
        15,
        7,
        54,
        11,
        44,
        13,
        4,
        28,
        12,
        43,
        32,
        7,
        73,
        25,
        32,
        13,
        12,
        18,
        58,
        8
      ],
      "2022-12": [
        75,
        8,
        20,
        21,
        16,
        66,
        25,
        56,
        36,
        15,
        28,
        33,
        30,
        21,
        7,
        74,
        26,
        47,
        21,
        25,
        5,
        17,
        76,
        6,
        62,
        19,
        23,
        24,
        11,
        21,
        0,
        43,
        40,
        16,
        56,
        20,
        8,
        13,
        17,
        4,
        36,
        8,
        30,
        12,
        4,
        25,
        17,
        33,
        35,
        9,
        48,
        17,
        22,
        11,
        6,
        16,
        33,
        14
      ],
      "2023-01": [
        67,
        6,
        14,
        8,
        8,
        47,
        20,
        33,
        22,
        16,
        19,
        41,
        21,
        11,
        2,
        41,
        18,
        39,
        15,
        24,
        3,
        13,
        65,
        6,
        59,
        7,
        17,
        14,
        7,
        18,
        0,
        40,
        35,
        13,
        51,
        13,
        9,
        16,
        15,
        2,
        20,
        4,
        13,
        10,
        6,
        11,
        12,
        26,
        16,
        6,
        44,
        7,
        20,
        10,
        9,
        4,
        22,
        9
      ],
      "2023-02": [
        80,
        15,
        12,
        8,
        13,
        66,
        32,
        41,
        19,
        24,
        21,
        32,
        22,
        18,
        5,
        48,
        13,
        45,
        21,
        31,
        6,
        14,
        69,
        7,
        42,
        10,
        14,
        21,
        7,
        31,
        0,
        40,
        33,
        11,
        50,
        12,
        17,
        20,
        23,
        5,
        34,
        11,
        33,
        5,
        10,
        15,
        11,
        27,
        19,
        5,
        34,
        18,
        15,
        11,
        7,
        4,
        36,
        12
      ],
      "2023-03": [
        114,
        27,
        42,
        29,
        12,
        129,
        52,
        97,
        76,
        48,
        59,
        71,
        54,
        41,
        12,
        98,
        38,
        79,
        39,
        55,
        12,
        45,
        117,
        16,
        99,
        31,
        59,
        39,
        16,
        42,
        0,
        79,
        86,
        46,
        115,
        24,
        25,
        49,
        38,
        14,
        45,
        11,
        64,
        11,
        10,
        36,
        27,
        67,
        65,
        16,
        89,
        34,
        32,
        13,
        16,
        19,
        78,
        16
      ],
      "2023-04": [
        114,
        17,
        30,
        29,
        23,
        76,
        38,
        50,
        52,
        37,
        28,
        66,
        40,
        29,
        4,
        80,
        18,
        50,
        37,
        44,
        13,
        21,
        83,
        10,
        67,
        23,
        37,
        32,
        15,
        32,
        0,
        64,
        60,
        20,
        88,
        32,
        26,
        40,
        19,
        3,
        53,
        8,
        29,
        11,
        13,
        13,
        11,
        56,
        36,
        12,
        59,
        13,
        22,
        17,
        6,
        17,
        44,
        6
      ],
      "2023-05": [
        124,
        25,
        57,
        21,
        12,
        151,
        38,
        69,
        38,
        19,
        51,
        52,
        38,
        58,
        7,
        78,
        25,
        63,
        15,
        47,
        8,
        21,
        91,
        10,
        65,
        24,
        30,
        26,
        9,
        26,
        0,
        78,
        61,
        30,
        81,
        23,
        18,
        24,
        17,
        7,
        56,
        15,
        34,
        13,
        6,
        37,
        17,
        42,
        37,
        13,
        82,
        26,
        25,
        14,
        18,
        20,
        60,
        19
      ],
      "2023-06": [
        110,
        15,
        44,
        29,
        23,
        127,
        27,
        74,
        32,
        21,
        50,
        60,
        33,
        38,
        7,
        70,
        28,
        52,
        16,
        32,
        4,
        18,
        80,
        10,
        60,
        14,
        32,
        23,
        16,
        30,
        1,
        57,
        65,
        16,
        76,
        17,
        25,
        31,
        17,
        6,
        42,
        12,
        31,
        16,
        4,
        31,
        11,
        53,
        31,
        11,
        56,
        25,
        14,
        11,
        10,
        9,
        48,
        14
      ],
      "2023-07": [
        122,
        18,
        36,
        25,
        14,
        87,
        45,
        64,
        30,
        24,
        37,
        50,
        46,
        35,
        9,
        74,
        20,
        51,
        14,
        50,
        13,
        23,
        108,
        3,
        74,
        14,
        30,
        25,
        18,
        25,
        1,
        57,
        52,
        41,
        74,
        16,
        24,
        21,
        23,
        9,
        37,
        15,
        32,
        8,
        9,
        28,
        18,
        36,
        28,
        14,
        53,
        30,
        26,
        15,
        4,
        12,
        41,
        9
      ],
      "2023-08": [
        109,
        19,
        48,
        17,
        26,
        100,
        42,
        60,
        45,
        21,
        37,
        50,
        42,
        54,
        6,
        91,
        21,
        63,
        31,
        39,
        7,
        38,
        113,
        12,
        91,
        26,
        40,
        36,
        13,
        56,
        1,
        71,
        55,
        30,
        72,
        27,
        20,
        33,
        25,
        3,
        62,
        18,
        45,
        15,
        12,
        27,
        22,
        62,
        45,
        10,
        60,
        28,
        30,
        9,
        19,
        25,
        62,
        13
      ],
      "2023-09": [
        110,
        13,
        47,
        28,
        14,
        105,
        40,
        54,
        47,
        31,
        47,
        59,
        53,
        24,
        14,
        82,
        22,
        50,
        29,
        32,
        14,
        20,
        114,
        17,
        92,
        23,
        48,
        24,
        17,
        46,
        1,
        65,
        64,
        30,
        61,
        20,
        17,
        28,
        34,
        4,
        40,
        8,
        39,
        9,
        15,
        33,
        21,
        49,
        45,
        11,
        43,
        22,
        17,
        14,
        14,
        12,
        43,
        21
      ],
      "2023-10": [
        109,
        19,
        66,
        33,
        21,
        109,
        30,
        77,
        51,
        34,
        34,
        62,
        57,
        33,
        4,
        68,
        22,
        51,
        16,
        30,
        9,
        20,
        101,
        6,
        74,
        24,
        46,
        39,
        15,
        41,
        0,
        51,
        57,
        17,
        59,
        19,
        25,
        28,
        16,
        1,
        41,
        8,
        40,
        12,
        9,
        37,
        11,
        52,
        25,
        8,
        69,
        28,
        23,
        14,
        11,
        15,
        58,
        9
      ],
      "2023-11": [
        125,
        11,
        81,
        25,
        14,
        159,
        43,
        72,
        76,
        18,
        53,
        60,
        55,
        36,
        10,
        84,
        14,
        48,
        32,
        37,
        18,
        28,
        97,
        13,
        75,
        40,
        53,
        40,
        19,
        36,
        3,
        51,
        66,
        31,
        79,
        21,
        29,
        33,
        24,
        5,
        47,
        19,
        40,
        9,
        10,
        35,
        14,
        58,
        54,
        12,
        66,
        22,
        28,
        17,
        12,
        9,
        55,
        15
      ],
      "2023-12": [
        104,
        21,
        84,
        26,
        21,
        179,
        40,
        81,
        114,
        31,
        60,
        57,
        61,
        39,
        11,
        77,
        23,
        45,
        33,
        42,
        10,
        36,
        92,
        11,
        73,
        45,
        72,
        42,
        15,
        44,
        1,
        46,
        66,
        19,
        104,
        36,
        27,
        33,
        25,
        18,
        51,
        11,
        50,
        7,
        4,
        27,
        16,
        94,
        40,
        19,
        42,
        22,
        24,
        27,
        9,
        15,
        71,
        12
      ],
      "2024-01": [
        87,
        23,
        57,
        29,
        12,
        83,
        31,
        57,
        41,
        24,
        38,
        49,
        52,
        30,
        8,
        64,
        16,
        41,
        11,
        20,
        13,
        24,
        90,
        9,
        73,
        34,
        45,
        28,
        14,
        34,
        1,
        47,
        37,
        23,
        53,
        26,
        30,
        17,
        24,
        3,
        43,
        8,
        29,
        15,
        9,
        21,
        12,
        49,
        26,
        10,
        49,
        17,
        20,
        15,
        8,
        12,
        40,
        12
      ],
      "2024-02": [
        103,
        8,
        93,
        19,
        16,
        116,
        25,
        82,
        52,
        22,
        28,
        44,
        43,
        35,
        7,
        57,
        14,
        33,
        18,
        30,
        10,
        22,
        86,
        20,
        77,
        22,
        36,
        29,
        11,
        31,
        0,
        57,
        45,
        24,
        52,
        15,
        20,
        25,
        21,
        5,
        35,
        2,
        34,
        21,
        5,
        14,
        10,
        46,
        36,
        10,
        55,
        22,
        17,
        16,
        12,
        5,
        41,
        11
      ],
      "2024-03": [
        155,
        17,
        128,
        28,
        28,
        221,
        49,
        86,
        127,
        46,
        55,
        94,
        68,
        41,
        7,
        91,
        36,
        71,
        35,
        39,
        14,
        45,
        115,
        21,
        115,
        52,
        84,
        44,
        15,
        62,
        1,
        89,
        81,
        38,
        103,
        27,
        35,
        67,
        32,
        9,
        69,
        16,
        66,
        9,
        4,
        45,
        21,
        110,
        43,
        17,
        73,
        41,
        42,
        28,
        10,
        16,
        67,
        15
      ],
      "2024-04": [
        119,
        19,
        87,
        33,
        16,
        146,
        45,
        59,
        82,
        22,
        50,
        64,
        49,
        42,
        8,
        81,
        23,
        44,
        25,
        31,
        16,
        44,
        97,
        17,
        68,
        28,
        49,
        22,
        21,
        28,
        0,
        71,
        72,
        24,
        102,
        33,
        30,
        33,
        30,
        4,
        42,
        18,
        51,
        11,
        16,
        20,
        16,
        82,
        28,
        11,
        54,
        23,
        27,
        19,
        19,
        14,
        55,
        14
      ],
      "2024-05": [
        138,
        15,
        113,
        39,
        22,
        157,
        35,
        84,
        83,
        34,
        49,
        55,
        64,
        40,
        8,
        68,
        19,
        56,
        18,
        29,
        10,
        29,
        93,
        16,
        94,
        40,
        68,
        43,
        24,
        43,
        1,
        76,
        71,
        38,
        63,
        25,
        30,
        32,
        21,
        6,
        49,
        17,
        25,
        17,
        3,
        37,
        12,
        88,
        35,
        8,
        54,
        25,
        25,
        23,
        20,
        8,
        62,
        4
      ],
      "2024-06": [
        139,
        18,
        140,
        40,
        24,
        192,
        38,
        66,
        79,
        26,
        84,
        70,
        51,
        39,
        6,
        69,
        12,
        40,
        20,
        32,
        20,
        19,
        97,
        8,
        78,
        65,
        65,
        30,
        27,
        42,
        0,
        78,
        61,
        21,
        53,
        18,
        29,
        25,
        25,
        4,
        43,
        18,
        44,
        14,
        6,
        27,
        20,
        59,
        39,
        9,
        59,
        23,
        35,
        21,
        15,
        8,
        58,
        16
      ],
      "2024-07": [
        146,
        14,
        127,
        24,
        32,
        161,
        53,
        81,
        75,
        34,
        57,
        51,
        57,
        40,
        16,
        85,
        17,
        54,
        24,
        34,
        13,
        60,
        116,
        17,
        84,
        49,
        67,
        36,
        18,
        44,
        0,
        92,
        72,
        40,
        100,
        34,
        40,
        53,
        35,
        9,
        50,
        14,
        47,
        15,
        9,
        50,
        22,
        76,
        49,
        10,
        63,
        37,
        35,
        27,
        20,
        11,
        83,
        8
      ],
      "2024-08": [
        133,
        18,
        98,
        21,
        15,
        108,
        42,
        72,
        62,
        43,
        53,
        54,
        46,
        39,
        11,
        88,
        17,
        43,
        13,
        34,
        13,
        35,
        80,
        10,
        67,
        43,
        55,
        28,
        24,
        35,
        0,
        61,
        83,
        23,
        72,
        18,
        28,
        24,
        25,
        4,
        52,
        20,
        42,
        11,
        12,
        22,
        22,
        57,
        31,
        22,
        52,
        22,
        28,
        15,
        18,
        17,
        61,
        12
      ],
      "2024-09": [
        136,
        18,
        102,
        23,
        22,
        146,
        33,
        61,
        77,
        33,
        57,
        90,
        69,
        43,
        15,
        86,
        32,
        37,
        23,
        35,
        15,
        33,
        116,
        16,
        84,
        45,
        55,
        39,
        11,
        49,
        1,
        64,
        57,
        25,
        81,
        20,
        26,
        33,
        37,
        4,
        58,
        17,
        47,
        27,
        8,
        29,
        12,
        75,
        41,
        15,
        74,
        28,
        29,
        19,
        18,
        11,
        73,
        26
      ],
      "2024-10": [
        172,
        14,
        191,
        36,
        30,
        214,
        51,
        78,
        113,
        27,
        74,
        64,
        49,
        38,
        12,
        89,
        23,
        36,
        22,
        29,
        15,
        38,
        82,
        18,
        65,
        73,
        62,
        49,
        10,
        52,
        0,
        71,
        75,
        33,
        97,
        18,
        24,
        37,
        23,
        7,
        41,
        16,
        35,
        27,
        8,
        51,
        15,
        75,
        39,
        17,
        68,
        31,
        31,
        17,
        18,
        10,
        52,
        9
      ],
      "2024-11": [
        138,
        17,
        128,
        52,
        18,
        191,
        39,
        81,
        111,
        25,
        60,
        58,
        55,
        38,
        11,
        71,
        28,
        51,
        18,
        40,
        13,
        29,
        108,
        13,
        81,
        78,
        68,
        25,
        17,
        58,
        3,
        69,
        58,
        20,
        70,
        38,
        36,
        37,
        27,
        4,
        47,
        16,
        35,
        20,
        2,
        31,
        15,
        71,
        28,
        15,
        56,
        35,
        24,
        26,
        17,
        6,
        45,
        7
      ],
      "2024-12": [
        146,
        24,
        185,
        33,
        31,
        237,
        44,
        91,
        123,
        29,
        88,
        81,
        88,
        47,
        15,
        96,
        33,
        35,
        20,
        42,
        22,
        24,
        103,
        16,
        98,
        110,
        81,
        45,
        15,
        49,
        0,
        72,
        79,
        29,
        90,
        24,
        31,
        44,
        43,
        14,
        66,
        21,
        46,
        15,
        7,
        48,
        13,
        109,
        33,
        17,
        56,
        20,
        43,
        31,
        11,
        15,
        72,
        16
      ],
      "2025-01": [
        103,
        13,
        115,
        32,
        13,
        112,
        33,
        57,
        65,
        25,
        58,
        58,
        48,
        24,
        6,
        55,
        15,
        45,
        23,
        27,
        14,
        32,
        91,
        16,
        61,
        53,
        45,
        28,
        18,
        37,
        0,
        58,
        61,
        28,
        74,
        18,
        24,
        15,
        28,
        4,
        45,
        11,
        25,
        15,
        8,
        18,
        18,
        54,
        26,
        12,
        54,
        21,
        20,
        17,
        15,
        11,
        36,
        14
      ],
      "2025-02": [
        122,
        17,
        146,
        21,
        23,
        158,
        47,
        70,
        55,
        16,
        53,
        47,
        58,
        31,
        7,
        57,
        19,
        32,
        12,
        24,
        20,
        38,
        82,
        12,
        64,
        73,
        45,
        27,
        16,
        34,
        0,
        62,
        59,
        27,
        81,
        19,
        31,
        32,
        30,
        7,
        36,
        23,
        43,
        21,
        4,
        20,
        13,
        55,
        29,
        13,
        53,
        23,
        24,
        14,
        12,
        4,
        51,
        19
      ],
      "2025-03": [
        171,
        24,
        230,
        58,
        34,
        249,
        71,
        73,
        167,
        38,
        116,
        109,
        103,
        43,
        9,
        91,
        37,
        47,
        30,
        51,
        24,
        47,
        115,
        25,
        107,
        116,
        92,
        44,
        21,
        69,
        0,
        86,
        82,
        45,
        114,
        35,
        53,
        63,
        46,
        10,
        62,
        30,
        63,
        20,
        9,
        61,
        17,
        123,
        60,
        28,
        52,
        25,
        50,
        32,
        23,
        18,
        85,
        33
      ],
      "2025-04": [
        137,
        22,
        154,
        52,
        24,
        157,
        43,
        66,
        78,
        27,
        89,
        59,
        60,
        40,
        9,
        68,
        17,
        33,
        27,
        19,
        21,
        32,
        69,
        15,
        85,
        58,
        53,
        30,
        15,
        53,
        0,
        81,
        63,
        15,
        82,
        23,
        34,
        24,
        39,
        5,
        44,
        19,
        45,
        21,
        8,
        33,
        22,
        80,
        35,
        14,
        44,
        15,
        33,
        23,
        15,
        9,
        57,
        17
      ],
      "2025-05": [
        161,
        20,
        270,
        53,
        37,
        252,
        59,
        76,
        85,
        25,
        98,
        78,
        74,
        56,
        9,
        70,
        28,
        63,
        29,
        27,
        26,
        49,
        99,
        18,
        106,
        94,
        73,
        42,
        15,
        58,
        2,
        82,
        70,
        29,
        93,
        20,
        63,
        46,
        40,
        5,
        60,
        22,
        50,
        23,
        13,
        53,
        11,
        108,
        39,
        16,
        59,
        28,
        56,
        12,
        21,
        12,
        77,
        28
      ],
      "2025-06": [
        151,
        19,
        241,
        38,
        21,
        185,
        77,
        74,
        103,
        29,
        108,
        69,
        86,
        45,
        16,
        65,
        29,
        46,
        20,
        28,
        17,
        37,
        98,
        18,
        76,
        106,
        67,
        35,
        21,
        48,
        1,
        71,
        74,
        32,
        80,
        17,
        52,
        31,
        36,
        4,
        45,
        13,
        38,
        18,
        6,
        45,
        22,
        111,
        43,
        15,
        35,
        28,
        37,
        28,
        16,
        9,
        70,
        15
      ],
      "2025-07": [
        167,
        10,
        182,
        47,
        33,
        158,
        42,
        56,
        80,
        24,
        65,
        80,
        105,
        47,
        17,
        87,
        27,
        46,
        19,
        22,
        20,
        57,
        99,
        9,
        75,
        56,
        70,
        49,
        20,
        57,
        1,
        75,
        64,
        32,
        70,
        20,
        53,
        37,
        23,
        7,
        72,
        21,
        44,
        25,
        8,
        33,
        23,
        100,
        37,
        17,
        58,
        28,
        35,
        26,
        20,
        7,
        56,
        15
      ],
      "2025-08": [
        160,
        22,
        221,
        25,
        30,
        170,
        58,
        78,
        109,
        29,
        84,
        74,
        68,
        42,
        12,
        99,
        26,
        29,
        21,
        21,
        28,
        42,
        107,
        8,
        103,
        69,
        62,
        41,
        18,
        54,
        1,
        68,
        73,
        39,
        71,
        28,
        49,
        45,
        48,
        8,
        59,
        19,
        58,
        20,
        10,
        48,
        21,
        97,
        33,
        21,
        40,
        36,
        55,
        19,
        17,
        12,
        67,
        16
      ],
      "2025-09": [
        72,
        6,
        73,
        15,
        16,
        63,
        29,
        25,
        29,
        13,
        22,
        33,
        31,
        16,
        6,
        36,
        10,
        33,
        5,
        6,
        10,
        17,
        53,
        6,
        31,
        22,
        35,
        14,
        7,
        31,
        0,
        32,
        36,
        13,
        34,
        10,
        26,
        8,
        17,
        3,
        16,
        13,
        24,
        21,
        5,
        12,
        5,
        40,
        20,
        2,
        22,
        8,
        18,
        12,
        7,
        3,
        34,
        2
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI",
          "year": "2022-09",
          "abstract": "BACKGROUND AND PURPOSE: Deep learning has been demonstrated effective in many\nneuroimaging applications. However, in many scenarios, the number of imaging\nsequences capturing information related to small vessel disease lesions is\ninsufficient to support data-driven techniques. Additionally, cohort-based\nstudies may not always have the optimal or essential imaging sequences for\naccurate lesion detection. Therefore, it is necessary to determine which\nimaging sequences are crucial for precise detection. This study introduces a\nnovel deep learning framework to detect enlarged perivascular spaces (ePVS) and\naims to find the optimal combination of MRI sequences for deep learning-based\nquantification. MATERIALS AND METHODS: We implemented an effective lightweight\nU-Net adapted for ePVS detection and comprehensively investigated different\ncombinations of information from SWI, FLAIR, T1-weighted (T1w), and T2-weighted\n(T2w) MRI sequences. The training data included 21 participants, which were\nrandomly selected from the MESA cohort. Participants had ePVS 683 lesions on\naverage. For T1w, T2w, and FLAIR images, the MESA study collected 3D isotropic\nMRI scans at six different sites with Siemens scanners. Our training data\nincluded participants from all these sites and all the scanner models, and the\nproposed model was applied to the whole brain instead of selective regions.\nRESULTS: The experimental results showed that T2w MRI is the most important for\naccurate ePVS detection, and the incorporation of SWI, FLAIR and T1w MRI in the\ndeep neural network had minor improvements in accuracy and resulted in the\nhighest sensitivity and precision (sensitivity =0.82, precision =0.83). The\nproposed method achieved comparable accuracy at a minimal time cost compared to\nmanual reading.",
          "arxiv_id": "2209.13727v2"
        },
        {
          "title": "Segment Anything in Medical Images",
          "year": "2023-04",
          "abstract": "Medical image segmentation is a critical component in clinical practice,\nfacilitating accurate diagnosis, treatment planning, and disease monitoring.\nHowever, existing methods, often tailored to specific modalities or disease\ntypes, lack generalizability across the diverse spectrum of medical image\nsegmentation tasks. Here we present MedSAM, a foundation model designed for\nbridging this gap by enabling universal medical image segmentation. The model\nis developed on a large-scale medical image dataset with 1,570,263 image-mask\npairs, covering 10 imaging modalities and over 30 cancer types. We conduct a\ncomprehensive evaluation on 86 internal validation tasks and 60 external\nvalidation tasks, demonstrating better accuracy and robustness than\nmodality-wise specialist models. By delivering accurate and efficient\nsegmentation across a wide spectrum of tasks, MedSAM holds significant\npotential to expedite the evolution of diagnostic tools and the personalization\nof treatment plans.",
          "arxiv_id": "2304.12306v3"
        },
        {
          "title": "A Precision Diagnostic Framework of Renal Cell Carcinoma on Whole-Slide Images using Deep Learning",
          "year": "2021-10",
          "abstract": "Diagnostic pathology, which is the basis and gold standard of cancer\ndiagnosis, provides essential information on the prognosis of the disease and\nvital evidence for clinical treatment. Tumor region detection, subtype and\ngrade classification are the fundamental diagnostic indicators for renal cell\ncarcinoma (RCC) in whole-slide images (WSIs). However, pathological diagnosis\nis subjective, differences in observation and diagnosis between pathologists is\ncommon in hospitals with inadequate diagnostic capacity. The main challenge for\ndeveloping deep learning based RCC diagnostic system is the lack of large-scale\ndatasets with precise annotations. In this work, we proposed a deep\nlearning-based framework for analyzing histopathological images of patients\nwith renal cell carcinoma, which has the potential to achieve pathologist-level\naccuracy in diagnosis. A deep convolutional neural network (InceptionV3) was\ntrained on the high-quality annotated dataset of The Cancer Genome Atlas (TCGA)\nwhole-slide histopathological image for accurate tumor area detection,\nclassification of RCC subtypes, and ISUP grades classification of clear cell\ncarcinoma subtypes. These results suggest that our framework can help\npathologists in the detection of cancer region and classification of subtypes\nand grades, which could be applied to any cancer type, providing auxiliary\ndiagnosis and promoting clinical consensus.",
          "arxiv_id": "2110.13652v1"
        }
      ],
      "1": [
        {
          "title": "Learning Many-to-Many Mapping for Unpaired Real-World Image Super-resolution and Downscaling",
          "year": "2023-10",
          "abstract": "Learning based single image super-resolution (SISR) for real-world images has\nbeen an active research topic yet a challenging task, due to the lack of paired\nlow-resolution (LR) and high-resolution (HR) training images. Most of the\nexisting unsupervised real-world SISR methods adopt a two-stage training\nstrategy by synthesizing realistic LR images from their HR counterparts first,\nthen training the super-resolution (SR) models in a supervised manner. However,\nthe training of image degradation and SR models in this strategy are separate,\nignoring the inherent mutual dependency between downscaling and its inverse\nupscaling process. Additionally, the ill-posed nature of image degradation is\nnot fully considered. In this paper, we propose an image downscaling and SR\nmodel dubbed as SDFlow, which simultaneously learns a bidirectional\nmany-to-many mapping between real-world LR and HR images unsupervisedly. The\nmain idea of SDFlow is to decouple image content and degradation information in\nthe latent space, where content information distribution of LR and HR images is\nmatched in a common latent space. Degradation information of the LR images and\nthe high-frequency information of the HR images are fitted to an easy-to-sample\nconditional distribution. Experimental results on real-world image SR datasets\nindicate that SDFlow can generate diverse realistic LR and SR images both\nquantitatively and qualitatively.",
          "arxiv_id": "2310.04964v1"
        },
        {
          "title": "InstructIR: High-Quality Image Restoration Following Human Instructions",
          "year": "2024-01",
          "abstract": "Image restoration is a fundamental problem that involves recovering a\nhigh-quality clean image from its degraded observation. All-In-One image\nrestoration models can effectively restore images from various types and levels\nof degradation using degradation-specific information as prompts to guide the\nrestoration model. In this work, we present the first approach that uses\nhuman-written instructions to guide the image restoration model. Given natural\nlanguage prompts, our model can recover high-quality images from their degraded\ncounterparts, considering multiple degradation types. Our method, InstructIR,\nachieves state-of-the-art results on several restoration tasks including image\ndenoising, deraining, deblurring, dehazing, and (low-light) image enhancement.\nInstructIR improves +1dB over previous all-in-one restoration methods.\nMoreover, our dataset and results represent a novel benchmark for new research\non text-guided image restoration and enhancement. Our code, datasets and models\nare available at: https://github.com/mv-lab/InstructIR",
          "arxiv_id": "2401.16468v5"
        },
        {
          "title": "SR+Codec: a Benchmark of Super-Resolution for Video Compression Bitrate Reduction",
          "year": "2023-05",
          "abstract": "In recent years, there has been significant interest in Super-Resolution\n(SR), which focuses on generating a high-resolution image from a low-resolution\ninput. Deep learning-based methods for super-resolution have been particularly\npopular and have shown impressive results on various benchmarks. However,\nresearch indicates that these methods may not perform as well on strongly\ncompressed videos. We developed a super-resolution benchmark to analyze SR's\ncapacity to upscale compressed videos. Our dataset employed video codecs based\non five widely-used compression standards: H.264, H.265, H.266, AV1, and AVS3.\nWe assessed 19 popular SR models using our benchmark and evaluated their\nability to restore details and their susceptibility to compression artifacts.\nTo get an accurate perceptual ranking of SR models, we conducted a\ncrowd-sourced side-by-side comparison of their outputs. We found that some SR\nmodels, combined with compression, allow us to reduce the video bitrate without\nsignificant loss of quality. We also compared a range of image and video\nquality metrics with subjective scores to evaluate their accuracy on\nsuper-resolved compressed videos. The benchmark is publicly available at\nhttps://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html",
          "arxiv_id": "2305.04844v3"
        }
      ],
      "2": [
        {
          "title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis",
          "year": "2024-08",
          "abstract": "Vision-language models (VLMs) have shown impressive zero- and few-shot\nperformance on real-world visual question answering (VQA) benchmarks, alluding\nto their capabilities as visual reasoning engines. However, the benchmarks\nbeing used conflate \"pure\" visual reasoning with world knowledge, and also have\nquestions that involve a limited number of reasoning steps. Thus, it remains\nunclear whether a VLM's apparent visual reasoning performance is due to its\nworld knowledge, or due to actual visual reasoning capabilities.\n  To clarify this ambiguity, we systematically benchmark and dissect the\nzero-shot visual reasoning capabilities of VLMs through synthetic datasets that\nrequire minimal world knowledge, and allow for analysis over a broad range of\nreasoning steps. We focus on two novel aspects of zero-shot visual reasoning:\ni) evaluating the impact of conveying scene information as either visual\nembeddings or purely textual scene descriptions to the underlying large\nlanguage model (LLM) of the VLM, and ii) comparing the effectiveness of\nchain-of-thought prompting to standard prompting for zero-shot visual\nreasoning.\n  We find that the underlying LLMs, when provided textual scene descriptions,\nconsistently perform better compared to being provided visual embeddings. In\nparticular, 18% higher accuracy is achieved on the PTR dataset. We also find\nthat CoT prompting performs marginally better than standard prompting only for\nthe comparatively large GPT-3.5-Turbo (175B) model, and does worse for\nsmaller-scale models. This suggests the emergence of CoT abilities for visual\nreasoning in LLMs at larger scales even when world knowledge is limited.\nOverall, we find limitations in the abilities of VLMs and LLMs for more complex\nvisual reasoning, and highlight the important role that LLMs can play in visual\nreasoning.",
          "arxiv_id": "2409.00106v1"
        },
        {
          "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
          "year": "2024-06",
          "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
          "arxiv_id": "2406.13621v1"
        },
        {
          "title": "FiVL: A Framework for Improved Vision-Language Alignment through the Lens of Training, Evaluation and Explainability",
          "year": "2024-12",
          "abstract": "Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. Furthermore, current\nvision-language benchmarks are not specifically measuring the degree to which\nthe answer require the visual input. This limitation makes it challenging to\nconfirm that the image is truly necessary, particularly in tasks like visual\nquestion answering. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nalso evaluate their effectiveness in achieving it. We demonstrate the value of\nour datasets through three approaches. First, we introduce a novel training\ntask based on our augmented training dataset, resulting in better performance\nthan the baseline. Second, we present benchmarks to assess the model's ability\nto use image as substantive evidence, rather than relying solely on linguistic\npriors. Finally, we identify attention heads with the strongest vision-language\nalignment, enabling explainability on visual-driven hallucinations. The code is\navailable at https://github.com/IntelLabs/fivl.",
          "arxiv_id": "2412.14672v2"
        }
      ],
      "3": [
        {
          "title": "Open-Vocabulary Remote Sensing Image Semantic Segmentation",
          "year": "2024-09",
          "abstract": "Open-vocabulary image semantic segmentation (OVS) seeks to segment images\ninto semantic regions across an open set of categories. Existing OVS methods\ncommonly depend on foundational vision-language models and utilize similarity\ncomputation to tackle OVS tasks. However, these approaches are predominantly\ntailored to natural images and struggle with the unique characteristics of\nremote sensing images, such as rapidly changing orientations and significant\nscale variations. These challenges complicate OVS tasks in earth vision,\nrequiring specialized approaches. To tackle this dilemma, we propose the first\nOVS framework specifically designed for remote sensing imagery, drawing\ninspiration from the distinct remote sensing traits. Particularly, to address\nthe varying orientations, we introduce a rotation-aggregative similarity\ncomputation module that generates orientation-adaptive similarity maps as\ninitial semantic maps. These maps are subsequently refined at both spatial and\ncategorical levels to produce more accurate semantic maps. Additionally, to\nmanage significant scale changes, we integrate multi-scale image features into\nthe upsampling process, resulting in the final scale-aware semantic masks. To\nadvance OVS in earth vision and encourage reproducible research, we establish\nthe first open-sourced OVS benchmark for remote sensing imagery, including four\npublic remote sensing datasets. Extensive experiments on this benchmark\ndemonstrate our proposed method achieves state-of-the-art performance. All\ncodes and datasets are available at https://github.com/caoql98/OVRS.",
          "arxiv_id": "2409.07683v1"
        },
        {
          "title": "SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding",
          "year": "2022-11",
          "abstract": "Remote sensing images are useful for a wide variety of planet monitoring\napplications, from tracking deforestation to tackling illegal fishing. The\nEarth is extremely diverse -- the amount of potential tasks in remote sensing\nimages is massive, and the sizes of features range from several kilometers to\njust tens of centimeters. However, creating generalizable computer vision\nmethods is a challenge in part due to the lack of a large-scale dataset that\ncaptures these diverse features for many tasks. In this paper, we present\nSatlasPretrain, a remote sensing dataset that is large in both breadth and\nscale, combining Sentinel-2 and NAIP images with 302M labels under 137\ncategories and seven label types. We evaluate eight baselines and a proposed\nmethod on SatlasPretrain, and find that there is substantial room for\nimprovement in addressing research challenges specific to remote sensing,\nincluding processing image time series that consist of images from very\ndifferent types of sensors, and taking advantage of long-range spatial context.\nMoreover, we find that pre-training on SatlasPretrain substantially improves\nperformance on downstream tasks, increasing average accuracy by 18% over\nImageNet and 6% over the next best baseline. The dataset, pre-trained model\nweights, and code are available at https://satlas-pretrain.allen.ai/.",
          "arxiv_id": "2211.15660v3"
        },
        {
          "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models",
          "year": "2024-08",
          "abstract": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.",
          "arxiv_id": "2408.14744v4"
        }
      ],
      "4": [
        {
          "title": "Language Conditioned Imitation Learning over Unstructured Data",
          "year": "2020-05",
          "abstract": "Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io",
          "arxiv_id": "2005.07648v2"
        },
        {
          "title": "Latent Action Pretraining from Videos",
          "year": "2024-10",
          "abstract": "We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.",
          "arxiv_id": "2410.11758v2"
        },
        {
          "title": "Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation",
          "year": "2024-05",
          "abstract": "We seek to learn a generalizable goal-conditioned policy that enables\nzero-shot robot manipulation: interacting with unseen objects in novel scenes\nwithout test-time adaptation. While typical approaches rely on a large amount\nof demonstration data for such generalization, we propose an approach that\nleverages web videos to predict plausible interaction plans and learns a\ntask-agnostic transformation to obtain robot actions in the real world. Our\nframework,Track2Act predicts tracks of how points in an image should move in\nfuture time-steps based on a goal, and can be trained with diverse videos on\nthe web including those of humans and robots manipulating everyday objects. We\nuse these 2D track predictions to infer a sequence of rigid transforms of the\nobject to be manipulated, and obtain robot end-effector poses that can be\nexecuted in an open-loop manner. We then refine this open-loop plan by\npredicting residual actions through a closed loop policy trained with a few\nembodiment-specific demonstrations. We show that this approach of combining\nscalably learned track prediction with a residual policy requiring minimal\nin-domain robot-specific data enables diverse generalizable robot manipulation,\nand present a wide array of real-world robot manipulation results across unseen\ntasks, objects, and scenes. https://homangab.github.io/track2act/",
          "arxiv_id": "2405.01527v2"
        }
      ],
      "5": [
        {
          "title": "Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance",
          "year": "2024-03",
          "abstract": "Large-scale text-to-image diffusion models have achieved great success in\nsynthesizing high-quality and diverse images given target text prompts. Despite\nthe revolutionary image generation ability, current state-of-the-art models\nstill struggle to deal with multi-concept generation accurately in many cases.\nThis phenomenon is known as ``concept bleeding\" and displays as the unexpected\noverlapping or merging of various concepts. This paper presents a general\napproach for text-to-image diffusion models to address the mutual interference\nbetween different subjects and their attachments in complex scenes, pursuing\nbetter text-image consistency. The core idea is to isolate the synthesizing\nprocesses of different concepts. We propose to bind each attachment to\ncorresponding subjects separately with split text prompts. Besides, we\nintroduce a revision method to fix the concept bleeding problem in\nmulti-subject synthesis. We first depend on pre-trained object detection and\nsegmentation models to obtain the layouts of subjects. Then we isolate and\nresynthesize each subject individually with corresponding text prompts to avoid\nmutual interference. Overall, we achieve a training-free strategy, named\nIsolated Diffusion, to optimize multi-concept text-to-image synthesis. It is\ncompatible with the latest Stable Diffusion XL (SDXL) and prior Stable\nDiffusion (SD) models. We compare our approach with alternative methods using a\nvariety of multi-concept text prompts and demonstrate its effectiveness with\nclear advantages in text-image consistency and user study.",
          "arxiv_id": "2403.16954v2"
        },
        {
          "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
          "year": "2023-05",
          "abstract": "Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.",
          "arxiv_id": "2305.05189v4"
        },
        {
          "title": "Self-correcting LLM-controlled Diffusion Models",
          "year": "2023-11",
          "abstract": "Text-to-image generation has witnessed significant progress with the advent\nof diffusion models. Despite the ability to generate photorealistic images,\ncurrent text-to-image diffusion models still often struggle to accurately\ninterpret and follow complex input text prompts. In contrast to existing models\nthat aim to generate images only with their best effort, we introduce\nSelf-correcting LLM-controlled Diffusion (SLD). SLD is a framework that\ngenerates an image from the input prompt, assesses its alignment with the\nprompt, and performs self-corrections on the inaccuracies in the generated\nimage. Steered by an LLM controller, SLD turns text-to-image generation into an\niterative closed-loop process, ensuring correctness in the resulting image. SLD\nis not only training-free but can also be seamlessly integrated with diffusion\nmodels behind API access, such as DALL-E 3, to further boost the performance of\nstate-of-the-art diffusion models. Experimental results show that our approach\ncan rectify a majority of incorrect generations, particularly in generative\nnumeracy, attribute binding, and spatial relationships. Furthermore, by simply\nadjusting the instructions to the LLM, SLD can perform image editing tasks,\nbridging the gap between text-to-image generation and image editing pipelines.\nWe will make our code available for future research and applications.",
          "arxiv_id": "2311.16090v1"
        }
      ],
      "6": [
        {
          "title": "Temporal Action Segmentation with High-level Complex Activity Labels",
          "year": "2021-08",
          "abstract": "The temporal action segmentation task segments videos temporally and predicts\naction labels for all frames. Fully supervising such a segmentation model\nrequires dense frame-wise action annotations, which are expensive and tedious\nto collect.\n  This work is the first to propose a Constituent Action Discovery (CAD)\nframework that only requires the video-wise high-level complex activity label\nas supervision for temporal action segmentation. The proposed approach\nautomatically discovers constituent video actions using an activity\nclassification task. Specifically, we define a finite number of latent action\nprototypes to construct video-level dual representations with which these\nprototypes are learned collectively through the activity classification\ntraining. This setting endows our approach with the capability to discover\npotentially shared actions across multiple complex activities.\n  Due to the lack of action-level supervision, we adopt the Hungarian matching\nalgorithm to relate latent action prototypes to ground truth semantic classes\nfor evaluation. We show that with the high-level supervision, the Hungarian\nmatching can be extended from the existing video and activity levels to the\nglobal level. The global-level matching allows for action sharing across\nactivities, which has never been considered in the literature before. Extensive\nexperiments demonstrate that our discovered actions can help perform temporal\naction segmentation and activity recognition tasks.",
          "arxiv_id": "2108.06706v3"
        },
        {
          "title": "TA2N: Two-Stage Action Alignment Network for Few-shot Action Recognition",
          "year": "2021-07",
          "abstract": "Few-shot action recognition aims to recognize novel action classes (query)\nusing just a few samples (support). The majority of current approaches follow\nthe metric learning paradigm, which learns to compare the similarity between\nvideos. Recently, it has been observed that directly measuring this similarity\nis not ideal since different action instances may show distinctive temporal\ndistribution, resulting in severe misalignment issues across query and support\nvideos. In this paper, we arrest this problem from two distinct aspects --\naction duration misalignment and action evolution misalignment. We address them\nsequentially through a Two-stage Action Alignment Network (TA2N). The first\nstage locates the action by learning a temporal affine transform, which warps\neach video feature to its action duration while dismissing the\naction-irrelevant feature (e.g. background). Next, the second stage coordinates\nquery feature to match the spatial-temporal action evolution of support by\nperforming temporally rearrange and spatially offset prediction. Extensive\nexperiments on benchmark datasets show the potential of the proposed method in\nachieving state-of-the-art performance for few-shot action recognition.The code\nof this project can be found at https://github.com/R00Kie-Liu/TA2N",
          "arxiv_id": "2107.04782v4"
        },
        {
          "title": "Semi-Supervised Few-Shot Atomic Action Recognition",
          "year": "2020-11",
          "abstract": "Despite excellent progress has been made, the performance on action\nrecognition still heavily relies on specific datasets, which are difficult to\nextend new action classes due to labor-intensive labeling. Moreover, the high\ndiversity in Spatio-temporal appearance requires robust and representative\naction feature aggregation and attention. To address the above issues, we focus\non atomic actions and propose a novel model for semi-supervised few-shot atomic\naction recognition. Our model features unsupervised and contrastive video\nembedding, loose action alignment, multi-head feature comparison, and\nattention-based aggregation, together of which enables action recognition with\nonly a few training examples through extracting more representative features\nand allowing flexibility in spatial and temporal alignment and variations in\nthe action. Experiments show that our model can attain high accuracy on\nrepresentative atomic action datasets outperforming their respective\nstate-of-the-art classification accuracy in full supervision setting.",
          "arxiv_id": "2011.08410v1"
        }
      ],
      "7": [
        {
          "title": "Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM",
          "year": "2024-03",
          "abstract": "Numerous studies have demonstrated the susceptibility of deep neural networks\n(DNNs) to subtle adversarial perturbations, prompting the development of many\nadvanced adversarial defense methods aimed at mitigating adversarial attacks.\nCurrent defense strategies usually train DNNs for a specific adversarial attack\nmethod and can achieve good robustness in defense against this type of\nadversarial attack. Nevertheless, when subjected to evaluations involving\nunfamiliar attack modalities, empirical evidence reveals a pronounced\ndeterioration in the robustness of DNNs. Meanwhile, there is a trade-off\nbetween the classification accuracy of clean examples and adversarial examples.\nMost defense methods often sacrifice the accuracy of clean examples in order to\nimprove the adversarial robustness of DNNs. To alleviate these problems and\nenhance the overall robust generalization of DNNs, we propose the Test-Time\nPixel-Level Adversarial Purification (TPAP) method. This approach is based on\nthe robust overfitting characteristic of DNNs to the fast gradient sign method\n(FGSM) on training and test datasets. It utilizes FGSM for adversarial\npurification, to process images for purifying unknown adversarial perturbations\nfrom pixels at testing time in a \"counter changes with changelessness\" manner,\nthereby enhancing the defense capability of DNNs against various unknown\nadversarial attacks. Extensive experimental results show that our method can\neffectively improve both overall robust generalization of DNNs, notably over\nprevious methods.",
          "arxiv_id": "2403.11448v1"
        },
        {
          "title": "Random Transformation of Image Brightness for Adversarial Attack",
          "year": "2021-01",
          "abstract": "Deep neural networks are vulnerable to adversarial examples, which are\ncrafted by adding small, human-imperceptible perturbations to the original\nimages, but make the model output inaccurate predictions. Before deep neural\nnetworks are deployed, adversarial attacks can thus be an important method to\nevaluate and select robust models in safety-critical applications. However,\nunder the challenging black-box setting, the attack success rate, i.e., the\ntransferability of adversarial examples, still needs to be improved. Based on\nimage augmentation methods, we found that random transformation of image\nbrightness can eliminate overfitting in the generation of adversarial examples\nand improve their transferability. To this end, we propose an adversarial\nexample generation method based on this phenomenon, which can be integrated\nwith Fast Gradient Sign Method (FGSM)-related methods to build a more robust\ngradient-based attack and generate adversarial examples with better\ntransferability. Extensive experiments on the ImageNet dataset demonstrate the\nmethod's effectiveness. Whether on normally or adversarially trained networks,\nour method has a higher success rate for black-box attacks than other attack\nmethods based on data augmentation. We hope that this method can help to\nevaluate and improve the robustness of models.",
          "arxiv_id": "2101.04321v1"
        },
        {
          "title": "Towards Unified Robustness Against Both Backdoor and Adversarial Attacks",
          "year": "2024-05",
          "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to both backdoor and\nadversarial attacks. In the literature, these two types of attacks are commonly\ntreated as distinct robustness problems and solved separately, since they\nbelong to training-time and inference-time attacks respectively. However, this\npaper revealed that there is an intriguing connection between them: (1)\nplanting a backdoor into a model will significantly affect the model's\nadversarial examples; (2) for an infected model, its adversarial examples have\nsimilar features as the triggered images. Based on these observations, a novel\nProgressive Unified Defense (PUD) algorithm is proposed to defend against\nbackdoor and adversarial attacks simultaneously. Specifically, our PUD has a\nprogressive model purification scheme to jointly erase backdoors and enhance\nthe model's adversarial robustness. At the early stage, the adversarial\nexamples of infected models are utilized to erase backdoors. With the backdoor\ngradually erased, our model purification can naturally turn into a stage to\nboost the model's robustness against adversarial attacks. Besides, our PUD\nalgorithm can effectively identify poisoned images, which allows the initial\nextra dataset not to be completely clean. Extensive experimental results show\nthat, our discovered connection between backdoor and adversarial attacks is\nubiquitous, no matter what type of backdoor attack. The proposed PUD\noutperforms the state-of-the-art backdoor defense, including the model\nrepairing-based and data filtering-based methods. Besides, it also has the\nability to compete with the most advanced adversarial defense methods.",
          "arxiv_id": "2405.17929v1"
        }
      ],
      "8": [
        {
          "title": "NeRF: Neural Radiance Field in 3D Vision: A Comprehensive Review (Updated Post-Gaussian Splatting)",
          "year": "2022-10",
          "abstract": "In March 2020, Neural Radiance Field (NeRF) revolutionized Computer Vision,\nallowing for implicit, neural network-based scene representation and novel view\nsynthesis. NeRF models have found diverse applications in robotics, urban\nmapping, autonomous navigation, virtual reality/augmented reality, and more. In\nAugust 2023, Gaussian Splatting, a direct competitor to the NeRF-based\nframework, was proposed, gaining tremendous momentum and overtaking NeRF-based\nresearch in terms of interest as the dominant framework for novel view\nsynthesis. We present a comprehensive survey of NeRF papers from the past five\nyears (2020-2025). These include papers from the pre-Gaussian Splatting era,\nwhere NeRF dominated the field for novel view synthesis and 3D implicit and\nhybrid representation neural field learning. We also include works from the\npost-Gaussian Splatting era where NeRF and implicit/hybrid neural fields found\nmore niche applications.\n  Our survey is organized into architecture and application-based taxonomies in\nthe pre-Gaussian Splatting era, as well as a categorization of active research\nareas for NeRF, neural field, and implicit/hybrid neural representation\nmethods. We provide an introduction to the theory of NeRF and its training via\ndifferentiable volume rendering. We also present a benchmark comparison of the\nperformance and speed of classical NeRF, implicit and hybrid neural\nrepresentation, and neural field models, and an overview of key datasets.",
          "arxiv_id": "2210.00379v7"
        },
        {
          "title": "Deblurring 3D Gaussian Splatting",
          "year": "2024-01",
          "abstract": "Recent studies in Radiance Fields have paved the robust way for novel view\nsynthesis with their photorealistic rendering quality. Nevertheless, they\nusually employ neural networks and volumetric rendering, which are costly to\ntrain and impede their broad use in various real-time applications due to the\nlengthy rendering time. Lately 3D Gaussians splatting-based approach has been\nproposed to model the 3D scene, and it achieves remarkable visual quality while\nrendering the images in real-time. However, it suffers from severe degradation\nin the rendering quality if the training images are blurry. Blurriness commonly\noccurs due to the lens defocusing, object motion, and camera shake, and it\ninevitably intervenes in clean image acquisition. Several previous studies have\nattempted to render clean and sharp images from blurry input images using\nneural fields. The majority of those works, however, are designed only for\nvolumetric rendering-based neural radiance fields and are not straightforwardly\napplicable to rasterization-based 3D Gaussian splatting methods. Thus, we\npropose a novel real-time deblurring framework, Deblurring 3D Gaussian\nSplatting, using a small Multi-Layer Perceptron (MLP) that manipulates the\ncovariance of each 3D Gaussian to model the scene blurriness. While Deblurring\n3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct\nfine and sharp details from blurry images. A variety of experiments have been\nconducted on the benchmark, and the results have revealed the effectiveness of\nour approach for deblurring. Qualitative results are available at\nhttps://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/",
          "arxiv_id": "2401.00834v3"
        },
        {
          "title": "Recent Advances in 3D Gaussian Splatting",
          "year": "2024-03",
          "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the\nrendering speed of novel view synthesis. Unlike neural implicit representations\nlike Neural Radiance Fields (NeRF) that represent a 3D scene with position and\nviewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of\nGaussian ellipsoids to model the scene so that efficient rendering can be\naccomplished by rasterizing Gaussian ellipsoids into images. Apart from the\nfast rendering speed, the explicit representation of 3D Gaussian Splatting\nfacilitates editing tasks like dynamic reconstruction, geometry editing, and\nphysical simulation. Considering the rapid change and growing number of works\nin this field, we present a literature review of recent 3D Gaussian Splatting\nmethods, which can be roughly classified into 3D reconstruction, 3D editing,\nand other downstream applications by functionality. Traditional point-based\nrendering methods and the rendering formulation of 3D Gaussian Splatting are\nalso illustrated for a better understanding of this technique. This survey aims\nto help beginners get into this field quickly and provide experienced\nresearchers with a comprehensive overview, which can stimulate the future\ndevelopment of the 3D Gaussian Splatting representation.",
          "arxiv_id": "2403.11134v2"
        }
      ],
      "9": [
        {
          "title": "Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection",
          "year": "2021-03",
          "abstract": "The ability to accurately detect and localize objects is recognized as being\nthe most important for the perception of self-driving cars. From 2D to 3D\nobject detection, the most difficult is to determine the distance from the\nego-vehicle to objects. Expensive technology like LiDAR can provide a precise\nand accurate depth information, so most studies have tended to focus on this\nsensor showing a performance gap between LiDAR-based methods and camera-based\nmethods. Although many authors have investigated how to fuse LiDAR with RGB\ncameras, as far as we know there are no studies to fuse LiDAR and stereo in a\ndeep neural network for the 3D object detection task. This paper presents\nSLS-Fusion, a new approach to fuse data from 4-beam LiDAR and a stereo camera\nvia a neural network for depth estimation to achieve better dense depth maps\nand thereby improves 3D object detection performance. Since 4-beam LiDAR is\ncheaper than the well-known 64-beam LiDAR, this approach is also classified as\na low-cost sensors-based method. Through evaluation on the KITTI benchmark, it\nis shown that the proposed method significantly improves depth estimation\nperformance compared to a baseline method. Also, when applying it to 3D object\ndetection, a new state of the art on low-cost sensor based method is achieved.",
          "arxiv_id": "2103.03977v3"
        },
        {
          "title": "SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection",
          "year": "2024-11",
          "abstract": "More and more research works fuse the LiDAR and camera information to improve\nthe 3D object detection of the autonomous driving system. Recently, a simple\nyet effective fusion framework has achieved an excellent detection performance,\nfusing the LiDAR and camera features in a unified bird's-eye-view (BEV) space.\nIn this paper, we propose a LiDAR-camera fusion framework, named SimpleBEV, for\naccurate 3D object detection, which follows the BEV-based fusion framework and\nimproves the camera and LiDAR encoders, respectively. Specifically, we perform\nthe camera-based depth estimation using a cascade network and rectify the depth\nresults with the depth information derived from the LiDAR points. Meanwhile, an\nauxiliary branch that implements the 3D object detection using only the\ncamera-BEV features is introduced to exploit the camera information during the\ntraining phase. Besides, we improve the LiDAR feature extractor by fusing the\nmulti-scaled sparse convolutional features. Experimental results demonstrate\nthe effectiveness of our proposed method. Our method achieves 77.6\\% NDS\naccuracy on the nuScenes dataset, showcasing superior performance in the 3D\nobject detection track.",
          "arxiv_id": "2411.05292v1"
        },
        {
          "title": "RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection",
          "year": "2024-03",
          "abstract": "Three-dimensional object detection is one of the key tasks in autonomous\ndriving. To reduce costs in practice, low-cost multi-view cameras for 3D object\ndetection are proposed to replace the expansive LiDAR sensors. However, relying\nsolely on cameras is difficult to achieve highly accurate and robust 3D object\ndetection. An effective solution to this issue is combining multi-view cameras\nwith the economical millimeter-wave radar sensor to achieve more reliable\nmulti-modal 3D object detection. In this paper, we introduce RCBEVDet, a\nradar-camera fusion 3D object detection method in the bird's eye view (BEV).\nSpecifically, we first design RadarBEVNet for radar BEV feature extraction.\nRadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section\n(RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based\nencoder and a transformer-based encoder are proposed to extract radar features,\nwith an injection and extraction module to facilitate communication between the\ntwo encoders. The RCS-aware BEV encoder takes RCS as the object size prior to\nscattering the point feature in BEV. Besides, we present the Cross-Attention\nMulti-layer Fusion module to automatically align the multi-modal BEV feature\nfrom radar and camera with the deformable attention mechanism, and then fuse\nthe feature with channel and spatial fusion layers. Experimental results show\nthat RCBEVDet achieves new state-of-the-art radar-camera fusion results on\nnuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore,\nRCBEVDet achieves better 3D detection results than all real-time camera-only\nand radar-camera 3D object detectors with a faster inference speed at 21~28\nFPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.",
          "arxiv_id": "2403.16440v1"
        }
      ],
      "10": [
        {
          "title": "Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs",
          "year": "2024-06",
          "abstract": "Video understanding is a crucial next step for multimodal large language\nmodels (MLLMs). Various benchmarks are introduced for better evaluating the\nMLLMs. Nevertheless, current video benchmarks are still inefficient for\nevaluating video models during iterative development due to the high cost of\nconstructing datasets and the difficulty in isolating specific skills. In this\npaper, we propose VideoNIAH (Video Needle In A Haystack), a benchmark\nconstruction framework through synthetic video generation. VideoNIAH decouples\nvideo content from their query-responses by inserting unrelated visual\n'needles' into original videos. The framework automates the generation of\nquery-response pairs using predefined rules, minimizing manual labor. The\nqueries focus on specific aspects of video understanding, enabling more\nskill-specific evaluations. The separation between video content and the\nqueries also allow for increased video variety and evaluations across different\nlengths. Utilizing VideoNIAH, we compile a video benchmark VNBench, which\nincludes tasks such as retrieval, ordering, and counting to evaluate three key\naspects of video understanding: temporal perception, chronological ordering,\nand spatio-temporal coherence. We conduct a comprehensive evaluation of both\nproprietary and open-source models, uncovering significant differences in their\nvideo understanding capabilities across various tasks. Additionally, we perform\nan in-depth analysis of the test results and model configurations. Based on\nthese findings, we provide some advice for improving video MLLM training,\noffering valuable insights to guide future research and model development. The\ncode and data are available at https://github.com/joez17/VideoNIAH.",
          "arxiv_id": "2406.09367v3"
        },
        {
          "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
          "year": "2025-07",
          "abstract": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.",
          "arxiv_id": "2507.13353v1"
        },
        {
          "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding",
          "year": "2025-07",
          "abstract": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.",
          "arxiv_id": "2507.17047v2"
        }
      ],
      "11": [
        {
          "title": "ScaleDepth: Decomposing Metric Depth Estimation into Scale Prediction and Relative Depth Estimation",
          "year": "2024-07",
          "abstract": "Estimating depth from a single image is a challenging visual task. Compared\nto relative depth estimation, metric depth estimation attracts more attention\ndue to its practical physical significance and critical applications in\nreal-life scenarios. However, existing metric depth estimation methods are\ntypically trained on specific datasets with similar scenes, facing challenges\nin generalizing across scenes with significant scale variations. To address\nthis challenge, we propose a novel monocular depth estimation method called\nScaleDepth. Our method decomposes metric depth into scene scale and relative\ndepth, and predicts them through a semantic-aware scale prediction (SASP)\nmodule and an adaptive relative depth estimation (ARDE) module, respectively.\nThe proposed ScaleDepth enjoys several merits. First, the SASP module can\nimplicitly combine structural and semantic features of the images to predict\nprecise scene scales. Second, the ARDE module can adaptively estimate the\nrelative depth distribution of each image within a normalized depth space.\nThird, our method achieves metric depth estimation for both indoor and outdoor\nscenes in a unified framework, without the need for setting the depth range or\nfine-tuning model. Extensive experiments demonstrate that our method attains\nstate-of-the-art performance across indoor, outdoor, unconstrained, and unseen\nscenes. Project page: https://ruijiezhu94.github.io/ScaleDepth",
          "arxiv_id": "2407.08187v1"
        },
        {
          "title": "Non-learning Stereo-aided Depth Completion under Mis-projection via Selective Stereo Matching",
          "year": "2022-10",
          "abstract": "We propose a non-learning depth completion method for a sparse depth map\ncaptured using a light detection and ranging (LiDAR) sensor guided by a pair of\nstereo images. Generally, conventional stereo-aided depth completion methods\nhave two limiations. (i) They assume the given sparse depth map is accurately\naligned to the input image, whereas the alignment is difficult to achieve in\npractice. (ii) They have limited accuracy in the long range because the depth\nis estimated by pixel disparity. To solve the abovementioned limitations, we\npropose selective stereo matching (SSM) that searches the most appropriate\ndepth value for each image pixel from its neighborly projected LiDAR points\nbased on an energy minimization framework. This depth selection approach can\nhandle any type of mis-projection. Moreover, SSM has an advantage in terms of\nlong-range depth accuracy because it directly uses the LiDAR measurement rather\nthan the depth acquired from the stereo. SSM is a discrete process; thus, we\napply variational smoothing with binary anisotropic diffusion tensor (B-ADT) to\ngenerate a continuous depth map while preserving depth discontinuity across\nobject boundaries. Experimentally, compared with the previous state-of-the-art\nstereo-aided depth completion, the proposed method reduced the mean absolute\nerror (MAE) of the depth estimation to 0.65 times and demonstrated\napproximately twice more accurate estimation in the long range. Moreover, under\nvarious LiDAR-camera calibration errors, the proposed method reduced the depth\nestimation MAE to 0.34-0.93 times from previous depth completion methods.",
          "arxiv_id": "2210.01436v1"
        },
        {
          "title": "Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning",
          "year": "2022-08",
          "abstract": "Self-supervised monocular methods can efficiently learn depth information of\nweakly textured surfaces or reflective objects. However, the depth accuracy is\nlimited due to the inherent ambiguity in monocular geometric modeling. In\ncontrast, multi-frame depth estimation methods improve the depth accuracy\nthanks to the success of Multi-View Stereo (MVS), which directly makes use of\ngeometric constraints. Unfortunately, MVS often suffers from texture-less\nregions, non-Lambertian surfaces, and moving objects, especially in real-world\nvideo sequences without known camera motion and depth supervision. Therefore,\nwe propose MOVEDepth, which exploits the MOnocular cues and VElocity guidance\nto improve multi-frame Depth learning. Unlike existing methods that enforce\nconsistency between MVS depth and monocular depth, MOVEDepth boosts multi-frame\ndepth learning by directly addressing the inherent problems of MVS. The key of\nour approach is to utilize monocular depth as a geometric priority to construct\nMVS cost volume, and adjust depth candidates of cost volume under the guidance\nof predicted camera velocity. We further fuse monocular depth and MVS depth by\nlearning uncertainty in the cost volume, which results in a robust depth\nestimation against ambiguity in multi-view geometry. Extensive experiments show\nMOVEDepth achieves state-of-the-art performance: Compared with Monodepth2 and\nPackNet, our method relatively improves the depth accuracy by 20\\% and 19.8\\%\non the KITTI benchmark. MOVEDepth also generalizes to the more challenging DDAD\nbenchmark, relatively outperforming ManyDepth by 7.2\\%. The code is available\nat https://github.com/JeffWang987/MOVEDepth.",
          "arxiv_id": "2208.09170v1"
        }
      ],
      "12": [
        {
          "title": "GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction",
          "year": "2024-02",
          "abstract": "Predicting the future trajectories of pedestrians on the road is an important\ntask for autonomous driving. The pedestrian trajectory prediction is affected\nby scene paths, pedestrian's intentions and decision-making, which is a\nmulti-modal problem. Most recent studies use past trajectories to predict a\nvariety of potential future trajectory distributions, which do not account for\nthe scene context and pedestrian targets. Instead of predicting the future\ntrajectory directly, we propose to use scene context and observed trajectory to\npredict the goal points first, and then reuse the goal points to predict the\nfuture trajectories. By leveraging the information from scene context and\nobserved trajectory, the uncertainty can be limited to a few target areas,\nwhich represent the \"goals\" of the pedestrians. In this paper, we propose\nGoalNet, a new trajectory prediction neural network based on the goal areas of\na pedestrian. Our network can predict both pedestrian's trajectories and\nbounding boxes. The overall model is efficient and modular, and its outputs can\nbe changed according to the usage scenario. Experimental results show that\nGoalNet significantly improves the previous state-of-the-art performance by\n48.7% on the JAAD and 40.8% on the PIE dataset.",
          "arxiv_id": "2402.19002v2"
        },
        {
          "title": "ScePT: Scene-consistent, Policy-based Trajectory Predictions for Planning",
          "year": "2022-06",
          "abstract": "Trajectory prediction is a critical functionality of autonomous systems that\nshare environments with uncontrolled agents, one prominent example being\nself-driving vehicles. Currently, most prediction methods do not enforce scene\nconsistency, i.e., there are a substantial amount of self-collisions between\npredicted trajectories of different agents in the scene. Moreover, many\napproaches generate individual trajectory predictions per agent instead of\njoint trajectory predictions of the whole scene, which makes downstream\nplanning difficult. In this work, we present ScePT, a policy planning-based\ntrajectory prediction model that generates accurate, scene-consistent\ntrajectory predictions suitable for autonomous system motion planning. It\nexplicitly enforces scene consistency and learns an agent interaction policy\nthat can be used for conditional prediction. Experiments on multiple real-world\npedestrians and autonomous vehicle datasets show that ScePT} matches current\nstate-of-the-art prediction accuracy with significantly improved scene\nconsistency. We also demonstrate ScePT's ability to work with a downstream\ncontingency planner.",
          "arxiv_id": "2206.13387v1"
        },
        {
          "title": "PiP: Planning-informed Trajectory Prediction for Autonomous Driving",
          "year": "2020-03",
          "abstract": "It is critical to predict the motion of surrounding vehicles for self-driving\nplanning, especially in a socially compliant and flexible way. However, future\nprediction is challenging due to the interaction and uncertainty in driving\nbehaviors. We propose planning-informed trajectory prediction (PiP) to tackle\nthe prediction problem in the multi-agent setting. Our approach is\ndifferentiated from the traditional manner of prediction, which is only based\non historical information and decoupled with planning. By informing the\nprediction process with the planning of ego vehicle, our method achieves the\nstate-of-the-art performance of multi-agent forecasting on highway datasets.\nMoreover, our approach enables a novel pipeline which couples the prediction\nand planning, by conditioning PiP on multiple candidate trajectories of the ego\nvehicle, which is highly beneficial for autonomous driving in interactive\nscenarios.",
          "arxiv_id": "2003.11476v2"
        }
      ],
      "13": [
        {
          "title": "DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition",
          "year": "2022-03",
          "abstract": "Unconstrained handwritten text recognition is a challenging computer vision\ntask. It is traditionally handled by a two-step approach, combining line\nsegmentation followed by text line recognition. For the first time, we propose\nan end-to-end segmentation-free architecture for the task of handwritten\ndocument recognition: the Document Attention Network. In addition to text\nrecognition, the model is trained to label text parts using begin and end tags\nin an XML-like fashion. This model is made up of an FCN encoder for feature\nextraction and a stack of transformer decoder layers for a recurrent\ntoken-by-token prediction process. It takes whole text documents as input and\nsequentially outputs characters, as well as logical layout tokens. Contrary to\nthe existing segmentation-based approaches, the model is trained without using\nany segmentation label. We achieve competitive results on the READ 2016 dataset\nat page level, as well as double-page level with a CER of 3.43% and 3.70%,\nrespectively. We also provide results for the RIMES 2009 dataset at page level,\nreaching 4.54% of CER.\n  We provide all source code and pre-trained model weights at\nhttps://github.com/FactoDeepLearning/DAN.",
          "arxiv_id": "2203.12273v4"
        },
        {
          "title": "An Efficient Language-Independent Multi-Font OCR for Arabic Script",
          "year": "2020-09",
          "abstract": "Optical Character Recognition (OCR) is the process of extracting digitized\ntext from images of scanned documents. While OCR systems have already matured\nin many languages, they still have shortcomings in cursive languages with\noverlapping letters such as the Arabic language. This paper proposes a complete\nArabic OCR system that takes a scanned image of Arabic Naskh script as an input\nand generates a corresponding digital document. Our Arabic OCR system consists\nof the following modules: Pre-processing, Word-level Feature Extraction,\nCharacter Segmentation, Character Recognition, and Post-processing. This paper\nalso proposes an improved font-independent character segmentation algorithm\nthat outperforms the state-of-the-art segmentation algorithms. Lastly, the\npaper proposes a neural network model for the character recognition task. The\nsystem has experimented on several open Arabic corpora datasets with an average\ncharacter segmentation accuracy 98.06%, character recognition accuracy 99.89%,\nand overall system accuracy 97.94% achieving outstanding results compared to\nthe state-of-the-art Arabic OCR systems.",
          "arxiv_id": "2009.09115v1"
        },
        {
          "title": "Enhancement of text recognition for hanja handwritten documents of Ancient Korea",
          "year": "2024-12",
          "abstract": "We implemented a high-performance optical character recognition model for\nclassical handwritten documents using data augmentation with highly variable\ncropping within the document region. Optical character recognition in\nhandwritten documents, especially classical documents, has been a challenging\ntopic in many countries and research organizations due to its difficulty.\nAlthough many researchers have conducted research on this topic, the quality of\nclassical texts over time and the unique stylistic characteristics of various\nauthors have made it difficult, and it is clear that the recognition of hanja\nhandwritten documents is a meaningful and special challenge, especially since\nhanja, which has been developed by reflecting the vocabulary, semantic, and\nsyntactic features of the Joseon Dynasty, is different from classical Chinese\ncharacters. To study this challenge, we used 1100 cursive documents, which are\nsmall in size, and augmented 100 documents per document by cropping a randomly\nsized region within each document for training, and trained them using a\ntwo-stage object detection model, High resolution neural network (HRNet), and\napplied the resulting model to achieve a high inference recognition rate of 90%\nfor cursive documents. Through this study, we also confirmed that the\nperformance of OCR is affected by the simplified characters, variants, variant\ncharacters, common characters, and alternators of Chinese characters that are\ndifficult to see in other studies, and we propose that the results of this\nstudy can be applied to optical character recognition of modern documents in\nmultiple languages as well as other typefaces in classical documents.",
          "arxiv_id": "2412.10647v1"
        }
      ],
      "14": [
        {
          "title": "PANDA : Perceptually Aware Neural Detection of Anomalies",
          "year": "2021-04",
          "abstract": "Semi-supervised methods of anomaly detection have seen substantial\nadvancement in recent years. Of particular interest are applications of such\nmethods to diverse, real-world anomaly detection problems where anomalous\nvariations can vary from the visually obvious to the very subtle. In this work,\nwe propose a novel fine-grained VAE-GAN architecture trained in a\nsemi-supervised manner in order to detect both visually distinct and subtle\nanomalies. With the use of a residually connected dual-feature extractor, a\nfine-grained discriminator and a perceptual loss function, we are able to\ndetect subtle, low inter-class (anomaly vs. normal) variant anomalies with\ngreater detection capability and smaller margins of deviation in AUC value\nduring inference compared to prior work whilst also remaining time-efficient\nduring inference. We achieve state of-the-art anomaly detection results when\ncompared extensively with prior semi-supervised approaches across a multitude\nof anomaly detection benchmark tasks including trivial leave-one out tasks\n(CIFAR-10 - AUPRCavg: 0.91; MNIST - AUPRCavg: 0.90) in addition to challenging\nreal-world anomaly detection tasks (plant leaf disease - AUC: 0.776; threat\nitem X-ray - AUC: 0.51), video frame-level anomaly detection (UCSDPed1 - AUC:\n0.95) and high frequency texture with object anomalous defect detection (MVTEC\n- AUCavg: 0.83).",
          "arxiv_id": "2104.13702v1"
        },
        {
          "title": "AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning",
          "year": "2021-10",
          "abstract": "Anomaly segmentation, which localizes defective areas, is an important\ncomponent in large-scale industrial manufacturing. However, most recent\nresearches have focused on anomaly detection. This paper proposes a novel\nanomaly segmentation network (AnoSeg) that can directly generate an accurate\nanomaly map using self-supervised learning. For highly accurate anomaly\nsegmentation, the proposed AnoSeg considers three novel techniques: Anomaly\ndata generation based on hard augmentation, self-supervised learning with\npixel-wise and adversarial losses, and coordinate channel concatenation. First,\nto generate synthetic anomaly images and reference masks for normal data, the\nproposed method uses hard augmentation to change the normal sample\ndistribution. Then, the proposed AnoSeg is trained in a self-supervised\nlearning manner from the synthetic anomaly data and normal data. Finally, the\ncoordinate channel, which represents the pixel location information, is\nconcatenated to an input of AnoSeg to consider the positional relationship of\neach pixel in the image. The estimated anomaly map can also be utilized to\nimprove the performance of anomaly detection. Our experiments show that the\nproposed method outperforms the state-of-the-art anomaly detection and anomaly\nsegmentation methods for the MVTec AD dataset. In addition, we compared the\nproposed method with the existing methods through the intersection over union\n(IoU) metric commonly used in segmentation tasks and demonstrated the\nsuperiority of our method for anomaly segmentation.",
          "arxiv_id": "2110.03396v1"
        },
        {
          "title": "3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly",
          "year": "2025-02",
          "abstract": "Industrial anomaly detection achieves progress thanks to datasets such as\nMVTec-AD and VisA. However, they suffer from limitations in terms of the number\nof defect samples, types of defects, and availability of real-world scenes.\nThese constraints inhibit researchers from further exploring the performance of\nindustrial detection with higher accuracy. To this end, we propose a new\nlarge-scale anomaly detection dataset called 3CAD, which is derived from real\n3C production lines. Specifically, the proposed 3CAD includes eight different\ntypes of manufactured parts, totaling 27,039 high-resolution images labeled\nwith pixel-level anomalies. The key features of 3CAD are that it covers\nanomalous regions of different sizes, multiple anomaly types, and the\npossibility of multiple anomalous regions and multiple anomaly types per\nanomaly image. This is the largest and first anomaly detection dataset\ndedicated to 3C product quality control for community exploration and\ndevelopment. Meanwhile, we introduce a simple yet effective framework for\nunsupervised anomaly detection: a Coarse-to-Fine detection paradigm with\nRecovery Guidance (CFRG). To detect small defect anomalies, the proposed CFRG\nutilizes a coarse-to-fine detection paradigm. Specifically, we utilize a\nheterogeneous distillation model for coarse localization and then fine\nlocalization through a segmentation model. In addition, to better capture\nnormal patterns, we introduce recovery features as guidance. Finally, we report\nthe results of our CFRG framework and popular anomaly detection methods on the\n3CAD dataset, demonstrating strong competitiveness and providing a highly\nchallenging benchmark to promote the development of the anomaly detection\nfield. Data and code are available: https://github.com/EnquanYang2022/3CAD.",
          "arxiv_id": "2502.05761v1"
        }
      ],
      "15": [
        {
          "title": "Enhancing Novel Object Detection via Cooperative Foundational Models",
          "year": "2023-11",
          "abstract": "In this work, we address the challenging and emergent problem of novel object\ndetection (NOD), focusing on the accurate detection of both known and novel\nobject categories during inference. Traditional object detection algorithms are\ninherently closed-set, limiting their capability to handle NOD. We present a\nnovel approach to transform existing closed-set detectors into open-set\ndetectors. This transformation is achieved by leveraging the complementary\nstrengths of pre-trained foundational models, specifically CLIP and SAM,\nthrough our cooperative mechanism. Furthermore, by integrating this mechanism\nwith state-of-the-art open-set detectors such as GDINO, we establish new\nbenchmarks in object detection performance. Our method achieves 17.42 mAP in\nnovel object detection and 42.08 mAP for known objects on the challenging LVIS\ndataset. Adapting our approach to the COCO OVD split, we surpass the current\nstate-of-the-art by a margin of 7.2 $ \\text{AP}_{50} $ for novel classes. Our\ncode is available at https://rohit901.github.io/coop-foundation-models/ .",
          "arxiv_id": "2311.12068v4"
        },
        {
          "title": "Leveraging Bottom-Up and Top-Down Attention for Few-Shot Object Detection",
          "year": "2020-07",
          "abstract": "Few-shot object detection aims at detecting objects with few annotated\nexamples, which remains a challenging research problem yet to be explored.\nRecent studies have shown the effectiveness of self-learned top-down attention\nmechanisms in object detection and other vision tasks. The top-down attention,\nhowever, is less effective at improving the performance of few-shot detectors.\nDue to the insufficient training data, object detectors cannot effectively\ngenerate attention maps for few-shot examples. To improve the performance and\ninterpretability of few-shot object detectors, we propose an attentive few-shot\nobject detection network (AttFDNet) that takes the advantages of both top-down\nand bottom-up attention. Being task-agnostic, the bottom-up attention serves as\na prior that helps detect and localize naturally salient objects. We further\naddress specific challenges in few-shot object detection by introducing two\nnovel loss terms and a hybrid few-shot learning strategy. Experimental results\nand visualization demonstrate the complementary nature of the two types of\nattention and their roles in few-shot object detection. Codes are available at\nhttps://github.com/chenxy99/AttFDNet.",
          "arxiv_id": "2007.12104v1"
        },
        {
          "title": "PROB: Probabilistic Objectness for Open World Object Detection",
          "year": "2022-12",
          "abstract": "Open World Object Detection (OWOD) is a new and challenging computer vision\ntask that bridges the gap between classic object detection (OD) benchmarks and\nobject detection in the real world. In addition to detecting and classifying\nseen/labeled objects, OWOD algorithms are expected to detect novel/unknown\nobjects - which can be classified and incrementally learned. In standard OD,\nobject proposals not overlapping with a labeled object are automatically\nclassified as background. Therefore, simply applying OD methods to OWOD fails\nas unknown objects would be predicted as background. The challenge of detecting\nunknown objects stems from the lack of supervision in distinguishing unknown\nobjects and background object proposals. Previous OWOD methods have attempted\nto overcome this issue by generating supervision using pseudo-labeling -\nhowever, unknown object detection has remained low. Probabilistic/generative\nmodels may provide a solution for this challenge. Herein, we introduce a novel\nprobabilistic framework for objectness estimation, where we alternate between\nprobability distribution estimation and objectness likelihood maximization of\nknown objects in the embedded feature space - ultimately allowing us to\nestimate the objectness probability of different proposals. The resulting\nProbabilistic Objectness transformer-based open-world detector, PROB,\nintegrates our framework into traditional object detection models, adapting\nthem for the open-world setting. Comprehensive experiments on OWOD benchmarks\nshow that PROB outperforms all existing OWOD methods in both unknown object\ndetection ($\\sim 2\\times$ unknown recall) and known object detection ($\\sim\n10\\%$ mAP). Our code will be made available upon publication at\nhttps://github.com/orrzohar/PROB.",
          "arxiv_id": "2212.01424v1"
        }
      ],
      "16": [
        {
          "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets",
          "year": "2021-07",
          "abstract": "Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.",
          "arxiv_id": "2107.00860v1"
        },
        {
          "title": "DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search",
          "year": "2020-03",
          "abstract": "Efficient search is a core issue in Neural Architecture Search (NAS). It is\ndifficult for conventional NAS algorithms to directly search the architectures\non large-scale tasks like ImageNet. In general, the cost of GPU hours for NAS\ngrows with regard to training dataset size and candidate set size. One common\nway is searching on a smaller proxy dataset (e.g., CIFAR-10) and then\ntransferring to the target task (e.g., ImageNet). These architectures optimized\non proxy data are not guaranteed to be optimal on the target task. Another\ncommon way is learning with a smaller candidate set, which may require expert\nknowledge and indeed betrays the essence of NAS. In this paper, we present\nDA-NAS that can directly search the architecture for large-scale target tasks\nwhile allowing a large candidate set in a more efficient manner. Our method is\nbased on an interesting observation that the learning speed for blocks in deep\nneural networks is related to the difficulty of recognizing distinct\ncategories. We carefully design a progressive data adapted pruning strategy for\nefficient architecture search. It will quickly trim low performed blocks on a\nsubset of target dataset (e.g., easy classes), and then gradually find the best\nblocks on the whole target dataset. At this time, the original candidate set\nbecomes as compact as possible, providing a faster search in the target task.\nExperiments on ImageNet verify the effectiveness of our approach. It is 2x\nfaster than previous methods while the accuracy is currently state-of-the-art,\nat 76.2% under small FLOPs constraint. It supports an argument search space\n(i.e., more candidate blocks) to efficiently search the best-performing\narchitecture.",
          "arxiv_id": "2003.12563v1"
        },
        {
          "title": "Automatic Pruning for Quantized Neural Networks",
          "year": "2020-02",
          "abstract": "Neural network quantization and pruning are two techniques commonly used to\nreduce the computational complexity and memory footprint of these models for\ndeployment. However, most existing pruning strategies operate on full-precision\nand cannot be directly applied to discrete parameter distributions after\nquantization. In contrast, we study a combination of these two techniques to\nachieve further network compression. In particular, we propose an effective\npruning strategy for selecting redundant low-precision filters. Furthermore, we\nleverage Bayesian optimization to efficiently determine the pruning ratio for\neach layer. We conduct extensive experiments on CIFAR-10 and ImageNet with\nvarious architectures and precisions. In particular, for ResNet-18 on ImageNet,\nwe prune 26.12% of the model size with Binarized Neural Network quantization,\nachieving a top-1 classification accuracy of 47.32% in a model of 2.47 MB and\n59.30% with a 2-bit DoReFa-Net in 4.36 MB.",
          "arxiv_id": "2002.00523v1"
        }
      ],
      "17": [
        {
          "title": "SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning",
          "year": "2021-01",
          "abstract": "This paper introduces SelfMatch, a semi-supervised learning method that\ncombines the power of contrastive self-supervised learning and consistency\nregularization. SelfMatch consists of two stages: (1) self-supervised\npre-training based on contrastive learning and (2) semi-supervised fine-tuning\nbased on augmentation consistency regularization. We empirically demonstrate\nthat SelfMatch achieves the state-of-the-art results on standard benchmark\ndatasets such as CIFAR-10 and SVHN. For example, for CIFAR-10 with 40 labeled\nexamples, SelfMatch achieves 93.19% accuracy that outperforms the strong\nprevious methods such as MixMatch (52.46%), UDA (70.95%), ReMixMatch (80.9%),\nand FixMatch (86.19%). We note that SelfMatch can close the gap between\nsupervised learning (95.87%) and semi-supervised learning (93.19%) by using\nonly a few labels for each class.",
          "arxiv_id": "2101.06480v1"
        },
        {
          "title": "Generalized Semi-Supervised Learning via Self-Supervised Feature Adaptation",
          "year": "2024-05",
          "abstract": "Traditional semi-supervised learning (SSL) assumes that the feature\ndistributions of labeled and unlabeled data are consistent which rarely holds\nin realistic scenarios. In this paper, we propose a novel SSL setting, where\nunlabeled samples are drawn from a mixed distribution that deviates from the\nfeature distribution of labeled samples. Under this setting, previous SSL\nmethods tend to predict wrong pseudo-labels with the model fitted on labeled\ndata, resulting in noise accumulation. To tackle this issue, we propose\nSelf-Supervised Feature Adaptation (SSFA), a generic framework for improving\nSSL performance when labeled and unlabeled data come from different\ndistributions. SSFA decouples the prediction of pseudo-labels from the current\nmodel to improve the quality of pseudo-labels. Particularly, SSFA incorporates\na self-supervised task into the SSL framework and uses it to adapt the feature\nextractor of the model to the unlabeled data. In this way, the extracted\nfeatures better fit the distribution of unlabeled data, thereby generating\nhigh-quality pseudo-labels. Extensive experiments show that our proposed SSFA\nis applicable to various pseudo-label-based SSL learners and significantly\nimproves performance in labeled, unlabeled, and even unseen distributions.",
          "arxiv_id": "2405.20596v1"
        },
        {
          "title": "CLAF: Contrastive Learning with Augmented Features for Imbalanced Semi-Supervised Learning",
          "year": "2023-12",
          "abstract": "Due to the advantages of leveraging unlabeled data and learning meaningful\nrepresentations, semi-supervised learning and contrastive learning have been\nprogressively combined to achieve better performances in popular applications\nwith few labeled data and abundant unlabeled data. One common manner is\nassigning pseudo-labels to unlabeled samples and selecting positive and\nnegative samples from pseudo-labeled samples to apply contrastive learning.\nHowever, the real-world data may be imbalanced, causing pseudo-labels to be\nbiased toward the majority classes and further undermining the effectiveness of\ncontrastive learning. To address the challenge, we propose Contrastive Learning\nwith Augmented Features (CLAF). We design a class-dependent feature\naugmentation module to alleviate the scarcity of minority class samples in\ncontrastive learning. For each pseudo-labeled sample, we select positive and\nnegative samples from labeled data instead of unlabeled data to compute\ncontrastive loss. Comprehensive experiments on imbalanced image classification\ndatasets demonstrate the effectiveness of CLAF in the context of imbalanced\nsemi-supervised learning.",
          "arxiv_id": "2312.09598v2"
        }
      ],
      "18": [
        {
          "title": "AnimePose: Multi-person 3D pose estimation and animation",
          "year": "2020-02",
          "abstract": "3D animation of humans in action is quite challenging as it involves using a\nhuge setup with several motion trackers all over the person's body to track the\nmovements of every limb. This is time-consuming and may cause the person\ndiscomfort in wearing exoskeleton body suits with motion sensors. In this work,\nwe present a trivial yet effective solution to generate 3D animation of\nmultiple persons from a 2D video using deep learning. Although significant\nimprovement has been achieved recently in 3D human pose estimation, most of the\nprior works work well in case of single person pose estimation and multi-person\npose estimation is still a challenging problem. In this work, we firstly\npropose a supervised multi-person 3D pose estimation and animation framework\nnamely AnimePose for a given input RGB video sequence. The pipeline of the\nproposed system consists of various modules: i) Person detection and\nsegmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information for\nperson localization iv) Person trajectory prediction and human pose tracking.\nOur proposed system produces comparable results on previous state-of-the-art 3D\nmulti-person pose estimation methods on publicly available datasets MuCo-3DHP\nand MuPoTS-3D datasets and it also outperforms previous state-of-the-art human\npose tracking methods by a significant margin of 11.7% performance gain on MOTA\nscore on Posetrack 2018 dataset.",
          "arxiv_id": "2002.02792v1"
        },
        {
          "title": "MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling",
          "year": "2023-06",
          "abstract": "Estimating 3D human poses only from a 2D human pose sequence is thoroughly\nexplored in recent years. Yet, prior to this, no such work has attempted to\nunify 2D and 3D pose representations in the shared feature space. In this\npaper, we propose \\mpm, a unified 2D-3D human pose representation framework via\nmasked pose modeling. We treat 2D and 3D poses as two different modalities like\nvision and language and build a single-stream transformer-based architecture.\nWe apply two pretext tasks, which are masked 2D pose modeling, and masked 3D\npose modeling to pre-train our network and use full-supervision to perform\nfurther fine-tuning. A high masking ratio of $71.8~\\%$ in total with a\nspatio-temporal mask sampling strategy leads to better relation modeling both\nin spatial and temporal domains. \\mpm~can handle multiple tasks including 3D\nhuman pose estimation, 3D pose estimation from occluded 2D pose, and 3D pose\ncompletion in a \\textbf{single} framework. We conduct extensive experiments and\nablation studies on several widely used human pose datasets and achieve\nstate-of-the-art performance on MPI-INF-3DHP.",
          "arxiv_id": "2306.17201v2"
        },
        {
          "title": "Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation",
          "year": "2024-08",
          "abstract": "Robust 3D human pose estimation is crucial to ensure safe and effective\nhuman-robot collaboration. Accurate human perception,however, is particularly\nchallenging in these scenarios due to strong occlusions and limited camera\nviewpoints. Current 3D human pose estimation approaches are rather vulnerable\nin such conditions. In this work we present a novel approach for robust 3D\nhuman pose estimation in the context of human-robot collaboration. Instead of\nrelying on noisy 2D features triangulation, we perform multi-view fusion on 3D\nskeletons provided by absolute monocular methods. Accurate 3D pose estimation\nis then obtained via reprojection error optimization, introducing limbs length\nsymmetry constraints. We evaluate our approach on the public dataset Human3.6M\nand on a novel version Human3.6M-Occluded, derived adding synthetic occlusions\non the camera views with the purpose of testing pose estimation algorithms\nunder severe occlusions. We further validate our method on real human-robot\ncollaboration workcells, in which we strongly surpass current 3D human pose\nestimation methods. Our approach outperforms state-of-the-art multi-view human\npose estimation techniques and demonstrates superior capabilities in handling\nchallenging scenarios with strong occlusions, representing a reliable and\neffective solution for real human-robot collaboration setups.",
          "arxiv_id": "2408.15810v1"
        }
      ],
      "19": [
        {
          "title": "Guiding GANs: How to control non-conditional pre-trained GANs for conditional image generation",
          "year": "2021-01",
          "abstract": "Generative Adversarial Networks (GANs) are an arrange of two neural networks\n-- the generator and the discriminator -- that are jointly trained to generate\nartificial data, such as images, from random inputs. The quality of these\ngenerated images has recently reached such levels that can often lead both\nmachines and humans into mistaking fake for real examples. However, the process\nperformed by the generator of the GAN has some limitations when we want to\ncondition the network to generate images from subcategories of a specific\nclass. Some recent approaches tackle this \\textit{conditional generation} by\nintroducing extra information prior to the training process, such as image\nsemantic segmentation or textual descriptions. While successful, these\ntechniques still require defining beforehand the desired subcategories and\ncollecting large labeled image datasets representing them to train the GAN from\nscratch. In this paper we present a novel and alternative method for guiding\ngeneric non-conditional GANs to behave as conditional GANs. Instead of\nre-training the GAN, our approach adds into the mix an encoder network to\ngenerate the high-dimensional random input vectors that are fed to the\ngenerator network of a non-conditional GAN to make it generate images from a\nspecific subcategory. In our experiments, when compared to training a\nconditional GAN from scratch, our guided GAN is able to generate artificial\nimages of perceived quality comparable to that of non-conditional GANs after\ntraining the encoder on just a few hundreds of images, which substantially\naccelerates the process and enables adding new subcategories seamlessly.",
          "arxiv_id": "2101.00990v1"
        },
        {
          "title": "Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey",
          "year": "2021-11",
          "abstract": "This is a tutorial and survey paper on Generative Adversarial Network (GAN),\nadversarial autoencoders, and their variants. We start with explaining\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\nand DCGAN. The mode collapse problem is introduced and various methods,\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\nintroduce some applications of GAN such as image-to-image translation\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\nGAN), text-to-image translation (including StackGAN), and mixing image\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\nautoencoders based on adversarial learning including adversarial autoencoder,\nPixelGAN, and implicit autoencoder.",
          "arxiv_id": "2111.13282v1"
        },
        {
          "title": "Unsupervised Image Generation with Infinite Generative Adversarial Networks",
          "year": "2021-08",
          "abstract": "Image generation has been heavily investigated in computer vision, where one\ncore research challenge is to generate images from arbitrarily complex\ndistributions with little supervision. Generative Adversarial Networks (GANs)\nas an implicit approach have achieved great successes in this direction and\ntherefore been employed widely. However, GANs are known to suffer from issues\nsuch as mode collapse, non-structured latent space, being unable to compute\nlikelihoods, etc. In this paper, we propose a new unsupervised non-parametric\nmethod named mixture of infinite conditional GANs or MIC-GANs, to tackle\nseveral GAN issues together, aiming for image generation with parsimonious\nprior knowledge. Through comprehensive evaluations across different datasets,\nwe show that MIC-GANs are effective in structuring the latent space and\navoiding mode collapse, and outperform state-of-the-art methods. MICGANs are\nadaptive, versatile, and robust. They offer a promising solution to several\nwell-known GAN issues. Code available: github.com/yinghdb/MICGANs.",
          "arxiv_id": "2108.07975v1"
        }
      ],
      "20": [
        {
          "title": "Conditioned Prompt-Optimization for Continual Deepfake Detection",
          "year": "2024-07",
          "abstract": "The rapid advancement of generative models has significantly enhanced the\nrealism and customization of digital content creation. The increasing power of\nthese tools, coupled with their ease of access, fuels the creation of\nphotorealistic fake content, termed deepfakes, that raises substantial concerns\nabout their potential misuse. In response, there has been notable progress in\ndeveloping detection mechanisms to identify content produced by these advanced\nsystems. However, existing methods often struggle to adapt to the continuously\nevolving landscape of deepfake generation. This paper introduces Prompt2Guard,\na novel solution for exemplar-free continual deepfake detection of images, that\nleverages Vision-Language Models (VLMs) and domain-specific multimodal prompts.\nCompared to previous VLM-based approaches that are either bounded by prompt\nselection accuracy or necessitate multiple forward passes, we leverage a\nprediction ensembling technique with read-only prompts. Read-only prompts do\nnot interact with VLMs internal representation, mitigating the need for\nmultiple forward passes. Thus, we enhance efficiency and accuracy in detecting\ngenerated content. Additionally, our method exploits a text-prompt conditioning\ntailored to deepfake detection, which we demonstrate is beneficial in our\nsetting. We evaluate Prompt2Guard on CDDB-Hard, a continual deepfake detection\nbenchmark composed of five deepfake detection datasets spanning multiple\ndomains and generators, achieving a new state-of-the-art. Additionally, our\nresults underscore the effectiveness of our approach in addressing the\nchallenges posed by continual deepfake detection, paving the way for more\nrobust and adaptable solutions in deepfake detection.",
          "arxiv_id": "2407.21554v1"
        },
        {
          "title": "Deep Learning Technology for Face Forgery Detection: A Survey",
          "year": "2024-09",
          "abstract": "Currently, the rapid development of computer vision and deep learning has\nenabled the creation or manipulation of high-fidelity facial images and videos\nvia deep generative approaches. This technology, also known as deepfake, has\nachieved dramatic progress and become increasingly popular in social media.\nHowever, the technology can generate threats to personal privacy and national\nsecurity by spreading misinformation. To diminish the risks of deepfake, it is\ndesirable to develop powerful forgery detection methods to distinguish fake\nfaces from real faces. This paper presents a comprehensive survey of recent\ndeep learning-based approaches for facial forgery detection. We attempt to\nprovide the reader with a deeper understanding of the current advances as well\nas the major challenges for deepfake detection based on deep learning. We\npresent an overview of deepfake techniques and analyse the characteristics of\nvarious deepfake datasets. We then provide a systematic review of different\ncategories of deepfake detection and state-of-the-art deepfake detection\nmethods. The drawbacks of existing detection methods are analyzed, and future\nresearch directions are discussed to address the challenges in improving both\nthe performance and generalization of deepfake detection.",
          "arxiv_id": "2409.14289v3"
        },
        {
          "title": "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection",
          "year": "2021-01",
          "abstract": "In recent years, the abuse of a face swap technique called deepfake has\nraised enormous public concerns. So far, a large number of deepfake videos\n(known as \"deepfakes\") have been crafted and uploaded to the internet, calling\nfor effective countermeasures. One promising countermeasure against deepfakes\nis deepfake detection. Several deepfake datasets have been released to support\nthe training and testing of deepfake detectors, such as DeepfakeDetection and\nFaceForensics++. While this has greatly advanced deepfake detection, most of\nthe real videos in these datasets are filmed with a few volunteer actors in\nlimited scenes, and the fake videos are crafted by researchers using a few\npopular deepfake softwares. Detectors developed on these datasets may become\nless effective against real-world deepfakes on the internet. To better support\ndetection against real-world deepfakes, in this paper, we introduce a new\ndataset WildDeepfake which consists of 7,314 face sequences extracted from 707\ndeepfake videos collected completely from the internet. WildDeepfake is a small\ndataset that can be used, in addition to existing datasets, to develop and test\nthe effectiveness of deepfake detectors against real-world deepfakes. We\nconduct a systematic evaluation of a set of baseline detection networks on both\nexisting and our WildDeepfake datasets, and show that WildDeepfake is indeed a\nmore challenging dataset, where the detection performance can decrease\ndrastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\nDetection Networks (ADDNets) to leverage the attention masks on real/fake faces\nfor improved detection. We empirically verify the effectiveness of ADDNets on\nboth existing datasets and WildDeepfake. The dataset is available at:\nhttps://github.com/OpenTAI/wild-deepfake.",
          "arxiv_id": "2101.01456v2"
        }
      ],
      "21": [
        {
          "title": "Interpretable Image Emotion Recognition: A Domain Adaptation Approach Using Facial Expressions",
          "year": "2020-11",
          "abstract": "This paper proposes a feature-based domain adaptation technique for\nidentifying emotions in generic images, encompassing both facial and non-facial\nobjects, as well as non-human components. This approach addresses the challenge\nof the limited availability of pre-trained models and well-annotated datasets\nfor Image Emotion Recognition (IER). Initially, a deep-learning-based Facial\nExpression Recognition (FER) system is developed, classifying facial images\ninto discrete emotion classes. Maintaining the same network architecture, this\nFER system is then adapted to recognize emotions in generic images through the\napplication of discrepancy loss, enabling the model to effectively learn IER\nfeatures while classifying emotions into categories such as 'happy,' 'sad,'\n'hate,' and 'anger.' Additionally, a novel interpretability method, Divide and\nConquer based Shap (DnCShap), is introduced to elucidate the visual features\nmost relevant for emotion recognition. The proposed IER system demonstrated\nemotion classification accuracies of 61.86% for the IAPSa dataset, 62.47 for\nthe ArtPhoto dataset, 70.78% for the FI dataset, and 59.72% for the EMOTIC\ndataset. The system effectively identifies the important visual features that\nlead to specific emotion classifications and also provides detailed embedding\nplots explaining the predictions, enhancing the understanding and trust in\nAI-driven emotion recognition systems.",
          "arxiv_id": "2011.08388v4"
        },
        {
          "title": "PERI: Part Aware Emotion Recognition In The Wild",
          "year": "2022-10",
          "abstract": "Emotion recognition aims to interpret the emotional states of a person based\non various inputs including audio, visual, and textual cues. This paper focuses\non emotion recognition using visual features. To leverage the correlation\nbetween facial expression and the emotional state of a person, pioneering\nmethods rely primarily on facial features. However, facial features are often\nunreliable in natural unconstrained scenarios, such as in crowded scenes, as\nthe face lacks pixel resolution and contains artifacts due to occlusion and\nblur. To address this, in the wild emotion recognition exploits full-body\nperson crops as well as the surrounding scene context. In a bid to use body\npose for emotion recognition, such methods fail to realize the potential that\nfacial expressions, when available, offer. Thus, the aim of this paper is\ntwo-fold. First, we demonstrate our method, PERI, to leverage both body pose\nand facial landmarks. We create part aware spatial (PAS) images by extracting\nkey regions from the input image using a mask generated from both body pose and\nfacial landmarks. This allows us to exploit body pose in addition to facial\ncontext whenever available. Second, to reason from the PAS images, we introduce\ncontext infusion (Cont-In) blocks. These blocks attend to part-specific\ninformation, and pass them onto the intermediate features of an emotion\nrecognition network. Our approach is conceptually simple and can be applied to\nany existing emotion recognition method. We provide our results on the publicly\navailable in the wild EMOTIC dataset. Compared to existing methods, PERI\nachieves superior performance and leads to significant improvements in the mAP\nof emotion categories, while decreasing Valence, Arousal and Dominance errors.\nImportantly, we observe that our method improves performance in both images\nwith fully visible faces as well as in images with occluded or blurred faces.",
          "arxiv_id": "2210.10130v1"
        },
        {
          "title": "AffectNet+: A Database for Enhancing Facial Expression Recognition with Soft-Labels",
          "year": "2024-10",
          "abstract": "Automated Facial Expression Recognition (FER) is challenging due to\nintra-class variations and inter-class similarities. FER can be especially\ndifficult when facial expressions reflect a mixture of various emotions (aka\ncompound expressions). Existing FER datasets, such as AffectNet, provide\ndiscrete emotion labels (hard-labels), where a single category of emotion is\nassigned to an expression. To alleviate inter- and intra-class challenges, as\nwell as provide a better facial expression descriptor, we propose a new\napproach to create FER datasets through a labeling method in which an image is\nlabeled with more than one emotion (called soft-labels), each with different\nconfidences. Specifically, we introduce the notion of soft-labels for facial\nexpression datasets, a new approach to affective computing for more realistic\nrecognition of facial expressions. To achieve this goal, we propose a novel\nmethodology to accurately calculate soft-labels: a vector representing the\nextent to which multiple categories of emotion are simultaneously present\nwithin a single facial expression. Finding smoother decision boundaries,\nenabling multi-labeling, and mitigating bias and imbalanced data are some of\nthe advantages of our proposed method. Building upon AffectNet, we introduce\nAffectNet+, the next-generation facial expression dataset. This dataset\ncontains soft-labels, three categories of data complexity subsets, and\nadditional metadata such as age, gender, ethnicity, head pose, facial\nlandmarks, valence, and arousal. AffectNet+ will be made publicly accessible to\nresearchers.",
          "arxiv_id": "2410.22506v1"
        }
      ],
      "22": [
        {
          "title": "Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation",
          "year": "2022-03",
          "abstract": "Semantic segmentation with limited annotations, such as weakly supervised\nsemantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS),\nis a challenging task that has attracted much attention recently. Most leading\nWSSS methods employ a sophisticated multi-stage training strategy to estimate\npseudo-labels as precise as possible, but they suffer from high model\ncomplexity. In contrast, there exists another research line that trains a\nsingle network with image-level labels in one training cycle. However, such a\nsingle-stage strategy often performs poorly because of the compounding effect\ncaused by inaccurate pseudo-label estimation. To address this issue, this paper\npresents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and\nSSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously\npredicts several complementary attentive LR representations from different\nviews of an image to learn precise pseudo-labels. Specifically, we reformulate\nthe LR representation learning as a collective matrix factorization problem and\noptimize it jointly with the network learning in an end-to-end manner. The\nresulting LR representation deprecates noisy information while capturing stable\nsemantics across different views, making it robust to the input variations,\nthereby reducing overfitting to self-supervision errors. The SLRNet can provide\na unified single-stage framework for various label-efficient semantic\nsegmentation settings: 1) WSSS with image-level labeled data, 2) SSSS with a\nfew pixel-level labeled data, and 3) SSSS with a few pixel-level labeled data\nand many image-level labeled data. Extensive experiments on the Pascal VOC\n2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both\nstate-of-the-art WSSS and SSSS methods with a variety of different settings,\nproving its good generalizability and efficacy.",
          "arxiv_id": "2203.10278v1"
        },
        {
          "title": "Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation",
          "year": "2021-06",
          "abstract": "Weakly supervised semantic segmentation is receiving great attention due to\nits low human annotation cost. In this paper, we aim to tackle bounding box\nsupervised semantic segmentation, i.e., training accurate semantic segmentation\nmodels using bounding box annotations as supervision. To this end, we propose\nAffinity Attention Graph Neural Network ($A^2$GNN). Following previous\npractices, we first generate pseudo semantic-aware seeds, which are then formed\ninto semantic graphs based on our newly proposed affinity Convolutional Neural\nNetwork (CNN). Then the built graphs are input to our $A^2$GNN, in which an\naffinity attention layer is designed to acquire the short- and long- distance\ninformation from soft graph edges to accurately propagate semantic labels from\nthe confident seeds to the unlabeled pixels. However, to guarantee the\nprecision of the seeds, we only adopt a limited number of confident pixel seed\nlabels for $A^2$GNN, which may lead to insufficient supervision for training.\nTo alleviate this issue, we further introduce a new loss function and a\nconsistency-checking mechanism to leverage the bounding box constraint, so that\nmore reliable guidance can be included for the model optimization. Experiments\nshow that our approach achieves new state-of-the-art performances on Pascal VOC\n2012 datasets (val: 76.5\\%, test: 75.2\\%). More importantly, our approach can\nbe readily applied to bounding box supervised instance segmentation task or\nother weakly supervised semantic segmentation tasks, with state-of-the-art or\ncomparable performance among almot all weakly supervised tasks on PASCAL VOC or\nCOCO dataset. Our source code will be available at\nhttps://github.com/zbf1991/A2GNN.",
          "arxiv_id": "2106.04054v1"
        },
        {
          "title": "Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised Semantic Segmentation",
          "year": "2021-03",
          "abstract": "Recently, several weakly supervised learning methods have been devoted to\nutilize bounding box supervision for training deep semantic segmentation\nmodels. Most existing methods usually leverage the generic proposal generators\n(e.g., dense CRF and MCG) to produce enhanced segmentation masks for further\ntraining segmentation models. These proposal generators, however, are generic\nand not specifically designed for box-supervised semantic segmentation, thereby\nleaving some leeway for improving segmentation performance. In this paper, we\naim at seeking for a more accurate learning-based class-agnostic pseudo mask\ngenerator tailored to box-supervised semantic segmentation. To this end, we\nresort to a pixel-level annotated auxiliary dataset where the class labels are\nnon-overlapped with those of the box-annotated dataset. For learning pseudo\nmask generator from the auxiliary dataset, we present a bi-level optimization\nformulation. In particular, the lower subproblem is used to learn\nbox-supervised semantic segmentation, while the upper subproblem is used to\nlearn an optimal class-agnostic pseudo mask generator. The learned pseudo\nsegmentation mask generator can then be deployed to the box-annotated dataset\nfor improving weakly supervised semantic segmentation. Experiments on PASCAL\nVOC 2012 dataset show that the learned pseudo mask generator is effective in\nboosting segmentation performance, and our method can further close the\nperformance gap between box-supervised and fully-supervised models. Our code\nwill be made publicly available at\nhttps://github.com/Vious/LPG_BBox_Segmentation .",
          "arxiv_id": "2103.05463v2"
        }
      ],
      "23": [
        {
          "title": "GI-SLAM: Gaussian-Inertial SLAM",
          "year": "2025-03",
          "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful\nrepresentation of geometry and appearance for dense Simultaneous Localization\nand Mapping (SLAM). Through rapid, differentiable rasterization of 3D\nGaussians, many 3DGS SLAM methods achieve near real-time rendering and\naccelerated training. However, these methods largely overlook inertial data,\nwitch is a critical piece of information collected from the inertial\nmeasurement unit (IMU). In this paper, we present GI-SLAM, a novel\ngaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking\nmodule and a realistic 3D Gaussian-based scene representation for mapping. Our\nmethod introduces an IMU loss that seamlessly integrates into the deep learning\nframework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the\naccuracy, robustness and efficiency of camera tracking. Moreover, our SLAM\nsystem supports a wide range of sensor configurations, including monocular,\nstereo, and RGBD cameras, both with and without IMU integration. Our method\nachieves competitive performance compared with existing state-of-the-art\nreal-time methods on the EuRoC and TUM-RGBD datasets.",
          "arxiv_id": "2503.18275v1"
        },
        {
          "title": "Inline Photometrically Calibrated Hybrid Visual SLAM",
          "year": "2024-09",
          "abstract": "This paper presents an integrated approach to Visual SLAM, merging online\nsequential photometric calibration within a Hybrid direct-indirect visual SLAM\n(H-SLAM). Photometric calibration helps normalize pixel intensity values under\ndifferent lighting conditions, and thereby improves the direct component of our\nH-SLAM. A tangential benefit also results to the indirect component of H-SLAM\ngiven that the detected features are more stable across variable lighting\nconditions. Our proposed photometrically calibrated H-SLAM is tested on several\ndatasets, including the TUM monoVO as well as on a dataset we created.\nCalibrated H-SLAM outperforms other state of the art direct, indirect, and\nhybrid Visual SLAM systems in all the experiments. Furthermore, in online SLAM\ntested at our site, it also significantly outperformed the other SLAM Systems.",
          "arxiv_id": "2409.16810v1"
        },
        {
          "title": "GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM",
          "year": "2024-03",
          "abstract": "Recent advancements in RGB-only dense Simultaneous Localization and Mapping\n(SLAM) have predominantly utilized grid-based neural implicit encodings and/or\nstruggle to efficiently realize global map and pose consistency. To this end,\nwe propose an efficient RGB-only dense SLAM system using a flexible neural\npoint cloud scene representation that adapts to keyframe poses and depth\nupdates, without needing costly backpropagation. Another critical challenge of\nRGB-only SLAM is the lack of geometric priors. To alleviate this issue, with\nthe aid of a monocular depth estimator, we introduce a novel DSPO layer for\nbundle adjustment which optimizes the pose and depth of keyframes along with\nthe scale of the monocular depth. Finally, our system benefits from loop\nclosure and online global bundle adjustment and performs either better or\ncompetitive to existing dense neural RGB SLAM methods in tracking, mapping and\nrendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source\ncode is available at https://github.com/zhangganlin/GlOIRE-SLAM",
          "arxiv_id": "2403.19549v3"
        }
      ],
      "24": [
        {
          "title": "Unsupervised Domain Expansion for Visual Categorization",
          "year": "2021-04",
          "abstract": "Expanding visual categorization into a novel domain without the need of extra\nannotation has been a long-term interest for multimedia intelligence.\nPreviously, this challenge has been approached by unsupervised domain\nadaptation (UDA). Given labeled data from a source domain and unlabeled data\nfrom a target domain, UDA seeks for a deep representation that is both\ndiscriminative and domain-invariant. While UDA focuses on the target domain, we\nargue that the performance on both source and target domains matters, as in\npractice which domain a test example comes from is unknown. In this paper we\nextend UDA by proposing a new task called unsupervised domain expansion (UDE),\nwhich aims to adapt a deep model for the target domain with its unlabeled data,\nmeanwhile maintaining the model's performance on the source domain. We propose\nKnowledge Distillation Domain Expansion (KDDE) as a general method for the UDE\ntask. Its domain-adaptation module can be instantiated with any existing model.\nWe develop a knowledge distillation based learning mechanism, enabling KDDE to\noptimize a single objective wherein the source and target domains are equally\ntreated. Extensive experiments on two major benchmarks, i.e., Office-Home and\nDomainNet, show that KDDE compares favorably against four competitive\nbaselines, i.e., DDC, DANN, DAAN, and CDAN, for both UDA and UDE tasks. Our\nstudy also reveals that the current UDA models improve their performance on the\ntarget domain at the cost of noticeable performance loss on the source domain.",
          "arxiv_id": "2104.00233v1"
        },
        {
          "title": "Cross-domain Self-supervised Learning for Domain Adaptation with Few Source Labels",
          "year": "2020-03",
          "abstract": "Existing unsupervised domain adaptation methods aim to transfer knowledge\nfrom a label-rich source domain to an unlabeled target domain. However,\nobtaining labels for some source domains may be very expensive, making complete\nlabeling as used in prior work impractical. In this work, we investigate a new\ndomain adaptation scenario with sparsely labeled source data, where only a few\nexamples in the source domain have been labeled, while the target domain is\nunlabeled. We show that when labeled source examples are limited, existing\nmethods often fail to learn discriminative features applicable for both source\nand target domains. We propose a novel Cross-Domain Self-supervised (CDS)\nlearning approach for domain adaptation, which learns features that are not\nonly domain-invariant but also class-discriminative. Our self-supervised\nlearning method captures apparent visual similarity with in-domain\nself-supervision in a domain adaptive manner and performs cross-domain feature\nmatching with across-domain self-supervision. In extensive experiments with\nthree standard benchmark datasets, our method significantly boosts performance\nof target accuracy in the new target domain with few source labels and is even\nhelpful on classical domain adaptation scenarios.",
          "arxiv_id": "2003.08264v1"
        },
        {
          "title": "Domain-Augmented Domain Adaptation",
          "year": "2022-02",
          "abstract": "Unsupervised domain adaptation (UDA) enables knowledge transfer from the\nlabelled source domain to the unlabeled target domain by reducing the\ncross-domain discrepancy. However, most of the studies were based on direct\nadaptation from the source domain to the target domain and have suffered from\nlarge domain discrepancies. To overcome this challenge, in this paper, we\npropose the domain-augmented domain adaptation (DADA) to generate pseudo\ndomains that have smaller discrepancies with the target domain, to enhance the\nknowledge transfer process by minimizing the discrepancy between the target\ndomain and pseudo domains. Furthermore, we design a pseudo-labeling method for\nDADA by projecting representations from the target domain to multiple pseudo\ndomains and taking the averaged predictions on the classification from the\npseudo domains as the pseudo labels. We conduct extensive experiments with the\nstate-of-the-art domain adaptation methods on four benchmark datasets: Office\nHome, Office-31, VisDA2017, and Digital datasets. The results demonstrate the\nsuperiority of our model.",
          "arxiv_id": "2202.10000v1"
        }
      ],
      "25": [
        {
          "title": "Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning",
          "year": "2023-05",
          "abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive image generation capabilities guided by text prompts. However,\nextending these techniques to video generation remains challenging, with\nexisting text-to-video (T2V) methods often struggling to produce high-quality\nand motion-consistent videos. In this work, we introduce Control-A-Video, a\ncontrollable T2V diffusion model that can generate videos conditioned on text\nprompts and reference control maps like edge and depth maps. To tackle video\nquality and motion consistency issues, we propose novel strategies to\nincorporate content prior and motion prior into the diffusion-based generation\nprocess. Specifically, we employ a first-frame condition scheme to transfer\nvideo generation from the image domain. Additionally, we introduce\nresidual-based and optical flow-based noise initialization to infuse motion\npriors from reference videos, promoting relevance among frame latents for\nreduced flickering. Furthermore, we present a Spatio-Temporal Reward Feedback\nLearning (ST-ReFL) algorithm that optimizes the video diffusion model using\nmultiple reward models for video quality and motion consistency, leading to\nsuperior outputs. Comprehensive experiments demonstrate that our framework\ngenerates higher-quality, more consistent videos compared to existing\nstate-of-the-art methods in controllable text-to-video generation",
          "arxiv_id": "2305.13840v3"
        },
        {
          "title": "LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer",
          "year": "2025-05",
          "abstract": "In recent years, large-scale pre-trained diffusion transformer models have\nmade significant progress in video generation. While current DiT models can\nproduce high-definition, high-frame-rate, and highly diverse videos, there is a\nlack of fine-grained control over the video content. Controlling the motion of\nsubjects in videos using only prompts is challenging, especially when it comes\nto describing complex movements. Further, existing methods fail to control the\nmotion in image-to-video generation, as the subject in the reference image\noften differs from the subject in the reference video in terms of initial\nposition, size, and shape. To address this, we propose the Leveraging Motion\nPrior (LMP) framework for zero-shot video generation. Our framework harnesses\nthe powerful generative capabilities of pre-trained diffusion transformers to\nenable motion in the generated videos to reference user-provided motion videos\nin both text-to-video and image-to-video generation. To this end, we first\nintroduce a foreground-background disentangle module to distinguish between\nmoving subjects and backgrounds in the reference video, preventing interference\nin the target video generation. A reweighted motion transfer module is designed\nto allow the target video to reference the motion from the reference video. To\navoid interference from the subject in the reference video, we propose an\nappearance separation module to suppress the appearance of the reference\nsubject in the target video. We annotate the DAVIS dataset with detailed\nprompts for our experiments and design evaluation metrics to validate the\neffectiveness of our method. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance in generation quality,\nprompt-video consistency, and control capability. Our homepage is available at\nhttps://vpx-ecnu.github.io/LMP-Website/",
          "arxiv_id": "2505.14167v1"
        },
        {
          "title": "Controllable Video Generation: A Survey",
          "year": "2025-07",
          "abstract": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.",
          "arxiv_id": "2507.16869v1"
        }
      ],
      "26": [
        {
          "title": "GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models",
          "year": "2023-10",
          "abstract": "In recent times, the generation of 3D assets from text prompts has shown\nimpressive results. Both 2D and 3D diffusion models can help generate decent 3D\nobjects based on prompts. 3D diffusion models have good 3D consistency, but\ntheir quality and generalization are limited as trainable 3D data is expensive\nand hard to obtain. 2D diffusion models enjoy strong abilities of\ngeneralization and fine generation, but 3D consistency is hard to guarantee.\nThis paper attempts to bridge the power from the two types of diffusion models\nvia the recent explicit and efficient 3D Gaussian splatting representation. A\nfast 3D object generation framework, named as GaussianDreamer, is proposed,\nwhere the 3D diffusion model provides priors for initialization and the 2D\ndiffusion model enriches the geometry and appearance. Operations of noisy point\ngrowing and color perturbation are introduced to enhance the initialized\nGaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D\navatar within 15 minutes on one GPU, much faster than previous methods, while\nthe generated instances can be directly rendered in real time. Demos and code\nare available at https://taoranyi.com/gaussiandreamer/.",
          "arxiv_id": "2310.08529v3"
        },
        {
          "title": "Sharp-It: A Multi-view to Multi-view Diffusion Model for 3D Synthesis and Manipulation",
          "year": "2024-12",
          "abstract": "Advancements in text-to-image diffusion models have led to significant\nprogress in fast 3D content creation. One common approach is to generate a set\nof multi-view images of an object, and then reconstruct it into a 3D model.\nHowever, this approach bypasses the use of a native 3D representation of the\nobject and is hence prone to geometric artifacts and limited in controllability\nand manipulation capabilities. An alternative approach involves native 3D\ngenerative models that directly produce 3D representations. These models,\nhowever, are typically limited in their resolution, resulting in lower quality\n3D objects. In this work, we bridge the quality gap between methods that\ndirectly generate 3D representations and ones that reconstruct 3D objects from\nmulti-view images. We introduce a multi-view to multi-view diffusion model\ncalled Sharp-It, which takes a 3D consistent set of multi-view images rendered\nfrom a low-quality object and enriches its geometric details and texture. The\ndiffusion model operates on the multi-view set in parallel, in the sense that\nit shares features across the generated views. A high-quality 3D model can then\nbe reconstructed from the enriched multi-view set. By leveraging the advantages\nof both 2D and 3D approaches, our method offers an efficient and controllable\nmethod for high-quality 3D content creation. We demonstrate that Sharp-It\nenables various 3D applications, such as fast synthesis, editing, and\ncontrolled generation, while attaining high-quality assets.",
          "arxiv_id": "2412.02631v1"
        },
        {
          "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior",
          "year": "2023-12",
          "abstract": "Recently, 3D content creation from text prompts has demonstrated remarkable\nprogress by utilizing 2D and 3D diffusion models. While 3D diffusion models\nensure great multi-view consistency, their ability to generate high-quality and\ndiverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion\nmodels find a distillation approach that achieves excellent generalization and\nrich details without any 3D data. However, 2D lifting methods suffer from\ninherent view-agnostic ambiguity thereby leading to serious multi-face Janus\nissues, where text prompts fail to provide sufficient guidance to learn\ncoherent 3D results. Instead of retraining a costly viewpoint-aware model, we\nstudy how to fully exploit easily accessible coarse 3D knowledge to enhance the\nprompts and guide 2D lifting optimization for refinement. In this paper, we\npropose Sherpa3D, a new text-to-3D framework that achieves high-fidelity,\ngeneralizability, and geometric consistency simultaneously. Specifically, we\ndesign a pair of guiding strategies derived from the coarse 3D prior generated\nby the 3D diffusion model: a structural guidance for geometric fidelity and a\nsemantic guidance for 3D coherence. Employing the two types of guidance, the 2D\ndiffusion model enriches the 3D content with diversified and high-quality\nresults. Extensive experiments show the superiority of our Sherpa3D over the\nstate-of-the-art text-to-3D methods in terms of quality and 3D consistency.",
          "arxiv_id": "2312.06655v1"
        }
      ],
      "27": [
        {
          "title": "Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives",
          "year": "2023-11",
          "abstract": "Person re-identification (Re-ID) technology plays an increasingly crucial\nrole in intelligent surveillance systems. Widespread occlusion significantly\nimpacts the performance of person Re-ID. Occluded person Re-ID refers to a\npedestrian matching method that deals with challenges such as pedestrian\ninformation loss, noise interference, and perspective misalignment. It has\ngarnered extensive attention from researchers. Over the past few years, several\nocclusion-solving person Re-ID methods have been proposed, tackling various\nsub-problems arising from occlusion. However, there is a lack of comprehensive\nstudies that compare, summarize, and evaluate the potential of occluded person\nRe-ID methods in detail. In this review, we start by providing a detailed\noverview of the datasets and evaluation scheme used for occluded person Re-ID.\nNext, we scientifically classify and analyze existing deep learning-based\noccluded person Re-ID methods from various perspectives, summarizing them\nconcisely. Furthermore, we conduct a systematic comparison among these methods,\nidentify the state-of-the-art approaches, and present an outlook on the future\ndevelopment of occluded person Re-ID.",
          "arxiv_id": "2311.00603v1"
        },
        {
          "title": "Unsupervised Pre-training for Person Re-identification",
          "year": "2020-12",
          "abstract": "In this paper, we present a large scale unlabeled person re-identification\n(Re-ID) dataset \"LUPerson\" and make the first attempt of performing\nunsupervised pre-training for improving the generalization ability of the\nlearned person Re-ID feature representation. This is to address the problem\nthat all existing person Re-ID datasets are all of limited scale due to the\ncostly effort required for data annotation. Previous research tries to leverage\nmodels pre-trained on ImageNet to mitigate the shortage of person Re-ID data\nbut suffers from the large domain gap between ImageNet and person Re-ID data.\nLUPerson is an unlabeled dataset of 4M images of over 200K identities, which is\n30X larger than the largest existing Re-ID dataset. It also covers a much\ndiverse range of capturing environments (eg, camera settings, scenes, etc.).\nBased on this dataset, we systematically study the key factors for learning\nRe-ID features from two perspectives: data augmentation and contrastive loss.\nUnsupervised pre-training performed on this large-scale dataset effectively\nleads to a generic Re-ID feature that can benefit all existing person Re-ID\nmethods. Using our pre-trained model in some basic frameworks, our methods\nachieve state-of-the-art results without bells and whistles on four widely used\nRe-ID datasets: CUHK03, Market1501, DukeMTMC, and MSMT17. Our results also show\nthat the performance improvement is more significant on small-scale target\ndatasets or under few-shot setting.",
          "arxiv_id": "2012.03753v2"
        },
        {
          "title": "Deep Learning-based Occluded Person Re-identification: A Survey",
          "year": "2022-07",
          "abstract": "Occluded person re-identification (Re-ID) aims at addressing the occlusion\nproblem when retrieving the person of interest across multiple cameras. With\nthe promotion of deep learning technology and the increasing demand for\nintelligent video surveillance, the frequent occlusion in real-world\napplications has made occluded person Re-ID draw considerable interest from\nresearchers. A large number of occluded person Re-ID methods have been proposed\nwhile there are few surveys that focus on occlusion. To fill this gap and help\nboost future research, this paper provides a systematic survey of occluded\nperson Re-ID. Through an in-depth analysis of the occlusion in person Re-ID,\nmost existing methods are found to only consider part of the problems brought\nby occlusion. Therefore, we review occlusion-related person Re-ID methods from\nthe perspective of issues and solutions. We summarize four issues caused by\nocclusion in person Re-ID, i.e., position misalignment, scale misalignment,\nnoisy information, and missing information. The occlusion-related methods\naddressing different issues are then categorized and introduced accordingly.\nAfter that, we summarize and compare the performance of recent occluded person\nRe-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS,\nOccluded-ReID, and Occluded-DukeMTMC. Finally, we provide insights on promising\nfuture research directions.",
          "arxiv_id": "2207.14452v1"
        }
      ],
      "28": [
        {
          "title": "Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification",
          "year": "2024-11",
          "abstract": "Convolutional Neural Networks (CNNs) have seen significant performance\nimprovements in recent years. However, due to their size and complexity, they\nfunction as black-boxes, leading to transparency concerns. State-of-the-art\nsaliency methods generate local explanations that highlight the area in the\ninput image where a class is identified but cannot explain how a concept of\ninterest contributes to the prediction, which is essential for bias mitigation.\nOn the other hand, concept-based methods, such as TCAV (Testing with Concept\nActivation Vectors), provide insights into how sensitive is the network to a\nconcept, but cannot compute its attribution in a specific prediction nor show\nits location within the input image. This paper introduces a novel post-hoc\nexplainability framework, Visual-TCAV, which aims to bridge the gap between\nthese methods by providing both local and global explanations for CNN-based\nimage classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to\ngenerate saliency maps that show where concepts are recognized by the network.\nMoreover, it can estimate the attribution of these concepts to the output of\nany class using a generalization of Integrated Gradients. This framework is\nevaluated on popular CNN architectures, with its validity further confirmed via\nexperiments where ground truth for explanations is known, and a comparison with\nTCAV. Our code is available at\nhttps://github.com/DataSciencePolimi/Visual-TCAV.",
          "arxiv_id": "2411.05698v2"
        },
        {
          "title": "CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models",
          "year": "2021-09",
          "abstract": "We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.",
          "arxiv_id": "2109.01401v3"
        },
        {
          "title": "Solving the enigma: Enhancing faithfulness and comprehensibility in explanations of deep networks",
          "year": "2024-05",
          "abstract": "The accelerated progress of artificial intelligence (AI) has popularized deep\nlearning models across various domains, yet their inherent opacity poses\nchallenges, particularly in critical fields like healthcare, medicine, and the\ngeosciences. Explainable AI (XAI) has emerged to shed light on these 'black\nbox' models, aiding in deciphering their decision-making processes. However,\ndifferent XAI methods often produce significantly different explanations,\nleading to high inter-method variability that increases uncertainty and\nundermines trust in deep networks' predictions. In this study, we address this\nchallenge by introducing a novel framework designed to enhance the\nexplainability of deep networks through a dual focus on maximizing both\naccuracy and comprehensibility in the explanations. Our framework integrates\noutputs from multiple established XAI methods and leverages a non-linear neural\nnetwork model, termed the 'explanation optimizer,' to construct a unified,\noptimal explanation. The optimizer evaluates explanations using two key\nmetrics: faithfulness (accuracy in reflecting the network's decisions) and\ncomplexity (comprehensibility). By balancing these, it provides accurate and\naccessible explanations, addressing a key XAI limitation. Experiments on\nmulti-class and binary classification in 2D object and 3D neuroscience imaging\nconfirm its efficacy. Our optimizer achieved faithfulness scores 155% and 63%\nhigher than the best XAI methods in 3D and 2D tasks, respectively, while also\nreducing complexity for better understanding. These results demonstrate that\noptimal explanations based on specific quality criteria are achievable,\noffering a solution to the issue of inter-method variability in the current XAI\nliterature and supporting more trustworthy deep network predictions",
          "arxiv_id": "2405.10008v3"
        }
      ],
      "29": [
        {
          "title": "You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object Tracking",
          "year": "2023-04",
          "abstract": "In the classical tracking-by-detection (TBD) paradigm, detection and tracking\nare separately and sequentially conducted, and data association must be\nproperly performed to achieve satisfactory tracking performance. In this paper,\na new end-to-end multi-object tracking framework is proposed, which integrates\nobject detection and multi-object tracking into a single model. The proposed\ntracking framework eliminates the complex data association process in the\nclassical TBD paradigm, and requires no additional training. Secondly, the\nregression confidence of historical trajectories is investigated, and the\npossible states of a trajectory (weak object or strong object) in the current\nframe are predicted. Then, a confidence fusion module is designed to guide\nnon-maximum suppression for trajectories and detections to achieve ordered and\nrobust tracking. Thirdly, by integrating historical trajectory features, the\nregression performance of the detector is enhanced, which better reflects the\nocclusion and disappearance patterns of objects in real world. Lastly,\nextensive experiments are conducted on the commonly used KITTI and Waymo\ndatasets. The results show that the proposed framework can achieve robust\ntracking by using only a 2D detector and a 3D detector, and it is proven more\naccurate than many of the state-of-the-art TBD-based multi-modal tracking\nmethods. The source codes of the proposed method are available at\nhttps://github.com/wangxiyang2022/YONTD-MOT.",
          "arxiv_id": "2304.08709v2"
        },
        {
          "title": "Global Correlation Network: End-to-End Joint Multi-Object Detection and Tracking",
          "year": "2021-03",
          "abstract": "Multi-object tracking (MOT) has made great progress in recent years, but\nthere are still some problems. Most MOT algorithms follow tracking-by-detection\nframework, which separates detection and tracking into two independent parts.\nEarly tracking-by-detection algorithms need to do two feature extractions for\ndetection and tracking. Recently, some algorithms make the feature extraction\ninto one network, but the tracking part still relies on data association and\nneeds complex post-processing for life cycle management. Those methods do not\ncombine detection and tracking well. In this paper, we present a novel network\nto realize joint multi-object detection and tracking in an end-to-end way,\ncalled Global Correlation Network (GCNet). Different from most object detection\nmethods, GCNet introduces the global correlation layer for regression of\nabsolute size and coordinates of bounding boxes instead of offsets prediction.\nThe pipeline of detection and tracking by GCNet is conceptually simple, which\ndoes not need non-maximum suppression, data association, and other complicated\ntracking strategies. GCNet was evaluated on a multi-vehicle tracking dataset,\nUA-DETRAC, and demonstrates promising performance compared to the\nstate-of-the-art detectors and trackers.",
          "arxiv_id": "2103.12511v2"
        },
        {
          "title": "Single Object Tracking Research: A Survey",
          "year": "2022-04",
          "abstract": "Visual object tracking is an important task in computer vision, which has\nmany real-world applications, e.g., video surveillance, visual navigation.\nVisual object tracking also has many challenges, e.g., object occlusion and\ndeformation. To solve above problems and track the target accurately and\nefficiently, many tracking algorithms have emerged in recent years. This paper\npresents the rationale and representative works of two most popular tracking\nframeworks in past ten years, i.e., the corelation filter and Siamese network\nfor object tracking. Then we present some deep learning based tracking methods\ncategorized by different network structures. We also introduce some classical\nstrategies for handling the challenges in tracking problem. Further, this paper\ndetailedly present and compare the benchmarks and challenges for tracking, from\nwhich we summarize the development history and development trend of visual\ntracking. Focusing on the future development of object tracking, which we think\nwould be applied in real-world scenes before some problems to be addressed,\nsuch as the problems in long-term tracking, low-power high-speed tracking and\nattack-robust tracking. In the future, the integration of multimodal data,\ne.g., the depth image, thermal image with traditional color image, will provide\nmore solutions for visual tracking. Moreover, tracking task will go together\nwith some other tasks, e.g., video object detection and segmentation.",
          "arxiv_id": "2204.11410v1"
        }
      ],
      "30": [
        {
          "title": "The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models",
          "year": "2025-06",
          "abstract": "As Vision-Language Models (VLMs) demonstrate increasing capabilities across\nreal-world applications such as code generation and chatbot assistance,\nensuring their safety has become paramount. Unlike traditional Large Language\nModels (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,\nallowing adversaries to modify visual or textual inputs to bypass safety\nguardrails and trigger the generation of harmful content. Through systematic\nanalysis of VLM behavior under attack, we identify a novel phenomenon termed\n``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs\nmay initially be compromised to produce harmful content, but eventually\nrecognize the associated risks and attempt to self-correct. This pattern\nsuggests that VLMs retain their underlying safety awareness but experience a\ntemporal delay in their activation. Building on this insight, we hypothesize\nthat VLMs' safety awareness can be proactively reactivated through carefully\ndesigned prompts. To this end, we introduce ``The Safety Reminder'', a soft\nprompt tuning approach that optimizes learnable prompt tokens, which are\nperiodically injected during the text generation process to enhance safety\nawareness, effectively preventing harmful content generation. Additionally, our\nsafety reminder only activates when harmful content is detected, leaving normal\nconversations unaffected and preserving the model's performance on benign\ntasks. Through comprehensive evaluation across three established safety\nbenchmarks and one adversarial attacks, we demonstrate that our approach\nsignificantly reduces attack success rates while maintaining model utility,\noffering a practical solution for deploying safer VLMs in real-world\napplications.",
          "arxiv_id": "2506.15734v1"
        },
        {
          "title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models",
          "year": "2024-05",
          "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have underscored\ntheir superiority in various multimodal tasks. However, the adversarial\nrobustness of VLMs has not been fully explored. Existing methods mainly assess\nrobustness through unimodal adversarial attacks that perturb images, while\nassuming inherent resilience against text-based attacks. Different from\nexisting attacks, in this work we propose a more comprehensive strategy that\njointly attacks both text and image modalities to exploit a broader spectrum of\nvulnerability within VLMs. Specifically, we propose a dual optimization\nobjective aimed at guiding the model to generate affirmative responses with\nhigh toxicity. Our attack method begins by optimizing an adversarial image\nprefix from random noise to generate diverse harmful responses in the absence\nof text input, thus imbuing the image with toxic semantics. Subsequently, an\nadversarial text suffix is integrated and co-optimized with the adversarial\nimage prefix to maximize the probability of eliciting affirmative responses to\nvarious harmful instructions. The discovered adversarial image prefix and text\nsuffix are collectively denoted as a Universal Master Key (UMK). When\nintegrated into various malicious queries, UMK can circumvent the alignment\ndefenses of VLMs and lead to the generation of objectionable content, known as\njailbreaks. The experimental results demonstrate that our universal attack\nstrategy can effectively jailbreak MiniGPT-4 with a 96% success rate,\nhighlighting the vulnerability of VLMs and the urgent need for new alignment\nstrategies.",
          "arxiv_id": "2405.17894v2"
        },
        {
          "title": "Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks",
          "year": "2024-11",
          "abstract": "Pre-trained vision-language models (VLMs) have showcased remarkable\nperformance in image and natural language understanding, such as image\ncaptioning and response generation. As the practical applications of\nvision-language models become increasingly widespread, their potential safety\nand robustness issues raise concerns that adversaries may evade the system and\ncause these models to generate toxic content through malicious attacks.\nTherefore, evaluating the robustness of open-source VLMs against adversarial\nattacks has garnered growing attention, with transfer-based attacks as a\nrepresentative black-box attacking strategy. However, most existing\ntransfer-based attacks neglect the importance of the semantic correlations\nbetween vision and text modalities, leading to sub-optimal adversarial example\ngeneration and attack performance. To address this issue, we present Chain of\nAttack (CoA), which iteratively enhances the generation of adversarial examples\nbased on the multi-modal semantic update using a series of intermediate\nattacking steps, achieving superior adversarial transferability and efficiency.\nA unified attack success rate computing method is further proposed for\nautomatic evasion evaluation. Extensive experiments conducted under the most\nrealistic and high-stakes scenario, demonstrate that our attacking strategy can\neffectively mislead models to generate targeted responses using only black-box\nattacks without any knowledge of the victim models. The comprehensive\nrobustness evaluation in our paper provides insight into the vulnerabilities of\nVLMs and offers a reference for the safety considerations of future model\ndevelopments.",
          "arxiv_id": "2411.15720v1"
        }
      ],
      "31": [
        {
          "title": "A Close Look at Spatial Modeling: From Attention to Convolution",
          "year": "2022-12",
          "abstract": "Vision Transformers have shown great promise recently for many vision tasks\ndue to the insightful architecture design and attention mechanism. By\nrevisiting the self-attention responses in Transformers, we empirically observe\ntwo interesting issues. First, Vision Transformers present a queryirrelevant\nbehavior at deep layers, where the attention maps exhibit nearly consistent\ncontexts in global scope, regardless of the query patch position (also\nhead-irrelevant). Second, the attention maps are intrinsically sparse, few\ntokens dominate the attention weights; introducing the knowledge from ConvNets\nwould largely smooth the attention and enhance the performance. Motivated by\nabove observations, we generalize self-attention formulation to abstract a\nqueryirrelevant global context directly and further integrate the global\ncontext into convolutions. The resulting model, a Fully Convolutional Vision\nTransformer (i.e., FCViT), purely consists of convolutional layers and firmly\ninherits the merits of both attention mechanism and convolutions, including\ndynamic property, weight sharing, and short- and long-range feature modeling,\netc. Experimental results demonstrate the effectiveness of FCViT. With less\nthan 14M parameters, our FCViT-S12 outperforms related work ResT-Lite by 3.7%\ntop1 accuracy on ImageNet-1K. When scaling FCViT to larger models, we still\nperform better than previous state-of-the-art ConvNeXt with even fewer\nparameters. FCViT-based models also demonstrate promising transferability to\ndownstream tasks, like object detection, instance segmentation, and semantic\nsegmentation. Codes and models are made available at:\nhttps://github.com/ma-xu/FCViT.",
          "arxiv_id": "2212.12552v1"
        },
        {
          "title": "You Only Need Less Attention at Each Stage in Vision Transformers",
          "year": "2024-06",
          "abstract": "The advent of Vision Transformers (ViTs) marks a substantial paradigm shift\nin the realm of computer vision. ViTs capture the global information of images\nthrough self-attention modules, which perform dot product computations among\npatchified image tokens. While self-attention modules empower ViTs to capture\nlong-range dependencies, the computational complexity grows quadratically with\nthe number of tokens, which is a major hindrance to the practical application\nof ViTs. Moreover, the self-attention mechanism in deep ViTs is also\nsusceptible to the attention saturation issue. Accordingly, we argue against\nthe necessity of computing the attention scores in every layer, and we propose\nthe Less-Attention Vision Transformer (LaViT), which computes only a few\nattention operations at each stage and calculates the subsequent feature\nalignments in other layers via attention transformations that leverage the\npreviously calculated attention scores. This novel approach can mitigate two\nprimary issues plaguing traditional self-attention modules: the heavy\ncomputational burden and attention saturation. Our proposed architecture offers\nsuperior efficiency and ease of implementation, merely requiring matrix\nmultiplications that are highly optimized in contemporary deep learning\nframeworks. Moreover, our architecture demonstrates exceptional performance\nacross various vision tasks including classification, detection and\nsegmentation.",
          "arxiv_id": "2406.00427v1"
        },
        {
          "title": "The Linear Attention Resurrection in Vision Transformer",
          "year": "2025-01",
          "abstract": "Vision Transformers (ViTs) have recently taken computer vision by storm.\nHowever, the softmax attention underlying ViTs comes with a quadratic\ncomplexity in time and memory, hindering the application of ViTs to\nhigh-resolution images. We revisit the attention design and propose a linear\nattention method to address the limitation, which doesn't sacrifice ViT's core\nadvantage of capturing global representation like existing methods (e.g. local\nwindow attention of Swin). We further investigate the key difference between\nlinear attention and softmax attention. Our empirical results suggest that\nlinear attention lacks a fundamental property of concentrating the distribution\nof the attention matrix. Inspired by this observation, we introduce a local\nconcentration module to enhance linear attention. By incorporating enhanced\nlinear global attention and local window attention, we propose a new ViT\narchitecture, dubbed L$^2$ViT. Notably, L$^2$ViT can effectively capture both\nglobal interactions and local representations while enjoying linear\ncomputational complexity. Extensive experiments demonstrate the strong\nperformance of L$^2$ViT. On image classification, L$^2$ViT achieves 84.4% Top-1\naccuracy on ImageNet-1K without any extra training data or label. By further\npre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution\n384$^2$. For downstream tasks, L$^2$ViT delivers favorable performance as a\nbackbone on object detection as well as semantic segmentation.",
          "arxiv_id": "2501.16182v1"
        }
      ],
      "32": [
        {
          "title": "Meta Navigator: Search for a Good Adaptation Policy for Few-shot Learning",
          "year": "2021-09",
          "abstract": "Few-shot learning aims to adapt knowledge learned from previous tasks to\nnovel tasks with only a limited amount of labeled data. Research literature on\nfew-shot learning exhibits great diversity, while different algorithms often\nexcel at different few-shot learning scenarios. It is therefore tricky to\ndecide which learning strategies to use under different task conditions.\nInspired by the recent success in Automated Machine Learning literature\n(AutoML), in this paper, we present Meta Navigator, a framework that attempts\nto solve the aforementioned limitation in few-shot learning by seeking a\nhigher-level strategy and proffer to automate the selection from various\nfew-shot learning designs. The goal of our work is to search for good parameter\nadaptation policies that are applied to different stages in the network for\nfew-shot classification. We present a search space that covers many popular\nfew-shot learning algorithms in the literature and develop a differentiable\nsearching and decoding algorithm based on meta-learning that supports\ngradient-based optimization. We demonstrate the effectiveness of our\nsearching-based method on multiple benchmark datasets. Extensive experiments\nshow that our approach significantly outperforms baselines and demonstrates\nperformance advantages over many state-of-the-art methods. Code and models will\nbe made publicly available.",
          "arxiv_id": "2109.05749v1"
        },
        {
          "title": "Semantics Disentangling for Generalized Zero-Shot Learning",
          "year": "2021-01",
          "abstract": "Generalized zero-shot learning (GZSL) aims to classify samples under the\nassumption that some classes are not observable during training. To bridge the\ngap between the seen and unseen classes, most GZSL methods attempt to associate\nthe visual features of seen classes with attributes or to generate unseen\nsamples directly. Nevertheless, the visual features used in the prior\napproaches do not necessarily encode semantically related information that the\nshared attributes refer to, which degrades the model generalization to unseen\nclasses. To address this issue, in this paper, we propose a novel semantics\ndisentangling framework for the generalized zero-shot learning task (SDGZSL),\nwhere the visual features of unseen classes are firstly estimated by a\nconditional VAE and then factorized into semantic-consistent and\nsemantic-unrelated latent vectors. In particular, a total correlation penalty\nis applied to guarantee the independence between the two factorized\nrepresentations, and the semantic consistency of which is measured by the\nderived relation network. Extensive experiments conducted on four GZSL\nbenchmark datasets have evidenced that the semantic-consistent features\ndisentangled by the proposed SDGZSL are more generalizable in tasks of\ncanonical and generalized zero-shot learning. Our source code is available at\nhttps://github.com/uqzhichen/SDGZSL.",
          "arxiv_id": "2101.07978v5"
        },
        {
          "title": "Domain Agnostic Few-Shot Learning For Document Intelligence",
          "year": "2021-10",
          "abstract": "Few-shot learning aims to generalize to novel classes with only a few samples\nwith class labels. Research in few-shot learning has borrowed techniques from\ntransfer learning, metric learning, meta-learning, and Bayesian methods. These\nmethods also aim to train models from limited training samples, and while\nencouraging performance has been achieved, they often fail to generalize to\nnovel domains. Many of the existing meta-learning methods rely on training data\nfor which the base classes are sampled from the same domain as the novel\nclasses used for meta-testing. However, in many applications in the industry,\nsuch as document classification, collecting large samples of data for\nmeta-learning is infeasible or impossible. While research in the field of the\ncross-domain few-shot learning exists, it is mostly limited to computer vision.\nTo our knowledge, no work yet exists that examines the use of few-shot learning\nfor classification of semi-structured documents (scans of paper documents)\ngenerated as part of a business workflow (forms, letters, bills, etc.). Here\nthe domain shift is significant, going from natural images to the\nsemi-structured documents of interest. In this work, we address the problem of\nfew-shot document image classification under domain shift. We evaluate our work\nby extensive comparisons with existing methods. Experimental results\ndemonstrate that the proposed method shows consistent improvements on the\nfew-shot classification performance under domain shift.",
          "arxiv_id": "2111.00007v1"
        }
      ],
      "33": [
        {
          "title": "HybridFusion: LiDAR and Vision Cross-Source Point Cloud Fusion",
          "year": "2023-04",
          "abstract": "Recently, cross-source point cloud registration from different sensors has\nbecome a significant research focus. However, traditional methods confront\nchallenges due to the varying density and structure of cross-source point\nclouds. In order to solve these problems, we propose a cross-source point cloud\nfusion algorithm called HybridFusion. It can register cross-source dense point\nclouds from different viewing angle in outdoor large scenes. The entire\nregistration process is a coarse-to-fine procedure. First, the point cloud is\ndivided into small patches, and a matching patch set is selected based on\nglobal descriptors and spatial distribution, which constitutes the coarse\nmatching process. To achieve fine matching, 2D registration is performed by\nextracting 2D boundary points from patches, followed by 3D adjustment. Finally,\nthe results of multiple patch pose estimates are clustered and fused to\ndetermine the final pose. The proposed approach is evaluated comprehensively\nthrough qualitative and quantitative experiments. In order to compare the\nrobustness of cross-source point cloud registration, the proposed method and\ngeneralized iterative closest point method are compared. Furthermore, a metric\nfor describing the degree of point cloud filling is proposed. The experimental\nresults demonstrate that our approach achieves state-of-the-art performance in\ncross-source point cloud registration.",
          "arxiv_id": "2304.04508v1"
        },
        {
          "title": "Robust Point Cloud Registration Framework Based on Deep Graph Matching",
          "year": "2021-03",
          "abstract": "3D point cloud registration is a fundamental problem in computer vision and\nrobotics. There has been extensive research in this area, but existing methods\nmeet great challenges in situations with a large proportion of outliers and\ntime constraints, but without good transformation initialization. Recently, a\nseries of learning-based algorithms have been introduced and show advantages in\nspeed. Many of them are based on correspondences between the two point clouds,\nso they do not rely on transformation initialization. However, these\nlearning-based methods are sensitive to outliers, which lead to more incorrect\ncorrespondences. In this paper, we propose a novel deep graph matchingbased\nframework for point cloud registration. Specifically, we first transform point\nclouds into graphs and extract deep features for each point. Then, we develop a\nmodule based on deep graph matching to calculate a soft correspondence matrix.\nBy using graph matching, not only the local geometry of each point but also its\nstructure and topology in a larger range are considered in establishing\ncorrespondences, so that more correct correspondences are found. We train the\nnetwork with a loss directly defined on the correspondences, and in the test\nstage the soft correspondences are transformed into hard one-to-one\ncorrespondences so that registration can be performed by singular value\ndecomposition. Furthermore, we introduce a transformer-based method to generate\nedges for graph construction, which further improves the quality of the\ncorrespondences. Extensive experiments on registering clean, noisy,\npartial-to-partial and unseen category point clouds show that the proposed\nmethod achieves state-of-the-art performance. The code will be made publicly\navailable at https://github.com/fukexue/RGM.",
          "arxiv_id": "2103.04256v1"
        },
        {
          "title": "Robust Point Cloud Registration Framework Based on Deep Graph Matching(TPAMI Version)",
          "year": "2022-11",
          "abstract": "3D point cloud registration is a fundamental problem in computer vision and\nrobotics. Recently, learning-based point cloud registration methods have made\ngreat progress. However, these methods are sensitive to outliers, which lead to\nmore incorrect correspondences. In this paper, we propose a novel deep graph\nmatching-based framework for point cloud registration. Specifically, we first\ntransform point clouds into graphs and extract deep features for each point.\nThen, we develop a module based on deep graph matching to calculate a soft\ncorrespondence matrix. By using graph matching, not only the local geometry of\neach point but also its structure and topology in a larger range are considered\nin establishing correspondences, so that more correct correspondences are\nfound. We train the network with a loss directly defined on the\ncorrespondences, and in the test stage the soft correspondences are transformed\ninto hard one-to-one correspondences so that registration can be performed by a\ncorrespondence-based solver. Furthermore, we introduce a transformer-based\nmethod to generate edges for graph construction, which further improves the\nquality of the correspondences. Extensive experiments on object-level and\nscene-level benchmark datasets show that the proposed method achieves\nstate-of-the-art performance. The code is available at:\n\\href{https://github.com/fukexue/RGM}{https://github.com/fukexue/RGM}.",
          "arxiv_id": "2211.04696v1"
        }
      ],
      "34": [
        {
          "title": "Learned Gridification for Efficient Point Cloud Processing",
          "year": "2023-07",
          "abstract": "Neural operations that rely on neighborhood information are much more\nexpensive when deployed on point clouds than on grid data due to the irregular\ndistances between points in a point cloud. In a grid, on the other hand, we can\ncompute the kernel only once and reuse it for all query positions. As a result,\noperations that rely on neighborhood information scale much worse for point\nclouds than for grid data, specially for large inputs and large neighborhoods.\n  In this work, we address the scalability issue of point cloud methods by\ntackling its root cause: the irregularity of the data. We propose learnable\ngridification as the first step in a point cloud processing pipeline to\ntransform the point cloud into a compact, regular grid. Thanks to\ngridification, subsequent layers can use operations defined on regular grids,\ne.g., Conv3D, which scale much better than native point cloud methods. We then\nextend gridification to point cloud to point cloud tasks, e.g., segmentation,\nby adding a learnable de-gridification step at the end of the point cloud\nprocessing pipeline to map the compact, regular grid back to its original point\ncloud form. Through theoretical and empirical analysis, we show that gridified\nnetworks scale better in terms of memory and time than networks directly\napplied on raw point cloud data, while being able to achieve competitive\nresults. Our code is publicly available at\nhttps://github.com/computri/gridifier.",
          "arxiv_id": "2307.14354v1"
        },
        {
          "title": "DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for Classification",
          "year": "2025-04",
          "abstract": "Deep neural networks have achieved significant success in 3D point cloud\nclassification while relying on large-scale, annotated point cloud datasets,\nwhich are labor-intensive to build. Compared to capturing data with LiDAR\nsensors and then performing annotation, it is relatively easier to sample point\nclouds from CAD models. Yet, data sampled from CAD models is regular, and does\nnot suffer from occlusion and missing points, which are very common for LiDAR\ndata, creating a large domain shift. Therefore, it is critical to develop\nmethods that can generalize well across different point cloud domains. %In this\npaper, we focus on the 3D point cloud domain generalization problem. Existing\n3D domain generalization methods employ point-based backbones to extract point\ncloud features. Yet, by analyzing point utilization of point-based methods and\nobserving the geometry of point clouds from different domains, we have found\nthat a large number of point features are discarded by point-based methods\nthrough the max-pooling operation. This is a significant waste especially\nconsidering the fact that domain generalization is more challenging than\nsupervised learning, and point clouds are already affected by missing points\nand occlusion to begin with. To address these issues, we propose a novel method\nfor 3D point cloud domain generalization, which can generalize to unseen\ndomains of point clouds. Our proposed method employs multiple 2D projections of\na 3D point cloud to alleviate the issue of missing points and involves a simple\nyet effective convolution-based model to extract features. The experiments,\nperformed on the PointDA-10 and Sim-to-Real benchmarks, demonstrate the\neffectiveness of our proposed method, which outperforms different baselines,\nand can transfer well from synthetic domain to real-world domain.",
          "arxiv_id": "2504.12456v1"
        },
        {
          "title": "Multi-scale Receptive Fields Graph Attention Network for Point Cloud Classification",
          "year": "2020-09",
          "abstract": "Understanding the implication of point cloud is still challenging to achieve\nthe goal of classification or segmentation due to the irregular and sparse\nstructure of point cloud. As we have known, PointNet architecture as a\nground-breaking work for point cloud which can learn efficiently shape features\ndirectly on unordered 3D point cloud and have achieved favorable performance.\nHowever, this model fail to consider the fine-grained semantic information of\nlocal structure for point cloud. Afterwards, many valuable works are proposed\nto enhance the performance of PointNet by means of semantic features of local\npatch for point cloud. In this paper, a multi-scale receptive fields graph\nattention network (named after MRFGAT) for point cloud classification is\nproposed. By focusing on the local fine features of point cloud and applying\nmulti attention modules based on channel affinity, the learned feature map for\nour network can well capture the abundant features information of point cloud.\nThe proposed MRFGAT architecture is tested on ModelNet10 and ModelNet40\ndatasets, and results show it achieves state-of-the-art performance in shape\nclassification tasks.",
          "arxiv_id": "2009.13289v1"
        }
      ],
      "35": [
        {
          "title": "LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer",
          "year": "2024-05",
          "abstract": "Animatable clothing transfer, aiming at dressing and animating garments\nacross characters, is a challenging problem. Most human avatar works entangle\nthe representations of the human body and clothing together, which leads to\ndifficulties for virtual try-on across identities. What's worse, the entangled\nrepresentations usually fail to exactly track the sliding motion of garments.\nTo overcome these limitations, we present Layered Gaussian Avatars (LayGA), a\nnew representation that formulates body and clothing as two separate layers for\nphotorealistic animatable clothing transfer from multi-view videos. Our\nrepresentation is built upon the Gaussian map-based avatar for its excellent\nrepresentation power of garment details. However, the Gaussian map produces\nunstructured 3D Gaussians distributed around the actual surface. The absence of\na smooth explicit surface raises challenges in accurate garment tracking and\ncollision handling between body and garments. Therefore, we propose two-stage\ntraining involving single-layer reconstruction and multi-layer fitting. In the\nsingle-layer reconstruction stage, we propose a series of geometric constraints\nto reconstruct smooth surfaces and simultaneously obtain the segmentation\nbetween body and clothing. Next, in the multi-layer fitting stage, we train two\nseparate models to represent body and clothing and utilize the reconstructed\nclothing geometries as 3D supervision for more accurate garment tracking.\nFurthermore, we propose geometry and rendering layers for both high-quality\ngeometric reconstruction and high-fidelity rendering. Overall, the proposed\nLayGA realizes photorealistic animations and virtual try-on, and outperforms\nother baseline methods. Our project page is\nhttps://jsnln.github.io/layga/index.html.",
          "arxiv_id": "2405.07319v1"
        },
        {
          "title": "HumanCoser: Layered 3D Human Generation via Semantic-Aware Diffusion Model",
          "year": "2024-08",
          "abstract": "This paper aims to generate physically-layered 3D humans from text prompts.\nExisting methods either generate 3D clothed humans as a whole or support only\ntight and simple clothing generation, which limits their applications to\nvirtual try-on and part-level editing. To achieve physically-layered 3D human\ngeneration with reusable and complex clothing, we propose a novel layer-wise\ndressed human representation based on a physically-decoupled diffusion model.\nSpecifically, to achieve layer-wise clothing generation, we propose a\ndual-representation decoupling framework for generating clothing decoupled from\nthe human body, in conjunction with an innovative multi-layer fusion volume\nrendering method. To match the clothing with different body shapes, we propose\nan SMPL-driven implicit field deformation network that enables the free\ntransfer and reuse of clothing. Extensive experiments demonstrate that our\napproach not only achieves state-of-the-art layered 3D human generation with\ncomplex clothing but also supports virtual try-on and layered human animation.",
          "arxiv_id": "2408.11357v1"
        },
        {
          "title": "Capturing and Animation of Body and Clothing from Monocular Video",
          "year": "2022-10",
          "abstract": "While recent work has shown progress on extracting clothed 3D human avatars\nfrom a single image, video, or a set of 3D scans, several limitations remain.\nMost methods use a holistic representation to jointly model the body and\nclothing, which means that the clothing and body cannot be separated for\napplications like virtual try-on. Other methods separately model the body and\nclothing, but they require training from a large set of 3D clothed human meshes\nobtained from 3D/4D scanners or physics simulations. Our insight is that the\nbody and clothing have different modeling requirements. While the body is well\nrepresented by a mesh-based parametric 3D model, implicit representations and\nneural radiance fields are better suited to capturing the large variety in\nshape and appearance present in clothing. Building on this insight, we propose\nSCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a\nmesh-based body with a neural radiance field. Integrating the mesh into the\nvolumetric rendering in combination with a differentiable rasterizer enables us\nto optimize SCARF directly from monocular videos, without any 3D supervision.\nThe hybrid modeling enables SCARF to (i) animate the clothed body avatar by\nchanging body poses (including hand articulation and facial expressions), (ii)\nsynthesize novel views of the avatar, and (iii) transfer clothing between\navatars in virtual try-on applications. We demonstrate that SCARF reconstructs\nclothing with higher visual quality than existing methods, that the clothing\ndeforms with changing body pose and body shape, and that clothing can be\nsuccessfully transferred between avatars of different subjects. The code and\nmodels are available at https://github.com/YadiraF/SCARF.",
          "arxiv_id": "2210.01868v1"
        }
      ],
      "36": [
        {
          "title": "Spectral DiffuserCam: lensless snapshot hyperspectral imaging with a spectral filter array",
          "year": "2020-06",
          "abstract": "Hyperspectral imaging is useful for applications ranging from medical\ndiagnostics to agricultural crop monitoring; however, traditional scanning\nhyperspectral imagers are prohibitively slow and expensive for widespread\nadoption. Snapshot techniques exist but are often confined to bulky benchtop\nsetups or have low spatio-spectral resolution. In this paper, we propose a\nnovel, compact, and inexpensive computational camera for snapshot hyperspectral\nimaging. Our system consists of a tiled spectral filter array placed directly\non the image sensor and a diffuser placed close to the sensor. Each point in\nthe world maps to a unique pseudorandom pattern on the spectral filter array,\nwhich encodes multiplexed spatio-spectral information. By solving a\nsparsity-constrained inverse problem, we recover the hyperspectral volume with\nsub-super-pixel resolution. Our hyperspectral imaging framework is flexible and\ncan be designed with contiguous or non-contiguous spectral filters that can be\nchosen for a given application. We provide theory for system design,\ndemonstrate a prototype device, and present experimental results with high\nspatio-spectral resolution.",
          "arxiv_id": "2006.08565v2"
        },
        {
          "title": "Cross-Scope Spatial-Spectral Information Aggregation for Hyperspectral Image Super-Resolution",
          "year": "2023-11",
          "abstract": "Hyperspectral image super-resolution has attained widespread prominence to\nenhance the spatial resolution of hyperspectral images. However,\nconvolution-based methods have encountered challenges in harnessing the global\nspatial-spectral information. The prevailing transformer-based methods have not\nadequately captured the long-range dependencies in both spectral and spatial\ndimensions. To alleviate this issue, we propose a novel cross-scope\nspatial-spectral Transformer (CST) to efficiently investigate long-range\nspatial and spectral similarities for single hyperspectral image\nsuper-resolution. Specifically, we devise cross-attention mechanisms in spatial\nand spectral dimensions to comprehensively model the long-range\nspatial-spectral characteristics. By integrating global information into the\nrectangle-window self-attention, we first design a cross-scope spatial\nself-attention to facilitate long-range spatial interactions. Then, by\nleveraging appropriately characteristic spatial-spectral features, we construct\na cross-scope spectral self-attention to effectively capture the intrinsic\ncorrelations among global spectral bands. Finally, we elaborate a concise\nfeed-forward neural network to enhance the feature representation capacity in\nthe Transformer structure. Extensive experiments over three hyperspectral\ndatasets demonstrate that the proposed CST is superior to other\nstate-of-the-art methods both quantitatively and visually. The code is\navailable at \\url{https://github.com/Tomchenshi/CST.git}.",
          "arxiv_id": "2311.17340v1"
        },
        {
          "title": "Spatial-Spectral Manifold Embedding of Hyperspectral Data",
          "year": "2020-07",
          "abstract": "In recent years, hyperspectral imaging, also known as imaging spectroscopy,\nhas been paid an increasing interest in geoscience and remote sensing\ncommunity. Hyperspectral imagery is characterized by very rich spectral\ninformation, which enables us to recognize the materials of interest lying on\nthe surface of the Earth more easier. We have to admit, however, that high\nspectral dimension inevitably brings some drawbacks, such as expensive data\nstorage and transmission, information redundancy, etc. Therefore, to reduce the\nspectral dimensionality effectively and learn more discriminative spectral\nlow-dimensional embedding, in this paper we propose a novel hyperspectral\nembedding approach by simultaneously considering spatial and spectral\ninformation, called spatial-spectral manifold embedding (SSME). Beyond the\npixel-wise spectral embedding approaches, SSME models the spatial and spectral\ninformation jointly in a patch-based fashion. SSME not only learns the spectral\nembedding by using the adjacency matrix obtained by similarity measurement\nbetween spectral signatures, but also models the spatial neighbours of a target\npixel in hyperspectral scene by sharing the same weights (or edges) in the\nprocess of learning embedding. Classification is explored as a potential\nstrategy to quantitatively evaluate the performance of learned embedding\nrepresentations. Classification is explored as a potential application for\nquantitatively evaluating the performance of these hyperspectral embedding\nalgorithms. Extensive experiments conducted on the widely-used hyperspectral\ndatasets demonstrate the superiority and effectiveness of the proposed SSME as\ncompared to several state-of-the-art embedding methods.",
          "arxiv_id": "2007.08767v1"
        }
      ],
      "37": [
        {
          "title": "Continual Learning via Manifold Expansion Replay",
          "year": "2023-10",
          "abstract": "In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.",
          "arxiv_id": "2310.08038v1"
        },
        {
          "title": "Susceptibility of Continual Learning Against Adversarial Attacks",
          "year": "2022-07",
          "abstract": "Recent continual learning approaches have primarily focused on mitigating\ncatastrophic forgetting. Nevertheless, two critical areas have remained\nrelatively unexplored: 1) evaluating the robustness of proposed methods and 2)\nensuring the security of learned tasks. This paper investigates the\nsusceptibility of continually learned tasks, including current and previously\nacquired tasks, to adversarial attacks. Specifically, we have observed that any\nclass belonging to any task can be easily targeted and misclassified as the\ndesired target class of any other task. Such susceptibility or vulnerability of\nlearned tasks to adversarial attacks raises profound concerns regarding data\nintegrity and privacy. To assess the robustness of continual learning\napproaches, we consider continual learning approaches in all three scenarios,\ni.e., task-incremental learning, domain-incremental learning, and\nclass-incremental learning. In this regard, we explore the robustness of three\nregularization-based methods, three replay-based approaches, and one hybrid\ntechnique that combines replay and exemplar approaches. We empirically\ndemonstrated that in any setting of continual learning, any class, whether\nbelonging to the current or previously learned tasks, is susceptible to\nmisclassification. Our observations identify potential limitations of continual\nlearning approaches against adversarial attacks and highlight that current\ncontinual learning algorithms could not be suitable for deployment in\nreal-world settings.",
          "arxiv_id": "2207.05225v5"
        },
        {
          "title": "Probing Representation Forgetting in Supervised and Unsupervised Continual Learning",
          "year": "2022-03",
          "abstract": "Continual Learning research typically focuses on tackling the phenomenon of\ncatastrophic forgetting in neural networks. Catastrophic forgetting is\nassociated with an abrupt loss of knowledge previously learned by a model when\nthe task, or more broadly the data distribution, being trained on changes. In\nsupervised learning problems this forgetting, resulting from a change in the\nmodel's representation, is typically measured or observed by evaluating the\ndecrease in old task performance. However, a model's representation can change\nwithout losing knowledge about prior tasks. In this work we consider the\nconcept of representation forgetting, observed by using the difference in\nperformance of an optimal linear classifier before and after a new task is\nintroduced. Using this tool we revisit a number of standard continual learning\nbenchmarks and observe that, through this lens, model representations trained\nwithout any explicit control for forgetting often experience small\nrepresentation forgetting and can sometimes be comparable to methods which\nexplicitly control for forgetting, especially in longer task sequences. We also\nshow that representation forgetting can lead to new insights on the effect of\nmodel capacity and loss function used in continual learning. Based on our\nresults, we show that a simple yet competitive approach is to learn\nrepresentations continually with standard supervised contrastive learning while\nconstructing prototypes of class samples when queried on old samples.",
          "arxiv_id": "2203.13381v2"
        }
      ],
      "38": [
        {
          "title": "Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation",
          "year": "2023-06",
          "abstract": "In the field of robotics, event-based cameras are emerging as a promising\nlow-power alternative to traditional frame-based cameras for capturing\nhigh-speed motion and high dynamic range scenes. This is due to their sparse\nand asynchronous event outputs. Spiking Neural Networks (SNNs) with their\nasynchronous event-driven compute, show great potential for extracting the\nspatio-temporal features from these event streams. In contrast, the standard\nAnalog Neural Networks (ANNs) fail to process event data effectively. However,\ntraining SNNs is difficult due to additional trainable parameters (thresholds\nand leaks), vanishing spikes at deeper layers, and a non-differentiable binary\nactivation function. Furthermore, an additional data structure, membrane\npotential, responsible for keeping track of temporal information, must be\nfetched and updated at every timestep in SNNs. To overcome these challenges, we\npropose a novel SNN-ANN hybrid architecture that combines the strengths of\nboth. Specifically, we leverage the asynchronous compute capabilities of SNN\nlayers to effectively extract the input temporal information. Concurrently, the\nANN layers facilitate training and efficient hardware deployment on traditional\nmachine learning hardware such as GPUs. We provide extensive experimental\nanalysis for assigning each layer to be spiking or analog, leading to a network\nconfiguration optimized for performance and ease of training. We evaluate our\nhybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle\nStereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid\nSNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE)\nwith 22% lower energy consumption compared to Full-SNN, and 48% lower AEE\ncompared to Full-ANN, while maintaining comparable energy usage.",
          "arxiv_id": "2306.02960v2"
        },
        {
          "title": "SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition",
          "year": "2023-10",
          "abstract": "Event cameras are bio-inspired sensors that respond to local changes in light\nintensity and feature low latency, high energy efficiency, and high dynamic\nrange. Meanwhile, Spiking Neural Networks (SNNs) have gained significant\nattention due to their remarkable efficiency and fault tolerance. By\nsynergistically harnessing the energy efficiency inherent in event cameras and\nthe spike-based processing capabilities of SNNs, their integration could enable\nultra-low-power application scenarios, such as action recognition tasks.\nHowever, existing approaches often entail converting asynchronous events into\nconventional frames, leading to additional data mapping efforts and a loss of\nsparsity, contradicting the design concept of SNNs and event cameras. To\naddress this challenge, we propose SpikePoint, a novel end-to-end point-based\nSNN architecture. SpikePoint excels at processing sparse event cloud data,\neffectively extracting both global and local features through a singular-stage\nstructure. Leveraging the surrogate training method, SpikePoint achieves high\naccuracy with few parameters and maintains low power consumption, specifically\nemploying the identity mapping feature extractor on diverse datasets.\nSpikePoint achieves state-of-the-art (SOTA) performance on four event-based\naction recognition datasets using only 16 timesteps, surpassing other SNN\nmethods. Moreover, it also achieves SOTA performance across all methods on\nthree datasets, utilizing approximately 0.3\\% of the parameters and 0.5\\% of\npower consumption employed by artificial neural networks (ANNs). These results\nemphasize the significance of Point Cloud and pave the way for many\nultra-low-power event-based data processing applications.",
          "arxiv_id": "2310.07189v2"
        },
        {
          "title": "Event-Based Angular Velocity Regression with Spiking Networks",
          "year": "2020-03",
          "abstract": "Spiking Neural Networks (SNNs) are bio-inspired networks that process\ninformation conveyed as temporal spikes rather than numeric values. A spiking\nneuron of an SNN only produces a spike whenever a significant number of spikes\noccur within a short period of time. Due to their spike-based computational\nmodel, SNNs can process output from event-based, asynchronous sensors without\nany pre-processing at extremely lower power unlike standard artificial neural\nnetworks. This is possible due to specialized neuromorphic hardware that\nimplements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have\nnot enjoyed the same rise of popularity as artificial neural networks. This not\nonly stems from the fact that their input format is rather unconventional but\nalso due to the challenges in training spiking networks. Despite their temporal\nnature and recent algorithmic advances, they have been mostly evaluated on\nclassification problems. We propose, for the first time, a temporal regression\nproblem of numerical values given events from an event camera. We specifically\ninvestigate the prediction of the 3-DOF angular velocity of a rotating event\ncamera with an SNN. The difficulty of this problem arises from the prediction\nof angular velocities continuously in time directly from irregular,\nasynchronous event-based input. Directly utilising the output of event cameras\nwithout any pre-processing ensures that we inherit all the benefits that they\nprovide over conventional cameras. That is high-temporal resolution,\nhigh-dynamic range and no motion blur. To assess the performance of SNNs on\nthis task, we introduce a synthetic event camera dataset generated from\nreal-world panoramic images and show that we can successfully train an SNN to\nperform angular velocity regression.",
          "arxiv_id": "2003.02790v1"
        }
      ],
      "39": [
        {
          "title": "AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars",
          "year": "2022-10",
          "abstract": "Although 2D generative models have made great progress in face image\ngeneration and animation, they often suffer from undesirable artifacts such as\n3D inconsistency when rendering images from different camera viewpoints. This\nprevents them from synthesizing video animations indistinguishable from real\nones. Recently, 3D-aware GANs extend 2D GANs for explicit disentanglement of\ncamera pose by leveraging 3D scene representations. These methods can well\npreserve the 3D consistency of the generated images across different views, yet\nthey cannot achieve fine-grained control over other attributes, among which\nfacial expression control is arguably the most useful and desirable for face\nanimation. In this paper, we propose an animatable 3D-aware GAN for multiview\nconsistent face animation generation. The key idea is to decompose the 3D\nrepresentation of the 3D-aware GAN into a template field and a deformation\nfield, where the former represents different identities with a canonical\nexpression, and the latter characterizes expression variations of each\nidentity. To achieve meaningful control over facial expressions via\ndeformation, we propose a 3D-level imitative learning scheme between the\ngenerator and a parametric 3D face model during adversarial training of the\n3D-aware GAN. This helps our method achieve high-quality animatable face image\ngeneration with strong visual 3D consistency, even though trained with only\nunstructured 2D images. Extensive experiments demonstrate our superior\nperformance over prior works. Project page:\nhttps://yuewuhkust.github.io/AniFaceGAN",
          "arxiv_id": "2210.06465v1"
        },
        {
          "title": "Image-to-Video Generation via 3D Facial Dynamics",
          "year": "2021-05",
          "abstract": "We present a versatile model, FaceAnime, for various video generation tasks\nfrom still images. Video generation from a single face image is an interesting\nproblem and usually tackled by utilizing Generative Adversarial Networks (GANs)\nto integrate information from the input face image and a sequence of sparse\nfacial landmarks. However, the generated face images usually suffer from\nquality loss, image distortion, identity change, and expression mismatching due\nto the weak representation capacity of the facial landmarks. In this paper, we\npropose to \"imagine\" a face video from a single face image according to the\nreconstructed 3D face dynamics, aiming to generate a realistic and\nidentity-preserving face video, with precisely predicted pose and facial\nexpression. The 3D dynamics reveal changes of the facial expression and motion,\nand can serve as a strong prior knowledge for guiding highly realistic face\nvideo generation. In particular, we explore face video prediction and exploit a\nwell-designed 3D dynamic prediction network to predict a 3D dynamic sequence\nfor a single face image. The 3D dynamics are then further rendered by the\nsparse texture mapping algorithm to recover structural details and sparse\ntextures for generating face frames. Our model is versatile for various AR/VR\nand entertainment applications, such as face video retargeting and face video\nprediction. Superior experimental results have well demonstrated its\neffectiveness in generating high-fidelity, identity-preserving, and visually\npleasant face video clips from a single source face image.",
          "arxiv_id": "2105.14678v1"
        },
        {
          "title": "Causal Representation Learning for Context-Aware Face Transfer",
          "year": "2021-10",
          "abstract": "Human face synthesis involves transferring knowledge about the identity and\nidentity-dependent face shape (IDFS) of a human face to target face images\nwhere the context (e.g., facial expressions, head poses, and other background\nfactors) may change dramatically. Human faces are non-rigid, so facial\nexpression leads to deformation of face shape, and head pose also affects the\nface observed in 2D images. A key challenge in face transfer is to match the\nface with unobserved new contexts, adapting the face appearance to different\nposes and expressions accordingly. In this work, we find a way to provide prior\nknowledge for generative models to reason about the appropriate appearance of a\nhuman face in response to various expressions and poses. We propose a novel\ncontext-aware face transfer method, called CarTrans, that incorporates causal\neffects of contextual factors into face representation, and thus is able to be\naware of the uncertainty of new contexts. We estimate the effect of facial\nexpression and head pose in terms of counterfactual inference by designing a\ncontrolled intervention trial, thus avoiding the requirement of a large number\nof observations to cover the pose-expression space well. Moreover, we propose a\nkernel regression-based encoder that eliminates the identity specificity of\ntarget faces when encoding contextual information from target images. The\nresulting method shows impressive performance, allowing fine-grained control\nover face shape and appearance under various contextual conditions.",
          "arxiv_id": "2110.01571v4"
        }
      ],
      "40": [
        {
          "title": "Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification",
          "year": "2020-01",
          "abstract": "Modern face recognition systems leverage datasets containing images of\nhundreds of thousands of specific individuals' faces to train deep\nconvolutional neural networks to learn an embedding space that maps an\narbitrary individual's face to a vector representation of their identity. The\nperformance of a face recognition system in face verification (1:1) and face\nidentification (1:N) tasks is directly related to the ability of an embedding\nspace to discriminate between identities. Recently, there has been significant\npublic scrutiny into the source and privacy implications of large-scale face\nrecognition training datasets such as MS-Celeb-1M and MegaFace, as many people\nare uncomfortable with their face being used to train dual-use technologies\nthat can enable mass surveillance. However, the impact of an individual's\ninclusion in training data on a derived system's ability to recognize them has\nnot previously been studied. In this work, we audit ArcFace, a\nstate-of-the-art, open source face recognition system, in a large-scale face\nidentification experiment with more than one million distractor images. We find\na Rank-1 face identification accuracy of 79.71% for individuals present in the\nmodel's training data and an accuracy of 75.73% for those not present. This\nmodest difference in accuracy demonstrates that face recognition systems using\ndeep learning work better for individuals they are trained on, which has\nserious privacy implications when one considers all major open source face\nrecognition training datasets do not obtain informed consent from individuals\nduring their collection.",
          "arxiv_id": "2001.03071v2"
        },
        {
          "title": "The Effect of Wearing a Face Mask on Face Image Quality",
          "year": "2021-10",
          "abstract": "Due to the COVID-19 situation, face masks have become a main part of our\ndaily life. Wearing mouth-and-nose protection has been made a mandate in many\npublic places, to prevent the spread of the COVID-19 virus. However, face masks\naffect the performance of face recognition, since a large area of the face is\ncovered. The effect of wearing a face mask on the different components of the\nface recognition system in a collaborative environment is a problem that is\nstill to be fully studied. This work studies, for the first time, the effect of\nwearing a face mask on face image quality by utilising state-of-the-art face\nimage quality assessment methods of different natures. This aims at providing\nbetter understanding on the effect of face masks on the operation of face\nrecognition as a whole system. In addition, we further studied the effect of\nsimulated masks on face image utility in comparison to real face masks. We\ndiscuss the correlation between the mask effect on face image quality and that\non the face verification performance by automatic systems and human experts,\nindicating a consistent trend between both factors. The evaluation is conducted\non the database containing (1) no-masked faces, (2) real face masks, and (3)\nsimulated face masks, by synthetically generating digital facial masks on\nno-masked faces. Finally, a visual interpretation of the face areas\ncontributing to the quality score of a selected set of quality assessment\nmethods is provided to give a deeper insight into the difference of network\ndecisions in masked and non-masked faces, among other variations.",
          "arxiv_id": "2110.11283v4"
        },
        {
          "title": "Analysis of face detection, face landmarking, and face recognition performance with masked face images",
          "year": "2022-06",
          "abstract": "Face recognition has become an essential task in our lives. However, the\ncurrent COVID-19 pandemic has led to the widespread use of face masks. The\neffect of wearing face masks is currently an understudied issue. The aim of\nthis paper is to analyze face detection, face landmarking, and face recognition\nperformance with masked face images. HOG and CNN face detectors are used for\nface detection in combination with 5-point and 68-point face landmark\npredictors and VGG16 face recognition model is used for face recognition on\nmasked and unmasked images. We found that the performance of face detection,\nface landmarking, and face recognition is negatively impacted by face masks",
          "arxiv_id": "2207.06478v1"
        }
      ],
      "41": [
        {
          "title": "A first step towards automated species recognition from camera trap images of mammals using AI in a European temperate forest",
          "year": "2021-03",
          "abstract": "Camera traps are used worldwide to monitor wildlife. Despite the increasing\navailability of Deep Learning (DL) models, the effective usage of this\ntechnology to support wildlife monitoring is limited. This is mainly due to the\ncomplexity of DL technology and high computing requirements. This paper\npresents the implementation of the light-weight and state-of-the-art YOLOv5\narchitecture for automated labeling of camera trap images of mammals in the\nBialowieza Forest (BF), Poland. The camera trapping data were organized and\nharmonized using TRAPPER software, an open source application for managing\nlarge-scale wildlife monitoring projects. The proposed image recognition\npipeline achieved an average accuracy of 85% F1-score in the identification of\nthe 12 most commonly occurring medium-size and large mammal species in BF using\na limited set of training and testing data (a total 2659 images with animals).\n  Based on the preliminary results, we concluded that the YOLOv5 object\ndetection and classification model is a promising light-weight DL solution\nafter the adoption of transfer learning technique. It can be efficiently\nplugged in via an API into existing web-based camera trapping data processing\nplatforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage\nand classify (manually) camera trapping datasets by many research groups in\nEurope, the implementation of AI-based automated species classification may\nsignificantly speed up the data processing workflow and thus better support\ndata-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers\nperform better performance on edge devices which may open a new chapter in\nanimal population monitoring in real time directly from camera trap devices.",
          "arxiv_id": "2103.11052v1"
        },
        {
          "title": "Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey",
          "year": "2022-06",
          "abstract": "Marine ecosystems and their fish habitats are becoming increasingly important\ndue to their integral role in providing a valuable food source and conservation\noutcomes. Due to their remote and difficult to access nature, marine\nenvironments and fish habitats are often monitored using underwater cameras.\nThese cameras generate a massive volume of digital data, which cannot be\nefficiently analysed by current manual processing methods, which involve a\nhuman observer. DL is a cutting-edge AI technology that has demonstrated\nunprecedented performance in analysing visual data. Despite its application to\na myriad of domains, its use in underwater fish habitat monitoring remains\nunder explored. In this paper, we provide a tutorial that covers the key\nconcepts of DL, which help the reader grasp a high-level understanding of how\nDL works. The tutorial also explains a step-by-step procedure on how DL\nalgorithms should be developed for challenging applications such as underwater\nfish monitoring. In addition, we provide a comprehensive survey of key deep\nlearning techniques for fish habitat monitoring including classification,\ncounting, localization, and segmentation. Furthermore, we survey publicly\navailable underwater fish datasets, and compare various DL techniques in the\nunderwater fish monitoring domains. We also discuss some challenges and\nopportunities in the emerging field of deep learning for fish habitat\nprocessing. This paper is written to serve as a tutorial for marine scientists\nwho would like to grasp a high-level understanding of DL, develop it for their\napplications by following our step-by-step tutorial, and see how it is evolving\nto facilitate their research efforts. At the same time, it is suitable for\ncomputer scientists who would like to survey state-of-the-art DL-based\nmethodologies for fish habitat monitoring.",
          "arxiv_id": "2206.05394v1"
        },
        {
          "title": "Is Underwater Image Enhancement All Object Detectors Need?",
          "year": "2023-11",
          "abstract": "Underwater object detection is a crucial and challenging problem in marine\nengineering and aquatic robot. The difficulty is partly because of the\ndegradation of underwater images caused by light selective absorption and\nscattering. Intuitively, enhancing underwater images can benefit high-level\napplications like underwater object detection. However, it is still unclear\nwhether all object detectors need underwater image enhancement as\npre-processing. We therefore pose the questions \"Does underwater image\nenhancement really improve underwater object detection?\" and \"How does\nunderwater image enhancement contribute to underwater object detection?\". With\nthese two questions, we conduct extensive studies. Specifically, we use 18\nstate-of-the-art underwater image enhancement algorithms, covering traditional,\nCNN-based, and GAN-based algorithms, to pre-process underwater object detection\ndata. Then, we retrain 7 popular deep learning-based object detectors using the\ncorresponding results enhanced by different algorithms, obtaining 126\nunderwater object detection models. Coupled with 7 object detection models\nretrained using raw underwater images, we employ these 133 models to\ncomprehensively analyze the effect of underwater image enhancement on\nunderwater object detection. We expect this study can provide sufficient\nexploration to answer the aforementioned questions and draw more attention of\nthe community to the joint problem of underwater image enhancement and\nunderwater object detection. The pre-trained models and results are publicly\navailable and will be regularly updated. Project page:\nhttps://github.com/BIGWangYuDong/lqit/tree/main/configs/detection/uw_enhancement_affect_detection.",
          "arxiv_id": "2311.18814v1"
        }
      ],
      "42": [
        {
          "title": "Category-Level and Open-Set Object Pose Estimation for Robotics",
          "year": "2025-04",
          "abstract": "Object pose estimation enables a variety of tasks in computer vision and\nrobotics, including scene understanding and robotic grasping. The complexity of\na pose estimation task depends on the unknown variables related to the target\nobject. While instance-level methods already excel for opaque and Lambertian\nobjects, category-level and open-set methods, where texture, shape, and size\nare partially or entirely unknown, still struggle with these basic material\nproperties. Since texture is unknown in these scenarios, it cannot be used for\ndisambiguating object symmetries, another core challenge of 6D object pose\nestimation. The complexity of estimating 6D poses with such a manifold of\nunknowns led to various datasets, accuracy metrics, and algorithmic solutions.\nThis paper compares datasets, accuracy metrics, and algorithms for solving 6D\npose estimation on the category-level. Based on this comparison, we analyze how\nto bridge category-level and open-set object pose estimation to reach\ngeneralization and provide actionable recommendations.",
          "arxiv_id": "2504.19572v1"
        },
        {
          "title": "StereoPose: Category-Level 6D Transparent Object Pose Estimation from Stereo Images via Back-View NOCS",
          "year": "2022-11",
          "abstract": "Most existing methods for category-level pose estimation rely on object point\nclouds. However, when considering transparent objects, depth cameras are\nusually not able to capture meaningful data, resulting in point clouds with\nsevere artifacts. Without a high-quality point cloud, existing methods are not\napplicable to challenging transparent objects. To tackle this problem, we\npresent StereoPose, a novel stereo image framework for category-level object\npose estimation, ideally suited for transparent objects. For a robust\nestimation from pure stereo images, we develop a pipeline that decouples\ncategory-level pose estimation into object size estimation, initial pose\nestimation, and pose refinement. StereoPose then estimates object pose based on\nrepresentation in the normalized object coordinate space~(NOCS). To address the\nissue of image content aliasing, we further define a back-view NOCS map for the\ntransparent object. The back-view NOCS aims to reduce the network learning\nambiguity caused by content aliasing, and leverage informative cues on the back\nof the transparent object for more accurate pose estimation. To further improve\nthe performance of the stereo framework, StereoPose is equipped with a parallax\nattention module for stereo feature fusion and an epipolar loss for improving\nthe stereo-view consistency of network predictions. Extensive experiments on\nthe public TOD dataset demonstrate the superiority of the proposed StereoPose\nframework for category-level 6D transparent object pose estimation.",
          "arxiv_id": "2211.01644v1"
        },
        {
          "title": "Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset",
          "year": "2022-06",
          "abstract": "6D object pose estimation is one of the fundamental problems in computer\nvision and robotics research. While a lot of recent efforts have been made on\ngeneralizing pose estimation to novel object instances within the same\ncategory, namely category-level 6D pose estimation, it is still restricted in\nconstrained environments given the limited number of annotated data. In this\npaper, we collect Wild6D, a new unlabeled RGBD object video dataset with\ndiverse instances and backgrounds. We utilize this data to generalize\ncategory-level 6D object pose estimation in the wild with semi-supervised\nlearning. We propose a new model, called Rendering for Pose estimation network\nRePoNet, that is jointly trained using the free ground-truths with the\nsynthetic data, and a silhouette matching objective function on the real-world\ndata. Without using any 3D annotations on real data, our method outperforms\nstate-of-the-art methods on the previous dataset and our Wild6D test set (with\nmanual annotations for evaluation) by a large margin. Project page with Wild6D\ndata: https://oasisyang.github.io/semi-pose .",
          "arxiv_id": "2206.15436v1"
        }
      ],
      "43": [
        {
          "title": "A Compendium of Autonomous Navigation using Object Detection and Tracking in Unmanned Aerial Vehicles",
          "year": "2025-05",
          "abstract": "Unmanned Aerial Vehicles (UAVs) are one of the most revolutionary inventions\nof 21st century. At the core of a UAV lies the central processing system that\nuses wireless signals to control their movement. The most popular UAVs are\nquadcopters that use a set of four motors, arranged as two on either side with\nopposite spin. An autonomous UAV is called a drone. Drones have been in service\nin the US army since the 90's for covert missions critical to national\nsecurity. It would not be wrong to claim that drones make up an integral part\nof the national security and provide the most valuable service during\nsurveillance operations. While UAVs are controlled using wireless signals,\nthere reside some challenges that disrupt the operation of such vehicles such\nas signal quality and range, real time processing, human expertise, robust\nhardware and data security. These challenges can be solved by programming UAVs\nto be autonomous, using object detection and tracking, through Computer Vision\nalgorithms. Computer Vision is an interdisciplinary field that seeks the use of\ndeep learning to gain a high-level understanding of digital images and videos\nfor the purpose of automating the task of human visual system. Using computer\nvision, algorithms for detecting and tracking various objects can be developed\nsuitable to the hardware so as to allow real time processing for immediate\njudgement. This paper attempts to review the various approaches several authors\nhave proposed for the purpose of autonomous navigation of UAVs by through\nvarious algorithms of object detection and tracking in real time, for the\npurpose of applications in various fields such as disaster management, dense\narea exploration, traffic vehicle surveillance etc.",
          "arxiv_id": "2506.05378v1"
        },
        {
          "title": "Evidential Detection and Tracking Collaboration: New Problem, Benchmark and Algorithm for Robust Anti-UAV System",
          "year": "2023-06",
          "abstract": "Unmanned Aerial Vehicles (UAVs) have been widely used in many areas,\nincluding transportation, surveillance, and military. However, their potential\nfor safety and privacy violations is an increasing issue and highly limits\ntheir broader applications, underscoring the critical importance of UAV\nperception and defense (anti-UAV). Still, previous works have simplified such\nan anti-UAV task as a tracking problem, where the prior information of UAVs is\nalways provided; such a scheme fails in real-world anti-UAV tasks (i.e. complex\nscenes, indeterminate-appear and -reappear UAVs, and real-time UAV\nsurveillance). In this paper, we first formulate a new and practical anti-UAV\nproblem featuring the UAVs perception in complex scenes without prior UAVs\ninformation. To benchmark such a challenging task, we propose the largest UAV\ndataset dubbed AntiUAV600 and a new evaluation metric. The AntiUAV600 comprises\n600 video sequences of challenging scenes with random, fast, and small-scale\nUAVs, with over 723K thermal infrared frames densely annotated with bounding\nboxes. Finally, we develop a novel anti-UAV approach via an evidential\ncollaboration of global UAVs detection and local UAVs tracking, which\neffectively tackles the proposed problem and can serve as a strong baseline for\nfuture research. Extensive experiments show our method outperforms SOTA\napproaches and validate the ability of AntiUAV600 to enhance UAV perception\nperformance due to its large scale and complexity. Our dataset, pretrained\nmodels, and source codes will be released publically.",
          "arxiv_id": "2306.15767v2"
        },
        {
          "title": "Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle Imagery: Review and Experimental Comparisons",
          "year": "2023-11",
          "abstract": "With the advancement of maritime unmanned aerial vehicles (UAVs) and deep\nlearning technologies, the application of UAV-based object detection has become\nincreasingly significant in the fields of maritime industry and ocean\nengineering. Endowed with intelligent sensing capabilities, the maritime UAVs\nenable effective and efficient maritime surveillance. To further promote the\ndevelopment of maritime UAV-based object detection, this paper provides a\ncomprehensive review of challenges, relative methods, and UAV aerial datasets.\nSpecifically, in this work, we first briefly summarize four challenges for\nobject detection on maritime UAVs, i.e., object feature diversity, device\nlimitation, maritime environment variability, and dataset scarcity. We then\nfocus on computational methods to improve maritime UAV-based object detection\nperformance in terms of scale-aware, small object detection, view-aware,\nrotated object detection, lightweight methods, and others. Next, we review the\nUAV aerial image/video datasets and propose a maritime UAV aerial dataset named\nMS2ship for ship detection. Furthermore, we conduct a series of experiments to\npresent the performance evaluation and robustness analysis of object detection\nmethods on maritime datasets. Eventually, we give the discussion and outlook on\nfuture works for maritime UAV-based object detection. The MS2ship dataset is\navailable at\n\\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.",
          "arxiv_id": "2311.07955v2"
        }
      ],
      "44": [
        {
          "title": "Face Reconstruction from Face Embeddings using Adapter to a Face Foundation Model",
          "year": "2024-11",
          "abstract": "Face recognition systems extract embedding vectors from face images and use\nthese embeddings to verify or identify individuals. Face reconstruction attack\n(also known as template inversion) refers to reconstructing face images from\nface embeddings and using the reconstructed face image to enter a face\nrecognition system. In this paper, we propose to use a face foundation model to\nreconstruct face images from the embeddings of a blackbox face recognition\nmodel. The foundation model is trained with 42M images to generate face images\nfrom the facial embeddings of a fixed face recognition model. We propose to use\nan adapter to translate target embeddings into the embedding space of the\nfoundation model. The generated images are evaluated on different face\nrecognition models and different datasets, demonstrating the effectiveness of\nour method to translate embeddings of different face recognition models. We\nalso evaluate the transferability of reconstructed face images when attacking\ndifferent face recognition models. Our experimental results show that our\nreconstructed face images outperform previous reconstruction attacks against\nface recognition models.",
          "arxiv_id": "2411.03960v1"
        },
        {
          "title": "3D Face Anti-spoofing with Factorized Bilinear Coding",
          "year": "2020-05",
          "abstract": "We have witnessed rapid advances in both face presentation attack models and\npresentation attack detection (PAD) in recent years. When compared with widely\nstudied 2D face presentation attacks, 3D face spoofing attacks are more\nchallenging because face recognition systems are more easily confused by the 3D\ncharacteristics of materials similar to real faces. In this work, we tackle the\nproblem of detecting these realistic 3D face presentation attacks, and propose\na novel anti-spoofing method from the perspective of fine-grained\nclassification. Our method, based on factorized bilinear coding of multiple\ncolor channels (namely MC\\_FBC), targets at learning subtle fine-grained\ndifferences between real and fake images. By extracting discriminative and\nfusing complementary information from RGB and YCbCr spaces, we have developed a\nprincipled solution to 3D face spoofing detection. A large-scale wax figure\nface database (WFFD) with both images and videos has also been collected as\nsuper-realistic attacks to facilitate the study of 3D face presentation attack\ndetection. Extensive experimental results show that our proposed method\nachieves the state-of-the-art performance on both our own WFFD and other face\nspoofing databases under various intra-database and inter-database testing\nscenarios.",
          "arxiv_id": "2005.06514v3"
        },
        {
          "title": "RSTAM: An Effective Black-Box Impersonation Attack on Face Recognition using a Mobile and Compact Printer",
          "year": "2022-06",
          "abstract": "Face recognition has achieved considerable progress in recent years thanks to\nthe development of deep neural networks, but it has recently been discovered\nthat deep neural networks are vulnerable to adversarial examples. This means\nthat face recognition models or systems based on deep neural networks are also\nsusceptible to adversarial examples. However, the existing methods of attacking\nface recognition models or systems with adversarial examples can effectively\ncomplete white-box attacks but not black-box impersonation attacks, physical\nattacks, or convenient attacks, particularly on commercial face recognition\nsystems. In this paper, we propose a new method to attack face recognition\nmodels or systems called RSTAM, which enables an effective black-box\nimpersonation attack using an adversarial mask printed by a mobile and compact\nprinter. First, RSTAM enhances the transferability of the adversarial masks\nthrough our proposed random similarity transformation strategy. Furthermore, we\npropose a random meta-optimization strategy for ensembling several pre-trained\nface models to generate more general adversarial masks. Finally, we conduct\nexperiments on the CelebA-HQ, LFW, Makeup Transfer (MT), and CASIA-FaceV5\ndatasets. The performance of the attacks is also evaluated on state-of-the-art\ncommercial face recognition systems: Face++, Baidu, Aliyun, Tencent, and\nMicrosoft. Extensive experiments show that RSTAM can effectively perform\nblack-box impersonation attacks on face recognition models or systems.",
          "arxiv_id": "2206.12590v1"
        }
      ],
      "45": [
        {
          "title": "Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues",
          "year": "2024-02",
          "abstract": "How to effectively interact audio with vision has garnered considerable\ninterest within the multi-modality research field. Recently, a novel\naudio-visual segmentation (AVS) task has been proposed, aiming to segment the\nsounding objects in video frames under the guidance of audio cues. However,\nmost existing AVS methods are hindered by a modality imbalance where the visual\nfeatures tend to dominate those of the audio modality, due to a unidirectional\nand insufficient integration of audio cues. This imbalance skews the feature\nrepresentation towards the visual aspect, impeding the learning of joint\naudio-visual representations and potentially causing segmentation inaccuracies.\nTo address this issue, we propose AVSAC. Our approach features a Bidirectional\nAudio-Visual Decoder (BAVD) with integrated bidirectional bridges, enhancing\naudio cues and fostering continuous interplay between audio and visual\nmodalities. This bidirectional interaction narrows the modality imbalance,\nfacilitating more effective learning of integrated audio-visual\nrepresentations. Additionally, we present a strategy for audio-visual\nframe-wise synchrony as fine-grained guidance of BAVD. This strategy enhances\nthe share of auditory components in visual features, contributing to a more\nbalanced audio-visual representation learning. Extensive experiments show that\nour method attains new benchmarks in AVS performance.",
          "arxiv_id": "2402.02327v2"
        },
        {
          "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
          "year": "2024-09",
          "abstract": "Video encompasses both visual and auditory data, creating a perceptually rich\nexperience where these two modalities complement each other. As such, videos\nare a valuable type of media for the investigation of the interplay between\naudio and visual elements. Previous studies of audio-visual modalities\nprimarily focused on either audio-visual representation learning or generative\nmodeling of a modality conditioned on the other, creating a disconnect between\nthese two branches. A unified framework that learns representation and\ngenerates modalities has not been developed yet. In this work, we introduce a\nnovel framework called Vision to Audio and Beyond (VAB) to bridge the gap\nbetween audio-visual representation learning and vision-to-audio generation.\nThe key approach of VAB is that rather than working with raw video frames and\naudio data, VAB performs representation learning and generative modeling within\nlatent spaces. In particular, VAB uses a pre-trained audio tokenizer and an\nimage encoder to obtain audio tokens and visual features, respectively. It then\nperforms the pre-training task of visual-conditioned masked audio token\nprediction. This training strategy enables the model to engage in contextual\nlearning and simultaneous video-to-audio generation. After the pre-training\nphase, VAB employs the iterative-decoding approach to rapidly generate audio\ntokens conditioned on visual features. Since VAB is a unified model, its\nbackbone can be fine-tuned for various audio-visual downstream tasks. Our\nexperiments showcase the efficiency of VAB in producing high-quality audio from\nvideo, and its capability to acquire semantic audio-visual features, leading to\ncompetitive results in audio-visual retrieval and classification.",
          "arxiv_id": "2409.19132v1"
        },
        {
          "title": "BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge",
          "year": "2023-08",
          "abstract": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate\nsounding sources by predicting pixel-wise maps. Previous methods assume that\neach sound component in an audio signal always has a visual counterpart in the\nimage. However, this assumption overlooks that off-screen sounds and background\nnoise often contaminate the audio recordings in real-world scenarios. They\nimpose significant challenges on building a consistent semantic mapping between\naudio and visual signals for AVS models and thus impede precise sound\nlocalization. In this work, we propose a two-stage bootstrapping audio-visual\nsegmentation framework by incorporating multi-modal foundation knowledge. In a\nnutshell, our BAVS is designed to eliminate the interference of background\nnoise or off-screen sounds in segmentation by establishing the audio-visual\ncorrespondences in an explicit manner. In the first stage, we employ a\nsegmentation model to localize potential sounding objects from visual data\nwithout being affected by contaminated audio signals. Meanwhile, we also\nutilize a foundation audio classification model to discern audio semantics.\nConsidering the audio tags provided by the audio foundation model are noisy,\nassociating object masks with audio tags is not trivial. Thus, in the second\nstage, we develop an audio-visual semantic integration strategy (AVIS) to\nlocalize the authentic-sounding objects. Here, we construct an audio-visual\ntree based on the hierarchical correspondence between sounds and object\ncategories. We then examine the label concurrency between the localized objects\nand classified audio tags by tracing the audio-visual tree. With AVIS, we can\neffectively segment real-sounding objects. Extensive experiments demonstrate\nthe superiority of our method on AVS datasets, particularly in scenarios\ninvolving background noise. Our project website is\nhttps://yenanliu.github.io/AVSS.github.io/.",
          "arxiv_id": "2308.10175v1"
        }
      ],
      "46": [
        {
          "title": "UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking",
          "year": "2020-01",
          "abstract": "We address Unsupervised Video Object Segmentation (UVOS), the task of\nautomatically generating accurate pixel masks for salient objects in a video\nsequence and of tracking these objects consistently through time, without any\ninput about which objects should be tracked. Towards solving this task, we\npresent UnOVOST (Unsupervised Offline Video Object Segmentation and Tracking)\nas a simple and generic algorithm which is able to track and segment a large\nvariety of objects. This algorithm builds up tracks in a number stages, first\ngrouping segments into short tracklets that are spatio-temporally consistent,\nbefore merging these tracklets into long-term consistent object tracks based on\ntheir visual similarity. In order to achieve this we introduce a novel\ntracklet-based Forest Path Cutting data association algorithm which builds up a\ndecision forest of track hypotheses before cutting this forest into paths that\nform long-term consistent object tracks. When evaluating our approach on the\nDAVIS 2017 Unsupervised dataset we obtain state-of-the-art performance with a\nmean J &F score of 67.9% on the val, 58% on the test-dev and 56.4% on the\ntest-challenge benchmarks, obtaining first place in the DAVIS 2019 Unsupervised\nVideo Object Segmentation Challenge. UnOVOST even performs competitively with\nmany semi-supervised video object segmentation algorithms even though it is not\ngiven any input as to which objects should be tracked and segmented.",
          "arxiv_id": "2001.05425v1"
        },
        {
          "title": "Efficient Track Anything",
          "year": "2024-11",
          "abstract": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video\nobject segmentation and tracking anything. Key components of SAM 2 that drive\nthe impressive video object segmentation performance include a large multistage\nimage encoder for frame feature extraction and a memory mechanism that stores\nmemory contexts from past frames to help current frame segmentation. The high\ncomputation complexity of multistage image encoder and memory module has\nlimited its applications in real-world tasks, e.g., video object segmentation\non mobile devices. To address this limitation, we propose EfficientTAMs,\nlightweight track anything models that produce high-quality results with low\nlatency and model size. Our idea is based on revisiting the plain,\nnonhierarchical Vision Transformer (ViT) as an image encoder for video object\nsegmentation, and introducing an efficient memory module, which reduces the\ncomplexity for both frame feature extraction and memory computation for current\nframe segmentation. We take vanilla lightweight ViTs and efficient memory\nmodule to build EfficientTAMs, and train the models on SA-1B and SA-V datasets\nfor video object segmentation and track anything tasks. We evaluate on multiple\nvideo segmentation benchmarks including semi-supervised VOS and promptable\nvideo segmentation, and find that our proposed EfficientTAM with vanilla ViT\nperform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and\n~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs\nalso perform favorably over original SAM with ~20x speedup on A100 and ~20x\nparameter reduction. On mobile devices such as iPhone 15 Pro Max, our\nEfficientTAMs can run at ~10 FPS for performing video object segmentation with\nreasonable quality, highlighting the capability of small models for on-device\nvideo object segmentation applications.",
          "arxiv_id": "2411.18933v1"
        },
        {
          "title": "The Second Place Solution for The 4th Large-scale Video Object Segmentation Challenge--Track 3: Referring Video Object Segmentation",
          "year": "2022-06",
          "abstract": "The referring video object segmentation task (RVOS) aims to segment object\ninstances in a given video referred by a language expression in all video\nframes. Due to the requirement of understanding cross-modal semantics within\nindividual instances, this task is more challenging than the traditional\nsemi-supervised video object segmentation where the ground truth object masks\nin the first frame are given. With the great achievement of Transformer in\nobject detection and object segmentation, RVOS has been made remarkable\nprogress where ReferFormer achieved the state-of-the-art performance. In this\nwork, based on the strong baseline framework--ReferFormer, we propose several\ntricks to boost further, including cyclical learning rates, semi-supervised\napproach, and test-time augmentation inference. The improved ReferFormer ranks\n2nd place on CVPR2022 Referring Youtube-VOS Challenge.",
          "arxiv_id": "2206.12035v1"
        }
      ],
      "47": [
        {
          "title": "MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model",
          "year": "2022-08",
          "abstract": "Human motion modeling is important for many modern graphics applications,\nwhich typically require professional skills. In order to remove the skill\nbarriers for laymen, recent motion generation methods can directly generate\nhuman motions conditioned on natural languages. However, it remains challenging\nto achieve diverse and fine-grained motion generation with various text inputs.\nTo address this problem, we propose MotionDiffuse, the first diffusion\nmodel-based text-driven motion generation framework, which demonstrates several\ndesired properties over existing methods. 1) Probabilistic Mapping. Instead of\na deterministic language-motion mapping, MotionDiffuse generates motions\nthrough a series of denoising steps in which variations are injected. 2)\nRealistic Synthesis. MotionDiffuse excels at modeling complicated data\ndistribution and generating vivid motion sequences. 3) Multi-Level\nManipulation. MotionDiffuse responds to fine-grained instructions on body\nparts, and arbitrary-length motion synthesis with time-varied text prompts. Our\nexperiments show MotionDiffuse outperforms existing SoTA methods by convincing\nmargins on text-driven motion generation and action-conditioned motion\ngeneration. A qualitative analysis further demonstrates MotionDiffuse's\ncontrollability for comprehensive motion generation. Homepage:\nhttps://mingyuan-zhang.github.io/projects/MotionDiffuse.html",
          "arxiv_id": "2208.15001v1"
        },
        {
          "title": "SMooGPT: Stylized Motion Generation using Large Language Models",
          "year": "2025-09",
          "abstract": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.",
          "arxiv_id": "2509.04058v1"
        },
        {
          "title": "MotionGPT: Human Motion as a Foreign Language",
          "year": "2023-06",
          "abstract": "Though the advancement of pre-trained large language models unfolds, the\nexploration of building a unified model for language and other multi-modal\ndata, such as motion, remains challenging and untouched so far. Fortunately,\nhuman motion displays a semantic coupling akin to human language, often\nperceived as a form of body language. By fusing language data with large-scale\nmotion models, motion-language pre-training that can enhance the performance of\nmotion-related tasks becomes feasible. Driven by this insight, we propose\nMotionGPT, a unified, versatile, and user-friendly motion-language model to\nhandle multiple motion-relevant tasks. Specifically, we employ the discrete\nvector quantization for human motion and transfer 3D motion into motion tokens,\nsimilar to the generation process of word tokens. Building upon this \"motion\nvocabulary\", we perform language modeling on both motion and text in a unified\nmanner, treating human motion as a specific language. Moreover, inspired by\nprompt learning, we pre-train MotionGPT with a mixture of motion-language data\nand fine-tune it on prompt-based question-and-answer tasks. Extensive\nexperiments demonstrate that MotionGPT achieves state-of-the-art performances\non multiple motion tasks including text-driven motion generation, motion\ncaptioning, motion prediction, and motion in-between.",
          "arxiv_id": "2306.14795v2"
        }
      ],
      "48": [
        {
          "title": "Latent Partition Implicit with Surface Codes for 3D Representation",
          "year": "2022-07",
          "abstract": "Deep implicit functions have shown remarkable shape modeling ability in\nvarious 3D computer vision tasks. One drawback is that it is hard for them to\nrepresent a 3D shape as multiple parts. Current solutions learn various\nprimitives and blend the primitives directly in the spatial space, which still\nstruggle to approximate the 3D shape accurately. To resolve this problem, we\nintroduce a novel implicit representation to represent a single 3D shape as a\nset of parts in the latent space, towards both highly accurate and plausibly\ninterpretable shape modeling. Our insight here is that both the part learning\nand the part blending can be conducted much easier in the latent space than in\nthe spatial space. We name our method Latent Partition Implicit (LPI), because\nof its ability of casting the global shape modeling into multiple local part\nmodeling, which partitions the global shape unity. LPI represents a shape as\nSigned Distance Functions (SDFs) using surface codes. Each surface code is a\nlatent code representing a part whose center is on the surface, which enables\nus to flexibly employ intrinsic attributes of shapes or additional surface\nproperties. Eventually, LPI can reconstruct both the shape and the parts on the\nshape, both of which are plausible meshes. LPI is a multi-level representation,\nwhich can partition a shape into different numbers of parts after training. LPI\ncan be learned without ground truth signed distances, point normals or any\nsupervision for part partition. LPI outperforms the latest methods under the\nwidely used benchmarks in terms of reconstruction accuracy and modeling\ninterpretability. Our code, data and models are available at\nhttps://github.com/chenchao15/LPI.",
          "arxiv_id": "2207.08631v3"
        },
        {
          "title": "Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape Modeling and Reconstruction from Raw Point Clouds",
          "year": "2020-12",
          "abstract": "Shape modeling and reconstruction from raw point clouds of objects stand as a\nfundamental challenge in vision and graphics research. Classical methods\nconsider analytic shape priors; however, their performance degraded when the\nscanned points deviate from the ideal conditions of cleanness and completeness.\nImportant progress has been recently made by data-driven approaches, which\nlearn global and/or local models of implicit surface representations from\nauxiliary sets of training shapes. Motivated from a universal phenomenon that\nself-similar shape patterns of local surface patches repeat across the entire\nsurface of an object, we aim to push forward the data-driven strategies and\npropose to learn a local implicit surface network for a shared, adaptive\nmodeling of the entire surface for a direct surface reconstruction from raw\npoint cloud; we also enhance the leveraging of surface self-similarities by\nimproving correlations among the optimized latent codes of individual surface\npatches. Given that orientations of raw points could be unavailable or noisy,\nwe extend sign agnostic learning into our local implicit model, which enables\nour recovery of signed implicit fields of local surfaces from the unsigned\ninputs. We term our framework as Sign-Agnostic Implicit Learning of Surface\nSelf-Similarities (SAIL-S3). With a global post-optimization of local sign\nflipping, SAIL-S3 is able to directly model raw, un-oriented point clouds and\nreconstruct high-quality object surfaces. Experiments show its superiority over\nexisting methods.",
          "arxiv_id": "2012.07498v3"
        },
        {
          "title": "LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions",
          "year": "2024-02",
          "abstract": "Implicit 3D surface reconstruction of an object from its partial and noisy 3D\npoint cloud scan is the classical geometry processing and 3D computer vision\nproblem. In the literature, various 3D shape representations have been\ndeveloped, differing in memory efficiency and shape retrieval effectiveness,\nsuch as volumetric, parametric, and implicit surfaces. Radial basis functions\nprovide memory-efficient parameterization of the implicit surface. However, we\nshow that training a neural network using the mean squared error between the\nground-truth implicit surface and the linear basis-based implicit surfaces does\nnot converge to the global solution. In this work, we propose locally supported\ncompact radial basis functions for a linear representation of the implicit\nsurface. This representation enables us to generate 3D shapes with arbitrary\ntopologies at any resolution due to their continuous nature. We then propose a\nneural network architecture for learning the linear implicit shape\nrepresentation of the 3D surface of an object. We learn linear implicit shapes\nwithin a supervised learning framework using ground truth Signed-Distance Field\n(SDF) data for guidance. The classical strategies face difficulties in finding\nlinear implicit shapes from a given 3D point cloud due to numerical issues\n(requires solving inverse of a large matrix) in basis and query point\nselection. The proposed approach achieves better Chamfer distance and\ncomparable F-score than the state-of-the-art approach on the benchmark dataset.\nWe also show the effectiveness of the proposed approach by using it for the 3D\nshape completion task.",
          "arxiv_id": "2402.07301v1"
        }
      ],
      "49": [
        {
          "title": "Audio-Driven Talking Face Generation with Diverse yet Realistic Facial Animations",
          "year": "2023-04",
          "abstract": "Audio-driven talking face generation, which aims to synthesize talking faces\nwith realistic facial animations (including accurate lip movements, vivid\nfacial expression details and natural head poses) corresponding to the audio,\nhas achieved rapid progress in recent years. However, most existing work\nfocuses on generating lip movements only without handling the closely\ncorrelated facial expressions, which degrades the realism of the generated\nfaces greatly. This paper presents DIRFA, a novel method that can generate\ntalking faces with diverse yet realistic facial animations from the same\ndriving audio. To accommodate fair variation of plausible facial animations for\nthe same audio, we design a transformer-based probabilistic mapping network\nthat can model the variational facial animation distribution conditioned upon\nthe input audio and autoregressively convert the audio signals into a facial\nanimation sequence. In addition, we introduce a temporally-biased mask into the\nmapping network, which allows to model the temporal dependency of facial\nanimations and produce temporally smooth facial animation sequence. With the\ngenerated facial animation sequence and a source image, photo-realistic talking\nfaces can be synthesized with a generic generation network. Extensive\nexperiments show that DIRFA can generate talking faces with realistic facial\nanimations effectively.",
          "arxiv_id": "2304.08945v1"
        },
        {
          "title": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis",
          "year": "2025-04",
          "abstract": "In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations. The code and models will be\navailable at https://thunder.is.tue.mpg.de/",
          "arxiv_id": "2504.13386v3"
        },
        {
          "title": "StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation",
          "year": "2022-08",
          "abstract": "We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.",
          "arxiv_id": "2208.10922v2"
        }
      ],
      "50": [
        {
          "title": "Trainable Activation Function in Image Classification",
          "year": "2020-04",
          "abstract": "In the current research of neural networks, the activation function is\nmanually specified by human and not able to change themselves during training.\nThis paper focus on how to make the activation function trainable for deep\nneural networks. We use series and linear combination of different activation\nfunctions make activation functions continuously variable. Also, we test the\nperformance of CNNs with Fourier series simulated activation(Fourier-CNN) and\nCNNs with linear combined activation function (LC-CNN) on Cifar-10 dataset. The\nresult shows our trainable activation function reveals better performance than\nthe most used ReLU activation function. Finally, we improves the performance of\nFourier-CNN with Autoencoder, and test the performance of PSO algorithm in\noptimizing the parameters of networks",
          "arxiv_id": "2004.13271v2"
        },
        {
          "title": "Deeper Learning with CoLU Activation",
          "year": "2021-12",
          "abstract": "In neural networks, non-linearity is introduced by activation functions. One\ncommonly used activation function is Rectified Linear Unit (ReLU). ReLU has\nbeen a popular choice as an activation but has flaws. State-of-the-art\nfunctions like Swish and Mish are now gaining attention as a better choice as\nthey combat many flaws presented by other activation functions. CoLU is an\nactivation function similar to Swish and Mish in properties. It is defined as\nf(x)=x/(1-xe^-(x+e^x)). It is smooth, continuously differentiable, unbounded\nabove, bounded below, non-saturating, and non-monotonic. Based on experiments\ndone with CoLU with different activation functions, it is observed that CoLU\nusually performs better than other functions on deeper neural networks. While\ntraining different neural networks on MNIST on an incrementally increasing\nnumber of convolutional layers, CoLU retained the highest accuracy for more\nlayers. On a smaller network with 8 convolutional layers, CoLU had the highest\nmean accuracy, closely followed by ReLU. On VGG-13 trained on Fashion-MNIST,\nCoLU had a 4.20% higher accuracy than Mish and 3.31% higher accuracy than ReLU.\nOn ResNet-9 trained on Cifar-10, CoLU had 0.05% higher accuracy than Swish,\n0.09% higher accuracy than Mish, and 0.29% higher accuracy than ReLU. It is\nobserved that activation functions may behave better than other activation\nfunctions based on different factors including the number of layers, types of\nlayers, number of parameters, learning rate, optimizer, etc. Further research\ncan be done on these factors and activation functions for more optimal\nactivation functions and more knowledge on their behavior.",
          "arxiv_id": "2112.12078v1"
        },
        {
          "title": "Growing Cosine Unit: A Novel Oscillatory Activation Function That Can Speedup Training and Reduce Parameters in Convolutional Neural Networks",
          "year": "2021-08",
          "abstract": "Convolutional neural networks have been successful in solving many socially\nimportant and economically significant problems. This ability to learn complex\nhigh-dimensional functions hierarchically can be attributed to the use of\nnonlinear activation functions. A key discovery that made training deep\nnetworks feasible was the adoption of the Rectified Linear Unit (ReLU)\nactivation function to alleviate the vanishing gradient problem caused by using\nsaturating activation functions. Since then, many improved variants of the ReLU\nactivation have been proposed. However, a majority of activation functions used\ntoday are non-oscillatory and monotonically increasing due to their biological\nplausibility. This paper demonstrates that oscillatory activation functions can\nimprove gradient flow and reduce network size. Two theorems on limits of\nnon-oscillatory activation functions are presented. A new oscillatory\nactivation function called Growing Cosine Unit(GCU) defined as $C(z) = z\\cos z$\nthat outperforms Sigmoids, Swish, Mish and ReLU on a variety of architectures\nand benchmarks is presented. The GCU activation has multiple zeros enabling\nsingle GCU neurons to have multiple hyperplanes in the decision boundary. This\nallows single GCU neurons to learn the XOR function without feature\nengineering. Experimental results indicate that replacing the activation\nfunction in the convolution layers with the GCU activation function\nsignificantly improves performance on CIFAR-10, CIFAR-100 and Imagenette.",
          "arxiv_id": "2108.12943v3"
        }
      ],
      "51": [
        {
          "title": "Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need",
          "year": "2023-02",
          "abstract": "The core of out-of-distribution (OOD) detection is to learn the\nin-distribution (ID) representation, which is distinguishable from OOD samples.\nPrevious work applied recognition-based methods to learn the ID features, which\ntend to learn shortcuts instead of comprehensive representations. In this work,\nwe find surprisingly that simply using reconstruction-based methods could boost\nthe performance of OOD detection significantly. We deeply explore the main\ncontributors of OOD detection and find that reconstruction-based pretext tasks\nhave the potential to provide a generally applicable and efficacious prior,\nwhich benefits the model in learning intrinsic data distributions of the ID\ndataset. Specifically, we take Masked Image Modeling as a pretext task for our\nOOD detection framework (MOOD). Without bells and whistles, MOOD outperforms\nprevious SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by\n3.0%, and near-distribution OOD detection by 2.1%. It even defeats the\n10-shot-per-class outlier exposure OOD detection, although we do not include\nany OOD samples for our detection",
          "arxiv_id": "2302.02615v2"
        },
        {
          "title": "In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation",
          "year": "2023-06",
          "abstract": "Out-of-distribution (OOD) detection is the problem of identifying inputs\nwhich are unrelated to the in-distribution task. The OOD detection performance\nwhen the in-distribution (ID) is ImageNet-1K is commonly being tested on a\nsmall range of test OOD datasets. We find that most of the currently used test\nOOD datasets, including datasets from the open set recognition (OSR)\nliterature, have severe issues: In some cases more than 50$\\%$ of the dataset\ncontains objects belonging to one of the ID classes. These erroneous samples\nheavily distort the evaluation of OOD detectors. As a solution, we introduce\nwith NINCO a novel test OOD dataset, each sample checked to be ID free, which\nwith its fine-grained range of OOD classes allows for a detailed analysis of an\nOOD detector's strengths and failure modes, particularly when paired with a\nnumber of synthetic \"OOD unit-tests\". We provide detailed evaluations across a\nlarge set of architectures and OOD detection methods on NINCO and the\nunit-tests, revealing new insights about model weaknesses and the effects of\npretraining on OOD detection performance. We provide code and data at\nhttps://github.com/j-cb/NINCO.",
          "arxiv_id": "2306.00826v1"
        },
        {
          "title": "Unsupervised Evaluation of Out-of-distribution Detection: A Data-centric Perspective",
          "year": "2023-02",
          "abstract": "Out-of-distribution (OOD) detection methods assume that they have test ground\ntruths, i.e., whether individual test samples are in-distribution (IND) or OOD.\nHowever, in the real world, we do not always have such ground truths, and thus\ndo not know which sample is correctly detected and cannot compute the metric\nlike AUROC to evaluate the performance of different OOD detection methods. In\nthis paper, we are the first to introduce the unsupervised evaluation problem\nin OOD detection, which aims to evaluate OOD detection methods in real-world\nchanging environments without OOD labels. We propose three methods to compute\nGscore as an unsupervised indicator of OOD detection performance. We further\nintroduce a new benchmark Gbench, which has 200 real-world OOD datasets of\nvarious label spaces to train and evaluate our method. Through experiments, we\nfind a strong quantitative correlation betwwen Gscore and the OOD detection\nperformance. Extensive experiments demonstrate that our Gscore achieves\nstate-of-the-art performance. Gscore also generalizes well with different\nIND/OOD datasets, OOD detection methods, backbones and dataset sizes. We\nfurther provide interesting analyses of the effects of backbones and IND/OOD\ndatasets on OOD detection performance. The data and code will be available.",
          "arxiv_id": "2302.08287v2"
        }
      ],
      "52": [
        {
          "title": "Video Anomaly Detection for Smart Surveillance",
          "year": "2020-04",
          "abstract": "In modern intelligent video surveillance systems, automatic anomaly detection\nthrough computer vision analytics plays a pivotal role which not only\nsignificantly increases monitoring efficiency but also reduces the burden on\nlive monitoring. Anomalies in videos are broadly defined as events or\nactivities that are unusual and signify irregular behavior. The goal of anomaly\ndetection is to temporally or spatially localize the anomaly events in video\nsequences. Temporal localization (i.e. indicating the start and end frames of\nthe anomaly event in a video) is referred to as frame-level detection. Spatial\nlocalization, which is more challenging, means to identify the pixels within\neach anomaly frame that correspond to the anomaly event. This setting is\nusually referred to as pixel-level detection. In this paper, we provide a brief\noverview of the recent research progress on video anomaly detection and\nhighlight a few future research directions.",
          "arxiv_id": "2004.00222v3"
        },
        {
          "title": "Weakly Supervised Video Anomaly Detection via Center-guided Discriminative Learning",
          "year": "2021-04",
          "abstract": "Anomaly detection in surveillance videos is a challenging task due to the\ndiversity of anomalous video content and duration. In this paper, we consider\nvideo anomaly detection as a regression problem with respect to anomaly scores\nof video clips under weak supervision. Hence, we propose an anomaly detection\nframework, called Anomaly Regression Net (AR-Net), which only requires\nvideo-level labels in training stage. Further, to learn discriminative features\nfor anomaly detection, we design a dynamic multiple-instance learning loss and\na center loss for the proposed AR-Net. The former is used to enlarge the\ninter-class distance between anomalous and normal instances, while the latter\nis proposed to reduce the intra-class distance of normal instances.\nComprehensive experiments are performed on a challenging benchmark:\nShanghaiTech. Our method yields a new state-of-the-art result for video anomaly\ndetection on ShanghaiTech dataset",
          "arxiv_id": "2104.07268v1"
        },
        {
          "title": "Anomaly Detection in Video Sequences: A Benchmark and Computational Model",
          "year": "2021-06",
          "abstract": "Anomaly detection has attracted considerable search attention. However,\nexisting anomaly detection databases encounter two major problems. Firstly,\nthey are limited in scale. Secondly, training sets contain only video-level\nlabels indicating the existence of an abnormal event during the full video\nwhile lacking annotations of precise time durations. To tackle these problems,\nwe contribute a new Large-scale Anomaly Detection (LAD) database as the\nbenchmark for anomaly detection in video sequences, which is featured in two\naspects. 1) It contains 2000 video sequences including normal and abnormal\nvideo clips with 14 anomaly categories including crash, fire, violence, etc.\nwith large scene varieties, making it the largest anomaly analysis database to\ndate. 2) It provides the annotation data, including video-level labels\n(abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal\nvideo frame) to facilitate anomaly detection. Leveraging the above benefits\nfrom the LAD database, we further formulate anomaly detection as a\nfully-supervised learning problem and propose a multi-task deep neural network\nto solve it. We first obtain the local spatiotemporal contextual feature by\nusing an Inflated 3D convolutional (I3D) network. Then we construct a recurrent\nconvolutional neural network fed the local spatiotemporal contextual feature to\nextract the spatiotemporal contextual feature. With the global spatiotemporal\ncontextual feature, the anomaly type and score can be computed simultaneously\nby a multi-task neural network. Experimental results show that the proposed\nmethod outperforms the state-of-the-art anomaly detection methods on our\ndatabase and other public databases of anomaly detection. Codes are available\nat https://github.com/wanboyang/anomaly_detection_LAD2000.",
          "arxiv_id": "2106.08570v1"
        }
      ],
      "53": [
        {
          "title": "Shape-guided Object Inpainting",
          "year": "2022-04",
          "abstract": "Previous works on image inpainting mainly focus on inpainting background or\npartially missing objects, while the problem of inpainting an entire missing\nobject remains unexplored. This work studies a new image inpainting task, i.e.\nshape-guided object inpainting. Given an incomplete input image, the goal is to\nfill in the hole by generating an object based on the context and implicit\nguidance given by the hole shape. Since previous methods for image inpainting\nare mainly designed for background inpainting, they are not suitable for this\ntask. Therefore, we propose a new data preparation method and a novel\nContextual Object Generator (CogNet) for the object inpainting task. On the\ndata side, we incorporate object priors into training data by using object\ninstances as holes. The CogNet has a two-stream architecture that combines the\nstandard bottom-up image completion process with a top-down object generation\nprocess. A predictive class embedding module bridges the two streams by\npredicting the class of the missing object from the bottom-up features, from\nwhich a semantic object map is derived as the input of the top-down stream.\nExperiments demonstrate that the proposed method can generate realistic objects\nthat fit the context in terms of both visual appearance and semantic meanings.\nCode can be found at the project page\n\\url{https://zengxianyu.github.io/objpaint}",
          "arxiv_id": "2204.07845v1"
        },
        {
          "title": "Line Drawing Guided Progressive Inpainting of Mural Damage",
          "year": "2022-11",
          "abstract": "Mural image inpainting is far less explored compared to its natural image\ncounterpart and remains largely unsolved. Most existing image-inpainting\nmethods tend to take the target image as the only input and directly repair the\ndamage to generate a visually plausible result. These methods obtain high\nperformance in restoration or completion of some pre-defined objects, e.g.,\nhuman face, fabric texture, and printed texts, etc., however, are not suitable\nfor repairing murals with varying subjects and large damaged areas. Moreover,\ndue to discrete colors in paints, mural inpainting may suffer from apparent\ncolor bias. To this end, in this paper, we propose a line drawing guided\nprogressive mural inpainting method. It divides the inpainting process into two\nsteps: structure reconstruction and color correction, implemented by a\nstructure reconstruction network (SRN) and a color correction network (CCN),\nrespectively. In structure reconstruction, SRN utilizes the line drawing as an\nassistant to achieve large-scale content authenticity and structural stability.\nIn color correction, CCN operates a local color adjustment for missing pixels\nwhich reduces the negative effects of color bias and edge jumping. The proposed\napproach is evaluated against the current state-of-the-art image inpainting\nmethods. Qualitative and quantitative results demonstrate the superiority of\nthe proposed method in mural image inpainting. The codes and data are available\nat https://github.com/qinnzou/mural-image-inpainting.",
          "arxiv_id": "2211.06649v2"
        },
        {
          "title": "Perceptual Artifacts Localization for Inpainting",
          "year": "2022-08",
          "abstract": "Image inpainting is an essential task for multiple practical applications\nlike object removal and image editing. Deep GAN-based models greatly improve\nthe inpainting performance in structures and textures within the hole, but\nmight also generate unexpected artifacts like broken structures or color blobs.\nUsers perceive these artifacts to judge the effectiveness of inpainting models,\nand retouch these imperfect areas to inpaint again in a typical retouching\nworkflow. Inspired by this workflow, we propose a new learning task of\nautomatic segmentation of inpainting perceptual artifacts, and apply the model\nfor inpainting model evaluation and iterative refinement. Specifically, we\nfirst construct a new inpainting artifacts dataset by manually annotating\nperceptual artifacts in the results of state-of-the-art inpainting models. Then\nwe train advanced segmentation networks on this dataset to reliably localize\ninpainting artifacts within inpainted images. Second, we propose a new\ninterpretable evaluation metric called Perceptual Artifact Ratio (PAR), which\nis the ratio of objectionable inpainted regions to the entire inpainted area.\nPAR demonstrates a strong correlation with real user preference. Finally, we\nfurther apply the generated masks for iterative image inpainting by combining\nour approach with multiple recent inpainting methods. Extensive experiments\ndemonstrate the consistent decrease of artifact regions and inpainting quality\nimprovement across the different methods.",
          "arxiv_id": "2208.03357v1"
        }
      ],
      "54": [
        {
          "title": "Improving Sign Recognition with Phonology",
          "year": "2023-02",
          "abstract": "We use insights from research on American Sign Language (ASL) phonology to\ntrain models for isolated sign language recognition (ISLR), a step towards\nautomatic sign language understanding. Our key insight is to explicitly\nrecognize the role of phonology in sign production to achieve more accurate\nISLR than existing work which does not consider sign language phonology. We\ntrain ISLR models that take in pose estimations of a signer producing a single\nsign to predict not only the sign but additionally its phonological\ncharacteristics, such as the handshape. These auxiliary predictions lead to a\nnearly 9% absolute gain in sign recognition accuracy on the WLASL benchmark,\nwith consistent improvements in ISLR regardless of the underlying prediction\nmodel architecture. This work has the potential to accelerate linguistic\nresearch in the domain of signed languages and reduce communication barriers\nbetween deaf and hearing people.",
          "arxiv_id": "2302.05759v1"
        },
        {
          "title": "Indian Sign Language Recognition Using Mediapipe Holistic",
          "year": "2023-04",
          "abstract": "Deaf individuals confront significant communication obstacles on a daily\nbasis. Their inability to hear makes it difficult for them to communicate with\nthose who do not understand sign language. Moreover, it presents difficulties\nin educational, occupational, and social contexts. By providing alternative\ncommunication channels, technology can play a crucial role in overcoming these\nobstacles. One such technology that can facilitate communication between deaf\nand hearing individuals is sign language recognition. We will create a robust\nsystem for sign language recognition in order to convert Indian Sign Language\nto text or speech. We will evaluate the proposed system and compare CNN and\nLSTM models. Since there are both static and gesture sign languages, a robust\nmodel is required to distinguish between them. In this study, we discovered\nthat a CNN model captures letters and characters for recognition of static sign\nlanguage better than an LSTM model, but it outperforms CNN by monitoring hands,\nfaces, and pose in gesture sign language phrases and sentences. The creation of\na text-to-sign language paradigm is essential since it will enhance the sign\nlanguage-dependent deaf and hard-of-hearing population's communication skills.\nEven though the sign-to-text translation is just one side of communication, not\nall deaf or hard-of-hearing people are proficient in reading or writing text.\nSome may have difficulty comprehending written language due to educational or\nliteracy issues. Therefore, a text-to-sign language paradigm would allow them\nto comprehend text-based information and participate in a variety of social,\neducational, and professional settings.\n  Keywords: deaf and hard-of-hearing, DHH, Indian sign language, CNN, LSTM,\nstatic and gesture sign languages, text-to-sign language model, MediaPipe\nHolistic, sign language recognition, SLR, SLT",
          "arxiv_id": "2304.10256v1"
        },
        {
          "title": "Continuous Sign Language Recognition System using Deep Learning with MediaPipe Holistic",
          "year": "2024-11",
          "abstract": "Sign languages are the language of hearing-impaired people who use visuals\nlike the hand, facial, and body movements for communication. There are\ndifferent signs and gestures representing alphabets, words, and phrases.\nNowadays approximately 300 sign languages are being practiced worldwide such as\nAmerican Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language\n(ISL), and many more. Sign languages are dependent on the vocal language of a\nplace. Unlike vocal or spoken languages, there are no helping words in sign\nlanguage like is, am, are, was, were, will, be, etc. As only a limited\npopulation is well-versed in sign language, this lack of familiarity of sign\nlanguage hinders hearing-impaired people from communicating freely and easily\nwith everyone. This issue can be addressed by a sign language recognition (SLR)\nsystem which has the capability to translate the sign language into vocal\nlanguage. In this paper, a continuous SLR system is proposed using a deep\nlearning model employing Long Short-Term Memory (LSTM), trained and tested on\nan ISL primary dataset. This dataset is created using MediaPipe Holistic\npipeline for tracking face, hand, and body movements and collecting landmarks.\nThe system recognizes the signs and gestures in real-time with 88.23% accuracy.",
          "arxiv_id": "2411.04517v1"
        }
      ],
      "55": [
        {
          "title": "Towards Source-free Domain Adaptive Semantic Segmentation via Importance-aware and Prototype-contrast Learning",
          "year": "2023-06",
          "abstract": "Domain adaptive semantic segmentation enables robust pixel-wise understanding\nin real-world driving scenes. Source-free domain adaptation, as a more\npractical technique, addresses the concerns of data privacy and storage\nlimitations in typical unsupervised domain adaptation methods, making it\nespecially relevant in the context of intelligent vehicles. It utilizes a\nwell-trained source model and unlabeled target data to achieve adaptation in\nthe target domain. However, in the absence of source data and target labels,\ncurrent solutions cannot sufficiently reduce the impact of domain shift and\nfully leverage the information from the target data. In this paper, we propose\nan end-to-end source-free domain adaptation semantic segmentation method via\nImportance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC\nframework effectively extracts domain-invariant knowledge from the well-trained\nsource model and learns domain-specific knowledge from the unlabeled target\ndomain. Specifically, considering the problem of domain shift in the prediction\nof the target domain by the source model, we put forward an importance-aware\nmechanism for the biased target prediction probability distribution to extract\ndomain-invariant knowledge from the source model. We further introduce a\nprototype-contrast strategy, which includes a prototype-symmetric cross-entropy\nloss and a prototype-enhanced cross-entropy loss, to learn target intra-domain\nknowledge without relying on labels. A comprehensive variety of experiments on\ntwo domain adaptive semantic segmentation benchmarks demonstrates that the\nproposed end-to-end IAPC solution outperforms existing state-of-the-art\nmethods. The source code is publicly available at\nhttps://github.com/yihong-97/Source-free-IAPC.",
          "arxiv_id": "2306.01598v3"
        },
        {
          "title": "WUDA: Unsupervised Domain Adaptation Based on Weak Source Domain Labels",
          "year": "2022-10",
          "abstract": "Unsupervised domain adaptation (UDA) for semantic segmentation addresses the\ncross-domain problem with fine source domain labels. However, the acquisition\nof semantic labels has always been a difficult step, many scenarios only have\nweak labels (e.g. bounding boxes). For scenarios where weak supervision and\ncross-domain problems coexist, this paper defines a new task: unsupervised\ndomain adaptation based on weak source domain labels (WUDA). To explore\nsolutions for this task, this paper proposes two intuitive frameworks: 1)\nPerform weakly supervised semantic segmentation in the source domain, and then\nimplement unsupervised domain adaptation; 2) Train an object detection model\nusing source domain data, then detect objects in the target domain and\nimplement weakly supervised semantic segmentation. We observe that the two\nframeworks behave differently when the datasets change. Therefore, we construct\ndataset pairs with a wide range of domain shifts and conduct extended\nexperiments to analyze the impact of different domain shifts on the two\nframeworks. In addition, to measure domain shift, we apply the metric\nrepresentation shift to urban landscape image segmentation for the first time.\nThe source code and constructed datasets are available at\n\\url{https://github.com/bupt-ai-cz/WUDA}.",
          "arxiv_id": "2210.02088v1"
        },
        {
          "title": "Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation",
          "year": "2021-03",
          "abstract": "Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models\ntrained on multiple labeled source domains to an unlabeled target domain. In\nthis paper, we propose a novel multi-source domain adaptation framework based\non collaborative learning for semantic segmentation. Firstly, a simple image\ntranslation method is introduced to align the pixel value distribution to\nreduce the gap between source domains and target domain to some extent. Then,\nto fully exploit the essential semantic information across source domains, we\npropose a collaborative learning method for domain adaptation without seeing\nany data from target domain. In addition, similar to the setting of\nunsupervised domain adaptation, unlabeled target domain data is leveraged to\nfurther improve the performance of domain adaptation. This is achieved by\nadditionally constraining the outputs of multiple adaptation models with pseudo\nlabels online generated by an ensembled model. Extensive experiments and\nablation studies are conducted on the widely-used domain adaptation benchmark\ndatasets in semantic segmentation. Our proposed method achieves 59.0\\% mIoU on\nthe validation set of Cityscapes by training on the labeled Synscapes and GTA5\ndatasets and unlabeled training set of Cityscapes. It significantly outperforms\nall previous state-of-the-arts single-source and multi-source unsupervised\ndomain adaptation methods.",
          "arxiv_id": "2103.04717v3"
        }
      ],
      "56": [
        {
          "title": "Locally Linear Region Knowledge Distillation",
          "year": "2020-10",
          "abstract": "Knowledge distillation (KD) is an effective technique to transfer knowledge\nfrom one neural network (teacher) to another (student), thus improving the\nperformance of the student. To make the student better mimic the behavior of\nthe teacher, the existing work focuses on designing different criteria to align\ntheir logits or representations. Different from these efforts, we address\nknowledge distillation from a novel data perspective. We argue that\ntransferring knowledge at sparse training data points cannot enable the student\nto well capture the local shape of the teacher function. To address this issue,\nwe propose locally linear region knowledge distillation ($\\rm L^2$RKD) which\ntransfers the knowledge in local, linear regions from a teacher to a student.\nThis is achieved by enforcing the student to mimic the outputs of the teacher\nfunction in local, linear regions. To the end, the student is able to better\ncapture the local shape of the teacher function and thus achieves a better\nperformance. Despite its simplicity, extensive experiments demonstrate that\n$\\rm L^2$RKD is superior to the original KD in many aspects as it outperforms\nKD and the other state-of-the-art approaches by a large margin, shows\nrobustness and superiority under few-shot settings, and is more compatible with\nthe existing distillation approaches to further improve their performances\nsignificantly.",
          "arxiv_id": "2010.04812v2"
        },
        {
          "title": "Interactive Knowledge Distillation",
          "year": "2020-07",
          "abstract": "Knowledge distillation is a standard teacher-student learning framework to\ntrain a light-weight student network under the guidance of a well-trained large\nteacher network. As an effective teaching strategy, interactive teaching has\nbeen widely employed at school to motivate students, in which teachers not only\nprovide knowledge but also give constructive feedback to students upon their\nresponses, to improve their learning performance. In this work, we propose an\nInterActive Knowledge Distillation (IAKD) scheme to leverage the interactive\nteaching strategy for efficient knowledge distillation. In the distillation\nprocess, the interaction between teacher and student networks is implemented by\na swapping-in operation: randomly replacing the blocks in the student network\nwith the corresponding blocks in the teacher network. In the way, we directly\ninvolve the teacher's powerful feature transformation ability to largely boost\nthe student's performance. Experiments with typical settings of teacher-student\nnetworks demonstrate that the student networks trained by our IAKD achieve\nbetter performance than those trained by conventional knowledge distillation\nmethods on diverse image classification datasets.",
          "arxiv_id": "2007.01476v3"
        },
        {
          "title": "Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation",
          "year": "2024-09",
          "abstract": "Knowledge distillation has become widely recognized for its ability to\ntransfer knowledge from a large teacher network to a compact and more\nstreamlined student network. Traditional knowledge distillation methods\nprimarily follow a teacher-oriented paradigm that imposes the task of learning\nthe teacher's complex knowledge onto the student network. However, significant\ndisparities in model capacity and architectural design hinder the student's\ncomprehension of the complex knowledge imparted by the teacher, resulting in\nsub-optimal performance. This paper introduces a novel perspective emphasizing\nstudent-oriented and refining the teacher's knowledge to better align with the\nstudent's needs, thereby improving knowledge transfer effectiveness.\nSpecifically, we present the Student-Oriented Knowledge Distillation (SoKD),\nwhich incorporates a learnable feature augmentation strategy during training to\nrefine the teacher's knowledge of the student dynamically. Furthermore, we\ndeploy the Distinctive Area Detection Module (DAM) to identify areas of mutual\ninterest between the teacher and student, concentrating knowledge transfer\nwithin these critical areas to avoid transferring irrelevant information. This\ncustomized module ensures a more focused and effective knowledge distillation\nprocess. Our approach, functioning as a plug-in, could be integrated with\nvarious knowledge distillation methods. Extensive experimental results\ndemonstrate the efficacy and generalizability of our method.",
          "arxiv_id": "2409.18785v1"
        }
      ],
      "57": [
        {
          "title": "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement",
          "year": "2022-12",
          "abstract": "Prior works on improving speech quality with visual input typically study\neach type of auditory distortion separately (e.g., separation, inpainting,\nvideo-to-speech) and present tailored algorithms. This paper proposes to unify\nthese subjects and study Generalized Speech Enhancement, where the goal is not\nto reconstruct the exact reference clean signal, but to focus on improving\ncertain aspects of speech. In particular, this paper concerns intelligibility,\nquality, and video synchronization. We cast the problem as audio-visual speech\nresynthesis, which is composed of two steps: pseudo audio-visual speech\nrecognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and\nP-TTS are connected by discrete units derived from a self-supervised speech\nmodel. Moreover, we utilize self-supervised audio-visual speech model to\ninitialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first\nhigh-quality model for in-the-wild video-to-speech synthesis and achieves\nsuperior performance on all LRS3 audio-visual enhancement tasks with a single\nmodel. To demonstrates its applicability in the real world, ReVISE is also\nevaluated on EasyCom, an audio-visual benchmark collected under challenging\nacoustic conditions with only 1.6 hours of training data. Similarly, ReVISE\ngreatly suppresses noise and improves quality. Project page:\nhttps://wnhsu.github.io/ReVISE.",
          "arxiv_id": "2212.11377v1"
        },
        {
          "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction",
          "year": "2022-01",
          "abstract": "Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert",
          "arxiv_id": "2201.02184v2"
        },
        {
          "title": "AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation",
          "year": "2023-12",
          "abstract": "This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.",
          "arxiv_id": "2312.02512v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T18:53:36Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}