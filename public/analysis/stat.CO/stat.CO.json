{
  "topics": {
    "data": {
      "0": {
        "name": "0_Langevin_Monte_Carlo_Metropolis",
        "keywords": [
          [
            "Langevin",
            0.021992272382117144
          ],
          [
            "Monte",
            0.0217805815262026
          ],
          [
            "Carlo",
            0.0217805815262026
          ],
          [
            "Metropolis",
            0.020465255364098062
          ],
          [
            "Markov",
            0.01849872484384124
          ],
          [
            "target",
            0.018397589650476203
          ],
          [
            "sampling",
            0.017919706587404292
          ],
          [
            "algorithm",
            0.01765354143136938
          ],
          [
            "Hamiltonian",
            0.017016135160320245
          ],
          [
            "HMC",
            0.016423152656514103
          ]
        ],
        "count": 348
      },
      "1": {
        "name": "1_algorithm_regression_matrix_sparse",
        "keywords": [
          [
            "algorithm",
            0.020307122464010827
          ],
          [
            "regression",
            0.017689694426751026
          ],
          [
            "matrix",
            0.01501963414901599
          ],
          [
            "sparse",
            0.013954298344805573
          ],
          [
            "method",
            0.013903386129760364
          ],
          [
            "problem",
            0.013343112758790916
          ],
          [
            "optimization",
            0.013185024891850493
          ],
          [
            "data",
            0.012870630711738886
          ],
          [
            "linear",
            0.012682922630593545
          ],
          [
            "problems",
            0.012418640902666986
          ]
        ],
        "count": 301
      },
      "2": {
        "name": "2_data_survival_model_package",
        "keywords": [
          [
            "data",
            0.02258196619751039
          ],
          [
            "survival",
            0.019154354417793423
          ],
          [
            "model",
            0.018844762205020935
          ],
          [
            "package",
            0.01860949769927633
          ],
          [
            "effects",
            0.01702969787425923
          ],
          [
            "models",
            0.014755949527323477
          ],
          [
            "time",
            0.014451337662507694
          ],
          [
            "analysis",
            0.01404340398777038
          ],
          [
            "causal",
            0.013942420311504573
          ],
          [
            "longitudinal",
            0.010840039069161686
          ]
        ],
        "count": 287
      },
      "3": {
        "name": "3_state_model_Bayesian_particle",
        "keywords": [
          [
            "state",
            0.016493645984259897
          ],
          [
            "model",
            0.015420089073531554
          ],
          [
            "Bayesian",
            0.014884564826271646
          ],
          [
            "particle",
            0.0147156357813511
          ],
          [
            "problems",
            0.014269266928046717
          ],
          [
            "Gaussian",
            0.012785068682091078
          ],
          [
            "models",
            0.01277576894420277
          ],
          [
            "Kalman",
            0.012701981139161005
          ],
          [
            "time",
            0.012563618976986735
          ],
          [
            "filter",
            0.012464108265707046
          ]
        ],
        "count": 275
      },
      "4": {
        "name": "4_spatial_data_Gaussian_model",
        "keywords": [
          [
            "spatial",
            0.039731765267665926
          ],
          [
            "data",
            0.026596487075155165
          ],
          [
            "Gaussian",
            0.01915638726404419
          ],
          [
            "model",
            0.016930433807998493
          ],
          [
            "models",
            0.0162075925253911
          ],
          [
            "large",
            0.01374405038262415
          ],
          [
            "covariance",
            0.012254911696983431
          ],
          [
            "process",
            0.012001625450254745
          ],
          [
            "processes",
            0.011691137654882994
          ],
          [
            "approach",
            0.010423267372836316
          ]
        ],
        "count": 212
      },
      "5": {
        "name": "5_Bayesian_MCMC_posterior_inference",
        "keywords": [
          [
            "Bayesian",
            0.02912853015660235
          ],
          [
            "MCMC",
            0.02789839763060783
          ],
          [
            "posterior",
            0.023360523981067794
          ],
          [
            "inference",
            0.019156255243320028
          ],
          [
            "ABC",
            0.019102391047800347
          ],
          [
            "likelihood",
            0.018728095171986873
          ],
          [
            "methods",
            0.016315994301873426
          ],
          [
            "Carlo",
            0.016271857581233933
          ],
          [
            "Monte",
            0.016271857581233933
          ],
          [
            "Markov",
            0.015232760674428535
          ]
        ],
        "count": 176
      },
      "6": {
        "name": "6_series_time_time series_model",
        "keywords": [
          [
            "series",
            0.029432845569586228
          ],
          [
            "time",
            0.028552645227401148
          ],
          [
            "time series",
            0.028337219285294517
          ],
          [
            "model",
            0.018488380583472193
          ],
          [
            "data",
            0.01796683518977469
          ],
          [
            "models",
            0.017614742031181237
          ],
          [
            "forecasting",
            0.017372869913628523
          ],
          [
            "detection",
            0.016018861007974495
          ],
          [
            "change",
            0.014785874348870994
          ],
          [
            "volatility",
            0.01250606215591914
          ]
        ],
        "count": 166
      },
      "7": {
        "name": "7_network_networks_graph_nodes",
        "keywords": [
          [
            "network",
            0.04880865356236374
          ],
          [
            "networks",
            0.04206947079200961
          ],
          [
            "graph",
            0.028076338372961644
          ],
          [
            "nodes",
            0.02042843822000072
          ],
          [
            "community",
            0.017518896314683105
          ],
          [
            "model",
            0.015696215991914984
          ],
          [
            "structure",
            0.014734286613527459
          ],
          [
            "edge",
            0.014627167435980505
          ],
          [
            "node",
            0.014362773117744684
          ],
          [
            "clustering",
            0.013135450776223676
          ]
        ],
        "count": 129
      },
      "8": {
        "name": "8_surrogate_uncertainty_fidelity_model",
        "keywords": [
          [
            "surrogate",
            0.028757429969235148
          ],
          [
            "uncertainty",
            0.025479640637470515
          ],
          [
            "fidelity",
            0.022425792766446934
          ],
          [
            "model",
            0.02162064520320976
          ],
          [
            "models",
            0.02091819018859246
          ],
          [
            "input",
            0.016138726369053905
          ],
          [
            "quantification",
            0.014150968272844726
          ],
          [
            "method",
            0.013418801515883719
          ],
          [
            "engineering",
            0.012841235177592062
          ],
          [
            "uncertainty quantification",
            0.012706011490325901
          ]
        ],
        "count": 108
      },
      "9": {
        "name": "9_distribution_test_sample_tests",
        "keywords": [
          [
            "distribution",
            0.032036330104419745
          ],
          [
            "test",
            0.02660501118406174
          ],
          [
            "sample",
            0.022358875301158914
          ],
          [
            "tests",
            0.02180233173515187
          ],
          [
            "confidence",
            0.019161766978506432
          ],
          [
            "values",
            0.017775350752054362
          ],
          [
            "statistics",
            0.016908492119424964
          ],
          [
            "method",
            0.016025730119640137
          ],
          [
            "distributions",
            0.015243147147650361
          ],
          [
            "type",
            0.013360165724943012
          ]
        ],
        "count": 96
      },
      "10": {
        "name": "10_design_designs_optimal_experimental",
        "keywords": [
          [
            "design",
            0.0679192257542954
          ],
          [
            "designs",
            0.04632389454526036
          ],
          [
            "optimal",
            0.035998770458876186
          ],
          [
            "experimental",
            0.034467222562619235
          ],
          [
            "experimental design",
            0.025742162757648495
          ],
          [
            "experiments",
            0.021635127737212493
          ],
          [
            "Design",
            0.01999253875250916
          ],
          [
            "optimal designs",
            0.016861542765743698
          ],
          [
            "information",
            0.0161807804383956
          ],
          [
            "Bayesian",
            0.01583377734501991
          ]
        ],
        "count": 86
      },
      "11": {
        "name": "11_data_package_code_statistical",
        "keywords": [
          [
            "data",
            0.039154130794081705
          ],
          [
            "package",
            0.025471896680713372
          ],
          [
            "code",
            0.025341315450368523
          ],
          [
            "statistical",
            0.022498125707661874
          ],
          [
            "data science",
            0.020110283617279255
          ],
          [
            "science",
            0.019448441319854803
          ],
          [
            "research",
            0.01665552270851411
          ],
          [
            "packages",
            0.015344578362105973
          ],
          [
            "software",
            0.015068790821636101
          ],
          [
            "reproducible",
            0.014677953961191904
          ]
        ],
        "count": 82
      },
      "12": {
        "name": "12_clustering_mixture_data_clusters",
        "keywords": [
          [
            "clustering",
            0.052615178986829825
          ],
          [
            "mixture",
            0.02985978669252492
          ],
          [
            "data",
            0.024758503192696048
          ],
          [
            "clusters",
            0.020462138558719842
          ],
          [
            "model",
            0.019022975011198807
          ],
          [
            "cluster",
            0.016485510931539823
          ],
          [
            "algorithm",
            0.01583576145037656
          ],
          [
            "Mixture",
            0.014153268674932638
          ],
          [
            "Bayesian",
            0.013499886792600758
          ],
          [
            "approach",
            0.013305027967030206
          ]
        ],
        "count": 79
      },
      "13": {
        "name": "13_model_disease_transmission_data",
        "keywords": [
          [
            "model",
            0.03025188221902898
          ],
          [
            "disease",
            0.029942286260865817
          ],
          [
            "transmission",
            0.025372215147678928
          ],
          [
            "data",
            0.024102503664646332
          ],
          [
            "models",
            0.02188900032456378
          ],
          [
            "epidemic",
            0.019710234424629144
          ],
          [
            "spread",
            0.018053423436565335
          ],
          [
            "infection",
            0.01754870849481144
          ],
          [
            "time",
            0.01709705483643522
          ],
          [
            "cases",
            0.014046965321640117
          ]
        ],
        "count": 73
      },
      "14": {
        "name": "14_random_sampling_algorithm_entropy",
        "keywords": [
          [
            "random",
            0.027774786040530325
          ],
          [
            "sampling",
            0.022249606448815972
          ],
          [
            "algorithm",
            0.018383902029126317
          ],
          [
            "entropy",
            0.017374638150055233
          ],
          [
            "number",
            0.01522685203115395
          ],
          [
            "distribution",
            0.014961918931479986
          ],
          [
            "generators",
            0.013282366697502514
          ],
          [
            "efficient",
            0.01245017324916035
          ],
          [
            "nets",
            0.012376842765172199
          ],
          [
            "algorithms",
            0.012267582328036396
          ]
        ],
        "count": 70
      }
    },
    "correlations": [
      [
        1.0,
        -0.6343276965459493,
        -0.6790888010305794,
        -0.6651218651652115,
        -0.6953567833477482,
        -0.136644622733501,
        -0.6965474717179376,
        -0.7120227395622962,
        -0.6129209783771132,
        -0.6529075818281715,
        -0.6951087220304724,
        -0.7005029178117803,
        -0.6980846879693063,
        -0.6861163010668684,
        -0.6658752659602227
      ],
      [
        -0.6343276965459493,
        1.0,
        -0.6771532840179129,
        -0.705094792445728,
        -0.709871168570953,
        -0.6772149408425494,
        -0.7118804334821915,
        -0.7251443754400106,
        -0.7059701626906503,
        -0.6840321835682686,
        -0.7024506350676587,
        -0.6885913226952586,
        -0.6941883343589347,
        -0.70369404312728,
        -0.7052048589011948
      ],
      [
        -0.6790888010305794,
        -0.6771532840179129,
        1.0,
        -0.725105513100007,
        -0.44340558797838764,
        -0.653166331242087,
        -0.6373812606904505,
        -0.6948852578324041,
        -0.4506535526261072,
        -0.6685941770044102,
        -0.6869257514413505,
        0.032208397535631095,
        -0.48009002634723036,
        -0.15841956921516753,
        -0.7339343222961335
      ],
      [
        -0.6651218651652115,
        -0.705094792445728,
        -0.725105513100007,
        1.0,
        -0.7170054784707025,
        -0.6385661770739056,
        -0.6853522316867218,
        -0.7525400841050838,
        -0.6776005393108104,
        -0.7224460463697553,
        -0.7351381449197543,
        -0.7440986392767093,
        -0.7250091541627774,
        -0.7186392497345647,
        -0.7381307495205929
      ],
      [
        -0.6953567833477482,
        -0.709871168570953,
        -0.44340558797838764,
        -0.7170054784707025,
        1.0,
        -0.6791330282091321,
        -0.6985354672007191,
        -0.7071327800496757,
        -0.6573176912321748,
        -0.7000552127199265,
        -0.7182688363890035,
        -0.4208013734555603,
        -0.49899420690228624,
        -0.46386372758267536,
        -0.7322373372677504
      ],
      [
        -0.136644622733501,
        -0.6772149408425494,
        -0.653166331242087,
        -0.6385661770739056,
        -0.6791330282091321,
        1.0,
        -0.7052362995637499,
        -0.6979176050822854,
        -0.587201624161757,
        -0.6706301728825319,
        -0.699973088780707,
        -0.6881604229195174,
        -0.6770553860088817,
        -0.6607837710684199,
        -0.6794111869736225
      ],
      [
        -0.6965474717179376,
        -0.7118804334821915,
        -0.6373812606904505,
        -0.6853522316867218,
        -0.6985354672007191,
        -0.7052362995637499,
        1.0,
        -0.7240049782579943,
        -0.677004287204262,
        -0.6995950029957796,
        -0.7288304122158654,
        -0.6644151505739422,
        -0.706596962263178,
        -0.6671936850829331,
        -0.7217389569676133
      ],
      [
        -0.7120227395622962,
        -0.7251443754400106,
        -0.6948852578324041,
        -0.7525400841050838,
        -0.7071327800496757,
        -0.6979176050822854,
        -0.7240049782579943,
        1.0,
        -0.6952696216215857,
        -0.7338194505417037,
        -0.7196754003276107,
        -0.70651209689291,
        -0.6855163908939645,
        -0.7094803047732106,
        -0.7394826328480952
      ],
      [
        -0.6129209783771132,
        -0.7059701626906503,
        -0.4506535526261072,
        -0.6776005393108104,
        -0.6573176912321748,
        -0.587201624161757,
        -0.677004287204262,
        -0.6952696216215857,
        1.0,
        -0.6824183492839919,
        -0.6827806783089212,
        -0.646837673420228,
        -0.683234084260543,
        -0.42333225019053233,
        -0.7249460895569294
      ],
      [
        -0.6529075818281715,
        -0.6840321835682686,
        -0.6685941770044102,
        -0.7224460463697553,
        -0.7000552127199265,
        -0.6706301728825319,
        -0.6995950029957796,
        -0.7338194505417037,
        -0.6824183492839919,
        1.0,
        -0.716532836228451,
        -0.6852886284723747,
        -0.7043295771638353,
        -0.6931368676604346,
        -0.6941037663312879
      ],
      [
        -0.6951087220304724,
        -0.7024506350676587,
        -0.6869257514413505,
        -0.7351381449197543,
        -0.7182688363890035,
        -0.699973088780707,
        -0.7288304122158654,
        -0.7196754003276107,
        -0.6827806783089212,
        -0.716532836228451,
        1.0,
        -0.6862149552842461,
        -0.722797507668411,
        -0.7120704925834433,
        -0.7243718112790865
      ],
      [
        -0.7005029178117803,
        -0.6885913226952586,
        0.032208397535631095,
        -0.7440986392767093,
        -0.4208013734555603,
        -0.6881604229195174,
        -0.6644151505739422,
        -0.70651209689291,
        -0.646837673420228,
        -0.6852886284723747,
        -0.6862149552842461,
        1.0,
        -0.4578395509994848,
        -0.4391867093185724,
        -0.7372043491205076
      ],
      [
        -0.6980846879693063,
        -0.6941883343589347,
        -0.48009002634723036,
        -0.7250091541627774,
        -0.49899420690228624,
        -0.6770553860088817,
        -0.706596962263178,
        -0.6855163908939645,
        -0.683234084260543,
        -0.7043295771638353,
        -0.722797507668411,
        -0.4578395509994848,
        1.0,
        -0.48915555296398405,
        -0.7376405536276908
      ],
      [
        -0.6861163010668684,
        -0.70369404312728,
        -0.15841956921516753,
        -0.7186392497345647,
        -0.46386372758267536,
        -0.6607837710684199,
        -0.6671936850829331,
        -0.7094803047732106,
        -0.42333225019053233,
        -0.6931368676604346,
        -0.7120704925834433,
        -0.4391867093185724,
        -0.48915555296398405,
        1.0,
        -0.7398485571291737
      ],
      [
        -0.6658752659602227,
        -0.7052048589011948,
        -0.7339343222961335,
        -0.7381307495205929,
        -0.7322373372677504,
        -0.6794111869736225,
        -0.7217389569676133,
        -0.7394826328480952,
        -0.7249460895569294,
        -0.6941037663312879,
        -0.7243718112790865,
        -0.7372043491205076,
        -0.7376405536276908,
        -0.7398485571291737,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        14,
        2,
        0,
        1,
        5,
        12,
        4,
        2,
        2,
        2,
        8,
        5,
        3,
        2,
        0
      ],
      "2020-02": [
        28,
        2,
        1,
        2,
        4,
        10,
        7,
        3,
        3,
        4,
        5,
        2,
        2,
        3,
        0
      ],
      "2020-03": [
        17,
        1,
        2,
        2,
        3,
        4,
        1,
        3,
        5,
        6,
        2,
        6,
        5,
        1,
        2
      ],
      "2020-04": [
        15,
        3,
        0,
        1,
        7,
        14,
        6,
        1,
        6,
        6,
        9,
        2,
        1,
        8,
        0
      ],
      "2020-05": [
        18,
        3,
        2,
        0,
        6,
        6,
        5,
        1,
        5,
        6,
        6,
        9,
        2,
        4,
        0
      ],
      "2020-06": [
        18,
        5,
        1,
        3,
        2,
        9,
        7,
        5,
        1,
        10,
        7,
        9,
        6,
        3,
        0
      ],
      "2020-07": [
        27,
        2,
        3,
        5,
        3,
        12,
        10,
        5,
        5,
        2,
        5,
        7,
        5,
        2,
        0
      ],
      "2020-08": [
        16,
        1,
        1,
        1,
        5,
        6,
        3,
        4,
        3,
        3,
        2,
        5,
        0,
        0,
        0
      ],
      "2020-09": [
        14,
        2,
        0,
        1,
        0,
        7,
        4,
        5,
        2,
        5,
        2,
        6,
        4,
        6,
        0
      ],
      "2020-10": [
        20,
        8,
        1,
        2,
        2,
        11,
        8,
        2,
        4,
        4,
        5,
        8,
        7,
        3,
        0
      ],
      "2020-11": [
        18,
        3,
        0,
        3,
        3,
        6,
        3,
        4,
        4,
        5,
        4,
        9,
        6,
        3,
        0
      ],
      "2020-12": [
        15,
        1,
        5,
        0,
        8,
        9,
        1,
        5,
        2,
        2,
        6,
        5,
        4,
        4,
        1
      ],
      "2021-01": [
        16,
        0,
        4,
        3,
        5,
        6,
        7,
        5,
        3,
        1,
        1,
        12,
        2,
        4,
        3
      ],
      "2021-02": [
        22,
        4,
        3,
        2,
        3,
        4,
        5,
        2,
        4,
        5,
        4,
        7,
        2,
        5,
        1
      ],
      "2021-03": [
        16,
        3,
        1,
        1,
        3,
        10,
        5,
        1,
        1,
        5,
        6,
        4,
        4,
        1,
        1
      ],
      "2021-04": [
        18,
        4,
        2,
        0,
        1,
        9,
        4,
        4,
        4,
        0,
        9,
        2,
        3,
        3,
        0
      ],
      "2021-05": [
        22,
        5,
        3,
        2,
        7,
        8,
        5,
        5,
        3,
        4,
        3,
        9,
        2,
        3,
        0
      ],
      "2021-06": [
        33,
        7,
        5,
        2,
        3,
        11,
        6,
        1,
        5,
        5,
        7,
        4,
        7,
        2,
        3
      ],
      "2021-07": [
        16,
        0,
        2,
        0,
        2,
        6,
        7,
        2,
        6,
        0,
        4,
        4,
        3,
        3,
        0
      ],
      "2021-08": [
        18,
        3,
        3,
        0,
        4,
        4,
        4,
        4,
        4,
        2,
        4,
        3,
        0,
        3,
        0
      ],
      "2021-09": [
        11,
        3,
        4,
        1,
        6,
        5,
        7,
        3,
        3,
        4,
        3,
        7,
        2,
        9,
        0
      ],
      "2021-10": [
        24,
        2,
        4,
        3,
        5,
        11,
        6,
        2,
        3,
        5,
        11,
        11,
        1,
        5,
        1
      ],
      "2021-11": [
        17,
        4,
        1,
        0,
        3,
        3,
        2,
        4,
        4,
        5,
        3,
        4,
        0,
        2,
        0
      ],
      "2021-12": [
        18,
        3,
        0,
        2,
        2,
        11,
        5,
        6,
        1,
        4,
        3,
        2,
        2,
        2,
        0
      ],
      "2022-01": [
        17,
        2,
        2,
        1,
        3,
        6,
        1,
        6,
        6,
        5,
        6,
        8,
        1,
        3,
        1
      ],
      "2022-02": [
        19,
        0,
        3,
        1,
        4,
        5,
        1,
        6,
        6,
        5,
        3,
        4,
        5,
        1,
        0
      ],
      "2022-03": [
        14,
        3,
        3,
        3,
        2,
        12,
        3,
        5,
        4,
        7,
        5,
        5,
        2,
        6,
        2
      ],
      "2022-04": [
        17,
        3,
        2,
        2,
        2,
        6,
        3,
        2,
        1,
        2,
        3,
        2,
        2,
        2,
        0
      ],
      "2022-05": [
        18,
        3,
        2,
        1,
        0,
        10,
        3,
        3,
        3,
        5,
        10,
        2,
        5,
        2,
        1
      ],
      "2022-06": [
        22,
        3,
        1,
        5,
        3,
        18,
        2,
        7,
        5,
        3,
        6,
        5,
        4,
        2,
        0
      ],
      "2022-07": [
        22,
        2,
        1,
        3,
        3,
        9,
        5,
        1,
        4,
        1,
        4,
        6,
        5,
        4,
        2
      ],
      "2022-08": [
        17,
        8,
        3,
        3,
        3,
        7,
        3,
        5,
        4,
        4,
        8,
        5,
        0,
        4,
        0
      ],
      "2022-09": [
        20,
        1,
        2,
        0,
        1,
        4,
        4,
        4,
        1,
        7,
        6,
        5,
        2,
        7,
        0
      ],
      "2022-10": [
        22,
        3,
        2,
        3,
        4,
        7,
        6,
        6,
        4,
        2,
        7,
        8,
        1,
        2,
        1
      ],
      "2022-11": [
        23,
        4,
        3,
        0,
        2,
        5,
        9,
        3,
        8,
        5,
        4,
        9,
        3,
        7,
        0
      ],
      "2022-12": [
        9,
        4,
        0,
        1,
        4,
        3,
        3,
        2,
        1,
        5,
        4,
        8,
        2,
        2,
        0
      ],
      "2023-01": [
        16,
        0,
        0,
        0,
        3,
        5,
        6,
        4,
        6,
        2,
        7,
        4,
        1,
        3,
        0
      ],
      "2023-02": [
        13,
        2,
        3,
        2,
        2,
        2,
        2,
        4,
        4,
        3,
        6,
        4,
        6,
        3,
        0
      ],
      "2023-03": [
        26,
        3,
        1,
        4,
        10,
        5,
        3,
        6,
        5,
        5,
        2,
        7,
        1,
        1,
        0
      ],
      "2023-04": [
        13,
        4,
        2,
        3,
        2,
        5,
        1,
        0,
        3,
        4,
        5,
        2,
        1,
        1,
        0
      ],
      "2023-05": [
        20,
        4,
        1,
        2,
        4,
        8,
        5,
        3,
        3,
        3,
        4,
        8,
        1,
        3,
        0
      ],
      "2023-06": [
        17,
        3,
        1,
        2,
        4,
        10,
        6,
        4,
        1,
        2,
        5,
        2,
        3,
        0,
        2
      ],
      "2023-07": [
        15,
        3,
        0,
        1,
        1,
        5,
        5,
        4,
        4,
        1,
        3,
        7,
        4,
        2,
        1
      ],
      "2023-08": [
        21,
        4,
        1,
        0,
        3,
        7,
        3,
        6,
        3,
        6,
        6,
        1,
        1,
        3,
        0
      ],
      "2023-09": [
        13,
        2,
        3,
        2,
        3,
        5,
        1,
        4,
        6,
        3,
        3,
        5,
        5,
        1,
        0
      ],
      "2023-10": [
        24,
        3,
        5,
        4,
        0,
        9,
        8,
        7,
        4,
        2,
        2,
        5,
        1,
        4,
        0
      ],
      "2023-11": [
        13,
        2,
        0,
        1,
        2,
        8,
        5,
        2,
        3,
        0,
        5,
        5,
        4,
        5,
        0
      ],
      "2023-12": [
        22,
        2,
        2,
        0,
        2,
        9,
        1,
        2,
        3,
        4,
        5,
        3,
        1,
        2,
        0
      ],
      "2024-01": [
        20,
        2,
        2,
        3,
        7,
        7,
        5,
        3,
        4,
        3,
        5,
        2,
        2,
        0,
        0
      ],
      "2024-02": [
        32,
        2,
        8,
        1,
        5,
        9,
        2,
        2,
        3,
        3,
        9,
        4,
        3,
        1,
        1
      ],
      "2024-03": [
        15,
        3,
        2,
        1,
        4,
        6,
        5,
        2,
        2,
        5,
        8,
        7,
        2,
        2,
        0
      ],
      "2024-04": [
        17,
        5,
        2,
        2,
        6,
        10,
        5,
        1,
        4,
        6,
        4,
        10,
        0,
        3,
        0
      ],
      "2024-05": [
        21,
        4,
        1,
        2,
        5,
        11,
        3,
        9,
        5,
        6,
        5,
        6,
        4,
        4,
        2
      ],
      "2024-06": [
        18,
        1,
        3,
        0,
        3,
        15,
        1,
        9,
        4,
        3,
        5,
        3,
        3,
        5,
        1
      ],
      "2024-07": [
        20,
        5,
        0,
        3,
        7,
        7,
        5,
        4,
        5,
        1,
        3,
        3,
        2,
        3,
        0
      ],
      "2024-08": [
        16,
        6,
        6,
        1,
        2,
        13,
        6,
        4,
        4,
        1,
        4,
        2,
        6,
        4,
        0
      ],
      "2024-09": [
        27,
        1,
        0,
        2,
        4,
        10,
        3,
        8,
        3,
        4,
        3,
        5,
        1,
        3,
        1
      ],
      "2024-10": [
        37,
        9,
        4,
        3,
        4,
        10,
        9,
        6,
        3,
        9,
        8,
        6,
        4,
        3,
        0
      ],
      "2024-11": [
        20,
        4,
        6,
        2,
        4,
        6,
        6,
        7,
        5,
        4,
        4,
        7,
        3,
        3,
        0
      ],
      "2024-12": [
        20,
        4,
        0,
        0,
        3,
        14,
        5,
        2,
        6,
        6,
        4,
        1,
        3,
        3,
        0
      ],
      "2025-01": [
        26,
        7,
        2,
        3,
        3,
        10,
        3,
        3,
        3,
        4,
        6,
        5,
        4,
        3,
        0
      ],
      "2025-02": [
        18,
        4,
        2,
        2,
        6,
        16,
        6,
        5,
        6,
        2,
        6,
        4,
        3,
        2,
        1
      ],
      "2025-03": [
        26,
        5,
        1,
        1,
        4,
        8,
        4,
        9,
        4,
        2,
        7,
        3,
        5,
        1,
        1
      ],
      "2025-04": [
        20,
        4,
        4,
        3,
        3,
        10,
        5,
        2,
        2,
        7,
        6,
        7,
        2,
        3,
        0
      ],
      "2025-05": [
        23,
        4,
        5,
        1,
        4,
        6,
        3,
        7,
        6,
        6,
        10,
        9,
        2,
        3,
        4
      ],
      "2025-06": [
        25,
        2,
        2,
        1,
        4,
        4,
        3,
        7,
        5,
        6,
        1,
        3,
        2,
        3,
        3
      ],
      "2025-07": [
        15,
        2,
        2,
        1,
        5,
        8,
        4,
        2,
        6,
        6,
        10,
        4,
        6,
        6,
        0
      ],
      "2025-08": [
        28,
        2,
        2,
        1,
        3,
        5,
        2,
        5,
        7,
        3,
        6,
        2,
        2,
        1,
        1
      ],
      "2025-09": [
        10,
        1,
        0,
        0,
        0,
        7,
        2,
        2,
        6,
        3,
        4,
        2,
        2,
        0,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Metropolis Augmented Hamiltonian Monte Carlo",
          "year": "2022-01",
          "abstract": "Hamiltonian Monte Carlo (HMC) is a powerful Markov Chain Monte Carlo (MCMC)\nmethod for sampling from complex high-dimensional continuous distributions.\nHowever, in many situations it is necessary or desirable to combine HMC with\nother Metropolis-Hastings (MH) samplers. The common HMC-within-Gibbs strategy\nimplies a trade-off between long HMC trajectories and more frequent other MH\nupdates. Addressing this trade-off has been the focus of several recent works.\nIn this paper we propose Metropolis Augmented Hamiltonian Monte Carlo (MAHMC),\nan HMC variant that allows MH updates within HMC and eliminates this trade-off.\nExperiments on two representative examples demonstrate MAHMC's efficiency and\nease of use when compared with within-Gibbs alternatives.",
          "arxiv_id": "2201.08044v2"
        },
        {
          "title": "Hamiltonian Assisted Metropolis Sampling",
          "year": "2020-05",
          "abstract": "Various Markov chain Monte Carlo (MCMC) methods are studied to improve upon\nrandom walk Metropolis sampling, for simulation from complex distributions.\nExamples include Metropolis-adjusted Langevin algorithms, Hamiltonian Monte\nCarlo, and other recent algorithms related to underdamped Langevin dynamics. We\npropose a broad class of irreversible sampling algorithms, called Hamiltonian\nassisted Metropolis sampling (HAMS), and develop two specific algorithms with\nappropriate tuning and preconditioning strategies. Our HAMS algorithms are\ndesigned to achieve two distinctive properties, while using an augmented target\ndensity with momentum as an auxiliary variable. One is generalized detailed\nbalance, which induces an irreversible exploration of the target. The other is\na rejection-free property, which allows our algorithms to perform\nsatisfactorily with relatively large step sizes. Furthermore, we formulate a\nframework of generalized Metropolis--Hastings sampling, which not only\nhighlights our construction of HAMS at a more abstract level, but also\nfacilitates possible further development of irreversible MCMC algorithms. We\npresent several numerical experiments, where the proposed algorithms are found\nto consistently yield superior results among existing ones.",
          "arxiv_id": "2005.08159v1"
        },
        {
          "title": "On Irreversible Metropolis Sampling Related to Langevin Dynamics",
          "year": "2021-06",
          "abstract": "There has been considerable interest in designing Markov chain Monte Carlo\nalgorithms by exploiting numerical methods for Langevin dynamics, which\nincludes Hamiltonian dynamics as a deterministic case. A prominent approach is\nHamiltonian Monte Carlo (HMC), where a leapfrog discretization of Hamiltonian\ndynamics is employed. We investigate a recently proposed class of irreversible\nsampling algorithms, called Hamiltonian assisted Metropolis sampling (HAMS),\nwhich uses an augmented target density similarly as in HMC, but involves a\nflexible proposal scheme and a carefully formulated acceptance-rejection scheme\nto achieve generalized reversibility. We show that as the step size tends to 0,\nthe HAMS proposal satisfies a class of stochastic differential equations\nincluding Langevin dynamics as a special case. We provide theoretical results\nfor HAMS under the univariate Gaussian setting, including the stationary\nvariance, the expected acceptance rate, and the spectral radius. From these\nresults, we derive default choices of tuning parameters for HAMS, such that\nonly the step size needs to be tuned in applications. Various relatively recent\nalgorithms for Langevin dynamics are also shown to fall in the class of HAMS\nproposals up to negligible differences. Our numerical experiments on sampling\nhigh-dimensional latent variables confirm that the HAMS algorithms consistently\nachieve superior performance, compared with several Metropolis-adjusted\nalgorithms based on popular integrators of Langevin dynamics.",
          "arxiv_id": "2106.03012v1"
        }
      ],
      "1": [
        {
          "title": "Estimating Multiple Precision Matrices with Cluster Fusion Regularization",
          "year": "2020-03",
          "abstract": "We propose a penalized likelihood framework for estimating multiple precision\nmatrices from different classes. Most existing methods either incorporate no\ninformation on relationships between the precision matrices, or require this\ninformation be known a priori. The framework proposed in this article allows\nfor simultaneous estimation of the precision matrices and relationships between\nthe precision matrices, jointly. Sparse and non-sparse estimators are proposed,\nboth of which require solving a non-convex optimization problem. To compute our\nproposed estimators, we use an iterative algorithm which alternates between a\nconvex optimization problem solved by blockwise coordinate descent and a\nk-means clustering problem. Blockwise updates for computing the sparse\nestimator require solving an elastic net penalized precision matrix estimation\nproblem, which we solve using a proximal gradient descent algorithm. We prove\nthat this subalgorithm has a linear rate of convergence. In simulation studies\nand two real data applications, we show that our method can outperform\ncompetitors that ignore relevant relationships between precision matrices and\nperforms similarly to methods which use prior information often uknown in\npractice.",
          "arxiv_id": "2003.00371v1"
        },
        {
          "title": "Grouped Variable Selection with Discrete Optimization: Computational and Statistical Perspectives",
          "year": "2021-04",
          "abstract": "We present a new algorithmic framework for grouped variable selection that is\nbased on discrete mathematical optimization. While there exist several\nappealing approaches based on convex relaxations and nonconvex heuristics, we\nfocus on optimal solutions for the $\\ell_0$-regularized formulation, a problem\nthat is relatively unexplored due to computational challenges. Our methodology\ncovers both high-dimensional linear regression and nonparametric sparse\nadditive modeling with smooth components. Our algorithmic framework consists of\napproximate and exact algorithms. The approximate algorithms are based on\ncoordinate descent and local search, with runtimes comparable to popular sparse\nlearning algorithms. Our exact algorithm is based on a standalone\nbranch-and-bound (BnB) framework, which can solve the associated mixed integer\nprogramming (MIP) problem to certified optimality. By exploiting the problem\nstructure, our custom BnB algorithm can solve to optimality problem instances\nwith $5 \\times 10^6$ features and $10^3$ observations in minutes to hours --\nover $1000$ times larger than what is currently possible using state-of-the-art\ncommercial MIP solvers. We also explore statistical properties of the\n$\\ell_0$-based estimators. We demonstrate, theoretically and empirically, that\nour proposed estimators have an edge over popular group-sparse estimators in\nterms of statistical performance in various regimes. We provide an open-source\nimplementation of our proposed framework.",
          "arxiv_id": "2104.07084v2"
        },
        {
          "title": "MARS: A second-order reduction algorithm for high-dimensional sparse precision matrices estimation",
          "year": "2021-06",
          "abstract": "Estimation of the precision matrix (or inverse covariance matrix) is of great\nimportance in statistical data analysis and machine learning. However, as the\nnumber of parameters scales quadratically with the dimension $p$, computation\nbecomes very challenging when $p$ is large. In this paper, we propose an\nadaptive sieving reduction algorithm to generate a solution path for the\nestimation of precision matrices under the $\\ell_1$ penalized D-trace loss,\nwith each subproblem being solved by a second-order algorithm. In each\niteration of our algorithm, we are able to greatly reduce the number of\nvariables in the {problem} based on the Karush-Kuhn-Tucker (KKT) conditions and\nthe sparse structure of the estimated precision matrix in the previous\niteration. As a result, our algorithm is capable of handling datasets with very\nhigh dimensions that may go beyond the capacity of the existing methods.\nMoreover, for the sub-problem in each iteration, other than solving the primal\nproblem directly, we develop a semismooth Newton augmented Lagrangian algorithm\nwith global linear convergence rate on the dual problem to improve the\nefficiency. Theoretical properties of our proposed algorithm have been\nestablished. In particular, we show that the convergence rate of our algorithm\nis asymptotically superlinear. The high efficiency and promising performance of\nour algorithm are illustrated via extensive simulation studies and real data\napplications, with comparison to several state-of-the-art solvers.",
          "arxiv_id": "2106.13508v2"
        }
      ],
      "2": [
        {
          "title": "spsurv: An R package for semi-parametric survival analysis",
          "year": "2020-03",
          "abstract": "Software development innovations and advances in computing have enabled more\ncomplex and less costly computations in medical research (survival analysis),\nengineering studies (reliability analysis), and social sciences event analysis\n(historical analysis). As a result, many semi-parametric modeling efforts\nemerged when it comes to time-to-event data analysis. In this context, this\nwork presents a flexible Bernstein polynomial (BP) based framework for survival\ndata modeling. This innovative approach is applied to existing families of\nmodels such as proportional hazards (PH), proportional odds (PO), and\naccelerated failure time (AFT) models to estimate unknown baseline functions.\nAlong with this contribution, this work also presents new automated routines in\nR, taking advantage of algorithms available in Stan. The proposed computation\nroutines are tested and explored through simulation studies based on artificial\ndatasets. The tools implemented to fit the proposed statistical models are\ncombined and organized in an R package. Also, the BP based proportional hazards\n(BPPH), proportional odds (BPPO), and accelerated failure time (BPAFT) models\nare illustrated in real applications related to cancer trial data using maximum\nlikelihood (ML) estimation and Markov chain Monte Carlo (MCMC) methods.",
          "arxiv_id": "2003.10548v1"
        },
        {
          "title": "Survival Data Simulation With the R Package rsurv",
          "year": "2024-06",
          "abstract": "In this paper we propose a novel R package, called rsurv, developed for\ngeneral survival data simulation purposes. The package is built under a new\napproach to simulate survival data that depends heavily on the use of dplyr\nverbs. The proposed package allows simulations of survival data from a wide\nrange of regression models, including accelerated failure time (AFT),\nproportional hazards (PH), proportional odds (PO), accelerated hazard (AH),\nYang and Prentice (YP), and extended hazard (EH) models. The package rsurv also\nstands out by its ability to generate survival data from an unlimited number of\nbaseline distributions provided that an implementation of the quantile function\nof the chosen baseline distribution is available in R. Another nice feature of\nthe package rsurv lies in the fact that linear predictors are specified using R\nformulas, facilitating the inclusion of categorical variables, interaction\nterms and offset variables. The functions implemented in the package rsurv can\nalso be employed to simulate survival data with more complex structures, such\nas survival data with different types of censoring mechanisms, survival data\nwith cure fraction, survival data with random effects (frailties), multivarite\nsurvival data, and competing risks survival data.",
          "arxiv_id": "2406.01750v1"
        },
        {
          "title": "TrialEmulation: An R Package to Emulate Target Trials for Causal Analysis of Observational Time-to-event Data",
          "year": "2024-02",
          "abstract": "Randomised controlled trials (RCTs) are regarded as the gold standard for\nestimating causal treatment effects on health outcomes. However, RCTs are not\nalways feasible, because of time, budget or ethical constraints. Observational\ndata such as those from electronic health records (EHRs) offer an alternative\nway to estimate the causal effects of treatments. Recently, the `target trial\nemulation' framework was proposed by Hernan and Robins (2016) to provide a\nformal structure for estimating causal treatment effects from observational\ndata. To promote more widespread implementation of target trial emulation in\npractice, we develop the R package TrialEmulation to emulate a sequence of\ntarget trials using observational time-to-event data, where individuals who\nstart to receive treatment and those who have not been on the treatment at the\nbaseline of the emulated trials are compared in terms of their risks of an\noutcome event. Specifically, TrialEmulation provides (1) data preparation for\nemulating a sequence of target trials, (2) calculation of the inverse\nprobability of treatment and censoring weights to handle treatment switching\nand dependent censoring, (3) fitting of marginal structural models for the\ntime-to-event outcome given baseline covariates, (4) estimation and inference\nof marginal intention to treat and per-protocol effects of the treatment in\nterms of marginal risk differences between treated and untreated for a\nuser-specified target trial population. In particular, TrialEmulation can\naccommodate large data sets (e.g., from EHRs) within memory constraints of R by\nprocessing data in chunks and applying case-control sampling. We demonstrate\nthe functionality of TrialEmulation using a simulated data set that mimics\ntypical observational time-to-event data in practice.",
          "arxiv_id": "2402.12083v1"
        }
      ],
      "3": [
        {
          "title": "Learning state and proposal dynamics in state-space models using differentiable particle filters and neural networks",
          "year": "2024-11",
          "abstract": "State-space models are a popular statistical framework for analysing\nsequential data. Within this framework, particle filters are often used to\nperform inference on non-linear state-space models. We introduce a new method,\nStateMixNN, that uses a pair of neural networks to learn the proposal\ndistribution and transition distribution of a particle filter. Both\ndistributions are approximated using multivariate Gaussian mixtures. The\ncomponent means and covariances of these mixtures are learnt as outputs of\nlearned functions. Our method is trained targeting the log-likelihood, thereby\nrequiring only the observation series, and combines the interpretability of\nstate-space models with the flexibility and approximation power of artificial\nneural networks. The proposed method significantly improves recovery of the\nhidden state in comparison with the state-of-the-art, showing greater\nimprovement in highly non-linear scenarios.",
          "arxiv_id": "2411.15638v2"
        },
        {
          "title": "Local Sequential MCMC for Data Assimilation with Applications in Geoscience",
          "year": "2024-09",
          "abstract": "This paper presents a new data assimilation (DA) scheme based on a sequential\nMarkov Chain Monte Carlo (SMCMC) DA technique [Ruzayqat et al. 2024] which is\nprovably convergent and has been recently used for filtering, particularly for\nhigh-dimensional non-linear, and potentially, non-Gaussian state-space models.\nUnlike particle filters, which can be considered exact methods and can be used\nfor filtering non-linear, non-Gaussian models, SMCMC does not assign weights to\nthe samples/particles, and therefore, the method does not suffer from the issue\nof weight-degeneracy when a relatively small number of samples is used. We\ndesign a localization approach within the SMCMC framework that focuses on\nregions where observations are located and restricts the transition densities\nincluded in the filtering distribution of the state to these regions. This\nresults in immensely reducing the effective degrees of freedom and thus\nimproving the efficiency. We test the new technique on high-dimensional ($d\n\\sim 10^4 - 10^5$) linear Gaussian model and non-linear shallow water models\nwith Gaussian noise with real and synthetic observations. For two of the\nnumerical examples, the observations mimic the data generated by the Surface\nWater and Ocean Topography (SWOT) mission led by NASA, which is a swath of\nocean height observations that changes location at every assimilation time\nstep. We also use a set of ocean drifters' real observations in which the\ndrifters are moving according the ocean kinematics and assumed to have\nuncertain locations at the time of assimilation. We show that when higher\naccuracy is required, the proposed algorithm is superior in terms of efficiency\nand accuracy over competing ensemble methods and the original SMCMC filter.",
          "arxiv_id": "2409.07111v1"
        },
        {
          "title": "Optimal design of large-scale nonlinear Bayesian inverse problems under model uncertainty",
          "year": "2022-11",
          "abstract": "We consider optimal experimental design (OED) for Bayesian nonlinear inverse\nproblems governed by partial differential equations (PDEs) under model\nuncertainty. Specifically, we consider inverse problems in which, in addition\nto the inversion parameters, the governing PDEs include secondary uncertain\nparameters. We focus on problems with infinite-dimensional inversion and\nsecondary parameters and present a scalable computational framework for optimal\ndesign of such problems. The proposed approach enables Bayesian inversion and\nOED under uncertainty within a unified framework. We build on the Bayesian\napproximation error (BAE) approach, to incorporate modeling uncertainties in\nthe Bayesian inverse problem, and methods for A-optimal design of\ninfinite-dimensional Bayesian nonlinear inverse problems. Specifically, a\nGaussian approximation to the posterior at the maximum a posteriori probability\npoint is used to define an uncertainty aware OED objective that is tractable to\nevaluate and optimize. In particular, the OED objective can be computed at a\ncost, in the number of PDE solves, that does not grow with the dimension of the\ndiscretized inversion and secondary parameters. The OED problem is formulated\nas a binary bilevel PDE constrained optimization problem and a greedy\nalgorithm, which provides a pragmatic approach, is used to find optimal\ndesigns. We demonstrate the effectiveness of the proposed approach for a model\ninverse problem governed by an elliptic PDE on a three-dimensional domain. Our\ncomputational results also highlight the pitfalls of ignoring modeling\nuncertainties in the OED and/or inference stages.",
          "arxiv_id": "2211.03952v3"
        }
      ],
      "4": [
        {
          "title": "Incorporating Subsampling into Bayesian Models for High-Dimensional Spatial Data",
          "year": "2023-05",
          "abstract": "Additive spatial statistical models with weakly stationary process\nassumptions have become standard in spatial statistics. However, one\ndisadvantage of such models is the computation time, which rapidly increases\nwith the number of data points. The goal of this article is to apply an\nexisting subsampling strategy to standard spatial additive models and to derive\nthe spatial statistical properties. We call this strategy the ''spatial data\nsubset model'' (SDSM) approach, which can be applied to big datasets in a\ncomputationally feasible way. Our approach has the advantage that one does not\nrequire any additional restrictive model assumptions. That is, computational\ngains increase as model assumptions are removed when using our model framework.\nThis provides one solution to the computational bottlenecks that occur when\napplying methods such as Kriging to ''big data''. We provide several properties\nof this new spatial data subset model approach in terms of moments, sill,\nnugget, and range under several sampling designs. An advantage of our approach\nis that it subsamples without throwing away data, and can be implemented using\ndatasets of any size that can be stored. We present the results of the spatial\ndata subset model approach on simulated datasets, and on a large dataset\nconsists of 150,000 observations of daytime land surface temperatures measured\nby the MODIS instrument onboard the Terra satellite.",
          "arxiv_id": "2305.13221v3"
        },
        {
          "title": "A Scalable Partitioned Approach to Model Massive Nonstationary Non-Gaussian Spatial Datasets",
          "year": "2020-11",
          "abstract": "Nonstationary non-Gaussian spatial data are common in many disciplines,\nincluding climate science, ecology, epidemiology, and social sciences. Examples\ninclude count data on disease incidence and binary satellite data on cloud mask\n(cloud/no-cloud). Modeling such datasets as stationary spatial processes can be\nunrealistic since they are collected over large heterogeneous domains (i.e.,\nspatial behavior differs across subregions). Although several approaches have\nbeen developed for nonstationary spatial models, these have focused primarily\non Gaussian responses. In addition, fitting nonstationary models for large\nnon-Gaussian datasets is computationally prohibitive. To address these\nchallenges, we propose a scalable algorithm for modeling such data by\nleveraging parallel computing in modern high-performance computing systems. We\npartition the spatial domain into disjoint subregions and fit locally\nnonstationary models using a carefully curated set of spatial basis functions.\nThen, we combine the local processes using a novel neighbor-based weighting\nscheme. Our approach scales well to massive datasets (e.g., 1 million samples)\nand can be implemented in nimble, a popular software environment for Bayesian\nhierarchical modeling. We demonstrate our method to simulated examples and two\nlarge real-world datasets pertaining to infectious diseases and remote sensing.",
          "arxiv_id": "2011.13083v1"
        },
        {
          "title": "Modelling Big, Heterogeneous, Non-Gaussian Spatial and Spatio-Temporal Data using FRK",
          "year": "2021-10",
          "abstract": "Non-Gaussian spatial and spatio-temporal data are becoming increasingly\nprevalent, and their analysis is needed in a variety of disciplines. FRK is an\nR package for spatial/spatio-temporal modelling and prediction with very large\ndata sets that, to date, has only supported linear process models and Gaussian\ndata models. In this paper, we describe a major upgrade to FRK that allows for\nnon-Gaussian data to be analysed in a generalised linear mixed model framework.\nThese vastly more general spatial and spatio-temporal models are fitted using\nthe Laplace approximation via the software TMB. The existing functionality of\nFRK is retained with this advance into non-Gaussian models; in particular, it\nallows for automatic basis-function construction, it can handle both\npoint-referenced and areal data simultaneously, and it can predict process\nvalues at any spatial support from these data. This new version of FRK also\nallows for the use of a large number of basis functions when modelling the\nspatial process, and is thus often able to achieve more accurate predictions\nthan previous versions of the package in a Gaussian setting. We demonstrate\ninnovative features in this new version of FRK, highlight its ease of use, and\ncompare it to alternative packages using both simulated and real data sets.",
          "arxiv_id": "2110.02507v3"
        }
      ],
      "5": [
        {
          "title": "Scalable Approximate Bayesian Computation for Growing Network Models via Extrapolated and Sampled Summaries",
          "year": "2020-11",
          "abstract": "Approximate Bayesian computation (ABC) is a simulation-based likelihood-free\nmethod applicable to both model selection and parameter estimation. ABC\nparameter estimation requires the ability to forward simulate datasets from a\ncandidate model, but because the sizes of the observed and simulated datasets\nusually need to match, this can be computationally expensive. Additionally,\nsince ABC inference is based on comparisons of summary statistics computed on\nthe observed and simulated data, using computationally expensive summary\nstatistics can lead to further losses in efficiency. ABC has recently been\napplied to the family of mechanistic network models, an area that has\ntraditionally lacked tools for inference and model choice. Mechanistic models\nof network growth repeatedly add nodes to a network until it reaches the size\nof the observed network, which may be of the order of millions of nodes. With\nABC, this process can quickly become computationally prohibitive due to the\nresource intensive nature of network simulations and evaluation of summary\nstatistics. We propose two methodological developments to enable the use of ABC\nfor inference in models for large growing networks. First, to save time needed\nfor forward simulating model realizations, we propose a procedure to\nextrapolate (via both least squares and Gaussian processes) summary statistics\nfrom small to large networks. Second, to reduce computation time for evaluating\nsummary statistics, we use sample-based rather than census-based summary\nstatistics. We show that the ABC posterior obtained through this approach,\nwhich adds two additional layers of approximation to the standard ABC, is\nsimilar to a classic ABC posterior. Although we deal with growing network\nmodels, both extrapolated summaries and sampled summaries are expected to be\nrelevant in other ABC settings where the data are generated incrementally.",
          "arxiv_id": "2011.04532v1"
        },
        {
          "title": "Generative AI for Bayesian Computation",
          "year": "2023-05",
          "abstract": "Bayesian Generative AI (BayesGen-AI) methods are developed and applied to\nBayesian computation. BayesGen-AI reconstructs the posterior distribution by\ndirectly modeling the parameter of interest as a mapping (a.k.a. deep learner)\nfrom a large simulated dataset. This provides a generator that we can evaluate\nat the observed data and provide draws from the posterior distribution. This\nmethod applies to all forms of Bayesian inference including parametric models,\nlikelihood-free models, prediction and maximum expected utility problems.\nBayesian computation is then equivalent to high dimensional non-parametric\nregression. Bayes Gen-AI main advantage is that it is density-free and\ntherefore provides an alternative to Markov Chain Monte Carlo. It has a number\nof advantages over vanilla generative adversarial networks (GAN) and\napproximate Bayesian computation (ABC) methods due to the fact that the\ngenerator is simpler to learn than a GAN architecture and is more flexible than\nkernel smoothing implicit in ABC methods. Design of the Network Architecture\nrequires careful selection of features (a.k.a. dimension reduction) and\nnonlinear architecture for inference. As a generic architecture, we propose a\ndeep quantile neural network and a uniform base distribution at which to\nevaluate the generator. To illustrate our methodology, we provide two real data\nexamples, the first in traffic flow prediction and the second in building a\nsurrogate for satellite drag data-set. Finally, we conclude with directions for\nfuture research.",
          "arxiv_id": "2305.14972v3"
        },
        {
          "title": "Rare event ABC-SMC$^{2}$",
          "year": "2022-11",
          "abstract": "Approximate Bayesian computation (ABC) is a well-established family of Monte\nCarlo methods for performing approximate Bayesian inference in the case where\nan ``implicit'' model is used for the data: when the data model can be\nsimulated, but the likelihood cannot easily be pointwise evaluated. A\nfundamental property of standard ABC approaches is that the number of Monte\nCarlo points required to achieve a given accuracy scales exponentially with the\ndimension of the data. Prangle et al. (2018) proposes a Markov chain Monte\nCarlo (MCMC) method that uses a rare event sequential Monte Carlo (SMC)\napproach to estimating the ABC likelihood that avoids this exponential scaling,\nand thus allows ABC to be used on higher dimensional data. This paper builds on\nthe work of Prangle et al. (2018) by using the rare event SMC approach within\nan SMC algorithm, instead of within an MCMC algorithm. The new method has a\nsimilar structure to SMC$^{2}$ (Chopin et al., 2013), and requires less tuning\nthan the MCMC approach. We demonstrate the new approach, compared to existing\nABC-SMC methods, on a toy example and on a duplication-divergence random graph\nmodel used for modelling protein interaction networks.",
          "arxiv_id": "2211.02172v1"
        }
      ],
      "6": [
        {
          "title": "MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns",
          "year": "2021-07",
          "abstract": "The decomposition of time series into components is an important task that\nhelps to understand time series and can enable better forecasting. Nowadays,\nwith high sampling rates leading to high-frequency data (such as daily, hourly,\nor minutely data), many real-world datasets contain time series data that can\nexhibit multiple seasonal patterns. Although several methods have been proposed\nto decompose time series better under these circumstances, they are often\ncomputationally inefficient or inaccurate. In this study, we propose Multiple\nSeasonal-Trend decomposition using Loess (MSTL), an extension to the\ntraditional Seasonal-Trend decomposition using Loess (STL) procedure, allowing\nthe decomposition of time series with multiple seasonal patterns. In our\nevaluation on synthetic and a perturbed real-world time series dataset,\ncompared to other decomposition benchmarks, MSTL demonstrates competitive\nresults with lower computational cost. The implementation of MSTL is available\nin the R package forecast.",
          "arxiv_id": "2107.13462v1"
        },
        {
          "title": "Distributed ARIMA Models for Ultra-long Time Series",
          "year": "2020-07",
          "abstract": "Providing forecasts for ultra-long time series plays a vital role in various\nactivities, such as investment decisions, industrial production arrangements,\nand farm management. This paper develops a novel distributed forecasting\nframework to tackle challenges associated with forecasting ultra-long time\nseries by using the industry-standard MapReduce framework. The proposed model\ncombination approach facilitates distributed time series forecasting by\ncombining the local estimators of time series models delivered from worker\nnodes and minimizing a global loss function. In this way, instead of\nunrealistically assuming the data generating process (DGP) of an ultra-long\ntime series stays invariant, we make assumptions only on the DGP of subseries\nspanning shorter time periods. We investigate the performance of the proposed\napproach with AutoRegressive Integrated Moving Average (ARIMA) models using the\nreal data application as well as numerical simulations. Compared to directly\nfitting the whole data with ARIMA models, our approach results in improved\nforecasting accuracy and computational efficiency both in point forecasts and\nprediction intervals, especially for longer forecast horizons. Moreover, we\nexplore some potential factors that may affect the forecasting performance of\nour approach.",
          "arxiv_id": "2007.09577v4"
        },
        {
          "title": "A Semiparametric Approach to the Detection of Change-points in Volatility Dynamics of Financial Data",
          "year": "2022-10",
          "abstract": "One of the most important features of financial time series data is\nvolatility. There are often structural changes in volatility over time, and an\naccurate estimation of the volatility of financial time series requires careful\nidentification of change-points. A common approach to modeling the volatility\nof time series data is the well-known GARCH model. Although the problem of\nchange-point estimation of volatility dynamics derived from the GARCH model has\nbeen considered in the literature, these approaches rely on parametric\nassumptions of the conditional error distribution, which are often violated in\nfinancial time series. This may lead to inaccuracies in change-point detection\nresulting in unreliable GARCH volatility estimates. This paper introduces a\nnovel change-point detection algorithm based on a semiparametric GARCH model.\nThe proposed method retains the structural advantages of the GARCH process\nwhile incorporating the flexibility of nonparametric conditional error\ndistribution. The approach utilizes a penalized likelihood derived from a\nsemiparametric GARCH model and an efficient binary segmentation algorithm. The\nresults show that in terms of change-point estimation and detection accuracy,\nthe semiparametric method outperforms the commonly used Quasi-MLE (QMLE) and\nother variations of GARCH models in wide-ranging scenarios.",
          "arxiv_id": "2210.11520v1"
        }
      ],
      "7": [
        {
          "title": "Fast inference of latent space dynamics in huge relational event networks",
          "year": "2023-03",
          "abstract": "Relational events are a type of social interactions, that sometimes are\nreferred to as dynamic networks. Its dynamics typically depends on emerging\npatterns, so-called endogenous variables, or external forces, referred to as\nexogenous variables. Comprehensive information on the actors in the network,\nespecially for huge networks, is rare, however. A latent space approach in\nnetwork analysis has been a popular way to account for unmeasured covariates\nthat are driving network configurations. Bayesian and EM-type algorithms have\nbeen proposed for inferring the latent space, but both the sheer size many\nsocial network applications as well as the dynamic nature of the process, and\ntherefore the latent space, make computations prohibitively expensive. In this\nwork we propose a likelihood-based algorithm that can deal with huge relational\nevent networks. We propose a hierarchical strategy for inferring network\ncommunity dynamics embedded into an interpretable latent space. Node dynamics\nare described by smooth spline processes. To make the framework feasible for\nlarge networks we borrow from machine learning optimization methodology.\nModel-based clustering is carried out via a convex clustering penalization,\nencouraging shared trajectories for ease of interpretation. We propose a\nmodel-based approach for separating macro-microstructures and perform a\nhierarchical analysis within successive hierarchies. The method can fit\nmillions of nodes on a public Colab GPU in a few minutes. The code and a\ntutorial are available in a Github repository.",
          "arxiv_id": "2303.17460v1"
        },
        {
          "title": "Root and community inference on the latent growth process of a network",
          "year": "2021-07",
          "abstract": "Many existing statistical models for networks overlook the fact that many\nreal world networks are formed through a growth process. To address this, we\nintroduce the PAPER (Preferential Attachment Plus Erd\\H{o}s--R\\'{e}nyi) model\nfor random networks, where we let a random network G be the union of a\npreferential attachment (PA) tree T and additional Erd\\H{o}s--R\\'{e}nyi (ER)\nrandom edges. The PA tree component captures the underlying growth/recruitment\nprocess of a network where vertices and edges are added sequentially, while the\nER component can be regarded as random noise. Given only a single snapshot of\nthe final network G, we study the problem of constructing confidence sets for\nthe early history, in particular the root node, of the unobserved growth\nprocess; the root node can be patient zero in a disease infection network or\nthe source of fake news in a social media network. We propose an inference\nalgorithm based on Gibbs sampling that scales to networks with millions of\nnodes and provide theoretical analysis showing that the expected size of the\nconfidence set is small so long as the noise level of the ER edges is not too\nlarge. We also propose variations of the model in which multiple growth\nprocesses occur simultaneously, reflecting the growth of multiple communities,\nand we use these models to provide a new approach to community detection.",
          "arxiv_id": "2107.00153v4"
        },
        {
          "title": "Actor Heterogeneity and Explained Variance in Network Models -- A Scalable Approach through Variational Approximations",
          "year": "2022-04",
          "abstract": "The analysis of network data has gained considerable interest in recent\nyears. This also includes the analysis of large, high-dimensional networks with\nhundreds and thousands of nodes. While exponential random graph models serve as\nworkhorse for network data analyses, their applicability to very large networks\nis problematic via classical inference such as maximum likelihood or exact\nBayesian estimation owing to scaling and instability issues. The latter trace\nfrom the fact that classical network statistics consider nodes as exchangeable,\ni.e., actors in the network are assumed to be homogeneous. This is often\nquestionable. One way to circumvent the restrictive assumption is to include\nactor-specific random effects, which account for unobservable heterogeneity.\nHowever, this increases the number of unknowns considerably, thus making the\nmodel highly-parameterized. As a solution even for very large networks, we\npropose a scalable approach based on variational approximations, which not only\nleads to numerically stable estimation but is also applicable to\nhigh-dimensional directed as well as undirected networks. We furthermore\ndemonstrate that including node-specific covariates can reduce node\nheterogeneity, which we facilitate through versatile prior formulations and a\nnew measure that we call posterior explained variance. We illustrate our\napproach in three diverse examples, covering network data from the Italian\nParliament, international arms trading, and Facebook; and conduct detailed\nsimulation studies.",
          "arxiv_id": "2204.14214v2"
        }
      ],
      "8": [
        {
          "title": "AL-SPCE -- Reliability analysis for nondeterministic models using stochastic polynomial chaos expansions and active learning",
          "year": "2025-07",
          "abstract": "Reliability analysis typically relies on deterministic simulators, which\nyield repeatable outputs for identical inputs. However, many real-world systems\ndisplay intrinsic randomness, requiring stochastic simulators whose outputs are\nrandom variables. This inherent variability must be accounted for in\nreliability analysis. While Monte Carlo methods can handle this, their high\ncomputational cost is often prohibitive. To address this, stochastic emulators\nhave emerged as efficient surrogate models capable of capturing the random\nresponse of simulators at reduced cost. Although promising, current methods\nstill require large training sets to produce accurate reliability estimates,\nwhich limits their practicality for expensive simulations. This work introduces\nan active learning framework to further reduce the computational burden of\nreliability analysis using stochastic emulators. We focus on stochastic\npolynomial chaos expansions (SPCE) and propose a novel learning function that\ntargets regions of high predictive uncertainty relevant to failure probability\nestimation. To quantify this uncertainty, we exploit the asymptotic normality\nof the maximum likelihood estimator. The resulting method, named active\nlearning stochastic polynomial chaos expansions (AL-SPCE), is applied to three\ntest cases. Results demonstrate that AL-SPCE maintains high accuracy in\nreliability estimates while significantly improving efficiency compared to\nconventional surrogate-based methods and direct Monte Carlo simulation. This\nconfirms the potential of active learning in enhancing the practicality of\nstochastic reliability analysis for complex, computationally expensive models.",
          "arxiv_id": "2507.04553v1"
        },
        {
          "title": "Stochastic polynomial chaos expansions to emulate stochastic simulators",
          "year": "2022-02",
          "abstract": "In the context of uncertainty quantification, computational models are\nrequired to be repeatedly evaluated. This task is intractable for costly\nnumerical models. Such a problem turns out to be even more severe for\nstochastic simulators, the output of which is a random variable for a given set\nof input parameters. To alleviate the computational burden, surrogate models\nare usually constructed and evaluated instead. However, due to the random\nnature of the model response, classical surrogate models cannot be applied\ndirectly to the emulation of stochastic simulators. To efficiently represent\nthe probability distribution of the model output for any given input values, we\ndevelop a new stochastic surrogate model called stochastic polynomial chaos\nexpansions. To this aim, we introduce a latent variable and an additional noise\nvariable, on top of the well-defined input variables, to reproduce the\nstochasticity. As a result, for a given set of input parameters, the model\noutput is given by a function of the latent variable with an additive noise,\nthus a random variable. In this paper, we propose an adaptive algorithm which\ndoes not require repeated runs of the simulator for the same input parameters.\nThe performance of the proposed method is compared with the generalized lambda\nmodel and a state-of-the-art kernel estimator on two case studies in\nmathematical finance and epidemiology and on an analytical example whose\nresponse distribution is bimodal. The results show that the proposed method is\nable to accurately represent general response distributions, i.e., not only\nnormal or unimodal ones. In terms of accuracy, it generally outperforms both\nthe generalized lambda model and the kernel density estimator.",
          "arxiv_id": "2202.03344v3"
        },
        {
          "title": "Uncertainty-aware multi-fidelity surrogate modeling with noisy data",
          "year": "2024-01",
          "abstract": "Emulating high-accuracy computationally expensive models is crucial for tasks\nrequiring numerous model evaluations, such as uncertainty quantification and\noptimization. When lower-fidelity models are available, they can be used to\nimprove the predictions of high-fidelity models. Multi-fidelity surrogate\nmodels combine information from sources of varying fidelities to construct an\nefficient surrogate model. However, in real-world applications, uncertainty is\npresent in both high- and low-fidelity models due to measurement or numerical\nnoise, as well as lack of knowledge due to the limited experimental design\nbudget. This paper introduces a comprehensive framework for multi-fidelity\nsurrogate modeling that handles noise-contaminated data and is able to estimate\nthe underlying noise-free high-fidelity model. Our methodology quantitatively\nincorporates the different types of uncertainty affecting the problem and\nemphasizes on delivering precise estimates of the uncertainty in its\npredictions both with respect to the underlying high-fidelity model and unseen\nnoise-contaminated high-fidelity observations, presented through confidence and\nprediction intervals, respectively. Additionally, the proposed framework offers\na natural approach to combining physical experiments and computational models\nby treating noisy experimental data as high-fidelity sources and white-box\ncomputational models as their low-fidelity counterparts. The effectiveness of\nour methodology is showcased through synthetic examples and a wind turbine\napplication.",
          "arxiv_id": "2401.06447v2"
        }
      ],
      "9": [
        {
          "title": "Two-Sample Test for High-Dimensional Covariance Matrices: a normal-reference approach",
          "year": "2022-12",
          "abstract": "Testing the equality of the covariance matrices of two high-dimensional\nsamples is a fundamental inference problem in statistics. Several tests have\nbeen proposed but they are either too liberal or too conservative when the\nrequired assumptions are not satisfied which attests that they are not always\napplicable in real data analysis. To overcome this difficulty, a\nnormal-reference test is proposed and studied in this paper. It is shown that\nunder some regularity conditions and the null hypothesis, the proposed test\nstatistic and a chi-square-type mixture have the same limiting distribution. It\nis then justified to approximate the null distribution of the proposed test\nstatistic using that of the chi-square-type mixture. The distribution of the\nchi-square-type mixture can be well approximated using a three-cumulant matched\nchi-square-approximation with its approximation parameters consistently\nestimated from the data. The asymptotic power of the proposed test under a\nlocal alternative is also established. Simulation studies and a real data\nexample demonstrate that in terms of size control, the proposed test\noutperforms the existing competitors substantially.",
          "arxiv_id": "2212.12338v1"
        },
        {
          "title": "Fast Conservative Monte Carlo Confidence Intervals",
          "year": "2024-05",
          "abstract": "Extant \"fast\" algorithms for Monte Carlo confidence sets are limited to\nunivariate shift parameters for the one-sample and two-sample problems using\nthe sample mean as the test statistic; moreover, some do not converge reliably\nand most do not produce conservative confidence sets. We outline general\nmethods for constructing confidence sets for real-valued and multidimensional\nparameters by inverting Monte Carlo tests using any test statistic and a broad\nrange of randomization schemes. The method exploits two facts that, to our\nknowledge, had not been combined: (i) there are Monte Carlo tests that are\nconservative despite relying on simulation, and (ii) since the coverage\nprobability of confidence sets depends only on the significance level of the\ntest of the true null, every null can be tested using the same Monte Carlo\nsample. The Monte Carlo sample can be arbitrarily small, although the highest\nnontrivial attainable confidence level generally increases as the number $N$ of\nMonte Carlo replicates increases. We present open-source Python and R\nimplementations of new algorithms to compute conservative confidence sets for\nreal-valued parameters from Monte Carlo tests, for test statistics and\nrandomization schemes that yield $P$-values that are monotone or weakly\nunimodal in the parameter, with the data and Monte Carlo sample held fixed. In\nthis case, the new method finds conservative confidence sets for real-valued\nparameters in $O(n)$ time, where $n$ is the number of data. The values of some\ntest statistics for different simulations and parameter values have a simple\nrelationship that makes more savings possible.",
          "arxiv_id": "2405.05238v3"
        },
        {
          "title": "Two-sample Behrens--Fisher problems for high-dimensional data: a normal reference F-type test",
          "year": "2022-12",
          "abstract": "The problem of testing the equality of mean vectors for high-dimensional data\nhas been intensively investigated in the literature. However, most of the\nexisting tests impose strong assumptions on the underlying group covariance\nmatrices which may not be satisfied or hardly be checked in practice. In this\narticle, an F-type test for two-sample Behrens--Fisher problems for\nhigh-dimensional data is proposed and studied. When the two samples are\nnormally distributed and when the null hypothesis is valid, the proposed F-type\ntest statistic is shown to be an F-type mixture, a ratio of two independent\nchi-square-type mixtures. Under some regularity conditions and the null\nhypothesis, it is shown that the proposed F-type test statistic and the above\nF-type mixture have the same normal and non-normal limits. It is then justified\nto approximate the null distribution of the proposed F-type test statistic by\nthat of the F-type mixture, resulting in the so-called normal reference F-type\ntest. Since the F-type mixture is a ratio of two independent chi-square-type\nmixtures, we employ the Welch--Satterthwaite chi-square-approximation to the\ndistributions of the numerator and the denominator of the F-type mixture\nrespectively, resulting in an approximation F-distribution whose degrees of\nfreedom can be consistently estimated from the data. The asymptotic power of\nthe proposed F-type test is established. Two simulation studies are conducted\nand they show that in terms of size control, the proposed F-type test\noutperforms two existing competitors. The proposed F-type test is also\nillustrated by a real data example.",
          "arxiv_id": "2212.13372v1"
        }
      ],
      "10": [
        {
          "title": "Fast Computation of Highly G-optimal Exact Designs via Particle Swarm Optimization",
          "year": "2022-06",
          "abstract": "Computing proposed exact $G$-optimal designs for response surface models is a\ndifficult computation that has received incremental improvements via algorithm\ndevelopment in the last two-decades. These optimal designs have not been\nconsidered widely in applications in part due to the difficulty and cost\ninvolved with computing them. Three primary algorithms for constructing exact\n$G$-optimal designs are presented in the literature: the coordinate exchange\n(CEXCH), a genetic algorithm (GA), and the relatively new $G$-optimal via\n$I_\\lambda$-optimality algorithm ($G(I_\\lambda)$-CEXCH) which was developed in\npart to address large computational cost. Particle swarm optimization (PSO) has\nachieved widespread use in many applications, but to date, its broad-scale\nsuccess notwithstanding, has seen relatively few applications in optimal design\nproblems. In this paper we develop an extension of PSO to adapt it to the\noptimal design problem. We then employ PSO to generate optimal designs for\nseveral scenarios covering $K = 1, 2, 3, 4, 5$ design factors, which are common\nexperimental sizes in industrial experiments. We compare these results to all\n$G$-optimal designs published in last two decades of literature. Published\n$G$-optimal designs generated by GA for $K=1, 2, 3$ factors have stood\nunchallenged for 14 years. We demonstrate that PSO has found improved\n$G$-optimal designs for these scenarios, and it does this with comparable\ncomputational cost to the state-of-the-art algorithm $G(I_\\lambda)$-CEXCH.\nFurther, we show that PSO is able to produce equal or better $G$-optimal\ndesigns for $K= 4, 5$ factors than those currently known. These results suggest\nthat PSO is superior to existing approaches for efficiently generating highly\n$G$-optimal designs.",
          "arxiv_id": "2206.06498v1"
        },
        {
          "title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design",
          "year": "2021-03",
          "abstract": "We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of\nadaptive Bayesian experimental design that allows experiments to be run in\nreal-time. Traditional sequential Bayesian optimal experimental design\napproaches require substantial computation at each stage of the experiment.\nThis makes them unsuitable for most real-world applications, where decisions\nmust typically be made quickly. DAD addresses this restriction by learning an\namortized design network upfront and then using this to rapidly run (multiple)\nadaptive experiments at deployment time. This network represents a design\npolicy which takes as input the data from previous steps, and outputs the next\ndesign using a single forward pass; these design decisions can be made in\nmilliseconds during the live experiment. To train the network, we introduce\ncontrastive information bounds that are suitable objectives for the sequential\nsetting, and propose a customized network architecture that exploits key\nsymmetries. We demonstrate that DAD successfully amortizes the process of\nexperimental design, outperforming alternative strategies on a number of\nproblems.",
          "arxiv_id": "2103.02438v2"
        },
        {
          "title": "Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation",
          "year": "2020-02",
          "abstract": "Implicit stochastic models, where the data-generation distribution is\nintractable but sampling is possible, are ubiquitous in the natural sciences.\nThe models typically have free parameters that need to be inferred from data\ncollected in scientific experiments. A fundamental question is how to design\nthe experiments so that the collected data are most useful. The field of\nBayesian experimental design advocates that, ideally, we should choose designs\nthat maximise the mutual information (MI) between the data and the parameters.\nFor implicit models, however, this approach is severely hampered by the high\ncomputational cost of computing posteriors and maximising MI, in particular\nwhen we have more than a handful of design variables to optimise. In this\npaper, we propose a new approach to Bayesian experimental design for implicit\nmodels that leverages recent advances in neural MI estimation to deal with\nthese issues. We show that training a neural network to maximise a lower bound\non MI allows us to jointly determine the optimal design and the posterior.\nSimulation studies illustrate that this gracefully extends Bayesian\nexperimental design for implicit models to higher design dimensions.",
          "arxiv_id": "2002.08129v3"
        }
      ],
      "11": [
        {
          "title": "Packaging code for reproducible research in the public sector",
          "year": "2023-05",
          "abstract": "The effective and ethical use of data to inform decision-making offers huge\nvalue to the public sector, especially when delivered by transparent,\nreproducible, and robust data processing workflows. One way that governments\nare unlocking this value is through making their data publicly available,\nallowing more people and organisations to derive insights. However, open data\nis not enough in many cases: publicly available datasets need to be accessible\nin an analysis-ready form from popular data science tools, such as R and\nPython, for them to realise their full potential.\n  This paper explores ways to maximise the impact of open data with reference\nto a case study of packaging code to facilitate reproducible analysis. We\npresent the jtstats project, which consists of R and Python packages for\nimporting, processing, and visualising large and complex datasets representing\njourney times, for many modes and purposes at multiple geographic levels,\nreleased by the UK Department of Transport. jtstats shows how domain specific\npackages can enable reproducible research within the public sector and beyond,\nsaving duplicated effort and reducing the risks of errors from repeated\nanalyses. We hope that the jtstats project inspires others, particularly those\nin the public sector, to add value to their data sets by making them more\naccessible.",
          "arxiv_id": "2305.16205v1"
        },
        {
          "title": "Facilitating team-based data science: lessons learned from the DSC-WAV project",
          "year": "2021-06",
          "abstract": "While coursework provides undergraduate data science students with some\nrelevant analytic skills, many are not given the rich experiences with data and\ncomputing they need to be successful in the workplace. Additionally, students\noften have limited exposure to team-based data science and the principles and\ntools of collaboration that are encountered outside of school. In this paper,\nwe describe the DSC-WAV program, an NSF-funded data science workforce\ndevelopment project in which teams of undergraduate sophomores and juniors work\nwith a local non-profit organization on a data-focused problem. To help\nstudents develop a sense of agency and improve confidence in their technical\nand non-technical data science skills, the project promoted a team-based\napproach to data science, adopting several processes and tools intended to\nfacilitate this collaboration. Evidence from the project evaluation, including\nparticipant survey and interview data, is presented to document the degree to\nwhich the project was successful in engaging students in team-based data\nscience, and how the project changed the students' perceptions of their\ntechnical and non-technical skills. We also examine opportunities for\nimprovement and offer insight to other data science educators who may want to\nimplement a similar team-based approach to data science projects at their own\ninstitutions.",
          "arxiv_id": "2106.11209v4"
        },
        {
          "title": "Best Practices in Statistical Computing",
          "year": "2021-01",
          "abstract": "The world is becoming increasingly complex, both in terms of the rich sources\nof data we have access to as well as in terms of the statistical and\ncomputational methods we can use on those data. These factors create an\never-increasing risk for errors in our code and sensitivity in our findings to\ndata preparation and execution of complex statistical and computing methods.\nThe consequences of coding and data mistakes can be substantial. Openness\n(e.g., providing others with data code) and transparency (e.g., requiring that\ndata processing and code follow standards) are two key solutions to help\nalleviate concerns about replicability and errors. In this paper, we describe\nthe key steps for implementing a code quality assurance (QA) process for\nresearchers to follow to improve their coding practices throughout a project to\nassure the quality of the final data, code, analyses and ultimately the\nresults. These steps include: (i) adherence to principles for code writing and\nstyle that follow best practices, (ii) clear written documentation that\ndescribes code, workflow and key analytic decisions; (iii) careful version\ncontrol, (iv) good data management; and (iv) regular testing and review.\nFollowing all these steps will greatly improve the ability of a study to assure\nresults are accurate and reproducible. The responsibility for code QA falls not\nonly on individual researchers but institutions, journals, and funding agencies\nas well.",
          "arxiv_id": "2101.11857v3"
        }
      ],
      "12": [
        {
          "title": "A Bayesian approach for clustering skewed data using mixtures of multivariate normal-inverse Gaussian distributions",
          "year": "2020-05",
          "abstract": "Non-Gaussian mixture models are gaining increasing attention for mixture\nmodel-based clustering particularly when dealing with data that exhibit\nfeatures such as skewness and heavy tails. Here, such a mixture distribution is\npresented, based on the multivariate normal inverse Gaussian (MNIG)\ndistribution. For parameter estimation of the mixture, a Bayesian approach via\nGibbs sampler is used; for this, a novel approach to simulate univariate\ngeneralized inverse Gaussian random variables and matrix generalized inverse\nGaussian random matrices is provided. The proposed algorithm will be applied to\nboth simulated and real data. Through simulation studies and real data\nanalysis, we show parameter recovery and that our approach provides competitive\nclustering results compared to other clustering approaches.",
          "arxiv_id": "2005.02585v1"
        },
        {
          "title": "Escaping the curse of dimensionality in Bayesian model based clustering",
          "year": "2020-06",
          "abstract": "Bayesian mixture models are widely used for clustering of high-dimensional\ndata with appropriate uncertainty quantification. However, as the dimension of\nthe observations increases, posterior inference often tends to favor too many\nor too few clusters. This article explains this behavior by studying the random\npartition posterior in a non-standard setting with a fixed sample size and\nincreasing data dimensionality. We provide conditions under which the finite\nsample posterior tends to either assign every observation to a different\ncluster or all observations to the same cluster as the dimension grows.\nInterestingly, the conditions do not depend on the choice of clustering prior,\nas long as all possible partitions of observations into clusters have positive\nprior probabilities, and hold irrespective of the true data-generating model.\nWe then propose a class of latent mixtures for Bayesian clustering (Lamb) on a\nset of low-dimensional latent variables inducing a partition on the observed\ndata. The model is amenable to scalable posterior inference and we show that it\ncan avoid the pitfalls of high-dimensionality under mild assumptions. The\nproposed approach is shown to have good performance in simulation studies and\nan application to inferring cell types based on scRNAseq.",
          "arxiv_id": "2006.02700v5"
        },
        {
          "title": "Distributed Bayesian clustering using finite mixture of mixtures",
          "year": "2020-03",
          "abstract": "In many modern applications, there is interest in analyzing enormous data\nsets that cannot be easily moved across computers or loaded into memory on a\nsingle computer. In such settings, it is very common to be interested in\nclustering. Existing distributed clustering algorithms are mostly distance or\ndensity based without a likelihood specification, precluding the possibility of\nformal statistical inference. Model-based clustering allows statistical\ninference, yet research on distributed inference has emphasized nonparametric\nBayesian mixture models over finite mixture models. To fill this gap, we\nintroduce a nearly embarrassingly parallel algorithm for clustering under a\nBayesian overfitted finite mixture of Gaussian mixtures, which we term\ndistributed Bayesian clustering (DIB-C). DIB-C can flexibly accommodate data\nsets with various shapes (e.g. skewed or multi-modal). With data randomly\npartitioned and distributed, we first run Markov chain Monte Carlo in an\nembarrassingly parallel manner to obtain local clustering draws and then refine\nacross workers for a final clustering estimate based on any loss function on\nthe space of partitions. DIB-C can also estimate cluster densities, quickly\nclassify new subjects and provide a posterior predictive distribution. Both\nsimulation studies and real data applications show superior performance of\nDIB-C in terms of robustness and computational efficiency.",
          "arxiv_id": "2003.13936v2"
        }
      ],
      "13": [
        {
          "title": "Bike Share's Impact on COVID-19 Transmission and Bike Share's Responses to COVID-19: A case study of Washington DC",
          "year": "2022-05",
          "abstract": "Due to the wide-ranging travel restrictions and lockdowns applied to limit\nthe diffusion of the SARS-CoV2 virus, the coronavirus disease of 2019\n(COVID-19) pandemic has had an immediate and significant effect on human\nmobility at the global, national, and local levels. At the local level,\nbike-sharing played a significant role in urban transport during the pandemic\nsince riders could travel outdoors with reduced infection risk. However, based\non different data resources, this non-motorized mode of transportation was\nstill negatively affected by the pandemic (i.e., relative reduction in\nridership). This study has two objectives: 1) to investigate the impact of the\nCOVID-19 pandemic on the numbers and duration of trips conducted through a\nbike-sharing system -- the Capital Bikeshare in Washington, DC, USA; and 2) to\nexplore whether land use and household income in the nation's capital influence\nthe spatial variation of ridership during the pandemic. Towards realizing these\nobjectives, this research looks at the relationship between bike sharing and\nCOVID-19 transmission as a two-directional relationship rather than a\none-directional causal relationship. Accordingly, this study models i) the\nimpact of COVID-19 infection numbers and rates on the use of the Capital\nBikeshare system and ii) the risk of COVID-19 transmission among individual\nbike-sharing users. In other words, we examine i) the cyclist's behavior as a\nfunction of the COVID-19 transmission evolution in an urban environment and ii)\nthe possible relationship between the bike share usage and the COVID-19\ntransmission through adopting a probabilistic contagion model. The findings\nshow the risk of using a bike-sharing system during the pandemic and whether\nbike sharing remains a healthier alternative mode of transportation in terms of\ninfection risk.",
          "arxiv_id": "2205.05011v2"
        },
        {
          "title": "Dynamic SIR/SEIR-like models comprising a time-dependent transmission rate: Hamiltonian Monte Carlo approach with applications to COVID-19",
          "year": "2023-01",
          "abstract": "A study of changes in the transmission of a disease, in particular, a new\ndisease like COVID-19, requires very flexible models which can capture, among\nothers, the effects of non-pharmacological and pharmacological measures,\nchanges in population behaviour and random events. In this work, we give\npriority to data-driven approaches and choose to avoid a priori and ad-hoc\nmethods. We introduce a generalised family of epidemiologically informed\nmechanistic models, guided by Ordinary Differential Equations and embedded in a\nprobabilistic model. The mechanistic models SIKR and SEMIKR with K Infectious\nand M Exposed sub-compartments (resulting in non-exponential infectious and\nexposed periods) are enriched with a time-dependent transmission rate,\nparametrized using Bayesian P-splines. This enables an extensive flexibility in\nthe transmission dynamics, with no ad-hoc intervention, while maintaining good\ndifferentiability properties. Our probabilistic model relies on the solutions\nof the mechanistic model and benefits from access to the information about\nunder-reporting of new infected cases, a crucial property when studying\ndiseases with a large fraction of asymptomatic infections. Such a model can be\nefficiently differentiated, which facilitates the use of Hamiltonian Monte\nCarlo for sampling from the posterior distribution of the model parameters. The\nfeatures and advantages of the proposed approach are demonstrated through\ncomparison with state-of-the-art methods using a synthetic dataset.\nFurthermore, we successfully apply our methodology to the study of the\ntransmission dynamics of COVID-19 in the Basque Country (Spain) for almost a\nyear, from mid February 2020 to the end of January 2021.",
          "arxiv_id": "2301.06385v1"
        },
        {
          "title": "The Framework for the Prediction of the Critical Turning Period for Outbreak of COVID-19 Spread in China based on the iSEIR Model",
          "year": "2020-04",
          "abstract": "The goal of this study is to establish a general framework for predicting the\nso-called critical Turning Period in an infectious disease epidemic such as the\nCOVID-19 outbreak in China early this year. This framework enabled a timely\nprediction of the turning period when applied to Wuhan COVID-19 epidemic and\ninformed the relevant authority for taking appropriate and timely actions to\ncontrol the epidemic. It is expected to provide insightful information on\nturning period for the world's current battle against the COVID-19 pandemic.\nThe underlying mathematical model in our framework is the individual\nSusceptible-Exposed- Infective-Removed (iSEIR) model, which is a set of\ndifferential equations extending the classic SEIR model. We used the observed\ndaily cases of COVID-19 in Wuhan from February 6 to 10, 2020 as the input to\nthe iSEIR model and were able to generate the trajectory of COVID-19 cases\ndynamics for the following days at midnight of February 10 based on the updated\nmodel, from which we predicted that the turning period of CIVID-19 outbreak in\nWuhan would arrive within one week after February 14. This prediction turned to\nbe timely and accurate, providing adequate time for the government, hospitals,\nessential industry sectors and services to meet peak demands and to prepare\naftermath planning. Our study also supports the observed effectiveness on\nflatting the epidemic curve by decisively imposing the Lockdown and Isolation\nControl Program in Wuhan since January 23, 2020. The Wuhan experience provides\nan exemplary lesson for the whole world to learn in combating COVID-19.",
          "arxiv_id": "2004.02278v1"
        }
      ],
      "14": [
        {
          "title": "Efficient Rejection Sampling in the Entropy-Optimal Range",
          "year": "2025-04",
          "abstract": "The problem of generating a random variate $X$ from a finite discrete\nprobability distribution $P$ using an entropy source of independent unbiased\ncoin flips is considered. The Knuth and Yao complexity theory of nonuniform\nrandom number generation furnishes a family of \"entropy-optimal\" sampling\nalgorithms that consume between $H(P)$ and $H(P)+2$ coin flips per generated\noutput, where $H$ is the Shannon entropy function. However, the space\ncomplexity of entropy-optimal samplers scales exponentially with the number of\nbits required to encode $P$. This article introduces a family of efficient\nrejection samplers and characterizes their entropy, space, and time complexity.\nWithin this family is a distinguished sampling algorithm that requires\nlinearithmic space and preprocessing time, and whose expected entropy cost\nalways falls in the entropy-optimal range $[H(P), H(P)+2)$. No previous sampler\nfor discrete probability distributions is known to achieve these\ncharacteristics. Numerical experiments demonstrate performance improvements in\nruntime and entropy of the proposed algorithm compared to the celebrated alias\nmethod.",
          "arxiv_id": "2504.04267v1"
        },
        {
          "title": "Efficient Online Random Sampling via Randomness Recycling",
          "year": "2025-05",
          "abstract": "``Randomness recycling'' is a powerful algorithmic technique for reusing a\nfraction of the random information consumed by a probabilistic algorithm to\nreduce its entropy requirements. This article presents a family of randomness\nrecycling algorithms for efficiently sampling a sequence $X_1, X_2, X_3, \\dots$\nof discrete random variables whose joint distribution follows an arbitrary\nstochastic process. We develop randomness recycling techniques to reduce the\nentropy cost of a variety of prominent sampling algorithms, which include\nuniform sampling, inverse transform sampling, lookup-table sampling, alias\nsampling, and discrete distribution generating (DDG) tree sampling. Our method\nachieves an expected amortized entropy cost of $H(X_1,\\dots,X_k)/k +\n\\varepsilon$ input bits per output sample using $O(\\log(1/\\varepsilon))$ space\nas $k\\to\\infty$, which is arbitrarily close to the optimal Shannon entropy rate\nof $H(X_1,\\dots,X_k)/k$ bits per sample. The combination of space, time, and\nentropy properties of our method improves upon the Knuth and Yao\nentropy-optimal algorithm and Han and Hoshi interval algorithm for sampling a\ndiscrete random sequence.\n  On the empirical side, we show that randomness recycling enables\nstate-of-the-art runtime performance on the Fisher-Yates shuffle when using a\ncryptographically secure pseudorandom number generator; and it can also speed\nup discrete Gaussian samplers. Accompanying the manuscript is a performant\nsoftware library in the C programming language that uses randomness recycling\nto accelerate several existing algorithms for random sampling.",
          "arxiv_id": "2505.18879v2"
        },
        {
          "title": "Optimal Approximate Sampling from Discrete Probability Distributions",
          "year": "2020-01",
          "abstract": "This paper addresses a fundamental problem in random variate generation:\ngiven access to a random source that emits a stream of independent fair bits,\nwhat is the most accurate and entropy-efficient algorithm for sampling from a\ndiscrete probability distribution $(p_1, \\dots, p_n)$, where the probabilities\nof the output distribution $(\\hat{p}_1, \\dots, \\hat{p}_n)$ of the sampling\nalgorithm must be specified using at most $k$ bits of precision? We present a\ntheoretical framework for formulating this problem and provide new techniques\nfor finding sampling algorithms that are optimal both statistically (in the\nsense of sampling accuracy) and information-theoretically (in the sense of\nentropy consumption). We leverage these results to build a system that, for a\nbroad family of measures of statistical accuracy, delivers a sampling algorithm\nwhose expected entropy usage is minimal among those that induce the same\ndistribution (i.e., is \"entropy-optimal\") and whose output distribution\n$(\\hat{p}_1, \\dots, \\hat{p}_n)$ is a closest approximation to the target\ndistribution $(p_1, \\dots, p_n)$ among all entropy-optimal sampling algorithms\nthat operate within the specified $k$-bit precision. This optimal approximate\nsampler is also a closer approximation than any (possibly entropy-suboptimal)\nsampler that consumes a bounded amount of entropy with the specified precision,\na class which includes floating-point implementations of inversion sampling and\nrelated methods found in many software libraries. We evaluate the accuracy,\nentropy consumption, precision requirements, and wall-clock runtime of our\noptimal approximate sampling algorithms on a broad set of distributions,\ndemonstrating the ways that they are superior to existing approximate samplers\nand establishing that they often consume significantly fewer resources than are\nneeded by exact samplers.",
          "arxiv_id": "2001.04555v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T20:07:27Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}