{
  "topics": {
    "data": {
      "0": {
        "name": "0_COVID_chest_lung_CT",
        "keywords": [
          [
            "COVID",
            0.028887420344539068
          ],
          [
            "chest",
            0.014744155266164433
          ],
          [
            "lung",
            0.014255721026612874
          ],
          [
            "CT",
            0.013142934905122108
          ],
          [
            "ray",
            0.011025191874588822
          ],
          [
            "learning",
            0.009204062892410575
          ],
          [
            "medical",
            0.009202284725929488
          ],
          [
            "diagnosis",
            0.008976828903604305
          ],
          [
            "classification",
            0.008871829054879139
          ],
          [
            "CXR",
            0.008766898390010427
          ]
        ],
        "count": 1462
      },
      "1": {
        "name": "1_image_images_adversarial_style",
        "keywords": [
          [
            "image",
            0.014049236246031965
          ],
          [
            "images",
            0.010611198544743981
          ],
          [
            "adversarial",
            0.010441635094959172
          ],
          [
            "style",
            0.009962276037110461
          ],
          [
            "GAN",
            0.009804141431061328
          ],
          [
            "face",
            0.008988442539929063
          ],
          [
            "GANs",
            0.008221060581832331
          ],
          [
            "models",
            0.008138134787697954
          ],
          [
            "generation",
            0.007966448344603161
          ],
          [
            "model",
            0.007630751674305509
          ]
        ],
        "count": 1109
      },
      "2": {
        "name": "2_segmentation_medical_image segmentation_medical image",
        "keywords": [
          [
            "segmentation",
            0.02887245022832302
          ],
          [
            "medical",
            0.0177027112761198
          ],
          [
            "image segmentation",
            0.015322541676737363
          ],
          [
            "medical image",
            0.013574580455778959
          ],
          [
            "medical image segmentation",
            0.01244543939744544
          ],
          [
            "Segmentation",
            0.010836674358216536
          ],
          [
            "image",
            0.010081561791181418
          ],
          [
            "Net",
            0.009184209120203167
          ],
          [
            "Medical",
            0.008803056460332961
          ],
          [
            "3D",
            0.007866927479359501
          ]
        ],
        "count": 1101
      },
      "3": {
        "name": "3_imaging_phase_microscopy_resolution",
        "keywords": [
          [
            "imaging",
            0.021079174105129925
          ],
          [
            "phase",
            0.017216498698656577
          ],
          [
            "microscopy",
            0.014658631958853998
          ],
          [
            "resolution",
            0.011443227337584953
          ],
          [
            "optical",
            0.010566863145351943
          ],
          [
            "reconstruction",
            0.010039476175664856
          ],
          [
            "light",
            0.009950193575400144
          ],
          [
            "high",
            0.00969645749094443
          ],
          [
            "single",
            0.009388898402268772
          ],
          [
            "method",
            0.008300974100164098
          ]
        ],
        "count": 1063
      },
      "4": {
        "name": "4_object_detection_data_3D",
        "keywords": [
          [
            "object",
            0.011182217654079157
          ],
          [
            "detection",
            0.010487817791023889
          ],
          [
            "data",
            0.007586628202374437
          ],
          [
            "3D",
            0.007575704267645789
          ],
          [
            "real",
            0.007037677843727627
          ],
          [
            "video",
            0.006831835172575367
          ],
          [
            "objects",
            0.006616917710093047
          ],
          [
            "dataset",
            0.006610248570386171
          ],
          [
            "semantic",
            0.006473344087928845
          ],
          [
            "performance",
            0.006253798635977416
          ]
        ],
        "count": 1052
      },
      "5": {
        "name": "5_MRI_reconstruction_MR_data",
        "keywords": [
          [
            "MRI",
            0.026849814553973894
          ],
          [
            "reconstruction",
            0.020381890722909125
          ],
          [
            "MR",
            0.012127158013282359
          ],
          [
            "data",
            0.011194608511263955
          ],
          [
            "motion",
            0.010851545737706981
          ],
          [
            "space",
            0.009951531536722414
          ],
          [
            "resolution",
            0.00940456655085857
          ],
          [
            "imaging",
            0.009395496155002486
          ],
          [
            "acquisition",
            0.009123640911810968
          ],
          [
            "image",
            0.008810694613524342
          ]
        ],
        "count": 998
      },
      "6": {
        "name": "6_medical_images_data_image",
        "keywords": [
          [
            "medical",
            0.01356112919348139
          ],
          [
            "images",
            0.013547522283768742
          ],
          [
            "data",
            0.012647657593413408
          ],
          [
            "image",
            0.011801402665410353
          ],
          [
            "domain",
            0.01137123197299147
          ],
          [
            "synthetic",
            0.010176022271694668
          ],
          [
            "models",
            0.009089962985055577
          ],
          [
            "MRI",
            0.009068236270794734
          ],
          [
            "CT",
            0.008998064381869234
          ],
          [
            "segmentation",
            0.008968657733683145
          ]
        ],
        "count": 909
      },
      "7": {
        "name": "7_SAR_data_imagery_satellite",
        "keywords": [
          [
            "SAR",
            0.018743793042237666
          ],
          [
            "data",
            0.012412387349426193
          ],
          [
            "imagery",
            0.01194732108608005
          ],
          [
            "satellite",
            0.010987978390664071
          ],
          [
            "remote sensing",
            0.010852636372808656
          ],
          [
            "remote",
            0.010815871444959104
          ],
          [
            "sensing",
            0.01030108331994368
          ],
          [
            "resolution",
            0.009796728840832648
          ],
          [
            "images",
            0.009121605460881175
          ],
          [
            "learning",
            0.007993506094027445
          ]
        ],
        "count": 861
      },
      "8": {
        "name": "8_CT_reconstruction_dose_image",
        "keywords": [
          [
            "CT",
            0.019459883848588544
          ],
          [
            "reconstruction",
            0.018327568614274654
          ],
          [
            "dose",
            0.014079325815278715
          ],
          [
            "image",
            0.012375999987155733
          ],
          [
            "inverse",
            0.010983282949946467
          ],
          [
            "problems",
            0.01053178832788308
          ],
          [
            "denoising",
            0.010221639704826655
          ],
          [
            "inverse problems",
            0.009119647725442293
          ],
          [
            "methods",
            0.009079505677519492
          ],
          [
            "imaging",
            0.008790090622346623
          ]
        ],
        "count": 842
      },
      "9": {
        "name": "9_compression_coding_image compression_video",
        "keywords": [
          [
            "compression",
            0.04065615361964331
          ],
          [
            "coding",
            0.02142761825042815
          ],
          [
            "image compression",
            0.019733933752099194
          ],
          [
            "video",
            0.018783207016406834
          ],
          [
            "Compression",
            0.016185275229657823
          ],
          [
            "rate",
            0.01580271948183268
          ],
          [
            "image",
            0.013600063129314954
          ],
          [
            "distortion",
            0.011311556232065001
          ],
          [
            "codecs",
            0.01024292463077584
          ],
          [
            "performance",
            0.010080426062954315
          ]
        ],
        "count": 759
      },
      "10": {
        "name": "10_retinal_OCT_fundus_images",
        "keywords": [
          [
            "retinal",
            0.02864895270048652
          ],
          [
            "OCT",
            0.022938899591504493
          ],
          [
            "fundus",
            0.016819693218479672
          ],
          [
            "images",
            0.011542253644390188
          ],
          [
            "segmentation",
            0.011061440965780265
          ],
          [
            "Retinal",
            0.009565383754686975
          ],
          [
            "fundus images",
            0.009266217213422006
          ],
          [
            "vessel",
            0.00910564354518431
          ],
          [
            "glaucoma",
            0.008596603943030485
          ],
          [
            "learning",
            0.007878051160516104
          ]
        ],
        "count": 741
      },
      "11": {
        "name": "11_denoising_HDR_image_noise",
        "keywords": [
          [
            "denoising",
            0.019532473127722477
          ],
          [
            "HDR",
            0.01915109281284476
          ],
          [
            "image",
            0.017727126291429765
          ],
          [
            "noise",
            0.016703959608301986
          ],
          [
            "light",
            0.015388014289749627
          ],
          [
            "low light",
            0.014171360896975528
          ],
          [
            "low",
            0.012611145605787924
          ],
          [
            "enhancement",
            0.011444128467042422
          ],
          [
            "images",
            0.011392346005968607
          ],
          [
            "methods",
            0.009301181185089727
          ]
        ],
        "count": 636
      },
      "12": {
        "name": "12_brain_AD_Alzheimer_fMRI",
        "keywords": [
          [
            "brain",
            0.028962479288564862
          ],
          [
            "AD",
            0.01981245661608912
          ],
          [
            "Alzheimer",
            0.01895090929741018
          ],
          [
            "fMRI",
            0.018642090097074206
          ],
          [
            "disease",
            0.01497330011531804
          ],
          [
            "functional",
            0.011828746514721324
          ],
          [
            "data",
            0.011677535594581526
          ],
          [
            "MRI",
            0.010906076242337094
          ],
          [
            "Disease",
            0.010220190137821902
          ],
          [
            "diagnosis",
            0.00986740123773534
          ]
        ],
        "count": 542
      },
      "13": {
        "name": "13_resolution_SR_super_super resolution",
        "keywords": [
          [
            "resolution",
            0.032161445365457814
          ],
          [
            "SR",
            0.027829207029170477
          ],
          [
            "super",
            0.02765352532716578
          ],
          [
            "super resolution",
            0.02639041620201824
          ],
          [
            "Super",
            0.021417957899057257
          ],
          [
            "Resolution",
            0.01923243525152228
          ],
          [
            "image",
            0.015927134140231933
          ],
          [
            "image super",
            0.01590104081885123
          ],
          [
            "SISR",
            0.012965336231437169
          ],
          [
            "Image",
            0.009847603399033868
          ]
        ],
        "count": 524
      },
      "14": {
        "name": "14_spectral_hyperspectral_HSI_Hyperspectral",
        "keywords": [
          [
            "spectral",
            0.03972341059762485
          ],
          [
            "hyperspectral",
            0.0312809635216468
          ],
          [
            "HSI",
            0.027295678532332678
          ],
          [
            "Hyperspectral",
            0.02195897809867265
          ],
          [
            "spatial",
            0.01777904250485928
          ],
          [
            "unmixing",
            0.013137173781777728
          ],
          [
            "hyperspectral image",
            0.011551892403150243
          ],
          [
            "Spectral",
            0.010878344927193622
          ],
          [
            "resolution",
            0.009634888795030386
          ],
          [
            "image",
            0.009132340265532675
          ]
        ],
        "count": 504
      },
      "15": {
        "name": "15_pathology_slide_cancer_WSIs",
        "keywords": [
          [
            "pathology",
            0.018655437830641303
          ],
          [
            "slide",
            0.017125932739912315
          ],
          [
            "cancer",
            0.014681677472670213
          ],
          [
            "WSIs",
            0.01371664520410914
          ],
          [
            "learning",
            0.011666438081297482
          ],
          [
            "WSI",
            0.011607449822393067
          ],
          [
            "histopathology",
            0.011075446603160977
          ],
          [
            "models",
            0.010101082511407034
          ],
          [
            "images",
            0.009936227577156578
          ],
          [
            "tissue",
            0.009693969140208808
          ]
        ],
        "count": 452
      },
      "16": {
        "name": "16_tumor_brain_brain tumor_segmentation",
        "keywords": [
          [
            "tumor",
            0.03944514871290294
          ],
          [
            "brain",
            0.026869160584931426
          ],
          [
            "brain tumor",
            0.020178867358957273
          ],
          [
            "segmentation",
            0.01854158645830556
          ],
          [
            "Brain",
            0.017125572162940094
          ],
          [
            "MRI",
            0.016802132636897785
          ],
          [
            "BraTS",
            0.015942396395785482
          ],
          [
            "tumors",
            0.014080145103030642
          ],
          [
            "Tumor",
            0.01366952776529099
          ],
          [
            "tumor segmentation",
            0.013328517377854955
          ]
        ],
        "count": 395
      },
      "17": {
        "name": "17_hardware_energy_accuracy_neural",
        "keywords": [
          [
            "hardware",
            0.01323083447211419
          ],
          [
            "energy",
            0.011892388818430607
          ],
          [
            "accuracy",
            0.011147314236183653
          ],
          [
            "neural",
            0.010619258901167054
          ],
          [
            "memory",
            0.010366959424191599
          ],
          [
            "network",
            0.009464151243765654
          ],
          [
            "pruning",
            0.009202926148032264
          ],
          [
            "quantization",
            0.009093713607412603
          ],
          [
            "Neural",
            0.009077847214066
          ],
          [
            "networks",
            0.008750347140568137
          ]
        ],
        "count": 372
      },
      "18": {
        "name": "18_defect_detection_inspection_defects",
        "keywords": [
          [
            "defect",
            0.020935762088148457
          ],
          [
            "detection",
            0.01657538601986696
          ],
          [
            "inspection",
            0.015129354523823134
          ],
          [
            "defects",
            0.014593709862271292
          ],
          [
            "manufacturing",
            0.011392612669724168
          ],
          [
            "defect detection",
            0.009571711261412581
          ],
          [
            "data",
            0.008927719515663
          ],
          [
            "anomaly",
            0.008544610385566403
          ],
          [
            "images",
            0.008012464241919062
          ],
          [
            "learning",
            0.00799381944292549
          ]
        ],
        "count": 291
      },
      "19": {
        "name": "19_coronary_vessel_artery_segmentation",
        "keywords": [
          [
            "coronary",
            0.02521778753954474
          ],
          [
            "vessel",
            0.024208358050810324
          ],
          [
            "artery",
            0.018123465047863153
          ],
          [
            "segmentation",
            0.017777538407716398
          ],
          [
            "vascular",
            0.016210747819008636
          ],
          [
            "vessels",
            0.013513175837304787
          ],
          [
            "3D",
            0.011420073058607927
          ],
          [
            "Coronary",
            0.011327995605435933
          ],
          [
            "coronary artery",
            0.011112926685436703
          ],
          [
            "angiography",
            0.010986628313505775
          ]
        ],
        "count": 284
      },
      "20": {
        "name": "20_cardiac_CMR_segmentation_myocardial",
        "keywords": [
          [
            "cardiac",
            0.03144887182143401
          ],
          [
            "CMR",
            0.020159076235873447
          ],
          [
            "segmentation",
            0.016534182293921524
          ],
          [
            "myocardial",
            0.01379749499511987
          ],
          [
            "left",
            0.01333915446045851
          ],
          [
            "Cardiac",
            0.012987763073379028
          ],
          [
            "heart",
            0.012541052473506526
          ],
          [
            "LGE",
            0.01219232268216097
          ],
          [
            "LV",
            0.011897303713476448
          ],
          [
            "LA",
            0.010992769188339415
          ]
        ],
        "count": 283
      },
      "21": {
        "name": "21_breast_cancer_breast cancer_Breast",
        "keywords": [
          [
            "breast",
            0.05232129127088988
          ],
          [
            "cancer",
            0.02801343212595345
          ],
          [
            "breast cancer",
            0.02565541448371004
          ],
          [
            "Breast",
            0.023451706398414242
          ],
          [
            "mammography",
            0.012193093507358395
          ],
          [
            "classification",
            0.01018692993227282
          ],
          [
            "detection",
            0.009984716598148861
          ],
          [
            "learning",
            0.00922771926472572
          ],
          [
            "images",
            0.009210792425814724
          ],
          [
            "model",
            0.00915628555947674
          ]
        ],
        "count": 274
      },
      "22": {
        "name": "22_registration_image registration_Registration_image",
        "keywords": [
          [
            "registration",
            0.07050433515511814
          ],
          [
            "image registration",
            0.028350146140573157
          ],
          [
            "Registration",
            0.020249275744610128
          ],
          [
            "image",
            0.016025885362110037
          ],
          [
            "deformation",
            0.011785348579850903
          ],
          [
            "learning",
            0.010891484259415338
          ],
          [
            "deformable",
            0.01025452906290836
          ],
          [
            "methods",
            0.009816804065327445
          ],
          [
            "medical",
            0.009041453447793649
          ],
          [
            "medical image registration",
            0.008886311303630412
          ]
        ],
        "count": 253
      },
      "23": {
        "name": "23_bone_knee_fracture_radiographs",
        "keywords": [
          [
            "bone",
            0.01963321021848894
          ],
          [
            "knee",
            0.017356940578553307
          ],
          [
            "fracture",
            0.015891230550332005
          ],
          [
            "radiographs",
            0.009142383536944451
          ],
          [
            "learning",
            0.008953323851870306
          ],
          [
            "segmentation",
            0.008884062562103168
          ],
          [
            "ray",
            0.008484421462175337
          ],
          [
            "deep",
            0.00835574652420743
          ],
          [
            "clinical",
            0.008156512181716763
          ],
          [
            "cartilage",
            0.00815578747011101
          ]
        ],
        "count": 227
      },
      "24": {
        "name": "24_imaging_ultrasound_photoacoustic_Ultrasound",
        "keywords": [
          [
            "imaging",
            0.023350727418895677
          ],
          [
            "ultrasound",
            0.0230638682549567
          ],
          [
            "photoacoustic",
            0.015047509449071518
          ],
          [
            "Ultrasound",
            0.014895170608122222
          ],
          [
            "data",
            0.012523662697044751
          ],
          [
            "resolution",
            0.011966066507459325
          ],
          [
            "vivo",
            0.011868760263705893
          ],
          [
            "beamforming",
            0.011229212429916468
          ],
          [
            "image",
            0.010877601681333721
          ],
          [
            "tissue",
            0.00959976755316414
          ]
        ],
        "count": 227
      },
      "25": {
        "name": "25_stroke_brain_segmentation_MRI",
        "keywords": [
          [
            "stroke",
            0.02496193302075686
          ],
          [
            "brain",
            0.02310189713812757
          ],
          [
            "segmentation",
            0.017824406965727034
          ],
          [
            "MRI",
            0.012535758248221627
          ],
          [
            "ischemic",
            0.010260038764348719
          ],
          [
            "CT",
            0.00907615911723077
          ],
          [
            "Stroke",
            0.009011762688280234
          ],
          [
            "scans",
            0.008529448185104727
          ],
          [
            "Brain",
            0.008276426884036292
          ],
          [
            "model",
            0.008183759167600238
          ]
        ],
        "count": 226
      },
      "26": {
        "name": "26_skin_melanoma_Skin_lesion",
        "keywords": [
          [
            "skin",
            0.05451277150916819
          ],
          [
            "melanoma",
            0.021928028648413325
          ],
          [
            "Skin",
            0.021412589157057554
          ],
          [
            "lesion",
            0.02056648270693239
          ],
          [
            "skin lesion",
            0.0182797800228211
          ],
          [
            "skin cancer",
            0.01520328633985305
          ],
          [
            "lesions",
            0.013273775439914221
          ],
          [
            "cancer",
            0.012537679934313255
          ],
          [
            "skin lesions",
            0.012275070721399769
          ],
          [
            "diagnosis",
            0.011334690658243989
          ]
        ],
        "count": 214
      },
      "27": {
        "name": "27_cell_cells_microscopy_segmentation",
        "keywords": [
          [
            "cell",
            0.03582573980079063
          ],
          [
            "cells",
            0.023019350466667823
          ],
          [
            "microscopy",
            0.019083419026219635
          ],
          [
            "segmentation",
            0.014373215513361012
          ],
          [
            "learning",
            0.010352969722840792
          ],
          [
            "images",
            0.010267100553083289
          ],
          [
            "3D",
            0.009726692444229613
          ],
          [
            "analysis",
            0.009168220427119214
          ],
          [
            "microscopy images",
            0.008905440069590091
          ],
          [
            "Cell",
            0.00882052118052002
          ]
        ],
        "count": 212
      },
      "28": {
        "name": "28_video_quality_VQA_videos",
        "keywords": [
          [
            "video",
            0.047653721928289726
          ],
          [
            "quality",
            0.03832616788423131
          ],
          [
            "VQA",
            0.03302145847246131
          ],
          [
            "videos",
            0.030051193593198765
          ],
          [
            "video quality",
            0.02633434234032533
          ],
          [
            "UGC",
            0.024279849607096046
          ],
          [
            "Video",
            0.021756525897057247
          ],
          [
            "Quality",
            0.020113043910566026
          ],
          [
            "quality assessment",
            0.01794444843662344
          ],
          [
            "content",
            0.016850318687888385
          ]
        ],
        "count": 194
      },
      "29": {
        "name": "29_FL_Federated_privacy_federated",
        "keywords": [
          [
            "FL",
            0.0348480262988293
          ],
          [
            "Federated",
            0.03346663572279587
          ],
          [
            "privacy",
            0.0320982580892849
          ],
          [
            "federated",
            0.03183749148929712
          ],
          [
            "data",
            0.02564427437093513
          ],
          [
            "federated learning",
            0.02205557935174936
          ],
          [
            "learning",
            0.020119885135528773
          ],
          [
            "medical",
            0.016467377513836614
          ],
          [
            "client",
            0.014600452457935766
          ],
          [
            "model",
            0.013597644095095984
          ]
        ],
        "count": 194
      },
      "30": {
        "name": "30_prostate_prostate cancer_cancer_Prostate",
        "keywords": [
          [
            "prostate",
            0.051537599619284974
          ],
          [
            "prostate cancer",
            0.02514931001525941
          ],
          [
            "cancer",
            0.02500653513955834
          ],
          [
            "Prostate",
            0.020705067983022317
          ],
          [
            "Gleason",
            0.013260219889851087
          ],
          [
            "MRI",
            0.01261089114424635
          ],
          [
            "segmentation",
            0.011569669352153897
          ],
          [
            "PCa",
            0.010565962210247025
          ],
          [
            "model",
            0.00951110297590217
          ],
          [
            "biopsy",
            0.00949365794448147
          ]
        ],
        "count": 192
      },
      "31": {
        "name": "31_fetal_brain_fetal brain_Fetal",
        "keywords": [
          [
            "fetal",
            0.05523704264666352
          ],
          [
            "brain",
            0.024876197771846027
          ],
          [
            "fetal brain",
            0.022460382891097797
          ],
          [
            "Fetal",
            0.014644899410930749
          ],
          [
            "MRI",
            0.013547667488465674
          ],
          [
            "ultrasound",
            0.013479232560033664
          ],
          [
            "segmentation",
            0.013325001518263942
          ],
          [
            "data",
            0.008150080683179123
          ],
          [
            "motion",
            0.007952848106985358
          ],
          [
            "method",
            0.007403213947608743
          ]
        ],
        "count": 190
      },
      "32": {
        "name": "32_point_point cloud_cloud_clouds",
        "keywords": [
          [
            "point",
            0.07074588615145919
          ],
          [
            "point cloud",
            0.05343102455196529
          ],
          [
            "cloud",
            0.050311888916266465
          ],
          [
            "clouds",
            0.032055236733630074
          ],
          [
            "point clouds",
            0.031385002588681335
          ],
          [
            "compression",
            0.03065982331768347
          ],
          [
            "Point",
            0.028849798008292654
          ],
          [
            "geometry",
            0.02292891145249428
          ],
          [
            "PCC",
            0.021636794805754332
          ],
          [
            "Cloud",
            0.02091910834106099
          ]
        ],
        "count": 181
      },
      "33": {
        "name": "33_IQA_quality_image quality_quality assessment",
        "keywords": [
          [
            "IQA",
            0.048719315944018654
          ],
          [
            "quality",
            0.0390716879862985
          ],
          [
            "image quality",
            0.025605213340778955
          ],
          [
            "quality assessment",
            0.024923277611923065
          ],
          [
            "assessment",
            0.022119619458757987
          ],
          [
            "image",
            0.021031231091198102
          ],
          [
            "reference",
            0.020875909297270984
          ],
          [
            "image quality assessment",
            0.020624051301245897
          ],
          [
            "Quality",
            0.020415508581380103
          ],
          [
            "Assessment",
            0.018182268569546987
          ]
        ],
        "count": 164
      },
      "34": {
        "name": "34_polyp_polyps_colonoscopy_polyp segmentation",
        "keywords": [
          [
            "polyp",
            0.0381374360615751
          ],
          [
            "polyps",
            0.026514526932728297
          ],
          [
            "colonoscopy",
            0.021838275878067415
          ],
          [
            "polyp segmentation",
            0.019378573078595362
          ],
          [
            "segmentation",
            0.01764094343065229
          ],
          [
            "cancer",
            0.017560774982365256
          ],
          [
            "Polyp",
            0.015172275688132281
          ],
          [
            "colorectal",
            0.013850594207446847
          ],
          [
            "detection",
            0.011194926111410873
          ],
          [
            "colorectal cancer",
            0.010027905718460503
          ]
        ],
        "count": 160
      },
      "35": {
        "name": "35_depth_light field_light_LF",
        "keywords": [
          [
            "depth",
            0.03043242335995624
          ],
          [
            "light field",
            0.02870106975315652
          ],
          [
            "light",
            0.027659477321691394
          ],
          [
            "LF",
            0.024286364896502443
          ],
          [
            "field",
            0.023137067450901633
          ],
          [
            "Light",
            0.016279758941189014
          ],
          [
            "angular",
            0.013787585618226541
          ],
          [
            "disparity",
            0.01272084338360688
          ],
          [
            "Field",
            0.012424339799144648
          ],
          [
            "depth estimation",
            0.012316470095193656
          ]
        ],
        "count": 160
      },
      "36": {
        "name": "36_staining_stain_images_tissue",
        "keywords": [
          [
            "staining",
            0.035674002516284394
          ],
          [
            "stain",
            0.028577365415041554
          ],
          [
            "images",
            0.02058247016232242
          ],
          [
            "tissue",
            0.01915596001212589
          ],
          [
            "virtual",
            0.015358465331794384
          ],
          [
            "IHC",
            0.014705103442840482
          ],
          [
            "pathology",
            0.012999994711895772
          ],
          [
            "histopathology",
            0.011938435236306567
          ],
          [
            "virtual staining",
            0.0106754843719492
          ],
          [
            "stains",
            0.0106133940906253
          ]
        ],
        "count": 159
      }
    },
    "correlations": [
      [
        1.0,
        -0.7518094543439287,
        -0.7308837299714789,
        -0.7561923163906785,
        -0.7614717495637306,
        -0.7621517365507238,
        -0.71482517099017,
        -0.7539375075254602,
        -0.7258405687418279,
        -0.7609681954243204,
        -0.7631197232608788,
        -0.7584985587866443,
        -0.7502603466720723,
        -0.7600355237301477,
        -0.7633686143523759,
        -0.7465448512110697,
        -0.7574046306189559,
        -0.7460934070527957,
        -0.7446869831220109,
        -0.758866707891361,
        -0.7578863944224018,
        -0.7609965887769841,
        -0.7560748404832043,
        -0.7415387942242823,
        -0.7509565799347817,
        -0.7570932609287946,
        -0.7526839106119957,
        -0.7461944976720255,
        -0.7558944535245187,
        -0.7320650481526836,
        -0.7613857899951813,
        -0.764800072767821,
        -0.7563270861606168,
        -0.7529897290457923,
        -0.7548165270774561,
        -0.7586202042433773,
        -0.7427278979253488
      ],
      [
        -0.7518094543439287,
        1.0,
        -0.7278726445063166,
        -0.7455319180888308,
        -0.7484765206055847,
        -0.7377074599728795,
        -0.44936765559589853,
        -0.7324183011053382,
        -0.735208734816978,
        -0.7348319639438924,
        -0.7517263668297849,
        -0.41472649987202703,
        -0.7506759463301167,
        -0.7146936910548964,
        -0.7418733761469234,
        -0.742396056515798,
        -0.7497967735141564,
        -0.7462325637424632,
        -0.7481442979207729,
        -0.7544371336097415,
        -0.7538871798480038,
        -0.7532761286463664,
        -0.7458789180649412,
        -0.7450494227490239,
        -0.7522471209855888,
        -0.7488432823198374,
        -0.7543122330855949,
        -0.7393371996059661,
        -0.7273456283119755,
        -0.7311064916639505,
        -0.7604181836955634,
        -0.7594967556567706,
        -0.7477523606230799,
        -0.48672177544725725,
        -0.7523789669327194,
        -0.7355944883922005,
        -0.690865211052063
      ],
      [
        -0.7308837299714789,
        -0.7278726445063166,
        1.0,
        -0.7485638761627991,
        -0.7452487623499728,
        -0.7250401739227331,
        -0.29733618452298133,
        -0.73064637700888,
        -0.7260055767674902,
        -0.7578106922452724,
        -0.7304892784833317,
        -0.7371920391480966,
        -0.7438630381167763,
        -0.7453919755092137,
        -0.7539420293010856,
        -0.718512881608409,
        -0.6108286434317465,
        -0.7190153881470811,
        -0.7397537222623451,
        -0.6185334078254144,
        -0.6240905581459093,
        -0.7352567168766007,
        -0.7255640374811363,
        -0.7139045275909206,
        -0.7337820295353471,
        -0.6054700877491408,
        -0.7225842161805279,
        -0.6471617649140166,
        -0.7424932839547739,
        -0.703441232376882,
        -0.7227060374241312,
        -0.746470669337997,
        -0.7440831367863494,
        -0.7209652882404719,
        -0.6099516857444982,
        -0.7311167124414242,
        -0.7184264006062098
      ],
      [
        -0.7561923163906785,
        -0.7455319180888308,
        -0.7485638761627991,
        1.0,
        -0.7272384657779991,
        -0.7336883808782838,
        -0.7243435711369309,
        -0.7443019267343163,
        -0.7334008769064265,
        -0.7594869201465279,
        -0.757210402873327,
        -0.7351118949356322,
        -0.7577363449694643,
        -0.7315594569920462,
        -0.7327170933436071,
        -0.7470410069048214,
        -0.7550869948180258,
        -0.7448309807248771,
        -0.7510104629369189,
        -0.7598301972679872,
        -0.7494293321777802,
        -0.7534873863799469,
        -0.7478929722880541,
        -0.7430918065607435,
        -0.4023161230977632,
        -0.7575781865474607,
        -0.7628656031188379,
        -0.6922562176123643,
        -0.7473584292695344,
        -0.7466396901351906,
        -0.7612575123495647,
        -0.7595776840927211,
        -0.74331940625237,
        -0.7313196241956624,
        -0.7615032992279636,
        -0.7220564266772322,
        -0.7445827658594402
      ],
      [
        -0.7614717495637306,
        -0.7484765206055847,
        -0.7452487623499728,
        -0.7272384657779991,
        1.0,
        -0.7558109601827919,
        -0.7308732211691303,
        -0.7487031926434424,
        -0.7519598068439297,
        -0.7502017665171901,
        -0.763838091628001,
        -0.7378722958785917,
        -0.7635509888255059,
        -0.75130482330068,
        -0.75206729276425,
        -0.7538913954174975,
        -0.7623208158001356,
        -0.7424474487239852,
        -0.6875461215586947,
        -0.7592104278544591,
        -0.7603384678371756,
        -0.7610371636620679,
        -0.7598006490543148,
        -0.7435879151169291,
        -0.7481048090860855,
        -0.7617677928645552,
        -0.7655764556207767,
        -0.7450644535573265,
        -0.7403977009655092,
        -0.7511885645494532,
        -0.7638412921804281,
        -0.7640747284069425,
        -0.7339819538979233,
        -0.7482509361163873,
        -0.7610725116380048,
        -0.7394267494172275,
        -0.7422467416447511
      ],
      [
        -0.7621517365507238,
        -0.7377074599728795,
        -0.7250401739227331,
        -0.7336883808782838,
        -0.7558109601827919,
        1.0,
        -0.6875668438182276,
        -0.7340805163470205,
        -0.5105914764654425,
        -0.7553876334727369,
        -0.7620073491682073,
        -0.7344549711394149,
        -0.6892746335140765,
        -0.7284464756945428,
        -0.7482835241454551,
        -0.7411361856509419,
        -0.6378294896709391,
        -0.7307971780621125,
        -0.7501194398633662,
        -0.7514871903717874,
        -0.7034350396687902,
        -0.7542391120386098,
        -0.7242077711138202,
        -0.7171846081989709,
        -0.7357830197007759,
        -0.7007625774853437,
        -0.759588544589823,
        -0.7371662239694629,
        -0.7395965203236194,
        -0.716112568799758,
        -0.7430841588351494,
        -0.7236524156984788,
        -0.743926765334378,
        -0.6961462448037496,
        -0.7557292007201224,
        -0.742936223315505,
        -0.7325990682243642
      ],
      [
        -0.71482517099017,
        -0.44936765559589853,
        -0.29733618452298133,
        -0.7243435711369309,
        -0.7308732211691303,
        -0.6875668438182276,
        1.0,
        -0.4981242910502906,
        -0.6884185438537136,
        -0.7274056324487138,
        -0.7304373491623772,
        -0.5330119902449534,
        -0.7289495185051239,
        -0.7078100590730549,
        -0.7282399001161226,
        -0.7062568771146809,
        -0.6967918046775485,
        -0.7096800921190345,
        -0.7225189639213538,
        -0.7215796180809884,
        -0.7148066367179655,
        -0.7293782162579467,
        -0.7151450411418645,
        -0.7072214892014966,
        -0.7175313574744074,
        -0.7165751060826134,
        -0.7304973119743571,
        -0.6948658656394465,
        -0.7095691107612151,
        -0.41885328805240263,
        -0.7383967922337135,
        -0.7457063293102981,
        -0.7324619421313159,
        -0.5473798735180867,
        -0.7270642221881638,
        -0.7044978547803222,
        -0.1792101392605383
      ],
      [
        -0.7539375075254602,
        -0.7324183011053382,
        -0.73064637700888,
        -0.7443019267343163,
        -0.7487031926434424,
        -0.7340805163470205,
        -0.4981242910502906,
        1.0,
        -0.742026515983677,
        -0.7503064122794154,
        -0.7572035701194895,
        -0.7332056335869395,
        -0.7518981437516742,
        -0.7169525268844932,
        -0.6983629930237112,
        -0.7394534048411201,
        -0.7503732652874142,
        -0.7286113901160248,
        -0.7340741285172823,
        -0.7553153544813842,
        -0.7538582420844664,
        -0.7592383143204642,
        -0.7426999981077291,
        -0.7345453969037374,
        -0.7471896479143572,
        -0.7531421508905518,
        -0.7576112958225724,
        -0.7386764659989264,
        -0.7375746792131308,
        -0.41267012211201753,
        -0.7606275114839589,
        -0.7610763902442714,
        -0.721550660113198,
        -0.725583685020688,
        -0.7563571363599941,
        -0.7350273726763605,
        -0.7227810719778931
      ],
      [
        -0.7258405687418279,
        -0.735208734816978,
        -0.7260055767674902,
        -0.7334008769064265,
        -0.7519598068439297,
        -0.5105914764654425,
        -0.6884185438537136,
        -0.742026515983677,
        1.0,
        -0.7549657415095026,
        -0.7641931538996427,
        -0.7180727062178975,
        -0.755056622351232,
        -0.7302718313231727,
        -0.7437582197030952,
        -0.7413434136616646,
        -0.7385964616212414,
        -0.7277456143975461,
        -0.7567941531038069,
        -0.7433304557043641,
        -0.7487523901087914,
        -0.7512943404781263,
        -0.7360875674213216,
        -0.7257186717872928,
        -0.7426689464871654,
        -0.7475184098703025,
        -0.7600726258863422,
        -0.7415106512151626,
        -0.7398131871661964,
        -0.7301624092891892,
        -0.7547362088808757,
        -0.760919061971664,
        -0.7404497427439627,
        -0.6977857142685454,
        -0.7538150791453335,
        -0.744011708095254,
        -0.7252529565981163
      ],
      [
        -0.7609681954243204,
        -0.7348319639438924,
        -0.7578106922452724,
        -0.7594869201465279,
        -0.7502017665171901,
        -0.7553876334727369,
        -0.7274056324487138,
        -0.7503064122794154,
        -0.7549657415095026,
        1.0,
        -0.7641874953595644,
        -0.7385431534215539,
        -0.7632510237453916,
        -0.7437117922505049,
        -0.7520071544874463,
        -0.7587001544951721,
        -0.7632365302696038,
        -0.7349908729653086,
        -0.7604811933747185,
        -0.7631834244800817,
        -0.7642707986253364,
        -0.7622097946598043,
        -0.7599310319797138,
        -0.7573287894821934,
        -0.7615436531267498,
        -0.7635973732296966,
        -0.7630984526864839,
        -0.7576079939772129,
        -0.5793540590786187,
        -0.7454812548846691,
        -0.764633824867383,
        -0.7643981764771527,
        -0.6881796654235062,
        -0.7007129079492517,
        -0.7628824910701963,
        -0.7402823507561037,
        -0.7413274345386673
      ],
      [
        -0.7631197232608788,
        -0.7517263668297849,
        -0.7304892784833317,
        -0.757210402873327,
        -0.763838091628001,
        -0.7620073491682073,
        -0.7304373491623772,
        -0.7572035701194895,
        -0.7641931538996427,
        -0.7641874953595644,
        1.0,
        -0.7530309344474337,
        -0.7506604548981859,
        -0.7536849098199672,
        -0.7593501898015801,
        -0.7521116842119668,
        -0.754455393529532,
        -0.7529462256184156,
        -0.7452920464687333,
        -0.6195078678738052,
        -0.7501360685390355,
        -0.7642634925569654,
        -0.7507918878328628,
        -0.7492225912100774,
        -0.75492074520268,
        -0.7434338793131356,
        -0.7522231386904594,
        -0.7402186276078995,
        -0.7564533768010073,
        -0.7477125692793292,
        -0.7646230938825844,
        -0.7629806661729072,
        -0.7576531399226087,
        -0.7400649377217128,
        -0.7494251635096074,
        -0.7476002793283465,
        -0.7439742836894008
      ],
      [
        -0.7584985587866443,
        -0.41472649987202703,
        -0.7371920391480966,
        -0.7351118949356322,
        -0.7378722958785917,
        -0.7344549711394149,
        -0.5330119902449534,
        -0.7332056335869395,
        -0.7180727062178975,
        -0.7385431534215539,
        -0.7530309344474337,
        1.0,
        -0.7592350518402402,
        -0.7179699125771342,
        -0.734767611556135,
        -0.7462485772042808,
        -0.7561074949362102,
        -0.7284942913716563,
        -0.7518044257398431,
        -0.7560223646803677,
        -0.7579656988926431,
        -0.7592739329588394,
        -0.745902582398954,
        -0.7443935716942611,
        -0.7437238895455767,
        -0.7534817185698685,
        -0.7585675064689759,
        -0.7379531773519225,
        -0.716629364909114,
        -0.7361816058959918,
        -0.7622309200158794,
        -0.7607117197753634,
        -0.7444852340660046,
        -0.5362810980587011,
        -0.7585931233130836,
        -0.6748373931461041,
        -0.7031649792914532
      ],
      [
        -0.7502603466720723,
        -0.7506759463301167,
        -0.7438630381167763,
        -0.7577363449694643,
        -0.7635509888255059,
        -0.6892746335140765,
        -0.7289495185051239,
        -0.7518981437516742,
        -0.755056622351232,
        -0.7632510237453916,
        -0.7506604548981859,
        -0.7592350518402402,
        1.0,
        -0.7585919330849544,
        -0.7572363139196083,
        -0.7408234178975206,
        -0.6431710251211704,
        -0.7420699821264005,
        -0.7442337315335954,
        -0.7537221605547819,
        -0.7599673492520511,
        -0.7598911793080498,
        -0.7472883777089457,
        -0.7407012139018636,
        -0.7556668144548645,
        -0.569087060241693,
        -0.759087912107595,
        -0.7429402126663813,
        -0.7591633099771988,
        -0.7372161133502559,
        -0.7611106642289944,
        -0.6188824188903774,
        -0.7565967816520367,
        -0.7498831947311142,
        -0.7576712346068416,
        -0.7533128771593957,
        -0.748396061162482
      ],
      [
        -0.7600355237301477,
        -0.7146936910548964,
        -0.7453919755092137,
        -0.7315594569920462,
        -0.75130482330068,
        -0.7284464756945428,
        -0.7078100590730549,
        -0.7169525268844932,
        -0.7302718313231727,
        -0.7437117922505049,
        -0.7536849098199672,
        -0.7179699125771342,
        -0.7585919330849544,
        1.0,
        -0.7013460877915993,
        -0.7451987667815776,
        -0.7530199077960038,
        -0.7355414940496394,
        -0.7547571073440363,
        -0.756312584384324,
        -0.7564188333643806,
        -0.7597395329957928,
        -0.7439019254659679,
        -0.7432095502283382,
        -0.7362972497561633,
        -0.7547922779797498,
        -0.7635200728364439,
        -0.7324765010771692,
        -0.7146360022833097,
        -0.7427361242228518,
        -0.7640977509553875,
        -0.7534176471225809,
        -0.7428119474546941,
        -0.695054633641943,
        -0.7601356962234052,
        -0.7195190734796423,
        -0.7257789793943321
      ],
      [
        -0.7633686143523759,
        -0.7418733761469234,
        -0.7539420293010856,
        -0.7327170933436071,
        -0.75206729276425,
        -0.7482835241454551,
        -0.7282399001161226,
        -0.6983629930237112,
        -0.7437582197030952,
        -0.7520071544874463,
        -0.7593501898015801,
        -0.734767611556135,
        -0.7572363139196083,
        -0.7013460877915993,
        1.0,
        -0.7514577058287226,
        -0.7557519916898403,
        -0.7507680127959115,
        -0.7500259790232017,
        -0.7634234852002098,
        -0.758499906430498,
        -0.7607468373189101,
        -0.7533820971697582,
        -0.7480133821711448,
        -0.7425192692790448,
        -0.7617112434665725,
        -0.7635845643504917,
        -0.7479660334360255,
        -0.7439794794010384,
        -0.7429962756477317,
        -0.7644242666037464,
        -0.7631765007119272,
        -0.7412136688279196,
        -0.737519483393489,
        -0.7625791309195743,
        -0.7410560688139871,
        -0.7363051294657985
      ],
      [
        -0.7465448512110697,
        -0.742396056515798,
        -0.718512881608409,
        -0.7470410069048214,
        -0.7538913954174975,
        -0.7411361856509419,
        -0.7062568771146809,
        -0.7394534048411201,
        -0.7413434136616646,
        -0.7587001544951721,
        -0.7521116842119668,
        -0.7462485772042808,
        -0.7408234178975206,
        -0.7451987667815776,
        -0.7514577058287226,
        1.0,
        -0.7083156006950799,
        -0.7339445856318961,
        -0.7426768187436772,
        -0.7513504090227656,
        -0.7488769617490982,
        -0.641170348624177,
        -0.7246896294921439,
        -0.360400000053148,
        -0.7446135892599188,
        -0.7453111776179746,
        -0.7447450598594875,
        -0.44665142216496756,
        -0.7494692886709118,
        -0.7230363201011536,
        -0.6642033748028137,
        -0.7585675329614002,
        -0.7479509141315868,
        -0.7396454340059124,
        -0.6881611030825037,
        -0.7383653680158909,
        -0.6834578965443172
      ],
      [
        -0.7574046306189559,
        -0.7497967735141564,
        -0.6108286434317465,
        -0.7550869948180258,
        -0.7623208158001356,
        -0.6378294896709391,
        -0.6967918046775485,
        -0.7503732652874142,
        -0.7385964616212414,
        -0.7632365302696038,
        -0.754455393529532,
        -0.7561074949362102,
        -0.6431710251211704,
        -0.7530199077960038,
        -0.7557519916898403,
        -0.7083156006950799,
        1.0,
        -0.7446584837580825,
        -0.7512211803899617,
        -0.667339983617498,
        -0.6731240621197525,
        -0.7150685541697861,
        -0.7321105295044235,
        -0.7367632521484682,
        -0.7487311252345529,
        -0.48888263220089806,
        -0.7417447648680564,
        -0.6719383289487808,
        -0.7557368003143172,
        -0.728646757624031,
        -0.7372448039541053,
        -0.6597681498917285,
        -0.7580248465425679,
        -0.7414962667771303,
        -0.6437022581188707,
        -0.7545768986135182,
        -0.7348615692964382
      ],
      [
        -0.7460934070527957,
        -0.7462325637424632,
        -0.7190153881470811,
        -0.7448309807248771,
        -0.7424474487239852,
        -0.7307971780621125,
        -0.7096800921190345,
        -0.7286113901160248,
        -0.7277456143975461,
        -0.7349908729653086,
        -0.7529462256184156,
        -0.7284942913716563,
        -0.7420699821264005,
        -0.7355414940496394,
        -0.7507680127959115,
        -0.7339445856318961,
        -0.7446584837580825,
        1.0,
        -0.7434703374055088,
        -0.7518550042428179,
        -0.7458448106448958,
        -0.7505521400848107,
        -0.7464188888613246,
        -0.7332488859857167,
        -0.7487841870924548,
        -0.7526738357229666,
        -0.7562163499572511,
        -0.7324871109739983,
        -0.737998262088005,
        -0.7338987016711318,
        -0.7564545315624354,
        -0.7616023506565227,
        -0.7446868969745917,
        -0.7372060765822627,
        -0.7509722938069164,
        -0.7380391198518343,
        -0.7335445333567884
      ],
      [
        -0.7446869831220109,
        -0.7481442979207729,
        -0.7397537222623451,
        -0.7510104629369189,
        -0.6875461215586947,
        -0.7501194398633662,
        -0.7225189639213538,
        -0.7340741285172823,
        -0.7567941531038069,
        -0.7604811933747185,
        -0.7452920464687333,
        -0.7518044257398431,
        -0.7442337315335954,
        -0.7547571073440363,
        -0.7500259790232017,
        -0.7426768187436772,
        -0.7512211803899617,
        -0.7434703374055088,
        1.0,
        -0.7501868234726776,
        -0.7516909283897772,
        -0.7487845172922176,
        -0.7531109694624782,
        -0.7279639309164919,
        -0.7527497544211883,
        -0.7479870873057846,
        -0.7596231704682905,
        -0.7374064428342728,
        -0.7455541762517749,
        -0.7426523691978703,
        -0.7538547402073441,
        -0.759450397055107,
        -0.748347489943143,
        -0.7492572252849964,
        -0.7513362136831048,
        -0.7443862560856153,
        -0.7361504174295677
      ],
      [
        -0.758866707891361,
        -0.7544371336097415,
        -0.6185334078254144,
        -0.7598301972679872,
        -0.7592104278544591,
        -0.7514871903717874,
        -0.7215796180809884,
        -0.7553153544813842,
        -0.7433304557043641,
        -0.7631834244800817,
        -0.6195078678738052,
        -0.7560223646803677,
        -0.7537221605547819,
        -0.756312584384324,
        -0.7634234852002098,
        -0.7513504090227656,
        -0.667339983617498,
        -0.7518550042428179,
        -0.7501868234726776,
        1.0,
        -0.6107229473766735,
        -0.7606071163700561,
        -0.7402933227259969,
        -0.7488450414491427,
        -0.7424132059178947,
        -0.5719770317386611,
        -0.7526122985477226,
        -0.6676894342850102,
        -0.7564147538424905,
        -0.7493563666141355,
        -0.7560867817364765,
        -0.7533241367611647,
        -0.7512019292135266,
        -0.7487144519629936,
        -0.6058244220017244,
        -0.7532066755149165,
        -0.7421360134363584
      ],
      [
        -0.7578863944224018,
        -0.7538871798480038,
        -0.6240905581459093,
        -0.7494293321777802,
        -0.7603384678371756,
        -0.7034350396687902,
        -0.7148066367179655,
        -0.7538582420844664,
        -0.7487523901087914,
        -0.7642707986253364,
        -0.7501360685390355,
        -0.7579656988926431,
        -0.7599673492520511,
        -0.7564188333643806,
        -0.758499906430498,
        -0.7488769617490982,
        -0.6731240621197525,
        -0.7458448106448958,
        -0.7516909283897772,
        -0.6107229473766735,
        1.0,
        -0.7602868162797363,
        -0.7167234007758299,
        -0.7437211291735812,
        -0.7452351437602402,
        -0.6083048051782467,
        -0.7558970401859235,
        -0.6710254864171064,
        -0.7487014106299872,
        -0.7442352268341255,
        -0.7549932536589394,
        -0.7453910283717654,
        -0.7499772561485312,
        -0.7321408392433613,
        -0.6200618129661205,
        -0.757520921265405,
        -0.7380868594014123
      ],
      [
        -0.7609965887769841,
        -0.7532761286463664,
        -0.7352567168766007,
        -0.7534873863799469,
        -0.7610371636620679,
        -0.7542391120386098,
        -0.7293782162579467,
        -0.7592383143204642,
        -0.7512943404781263,
        -0.7622097946598043,
        -0.7642634925569654,
        -0.7592739329588394,
        -0.7598911793080498,
        -0.7597395329957928,
        -0.7607468373189101,
        -0.641170348624177,
        -0.7150685541697861,
        -0.7505521400848107,
        -0.7487845172922176,
        -0.7606071163700561,
        -0.7602868162797363,
        1.0,
        -0.7455573191665665,
        -0.7432358005862438,
        -0.7271560379197066,
        -0.758952169426296,
        -0.7400902148936788,
        -0.7263164723215305,
        -0.7598264031124295,
        -0.7475133821514176,
        -0.6220915252152415,
        -0.7644381509665379,
        -0.7606589796547472,
        -0.7521087324747586,
        -0.68670802141594,
        -0.7577054586243415,
        -0.7194832091877164
      ],
      [
        -0.7560748404832043,
        -0.7458789180649412,
        -0.7255640374811363,
        -0.7478929722880541,
        -0.7598006490543148,
        -0.7242077711138202,
        -0.7151450411418645,
        -0.7426999981077291,
        -0.7360875674213216,
        -0.7599310319797138,
        -0.7507918878328628,
        -0.745902582398954,
        -0.7472883777089457,
        -0.7439019254659679,
        -0.7533820971697582,
        -0.7246896294921439,
        -0.7321105295044235,
        -0.7464188888613246,
        -0.7531109694624782,
        -0.7402933227259969,
        -0.7167234007758299,
        -0.7455573191665665,
        1.0,
        -0.7183748622357586,
        -0.7463673256043795,
        -0.7354699990928155,
        -0.7544614427312175,
        -0.7278603088423401,
        -0.7507288842691378,
        -0.7473531812221871,
        -0.7396757940200924,
        -0.7368420720429044,
        -0.7426311886004963,
        -0.7390715625963339,
        -0.7536240444760065,
        -0.7457109095454091,
        -0.7281405375667139
      ],
      [
        -0.7415387942242823,
        -0.7450494227490239,
        -0.7139045275909206,
        -0.7430918065607435,
        -0.7435879151169291,
        -0.7171846081989709,
        -0.7072214892014966,
        -0.7345453969037374,
        -0.7257186717872928,
        -0.7573287894821934,
        -0.7492225912100774,
        -0.7443935716942611,
        -0.7407012139018636,
        -0.7432095502283382,
        -0.7480133821711448,
        -0.360400000053148,
        -0.7367632521484682,
        -0.7332488859857167,
        -0.7279639309164919,
        -0.7488450414491427,
        -0.7437211291735812,
        -0.7432358005862438,
        -0.7183748622357586,
        1.0,
        -0.7299249197403233,
        -0.7413740080890064,
        -0.7547096805581803,
        -0.40300850329032734,
        -0.7492149103161871,
        -0.7264779528706913,
        -0.7528579550571814,
        -0.7585425668112231,
        -0.7469197981175331,
        -0.7373781632109707,
        -0.7478725367660726,
        -0.7343677318545033,
        -0.7303419621539606
      ],
      [
        -0.7509565799347817,
        -0.7522471209855888,
        -0.7337820295353471,
        -0.4023161230977632,
        -0.7481048090860855,
        -0.7357830197007759,
        -0.7175313574744074,
        -0.7471896479143572,
        -0.7426689464871654,
        -0.7615436531267498,
        -0.75492074520268,
        -0.7437238895455767,
        -0.7556668144548645,
        -0.7362972497561633,
        -0.7425192692790448,
        -0.7446135892599188,
        -0.7487311252345529,
        -0.7487841870924548,
        -0.7527497544211883,
        -0.7424132059178947,
        -0.7452351437602402,
        -0.7271560379197066,
        -0.7463673256043795,
        -0.7299249197403233,
        1.0,
        -0.752952595936925,
        -0.7605937796287299,
        -0.7393662388739961,
        -0.7476725838644779,
        -0.7435776364990877,
        -0.7544089099668838,
        -0.7362955230531947,
        -0.7440082710921246,
        -0.7223277735784592,
        -0.7579008535072083,
        -0.7340882634071693,
        -0.7397080875963153
      ],
      [
        -0.7570932609287946,
        -0.7488432823198374,
        -0.6054700877491408,
        -0.7575781865474607,
        -0.7617677928645552,
        -0.7007625774853437,
        -0.7165751060826134,
        -0.7531421508905518,
        -0.7475184098703025,
        -0.7635973732296966,
        -0.7434338793131356,
        -0.7534817185698685,
        -0.569087060241693,
        -0.7547922779797498,
        -0.7617112434665725,
        -0.7453111776179746,
        -0.48888263220089806,
        -0.7526738357229666,
        -0.7479870873057846,
        -0.5719770317386611,
        -0.6083048051782467,
        -0.758952169426296,
        -0.7354699990928155,
        -0.7413740080890064,
        -0.752952595936925,
        1.0,
        -0.7425852840732262,
        -0.6485531652593151,
        -0.7553413206277257,
        -0.744638973840219,
        -0.7507126871678895,
        -0.5725495717953097,
        -0.7548605412202715,
        -0.7457084055127137,
        -0.5764308543938932,
        -0.7579616187591869,
        -0.7399987319419195
      ],
      [
        -0.7526839106119957,
        -0.7543122330855949,
        -0.7225842161805279,
        -0.7628656031188379,
        -0.7655764556207767,
        -0.759588544589823,
        -0.7304973119743571,
        -0.7576112958225724,
        -0.7600726258863422,
        -0.7630984526864839,
        -0.7522231386904594,
        -0.7585675064689759,
        -0.759087912107595,
        -0.7635200728364439,
        -0.7635845643504917,
        -0.7447450598594875,
        -0.7417447648680564,
        -0.7562163499572511,
        -0.7596231704682905,
        -0.7526122985477226,
        -0.7558970401859235,
        -0.7400902148936788,
        -0.7544614427312175,
        -0.7547096805581803,
        -0.7605937796287299,
        -0.7425852840732262,
        1.0,
        -0.7500766119039055,
        -0.761263032774748,
        -0.7484425273622697,
        -0.746240822082192,
        -0.7607344194457911,
        -0.7586631697345423,
        -0.7548030891033708,
        -0.7406274181769131,
        -0.7577451773892963,
        -0.7478383672760759
      ],
      [
        -0.7461944976720255,
        -0.7393371996059661,
        -0.6471617649140166,
        -0.6922562176123643,
        -0.7450644535573265,
        -0.7371662239694629,
        -0.6948658656394465,
        -0.7386764659989264,
        -0.7415106512151626,
        -0.7576079939772129,
        -0.7402186276078995,
        -0.7379531773519225,
        -0.7429402126663813,
        -0.7324765010771692,
        -0.7479660334360255,
        -0.44665142216496756,
        -0.6719383289487808,
        -0.7324871109739983,
        -0.7374064428342728,
        -0.6676894342850102,
        -0.6710254864171064,
        -0.7263164723215305,
        -0.7278603088423401,
        -0.40300850329032734,
        -0.7393662388739961,
        -0.6485531652593151,
        -0.7500766119039055,
        1.0,
        -0.7408108001929414,
        -0.7322800573234471,
        -0.743850790363505,
        -0.7540206922420445,
        -0.741089144192674,
        -0.7329016852716246,
        -0.6522949531196212,
        -0.7305324872508263,
        -0.68992733932372
      ],
      [
        -0.7558944535245187,
        -0.7273456283119755,
        -0.7424932839547739,
        -0.7473584292695344,
        -0.7403977009655092,
        -0.7395965203236194,
        -0.7095691107612151,
        -0.7375746792131308,
        -0.7398131871661964,
        -0.5793540590786187,
        -0.7564533768010073,
        -0.716629364909114,
        -0.7591633099771988,
        -0.7146360022833097,
        -0.7439794794010384,
        -0.7494692886709118,
        -0.7557368003143172,
        -0.737998262088005,
        -0.7455541762517749,
        -0.7564147538424905,
        -0.7487014106299872,
        -0.7598264031124295,
        -0.7507288842691378,
        -0.7492149103161871,
        -0.7476725838644779,
        -0.7553413206277257,
        -0.761263032774748,
        -0.7408108001929414,
        1.0,
        -0.7347398332208309,
        -0.7618402058846838,
        -0.7582682486606975,
        -0.7309936991857312,
        -0.33521494733096124,
        -0.7572871170951709,
        -0.7337208106683604,
        -0.7337749229579487
      ],
      [
        -0.7320650481526836,
        -0.7311064916639505,
        -0.703441232376882,
        -0.7466396901351906,
        -0.7511885645494532,
        -0.716112568799758,
        -0.41885328805240263,
        -0.41267012211201753,
        -0.7301624092891892,
        -0.7454812548846691,
        -0.7477125692793292,
        -0.7361816058959918,
        -0.7372161133502559,
        -0.7427361242228518,
        -0.7429962756477317,
        -0.7230363201011536,
        -0.728646757624031,
        -0.7338987016711318,
        -0.7426523691978703,
        -0.7493563666141355,
        -0.7442352268341255,
        -0.7475133821514176,
        -0.7473531812221871,
        -0.7264779528706913,
        -0.7435776364990877,
        -0.744638973840219,
        -0.7484425273622697,
        -0.7322800573234471,
        -0.7347398332208309,
        1.0,
        -0.7511590908085188,
        -0.7542066784304086,
        -0.7425457633594544,
        -0.7299299939407694,
        -0.7495572763199332,
        -0.7421858586168155,
        -0.7261657316939023
      ],
      [
        -0.7613857899951813,
        -0.7604181836955634,
        -0.7227060374241312,
        -0.7612575123495647,
        -0.7638412921804281,
        -0.7430841588351494,
        -0.7383967922337135,
        -0.7606275114839589,
        -0.7547362088808757,
        -0.764633824867383,
        -0.7646230938825844,
        -0.7622309200158794,
        -0.7611106642289944,
        -0.7640977509553875,
        -0.7644242666037464,
        -0.6642033748028137,
        -0.7372448039541053,
        -0.7564545315624354,
        -0.7538547402073441,
        -0.7560867817364765,
        -0.7549932536589394,
        -0.6220915252152415,
        -0.7396757940200924,
        -0.7528579550571814,
        -0.7544089099668838,
        -0.7507126871678895,
        -0.746240822082192,
        -0.743850790363505,
        -0.7618402058846838,
        -0.7511590908085188,
        1.0,
        -0.763081241519622,
        -0.7617412174475551,
        -0.7585846545205293,
        -0.6734988677216125,
        -0.7597483661924133,
        -0.7448339820593378
      ],
      [
        -0.764800072767821,
        -0.7594967556567706,
        -0.746470669337997,
        -0.7595776840927211,
        -0.7640747284069425,
        -0.7236524156984788,
        -0.7457063293102981,
        -0.7610763902442714,
        -0.760919061971664,
        -0.7643981764771527,
        -0.7629806661729072,
        -0.7607117197753634,
        -0.6188824188903774,
        -0.7534176471225809,
        -0.7631765007119272,
        -0.7585675329614002,
        -0.6597681498917285,
        -0.7616023506565227,
        -0.759450397055107,
        -0.7533241367611647,
        -0.7453910283717654,
        -0.7644381509665379,
        -0.7368420720429044,
        -0.7585425668112231,
        -0.7362955230531947,
        -0.5725495717953097,
        -0.7607344194457911,
        -0.7540206922420445,
        -0.7582682486606975,
        -0.7542066784304086,
        -0.763081241519622,
        1.0,
        -0.7619447363773918,
        -0.7507294535080999,
        -0.7572971269834634,
        -0.7601848278231425,
        -0.7554369342957197
      ],
      [
        -0.7563270861606168,
        -0.7477523606230799,
        -0.7440831367863494,
        -0.74331940625237,
        -0.7339819538979233,
        -0.743926765334378,
        -0.7324619421313159,
        -0.721550660113198,
        -0.7404497427439627,
        -0.6881796654235062,
        -0.7576531399226087,
        -0.7444852340660046,
        -0.7565967816520367,
        -0.7428119474546941,
        -0.7412136688279196,
        -0.7479509141315868,
        -0.7580248465425679,
        -0.7446868969745917,
        -0.748347489943143,
        -0.7512019292135266,
        -0.7499772561485312,
        -0.7606589796547472,
        -0.7426311886004963,
        -0.7469197981175331,
        -0.7440082710921246,
        -0.7548605412202715,
        -0.7586631697345423,
        -0.741089144192674,
        -0.7309936991857312,
        -0.7425457633594544,
        -0.7617412174475551,
        -0.7619447363773918,
        1.0,
        -0.726609369027406,
        -0.7598276181296124,
        -0.730774751764411,
        -0.7440105706812976
      ],
      [
        -0.7529897290457923,
        -0.48672177544725725,
        -0.7209652882404719,
        -0.7313196241956624,
        -0.7482509361163873,
        -0.6961462448037496,
        -0.5473798735180867,
        -0.725583685020688,
        -0.6977857142685454,
        -0.7007129079492517,
        -0.7400649377217128,
        -0.5362810980587011,
        -0.7498831947311142,
        -0.695054633641943,
        -0.737519483393489,
        -0.7396454340059124,
        -0.7414962667771303,
        -0.7372060765822627,
        -0.7492572252849964,
        -0.7487144519629936,
        -0.7321408392433613,
        -0.7521087324747586,
        -0.7390715625963339,
        -0.7373781632109707,
        -0.7223277735784592,
        -0.7457084055127137,
        -0.7548030891033708,
        -0.7329016852716246,
        -0.33521494733096124,
        -0.7299299939407694,
        -0.7585846545205293,
        -0.7507294535080999,
        -0.726609369027406,
        1.0,
        -0.7547538456748439,
        -0.7201705526572246,
        -0.691237650827776
      ],
      [
        -0.7548165270774561,
        -0.7523789669327194,
        -0.6099516857444982,
        -0.7615032992279636,
        -0.7610725116380048,
        -0.7557292007201224,
        -0.7270642221881638,
        -0.7563571363599941,
        -0.7538150791453335,
        -0.7628824910701963,
        -0.7494251635096074,
        -0.7585931233130836,
        -0.7576712346068416,
        -0.7601356962234052,
        -0.7625791309195743,
        -0.6881611030825037,
        -0.6437022581188707,
        -0.7509722938069164,
        -0.7513362136831048,
        -0.6058244220017244,
        -0.6200618129661205,
        -0.68670802141594,
        -0.7536240444760065,
        -0.7478725367660726,
        -0.7579008535072083,
        -0.5764308543938932,
        -0.7406274181769131,
        -0.6522949531196212,
        -0.7572871170951709,
        -0.7495572763199332,
        -0.6734988677216125,
        -0.7572971269834634,
        -0.7598276181296124,
        -0.7547538456748439,
        1.0,
        -0.7516391815390939,
        -0.7409525728508006
      ],
      [
        -0.7586202042433773,
        -0.7355944883922005,
        -0.7311167124414242,
        -0.7220564266772322,
        -0.7394267494172275,
        -0.742936223315505,
        -0.7044978547803222,
        -0.7350273726763605,
        -0.744011708095254,
        -0.7402823507561037,
        -0.7476002793283465,
        -0.6748373931461041,
        -0.7533128771593957,
        -0.7195190734796423,
        -0.7410560688139871,
        -0.7383653680158909,
        -0.7545768986135182,
        -0.7380391198518343,
        -0.7443862560856153,
        -0.7532066755149165,
        -0.757520921265405,
        -0.7577054586243415,
        -0.7457109095454091,
        -0.7343677318545033,
        -0.7340882634071693,
        -0.7579616187591869,
        -0.7577451773892963,
        -0.7305324872508263,
        -0.7337208106683604,
        -0.7421858586168155,
        -0.7597483661924133,
        -0.7601848278231425,
        -0.730774751764411,
        -0.7201705526572246,
        -0.7516391815390939,
        1.0,
        -0.725507443631323
      ],
      [
        -0.7427278979253488,
        -0.690865211052063,
        -0.7184264006062098,
        -0.7445827658594402,
        -0.7422467416447511,
        -0.7325990682243642,
        -0.1792101392605383,
        -0.7227810719778931,
        -0.7252529565981163,
        -0.7413274345386673,
        -0.7439742836894008,
        -0.7031649792914532,
        -0.748396061162482,
        -0.7257789793943321,
        -0.7363051294657985,
        -0.6834578965443172,
        -0.7348615692964382,
        -0.7335445333567884,
        -0.7361504174295677,
        -0.7421360134363584,
        -0.7380868594014123,
        -0.7194832091877164,
        -0.7281405375667139,
        -0.7303419621539606,
        -0.7397080875963153,
        -0.7399987319419195,
        -0.7478383672760759,
        -0.68992733932372,
        -0.7337749229579487,
        -0.7261657316939023,
        -0.7448339820593378,
        -0.7554369342957197,
        -0.7440105706812976,
        -0.691237650827776,
        -0.7409525728508006,
        -0.725507443631323,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        71,
        1,
        16,
        9,
        6,
        13,
        6,
        10,
        6,
        8,
        9,
        11,
        5,
        15,
        9,
        2,
        5,
        7,
        8,
        5,
        1,
        6,
        6,
        4,
        1,
        1,
        3,
        7,
        8,
        15,
        0,
        1,
        11,
        8,
        1,
        11,
        13
      ],
      "2020-02": [
        94,
        2,
        12,
        5,
        5,
        9,
        3,
        5,
        7,
        17,
        6,
        19,
        7,
        20,
        9,
        1,
        8,
        16,
        4,
        5,
        5,
        6,
        8,
        2,
        5,
        3,
        0,
        5,
        9,
        14,
        4,
        1,
        16,
        14,
        1,
        10,
        13
      ],
      "2020-03": [
        106,
        4,
        19,
        10,
        10,
        8,
        12,
        6,
        5,
        12,
        10,
        21,
        3,
        24,
        7,
        3,
        6,
        13,
        7,
        7,
        1,
        6,
        7,
        10,
        4,
        4,
        5,
        7,
        22,
        14,
        3,
        0,
        15,
        19,
        1,
        23,
        14
      ],
      "2020-04": [
        145,
        6,
        22,
        6,
        8,
        14,
        12,
        13,
        5,
        17,
        18,
        16,
        7,
        32,
        15,
        4,
        6,
        11,
        5,
        2,
        6,
        7,
        13,
        11,
        2,
        5,
        10,
        10,
        14,
        15,
        3,
        2,
        19,
        18,
        2,
        15,
        14
      ],
      "2020-05": [
        99,
        6,
        22,
        7,
        7,
        9,
        12,
        8,
        14,
        9,
        11,
        17,
        4,
        18,
        15,
        4,
        11,
        11,
        10,
        2,
        3,
        8,
        13,
        8,
        3,
        2,
        6,
        6,
        19,
        14,
        4,
        0,
        28,
        17,
        0,
        18,
        4
      ],
      "2020-06": [
        119,
        7,
        23,
        4,
        9,
        14,
        7,
        8,
        11,
        14,
        11,
        27,
        5,
        13,
        15,
        4,
        8,
        6,
        4,
        3,
        6,
        7,
        10,
        6,
        2,
        2,
        0,
        4,
        8,
        14,
        6,
        1,
        19,
        14,
        2,
        18,
        6
      ],
      "2020-07": [
        93,
        8,
        29,
        10,
        5,
        10,
        5,
        10,
        6,
        16,
        17,
        19,
        8,
        25,
        24,
        3,
        10,
        9,
        7,
        7,
        3,
        7,
        5,
        11,
        3,
        4,
        7,
        11,
        13,
        17,
        4,
        3,
        15,
        14,
        3,
        15,
        10
      ],
      "2020-08": [
        101,
        6,
        10,
        5,
        6,
        20,
        7,
        4,
        7,
        12,
        6,
        15,
        7,
        30,
        8,
        7,
        5,
        7,
        10,
        3,
        11,
        8,
        11,
        6,
        3,
        4,
        4,
        6,
        23,
        17,
        2,
        2,
        16,
        17,
        1,
        11,
        11
      ],
      "2020-09": [
        91,
        2,
        15,
        6,
        3,
        7,
        5,
        10,
        7,
        16,
        10,
        15,
        10,
        26,
        6,
        4,
        5,
        7,
        4,
        5,
        6,
        16,
        9,
        5,
        4,
        1,
        5,
        6,
        16,
        14,
        1,
        3,
        12,
        12,
        1,
        11,
        5
      ],
      "2020-10": [
        91,
        3,
        22,
        6,
        3,
        15,
        10,
        1,
        12,
        5,
        12,
        9,
        9,
        22,
        15,
        3,
        12,
        3,
        3,
        3,
        7,
        9,
        8,
        6,
        4,
        3,
        5,
        8,
        8,
        9,
        1,
        2,
        12,
        18,
        0,
        12,
        9
      ],
      "2020-11": [
        75,
        2,
        16,
        8,
        1,
        7,
        3,
        8,
        12,
        7,
        5,
        16,
        7,
        22,
        9,
        4,
        15,
        7,
        3,
        0,
        6,
        7,
        11,
        4,
        7,
        2,
        6,
        6,
        10,
        6,
        5,
        2,
        13,
        7,
        0,
        13,
        11
      ],
      "2020-12": [
        76,
        4,
        18,
        5,
        0,
        8,
        3,
        14,
        5,
        10,
        9,
        17,
        7,
        21,
        17,
        3,
        11,
        5,
        4,
        1,
        7,
        8,
        9,
        4,
        0,
        3,
        2,
        6,
        6,
        6,
        2,
        0,
        9,
        12,
        5,
        8,
        10
      ],
      "2021-01": [
        62,
        1,
        18,
        4,
        3,
        5,
        9,
        4,
        4,
        5,
        4,
        16,
        9,
        17,
        10,
        6,
        10,
        6,
        11,
        3,
        3,
        8,
        5,
        9,
        1,
        3,
        6,
        7,
        14,
        6,
        3,
        0,
        15,
        17,
        4,
        5,
        8
      ],
      "2021-02": [
        58,
        3,
        18,
        5,
        4,
        7,
        4,
        6,
        8,
        11,
        2,
        14,
        2,
        18,
        9,
        2,
        7,
        7,
        2,
        1,
        1,
        4,
        6,
        5,
        2,
        1,
        3,
        10,
        8,
        2,
        1,
        0,
        10,
        9,
        1,
        14,
        8
      ],
      "2021-03": [
        86,
        3,
        36,
        6,
        2,
        10,
        6,
        9,
        10,
        13,
        8,
        13,
        11,
        21,
        21,
        2,
        8,
        7,
        5,
        2,
        3,
        2,
        8,
        3,
        2,
        2,
        3,
        5,
        12,
        11,
        2,
        3,
        12,
        14,
        2,
        15,
        6
      ],
      "2021-04": [
        68,
        3,
        12,
        2,
        2,
        7,
        9,
        7,
        9,
        16,
        5,
        12,
        7,
        20,
        12,
        2,
        7,
        4,
        3,
        0,
        2,
        4,
        7,
        1,
        2,
        3,
        6,
        4,
        14,
        10,
        0,
        0,
        6,
        10,
        1,
        12,
        10
      ],
      "2021-05": [
        68,
        3,
        19,
        6,
        3,
        20,
        3,
        8,
        5,
        14,
        4,
        10,
        6,
        22,
        11,
        0,
        7,
        2,
        6,
        4,
        1,
        4,
        8,
        4,
        2,
        4,
        5,
        5,
        7,
        8,
        7,
        1,
        10,
        14,
        4,
        5,
        6
      ],
      "2021-06": [
        60,
        3,
        15,
        10,
        1,
        15,
        7,
        4,
        10,
        14,
        9,
        11,
        3,
        26,
        13,
        4,
        5,
        7,
        3,
        2,
        3,
        5,
        7,
        4,
        1,
        0,
        7,
        5,
        11,
        6,
        7,
        3,
        16,
        15,
        2,
        7,
        8
      ],
      "2021-07": [
        76,
        5,
        20,
        5,
        0,
        5,
        3,
        5,
        8,
        11,
        13,
        18,
        6,
        16,
        9,
        2,
        10,
        4,
        0,
        5,
        7,
        6,
        11,
        4,
        2,
        1,
        2,
        3,
        6,
        11,
        2,
        5,
        10,
        16,
        2,
        12,
        10
      ],
      "2021-08": [
        49,
        2,
        15,
        4,
        2,
        8,
        8,
        8,
        8,
        10,
        13,
        14,
        7,
        19,
        7,
        5,
        8,
        3,
        7,
        1,
        2,
        7,
        7,
        1,
        1,
        2,
        4,
        5,
        3,
        7,
        1,
        2,
        9,
        16,
        1,
        6,
        11
      ],
      "2021-09": [
        66,
        1,
        27,
        5,
        0,
        13,
        13,
        2,
        2,
        14,
        13,
        12,
        5,
        14,
        11,
        2,
        14,
        4,
        0,
        5,
        9,
        9,
        8,
        2,
        4,
        3,
        7,
        2,
        13,
        10,
        1,
        3,
        7,
        5,
        0,
        9,
        11
      ],
      "2021-10": [
        68,
        2,
        22,
        4,
        2,
        5,
        8,
        7,
        14,
        6,
        13,
        20,
        5,
        18,
        5,
        3,
        7,
        3,
        6,
        3,
        6,
        7,
        10,
        8,
        1,
        2,
        6,
        5,
        11,
        6,
        3,
        2,
        7,
        18,
        3,
        5,
        8
      ],
      "2021-11": [
        53,
        3,
        17,
        4,
        2,
        12,
        10,
        5,
        3,
        18,
        5,
        20,
        7,
        24,
        12,
        4,
        11,
        3,
        1,
        6,
        2,
        11,
        7,
        2,
        4,
        2,
        4,
        7,
        7,
        6,
        2,
        3,
        14,
        11,
        3,
        11,
        6
      ],
      "2021-12": [
        55,
        1,
        16,
        5,
        1,
        9,
        5,
        5,
        6,
        18,
        8,
        13,
        9,
        21,
        9,
        6,
        9,
        1,
        3,
        3,
        1,
        6,
        12,
        4,
        1,
        0,
        4,
        1,
        5,
        11,
        4,
        0,
        12,
        13,
        0,
        12,
        10
      ],
      "2022-01": [
        64,
        2,
        23,
        5,
        0,
        16,
        7,
        8,
        11,
        13,
        7,
        13,
        5,
        24,
        9,
        3,
        14,
        6,
        3,
        1,
        6,
        9,
        5,
        7,
        5,
        0,
        1,
        6,
        6,
        7,
        0,
        2,
        6,
        8,
        3,
        9,
        3
      ],
      "2022-02": [
        54,
        4,
        24,
        4,
        6,
        13,
        6,
        1,
        11,
        18,
        12,
        10,
        4,
        24,
        8,
        5,
        6,
        5,
        5,
        4,
        1,
        10,
        16,
        5,
        2,
        3,
        3,
        11,
        13,
        11,
        2,
        1,
        12,
        10,
        3,
        9,
        6
      ],
      "2022-03": [
        73,
        4,
        48,
        7,
        4,
        18,
        7,
        6,
        13,
        25,
        13,
        16,
        9,
        22,
        15,
        5,
        13,
        9,
        4,
        1,
        5,
        11,
        14,
        8,
        0,
        4,
        6,
        14,
        22,
        19,
        2,
        4,
        12,
        23,
        6,
        10,
        9
      ],
      "2022-04": [
        47,
        0,
        17,
        4,
        2,
        13,
        8,
        7,
        5,
        13,
        3,
        9,
        6,
        25,
        14,
        7,
        10,
        10,
        6,
        2,
        0,
        6,
        11,
        7,
        1,
        2,
        5,
        3,
        13,
        14,
        0,
        2,
        13,
        15,
        3,
        14,
        8
      ],
      "2022-05": [
        55,
        2,
        14,
        6,
        1,
        12,
        6,
        6,
        11,
        14,
        5,
        9,
        5,
        31,
        16,
        5,
        5,
        4,
        4,
        3,
        7,
        6,
        9,
        2,
        1,
        1,
        4,
        5,
        10,
        14,
        1,
        5,
        16,
        12,
        4,
        8,
        5
      ],
      "2022-06": [
        75,
        1,
        21,
        3,
        1,
        14,
        9,
        10,
        5,
        14,
        11,
        11,
        12,
        23,
        4,
        4,
        14,
        8,
        6,
        2,
        7,
        7,
        9,
        6,
        4,
        3,
        2,
        5,
        16,
        14,
        4,
        3,
        15,
        16,
        4,
        15,
        9
      ],
      "2022-07": [
        90,
        7,
        24,
        4,
        3,
        12,
        3,
        3,
        7,
        25,
        9,
        18,
        7,
        31,
        8,
        1,
        9,
        4,
        6,
        4,
        4,
        8,
        5,
        7,
        5,
        1,
        5,
        6,
        14,
        13,
        5,
        1,
        12,
        11,
        1,
        9,
        10
      ],
      "2022-08": [
        49,
        1,
        25,
        3,
        1,
        12,
        8,
        5,
        9,
        11,
        8,
        13,
        9,
        12,
        9,
        0,
        7,
        5,
        3,
        3,
        6,
        4,
        12,
        2,
        3,
        2,
        4,
        1,
        10,
        8,
        6,
        4,
        14,
        16,
        3,
        14,
        11
      ],
      "2022-09": [
        59,
        2,
        17,
        5,
        6,
        13,
        6,
        2,
        7,
        13,
        12,
        18,
        12,
        25,
        13,
        1,
        6,
        9,
        3,
        4,
        11,
        7,
        7,
        3,
        1,
        3,
        4,
        6,
        4,
        8,
        2,
        7,
        14,
        16,
        1,
        11,
        3
      ],
      "2022-10": [
        71,
        2,
        19,
        9,
        4,
        15,
        7,
        5,
        7,
        14,
        9,
        11,
        12,
        18,
        7,
        2,
        12,
        8,
        2,
        3,
        4,
        5,
        7,
        7,
        1,
        0,
        7,
        4,
        8,
        10,
        2,
        0,
        9,
        17,
        1,
        9,
        7
      ],
      "2022-11": [
        76,
        1,
        21,
        4,
        0,
        17,
        8,
        9,
        15,
        8,
        2,
        14,
        16,
        31,
        8,
        3,
        11,
        3,
        6,
        3,
        5,
        11,
        8,
        6,
        1,
        3,
        7,
        5,
        12,
        12,
        2,
        4,
        11,
        20,
        3,
        17,
        12
      ],
      "2022-12": [
        47,
        3,
        14,
        3,
        6,
        8,
        7,
        5,
        10,
        11,
        8,
        14,
        6,
        15,
        2,
        1,
        6,
        5,
        2,
        1,
        1,
        1,
        5,
        1,
        2,
        0,
        1,
        3,
        4,
        6,
        3,
        0,
        9,
        13,
        1,
        12,
        5
      ],
      "2023-01": [
        41,
        1,
        14,
        3,
        1,
        9,
        6,
        3,
        6,
        9,
        2,
        9,
        4,
        9,
        12,
        4,
        11,
        4,
        4,
        3,
        3,
        3,
        6,
        2,
        1,
        2,
        3,
        7,
        9,
        6,
        4,
        0,
        7,
        10,
        4,
        7,
        7
      ],
      "2023-02": [
        43,
        2,
        21,
        1,
        2,
        7,
        9,
        3,
        5,
        16,
        14,
        8,
        7,
        19,
        9,
        3,
        3,
        2,
        4,
        2,
        3,
        3,
        3,
        2,
        4,
        1,
        1,
        2,
        4,
        11,
        3,
        0,
        11,
        10,
        2,
        7,
        3
      ],
      "2023-03": [
        84,
        5,
        26,
        2,
        3,
        12,
        14,
        5,
        11,
        23,
        9,
        20,
        10,
        28,
        22,
        7,
        8,
        10,
        6,
        2,
        0,
        5,
        16,
        5,
        3,
        4,
        6,
        10,
        22,
        15,
        8,
        1,
        19,
        20,
        2,
        11,
        15
      ],
      "2023-04": [
        68,
        1,
        27,
        6,
        2,
        11,
        8,
        8,
        8,
        11,
        10,
        15,
        6,
        18,
        11,
        8,
        18,
        2,
        5,
        4,
        3,
        15,
        5,
        4,
        2,
        6,
        4,
        6,
        13,
        10,
        4,
        2,
        9,
        9,
        3,
        10,
        6
      ],
      "2023-05": [
        55,
        2,
        24,
        3,
        2,
        14,
        6,
        8,
        13,
        17,
        9,
        15,
        9,
        20,
        11,
        1,
        17,
        4,
        6,
        4,
        4,
        10,
        12,
        2,
        5,
        0,
        9,
        5,
        8,
        9,
        8,
        4,
        15,
        20,
        3,
        10,
        13
      ],
      "2023-06": [
        56,
        1,
        35,
        0,
        4,
        17,
        11,
        13,
        9,
        21,
        9,
        12,
        11,
        22,
        15,
        4,
        9,
        6,
        6,
        3,
        3,
        6,
        10,
        2,
        3,
        3,
        4,
        2,
        7,
        12,
        2,
        1,
        11,
        16,
        3,
        10,
        13
      ],
      "2023-07": [
        53,
        2,
        36,
        0,
        3,
        12,
        8,
        9,
        16,
        18,
        8,
        14,
        6,
        28,
        16,
        6,
        16,
        6,
        8,
        8,
        4,
        6,
        18,
        4,
        1,
        2,
        5,
        5,
        7,
        9,
        5,
        1,
        17,
        22,
        4,
        19,
        10
      ],
      "2023-08": [
        69,
        2,
        26,
        4,
        0,
        16,
        8,
        16,
        12,
        13,
        9,
        15,
        8,
        16,
        13,
        4,
        13,
        4,
        4,
        7,
        3,
        12,
        15,
        3,
        9,
        3,
        3,
        8,
        5,
        14,
        3,
        3,
        19,
        11,
        4,
        10,
        10
      ],
      "2023-09": [
        75,
        1,
        26,
        4,
        4,
        21,
        11,
        9,
        9,
        18,
        8,
        16,
        13,
        15,
        13,
        4,
        8,
        5,
        3,
        1,
        6,
        3,
        11,
        4,
        3,
        3,
        6,
        3,
        7,
        5,
        5,
        1,
        10,
        16,
        1,
        9,
        9
      ],
      "2023-10": [
        51,
        2,
        29,
        6,
        3,
        14,
        4,
        7,
        12,
        12,
        5,
        13,
        16,
        19,
        13,
        5,
        12,
        2,
        2,
        5,
        6,
        14,
        13,
        0,
        3,
        1,
        10,
        3,
        9,
        16,
        2,
        5,
        21,
        10,
        3,
        10,
        4
      ],
      "2023-11": [
        64,
        3,
        28,
        5,
        1,
        10,
        5,
        13,
        8,
        9,
        16,
        7,
        6,
        20,
        20,
        2,
        10,
        5,
        6,
        7,
        8,
        20,
        11,
        5,
        2,
        2,
        4,
        7,
        10,
        12,
        2,
        3,
        11,
        15,
        0,
        10,
        9
      ],
      "2023-12": [
        67,
        3,
        21,
        2,
        2,
        17,
        4,
        6,
        14,
        13,
        7,
        15,
        12,
        21,
        15,
        1,
        4,
        3,
        5,
        3,
        3,
        14,
        3,
        3,
        3,
        3,
        6,
        5,
        9,
        7,
        2,
        3,
        9,
        14,
        3,
        13,
        7
      ],
      "2024-01": [
        49,
        2,
        28,
        0,
        2,
        9,
        6,
        9,
        2,
        23,
        6,
        5,
        9,
        19,
        11,
        5,
        16,
        3,
        3,
        3,
        1,
        10,
        7,
        5,
        1,
        5,
        4,
        3,
        8,
        9,
        1,
        6,
        10,
        9,
        2,
        6,
        5
      ],
      "2024-02": [
        59,
        2,
        21,
        3,
        3,
        11,
        4,
        6,
        17,
        29,
        9,
        5,
        4,
        18,
        13,
        5,
        13,
        4,
        5,
        6,
        3,
        5,
        13,
        1,
        2,
        0,
        2,
        2,
        13,
        9,
        2,
        2,
        13,
        14,
        0,
        14,
        4
      ],
      "2024-03": [
        93,
        3,
        36,
        8,
        1,
        15,
        8,
        6,
        13,
        15,
        7,
        19,
        18,
        28,
        10,
        7,
        14,
        6,
        6,
        5,
        8,
        11,
        13,
        2,
        2,
        5,
        4,
        7,
        17,
        15,
        8,
        3,
        18,
        22,
        3,
        10,
        11
      ],
      "2024-04": [
        50,
        1,
        18,
        3,
        1,
        6,
        8,
        7,
        9,
        15,
        8,
        17,
        4,
        24,
        15,
        5,
        7,
        3,
        7,
        4,
        5,
        13,
        8,
        7,
        3,
        3,
        1,
        4,
        17,
        7,
        5,
        2,
        11,
        26,
        0,
        9,
        7
      ],
      "2024-05": [
        61,
        1,
        23,
        1,
        3,
        16,
        6,
        7,
        11,
        24,
        8,
        9,
        6,
        21,
        12,
        7,
        10,
        3,
        5,
        2,
        4,
        10,
        9,
        4,
        1,
        1,
        4,
        4,
        8,
        12,
        1,
        2,
        10,
        19,
        4,
        10,
        5
      ],
      "2024-06": [
        58,
        3,
        28,
        4,
        3,
        11,
        10,
        7,
        11,
        16,
        8,
        12,
        7,
        20,
        16,
        5,
        14,
        9,
        5,
        8,
        5,
        11,
        13,
        2,
        0,
        1,
        4,
        13,
        9,
        13,
        10,
        1,
        8,
        19,
        4,
        11,
        12
      ],
      "2024-07": [
        55,
        2,
        37,
        4,
        2,
        17,
        13,
        8,
        6,
        23,
        9,
        13,
        13,
        18,
        14,
        10,
        9,
        3,
        5,
        7,
        6,
        11,
        14,
        8,
        4,
        3,
        5,
        7,
        14,
        16,
        4,
        6,
        11,
        15,
        7,
        13,
        15
      ],
      "2024-08": [
        64,
        2,
        27,
        2,
        1,
        6,
        6,
        8,
        10,
        15,
        13,
        9,
        5,
        19,
        13,
        9,
        5,
        4,
        1,
        4,
        4,
        11,
        14,
        5,
        5,
        3,
        5,
        5,
        9,
        11,
        1,
        3,
        10,
        18,
        2,
        10,
        8
      ],
      "2024-09": [
        67,
        1,
        29,
        3,
        5,
        16,
        11,
        6,
        11,
        19,
        13,
        19,
        12,
        24,
        14,
        6,
        19,
        5,
        6,
        7,
        3,
        15,
        13,
        1,
        2,
        2,
        8,
        10,
        9,
        11,
        6,
        3,
        15,
        22,
        2,
        12,
        10
      ],
      "2024-10": [
        70,
        1,
        32,
        7,
        0,
        13,
        11,
        6,
        12,
        29,
        9,
        11,
        18,
        31,
        11,
        4,
        13,
        2,
        4,
        3,
        10,
        10,
        11,
        10,
        3,
        1,
        6,
        5,
        12,
        17,
        6,
        3,
        13,
        16,
        2,
        10,
        4
      ],
      "2024-11": [
        62,
        2,
        18,
        1,
        2,
        18,
        4,
        7,
        14,
        21,
        12,
        15,
        11,
        23,
        18,
        7,
        15,
        3,
        4,
        6,
        6,
        12,
        14,
        7,
        2,
        3,
        9,
        7,
        12,
        16,
        5,
        6,
        14,
        18,
        1,
        7,
        6
      ],
      "2024-12": [
        78,
        3,
        26,
        2,
        5,
        11,
        3,
        11,
        8,
        18,
        5,
        13,
        9,
        21,
        14,
        5,
        14,
        5,
        3,
        3,
        6,
        4,
        10,
        6,
        0,
        4,
        3,
        5,
        5,
        8,
        3,
        2,
        13,
        19,
        3,
        16,
        5
      ],
      "2025-01": [
        71,
        2,
        29,
        4,
        0,
        17,
        6,
        10,
        11,
        11,
        13,
        11,
        15,
        16,
        14,
        6,
        11,
        3,
        3,
        4,
        2,
        15,
        9,
        4,
        1,
        3,
        8,
        3,
        4,
        8,
        10,
        3,
        12,
        16,
        1,
        6,
        8
      ],
      "2025-02": [
        52,
        3,
        25,
        3,
        2,
        10,
        4,
        11,
        13,
        15,
        10,
        9,
        7,
        13,
        12,
        1,
        14,
        0,
        8,
        2,
        3,
        5,
        9,
        7,
        2,
        2,
        2,
        3,
        11,
        14,
        7,
        3,
        10,
        13,
        3,
        9,
        4
      ],
      "2025-03": [
        115,
        6,
        25,
        3,
        3,
        12,
        3,
        13,
        9,
        28,
        13,
        15,
        9,
        25,
        14,
        9,
        12,
        1,
        6,
        6,
        4,
        15,
        15,
        4,
        1,
        2,
        8,
        6,
        11,
        20,
        2,
        5,
        17,
        23,
        3,
        10,
        9
      ],
      "2025-04": [
        50,
        0,
        23,
        3,
        3,
        11,
        4,
        13,
        8,
        16,
        9,
        11,
        2,
        14,
        9,
        7,
        9,
        4,
        3,
        4,
        5,
        5,
        6,
        2,
        0,
        1,
        4,
        1,
        6,
        6,
        2,
        2,
        14,
        12,
        2,
        7,
        9
      ],
      "2025-05": [
        98,
        1,
        24,
        2,
        2,
        22,
        1,
        11,
        16,
        24,
        13,
        12,
        11,
        28,
        19,
        4,
        6,
        6,
        2,
        2,
        6,
        11,
        14,
        3,
        4,
        3,
        4,
        11,
        11,
        15,
        5,
        4,
        18,
        19,
        3,
        10,
        10
      ],
      "2025-06": [
        67,
        2,
        26,
        4,
        1,
        18,
        7,
        9,
        17,
        16,
        4,
        11,
        8,
        27,
        17,
        1,
        9,
        3,
        8,
        7,
        5,
        10,
        9,
        1,
        2,
        2,
        7,
        4,
        8,
        10,
        7,
        6,
        21,
        17,
        1,
        3,
        14
      ],
      "2025-07": [
        72,
        2,
        31,
        8,
        3,
        20,
        4,
        8,
        5,
        11,
        5,
        10,
        9,
        13,
        20,
        6,
        15,
        3,
        9,
        8,
        10,
        14,
        14,
        6,
        4,
        4,
        6,
        3,
        11,
        7,
        5,
        2,
        6,
        23,
        3,
        13,
        5
      ],
      "2025-08": [
        94,
        0,
        12,
        1,
        5,
        14,
        4,
        5,
        9,
        10,
        4,
        8,
        13,
        19,
        10,
        6,
        9,
        5,
        8,
        3,
        6,
        18,
        13,
        8,
        1,
        5,
        6,
        8,
        14,
        11,
        4,
        1,
        13,
        15,
        2,
        4,
        9
      ],
      "2025-09": [
        26,
        0,
        8,
        1,
        0,
        4,
        1,
        2,
        3,
        5,
        6,
        4,
        4,
        3,
        6,
        3,
        3,
        1,
        1,
        1,
        2,
        1,
        6,
        2,
        1,
        1,
        1,
        2,
        4,
        3,
        0,
        3,
        5,
        7,
        0,
        2,
        3
      ]
    },
    "papers": {
      "0": [
        {
          "title": "COVID-DA: Deep Domain Adaptation from Typical Pneumonia to COVID-19",
          "year": "2020-04",
          "abstract": "The outbreak of novel coronavirus disease 2019 (COVID-19) has already\ninfected millions of people and is still rapidly spreading all over the globe.\nMost COVID-19 patients suffer from lung infection, so one important diagnostic\nmethod is to screen chest radiography images, e.g., X-Ray or CT images.\nHowever, such examinations are time-consuming and labor-intensive, leading to\nlimited diagnostic efficiency. To solve this issue, AI-based technologies, such\nas deep learning, have been used recently as effective computer-aided means to\nimprove diagnostic efficiency. However, one practical and critical difficulty\nis the limited availability of annotated COVID-19 data, due to the prohibitive\nannotation costs and urgent work of doctors to fight against the pandemic. This\nmakes the learning of deep diagnosis models very challenging. To address this,\nmotivated by that typical pneumonia has similar characteristics with COVID-19\nand many pneumonia datasets are publicly available, we propose to conduct\ndomain knowledge adaptation from typical pneumonia to COVID-19. There are two\nmain challenges: 1) the discrepancy of data distributions between domains; 2)\nthe task difference between the diagnosis of typical pneumonia and COVID-19. To\naddress them, we propose a new deep domain adaptation method for COVID-19\ndiagnosis, namely COVID-DA. Specifically, we alleviate the domain discrepancy\nvia feature adversarial adaptation and handle the task difference issue via a\nnovel classifier separation scheme. In this way, COVID-DA is able to diagnose\nCOVID-19 effectively with only a small number of COVID-19 annotations.\nExtensive experiments verify the effectiveness of COVID-DA and its great\npotential for real-world applications.",
          "arxiv_id": "2005.01577v1"
        },
        {
          "title": "Deep Learning on Chest X-ray Images to Detect and Evaluate Pneumonia Cases at the Era of COVID-19",
          "year": "2020-04",
          "abstract": "Coronavirus disease 2019 (COVID-19) is an infectious disease with first\nsymptoms similar to the flu. COVID-19 appeared first in China and very quickly\nspreads to the rest of the world, causing then the 2019-20 coronavirus\npandemic. In many cases, this disease causes pneumonia. Since pulmonary\ninfections can be observed through radiography images, this paper investigates\ndeep learning methods for automatically analyzing query chest X-ray images with\nthe hope to bring precision tools to health professionals towards screening the\nCOVID-19 and diagnosing confirmed patients. In this context, training datasets,\ndeep learning architectures and analysis strategies have been experimented from\npublicly open sets of chest X-ray images. Tailored deep learning models are\nproposed to detect pneumonia infection cases, notably viral cases. It is\nassumed that viral pneumonia cases detected during an epidemic COVID-19 context\nhave a high probability to presume COVID-19 infections. Moreover, easy-to-apply\nhealth indicators are proposed for estimating infection status and predicting\npatient status from the detected pneumonia cases. Experimental results show\npossibilities of training deep learning models over publicly open sets of chest\nX-ray images towards screening viral pneumonia. Chest X-ray test images of\nCOVID-19 infected patients are successfully diagnosed through detection models\nretained for their performances. The efficiency of proposed health indicators\nis highlighted through simulated scenarios of patients presenting infections\nand health problems by combining real and synthetic health data.",
          "arxiv_id": "2004.03399v1"
        },
        {
          "title": "Auto-Lesion Segmentation with a Novel Intensity Dark Channel Prior for COVID-19 Detection",
          "year": "2023-09",
          "abstract": "During the COVID-19 pandemic, medical imaging techniques like computed\ntomography (CT) scans have demonstrated effectiveness in combating the rapid\nspread of the virus. Therefore, it is crucial to conduct research on\ncomputerized models for the detection of COVID-19 using CT imaging. A novel\nprocessing method has been developed, utilizing radiomic features, to assist in\nthe CT-based diagnosis of COVID-19. Given the lower specificity of traditional\nfeatures in distinguishing between different causes of pulmonary diseases, the\nobjective of this study is to develop a CT-based radiomics framework for the\ndifferentiation of COVID-19 from other lung diseases. The model is designed to\nfocus on outlining COVID-19 lesions, as traditional features often lack\nspecificity in this aspect. The model categorizes images into three classes:\nCOVID-19, non-COVID-19, or normal. It employs enhancement auto-segmentation\nprinciples using intensity dark channel prior (IDCP) and deep neural networks\n(ALS-IDCP-DNN) within a defined range of analysis thresholds. A publicly\navailable dataset comprising COVID-19, normal, and non-COVID-19 classes was\nutilized to validate the proposed model's effectiveness. The best performing\nclassification model, Residual Neural Network with 50 layers (Resnet-50),\nattained an average accuracy, precision, recall, and F1-score of 98.8%, 99%,\n98%, and 98% respectively. These results demonstrate the capability of our\nmodel to accurately classify COVID-19 images, which could aid radiologists in\ndiagnosing suspected COVID-19 patients. Furthermore, our model's performance\nsurpasses that of more than 10 current state-of-the-art studies conducted on\nthe same dataset.",
          "arxiv_id": "2309.12638v2"
        }
      ],
      "1": [
        {
          "title": "Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey",
          "year": "2023-02",
          "abstract": "Generative Adversarial Networks (GANs) have been very successful for\nsynthesizing the images in a given dataset. The artificially generated images\nby GANs are very realistic. The GANs have shown potential usability in several\ncomputer vision applications, including image generation, image-to-image\ntranslation, video synthesis, and others. Conventionally, the generator network\nis the backbone of GANs, which generates the samples and the discriminator\nnetwork is used to facilitate the training of the generator network. The\ndiscriminator network is usually a Convolutional Neural Network (CNN). Whereas,\nthe generator network is usually either an Up-CNN for image generation or an\nEncoder-Decoder network for image-to-image translation. The convolution-based\nnetworks exploit the local relationship in a layer, which requires the deep\nnetworks to extract the abstract features. Hence, CNNs suffer to exploit the\nglobal relationship in the feature space. However, recently developed\nTransformer networks are able to exploit the global relationship at every\nlayer. The Transformer networks have shown tremendous performance improvement\nfor several problems in computer vision. Motivated from the success of\nTransformer networks and GANs, recent works have tried to exploit the\nTransformers in GAN framework for the image/video synthesis. This paper\npresents a comprehensive survey on the developments and advancements in GANs\nutilizing the Transformer networks for computer vision applications. The\nperformance comparison for several applications on benchmark datasets is also\nperformed and analyzed. The conducted survey will be very useful to deep\nlearning and computer vision community to understand the research trends \\&\ngaps related with Transformer-based GANs and to develop the advanced GAN\narchitectures by exploiting the global and local relationships for different\napplications.",
          "arxiv_id": "2302.08641v1"
        },
        {
          "title": "Generating Embroidery Patterns Using Image-to-Image Translation",
          "year": "2020-03",
          "abstract": "In many scenarios in computer vision, machine learning, and computer\ngraphics, there is a requirement to learn the mapping from an image of one\ndomain to an image of another domain, called Image-to-image translation. For\nexample, style transfer, object transfiguration, visually altering the\nappearance of weather conditions in an image, changing the appearance of a day\nimage into a night image or vice versa, photo enhancement, to name a few. In\nthis paper, we propose two machine learning techniques to solve the embroidery\nimage-to-image translation. Our goal is to generate a preview image which looks\nsimilar to an embroidered image, from a user-uploaded image. Our techniques are\nmodifications of two existing techniques, neural style transfer, and\ncycle-consistent generative-adversarial network. Neural style transfer renders\nthe semantic content of an image from one domain in the style of a different\nimage in another domain, whereas a cycle-consistent generative adversarial\nnetwork learns the mapping from an input image to output image without any\npaired training data, and also learn a loss function to train this mapping.\nFurthermore, the techniques we propose are independent of any embroidery\nattributes, such as elevation of the image, light-source, start, and endpoints\nof a stitch, type of stitch used, fabric type, etc. Given the user image, our\ntechniques can generate a preview image which looks similar to an embroidered\nimage. We train and test our propose techniques on an embroidery dataset which\nconsist of simple 2D images. To do so, we prepare an unpaired embroidery\ndataset with more than 8000 user-uploaded images along with embroidered images.\nEmpirical results show that these techniques successfully generate an\napproximate preview of an embroidered version of a user image, which can help\nusers in decision making.",
          "arxiv_id": "2003.02909v1"
        },
        {
          "title": "Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies",
          "year": "2020-12",
          "abstract": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains such as computer vision, medicine, and natural\nlanguage processing. Moreover, transforming an object or person to a desired\nshape become a well-studied research in the GANs. GANs are powerful models for\nlearning complex distributions to synthesize semantically meaningful samples.\nHowever, there is a lack of comprehensive review in this field, especially lack\nof a collection of GANs loss-variant, evaluation metrics, remedies for diverse\nimage generation, and stable training. Given the current fast GANs development,\nin this survey, we provide a comprehensive review of adversarial models for\nimage synthesis. We summarize the synthetic image generation methods, and\ndiscuss the categories including image-to-image translation, fusion image\ngeneration, label-to-image mapping, and text-to-image translation. We organize\nthe literature based on their base models, developed ideas related to\narchitectures, constraints, loss functions, evaluation metrics, and training\ndatasets. We present milestones of adversarial models, review an extensive\nselection of previous works in various categories, and present insights on the\ndevelopment route from the model-based to data-driven methods. Further, we\nhighlight a range of potential future research directions. One of the unique\nfeatures of this review is that all software implementations of these GAN\nmethods and datasets have been collected and made available in one place at\nhttps://github.com/pshams55/GAN-Case-Study.",
          "arxiv_id": "2012.13736v1"
        }
      ],
      "2": [
        {
          "title": "CATS v2: Hybrid encoders for robust medical segmentation",
          "year": "2023-08",
          "abstract": "Convolutional Neural Networks (CNNs) have exhibited strong performance in\nmedical image segmentation tasks by capturing high-level (local) information,\nsuch as edges and textures. However, due to the limited field of view of\nconvolution kernel, it is hard for CNNs to fully represent global information.\nRecently, transformers have shown good performance for medical image\nsegmentation due to their ability to better model long-range dependencies.\nNevertheless, transformers struggle to capture high-level spatial features as\neffectively as CNNs. A good segmentation model should learn a better\nrepresentation from local and global features to be both precise and\nsemantically accurate. In our previous work, we proposed CATS, which is a\nU-shaped segmentation network augmented with transformer encoder. In this work,\nwe further extend this model and propose CATS v2 with hybrid encoders.\nSpecifically, hybrid encoders consist of a CNN-based encoder path paralleled to\na transformer path with a shifted window, which better leverage both local and\nglobal information to produce robust 3D medical image segmentation. We fuse the\ninformation from the convolutional encoder and the transformer at the skip\nconnections of different resolutions to form the final segmentation. The\nproposed method is evaluated on three public challenge datasets: Beyond the\nCranial Vault (BTCV), Cross-Modality Domain Adaptation (CrossMoDA) and task 5\nof Medical Segmentation Decathlon (MSD-5), to segment abdominal organs,\nvestibular schwannoma (VS) and prostate, respectively. Compared with the\nstate-of-the-art methods, our approach demonstrates superior performance in\nterms of higher Dice scores. Our code is publicly available at\nhttps://github.com/MedICL-VU/CATS.",
          "arxiv_id": "2308.06377v3"
        },
        {
          "title": "From CNN to Transformer: A Review of Medical Image Segmentation Models",
          "year": "2023-08",
          "abstract": "Medical image segmentation is an important step in medical image analysis,\nespecially as a crucial prerequisite for efficient disease diagnosis and\ntreatment. The use of deep learning for image segmentation has become a\nprevalent trend. The widely adopted approach currently is U-Net and its\nvariants. Additionally, with the remarkable success of pre-trained models in\nnatural language processing tasks, transformer-based models like TransUNet have\nachieved desirable performance on multiple medical image segmentation datasets.\nIn this paper, we conduct a survey of the most representative four medical\nimage segmentation models in recent years. We theoretically analyze the\ncharacteristics of these models and quantitatively evaluate their performance\non two benchmark datasets (i.e., Tuberculosis Chest X-rays and ovarian tumors).\nFinally, we discuss the main challenges and future trends in medical image\nsegmentation. Our work can assist researchers in the related field to quickly\nestablish medical segmentation models tailored to specific regions.",
          "arxiv_id": "2308.05305v1"
        },
        {
          "title": "Towards Segment Anything Model (SAM) for Medical Image Segmentation: A Survey",
          "year": "2023-05",
          "abstract": "Due to the flexibility of prompting, foundation models have become the\ndominant force in the domains of natural language processing and image\ngeneration. With the recent introduction of the Segment Anything Model (SAM),\nthe prompt-driven paradigm has entered the realm of image segmentation,\nbringing with a range of previously unexplored capabilities. However, it\nremains unclear whether it can be applicable to medical image segmentation due\nto the significant differences between natural images and medical images.In\nthis work, we summarize recent efforts to extend the success of SAM to medical\nimage segmentation tasks, including both empirical benchmarking and\nmethodological adaptations, and discuss potential future directions for SAM in\nmedical image segmentation. Although directly applying SAM to medical image\nsegmentation cannot obtain satisfying performance on multi-modal and\nmulti-target medical datasets, many insights are drawn to guide future research\nto develop foundation models for medical image analysis. To facilitate future\nresearch, we maintain an active repository that contains up-to-date paper list\nand open-source project summary at https://github.com/YichiZhang98/SAM4MIS.",
          "arxiv_id": "2305.03678v3"
        }
      ],
      "3": [
        {
          "title": "Holographic single particle imaging for weakly scattering, heterogeneous nanoscale objects",
          "year": "2022-10",
          "abstract": "Single particle imaging (SPI) at X-ray free electron lasers (XFELs) is a\ntechnique to determine the 3D structure of nanoscale objects like biomolecules\nfrom a large number of diffraction patterns of copies of these objects in\nrandom orientations. Millions of low signal-to-noise diffraction patterns with\nunknown orientation are collected during an X-ray SPI experiment. The patterns\nare then analyzed and merged using a reconstruction algorithm to retrieve the\nfull 3D-structure of particle. The resolution of reconstruction is limited by\nbackground noise, signal-to-noise ratio in diffraction patterns and total\namount of data collected. We recently introduced a reference-enhanced\nholographic single particle imaging methodology [Optica 7,593-601(2020)] to\ncollect high enough signal-to-noise and background tolerant patterns and a\nreconstruction algorithm to recover missing parameters beyond orientation and\nthen directly retrieve the full Fourier model of the sample of interest. Here\nwe describe a phase retrieval algorithm based on maximum likelihood estimation\nusing pattern search dubbed as MaxLP, with better scalability for fine sampling\nof latent parameters and much better performance in the low signal limit.\nFurthermore, we show that structural variations within the target particle are\naveraged in real space, significantly improving robustness to conformational\nheterogeneity in comparison to conventional SPI. With these computational\nimprovements, we believe reference-enhanced SPI is capable of reaching sub-nm\nresolution biomolecule imaging.",
          "arxiv_id": "2210.10611v1"
        },
        {
          "title": "Large-scale phase retrieval",
          "year": "2021-04",
          "abstract": "High-throughput computational imaging requires efficient processing\nalgorithms to retrieve multi-dimensional and multi-scale information. In\ncomputational phase imaging, phase retrieval (PR) is required to reconstruct\nboth amplitude and phase in complex space from intensity-only measurements. The\nexisting PR algorithms suffer from the tradeoff among low computational\ncomplexity, robustness to measurement noise and strong generalization on\ndifferent modalities. In this work, we report an efficient large-scale phase\nretrieval technique termed as LPR. It extends the plug-and-play\ngeneralized-alternating-projection framework from real space to nonlinear\ncomplex space. The alternating projection solver and enhancing neural network\nare respectively derived to tackle the measurement formation and statistical\nprior regularization. This framework compensates the shortcomings of each\noperator, so as to realize high-fidelity phase retrieval with low computational\ncomplexity and strong generalization. We applied the technique for a series of\ncomputational phase imaging modalities including coherent diffraction imaging,\ncoded diffraction pattern imaging, and Fourier ptychographic microscopy.\nExtensive simulations and experiments validate that the technique outperforms\nthe existing PR algorithms with as much as 17dB enhancement on signal-to-noise\nratio, and more than one order-of-magnitude increased running efficiency.\nBesides, we for the first time demonstrate ultra-large-scale phase retrieval at\nthe 8K level (7680$\\times$4320 pixels) in minute-level time.",
          "arxiv_id": "2104.03148v1"
        },
        {
          "title": "Single-pixel coherent diffraction imaging",
          "year": "2020-03",
          "abstract": "Complex-field imaging is indispensable for numerous applications at\nwavelengths from X-ray to THz, with amplitude describing transmittance (or\nreflectivity) and phase revealing intrinsic structure of the target object.\nCoherent diffraction imaging (CDI) employs iterative phase retrieval algorithms\nto process diffraction measurements and is the predominant non-interferometric\nmethod to image complex fields. However, the working spectrum of CDI is quite\nnarrow, because the diffraction measurements on which it relies require dense\narray detection with ultra-high dynamic range. Here we report a single-pixel\nCDI technique that works for a wide waveband. A single-pixel detector instead\nof an array sensor is employed in the far field for detection. It repeatedly\nrecords the DC-only component of the diffracted wavefront scattered from an\nobject as it is illuminated by a sequence of binary modulation patterns. This\ndecreases the measurements' dynamic range by several orders of magnitude. We\nemploy an efficient single-pixel phase-retrieval algorithm to jointly recover\nthe object's 2D amplitude and phase maps from the 1D intensity-only\nmeasurements. No a priori object information is needed in the recovery process.\nWe validate the technique's quantitative phase imaging nature using both\ncalibrated phase objects and biological samples, and demonstrate its wide\nworking spectrum with both 488-nm visible light and 980-nm near-infrared light.\nOur approach paves the way for complex-field imaging in a wider waveband where\n2D detector arrays are not available, with broad applications in life and\nmaterial sciences.",
          "arxiv_id": "2003.14237v1"
        }
      ],
      "4": [
        {
          "title": "TinyVIRAT: Low-resolution Video Action Recognition",
          "year": "2020-07",
          "abstract": "The existing research in action recognition is mostly focused on high-quality\nvideos where the action is distinctly visible. In real-world surveillance\nenvironments, the actions in videos are captured at a wide range of\nresolutions. Most activities occur at a distance with a small resolution and\nrecognizing such activities is a challenging problem. In this work, we focus on\nrecognizing tiny actions in videos. We introduce a benchmark dataset,\nTinyVIRAT, which contains natural low-resolution activities. The actions in\nTinyVIRAT videos have multiple labels and they are extracted from surveillance\nvideos which makes them realistic and more challenging. We propose a novel\nmethod for recognizing tiny actions in videos which utilizes a progressive\ngenerative approach to improve the quality of low-resolution actions. The\nproposed method also consists of a weakly trained attention mechanism which\nhelps in focusing on the activity regions in the video. We perform extensive\nexperiments to benchmark the proposed TinyVIRAT dataset and observe that the\nproposed method significantly improves the action recognition performance over\nbaselines. We also evaluate the proposed approach on synthetically resized\naction recognition datasets and achieve state-of-the-art results when compared\nwith existing methods. The dataset and code is publicly available at\nhttps://github.com/UgurDemir/Tiny-VIRAT.",
          "arxiv_id": "2007.07355v1"
        },
        {
          "title": "Issues in Object Detection in Videos using Common Single-Image CNNs",
          "year": "2021-05",
          "abstract": "A growing branch of computer vision is object detection. Object detection is\nused in many applications such as industrial process, medical imaging analysis,\nand autonomous vehicles. The ability to detect objects in videos is crucial.\nObject detection systems are trained on large image datasets. For applications\nsuch as autonomous vehicles, it is crucial that the object detection system can\nidentify objects through multiple frames in video. There are many problems with\napplying these systems to video. Shadows or changes in brightness that can\ncause the system to incorrectly identify objects frame to frame and cause an\nunintended system response. There are many neural networks that have been used\nfor object detection and if there was a way of connecting objects between\nframes then these problems could be eliminated. For these neural networks to\nget better at identifying objects in video, they need to be re-trained. A\ndataset must be created with images that represent consecutive video frames and\nhave matching ground-truth layers. A method is proposed that can generate these\ndatasets. The ground-truth layer contains only moving objects. To generate this\nlayer, FlowNet2-Pytorch was used to create the flow mask using the novel\nMagnitude Method. As well, a segmentation mask will be generated using networks\nsuch as Mask R-CNN or Refinenet. These segmentation masks will contain all\nobjects detected in a frame. By comparing this segmentation mask to the flow\nmask ground-truth layer, a loss function is generated. This loss function can\nbe used to train a neural network to be better at making consistent predictions\non video. The system was tested on multiple video samples and a loss was\ngenerated for each frame, proving the Magnitude Method's ability to be used to\ntrain object detection neural networks in future work.",
          "arxiv_id": "2105.12822v1"
        },
        {
          "title": "End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection",
          "year": "2020-04",
          "abstract": "Reliable and accurate 3D object detection is a necessity for safe autonomous\ndriving. Although LiDAR sensors can provide accurate 3D point cloud estimates\nof the environment, they are also prohibitively expensive for many settings.\nRecently, the introduction of pseudo-LiDAR (PL) has led to a drastic reduction\nin the accuracy gap between methods based on LiDAR sensors and those based on\ncheap stereo cameras. PL combines state-of-the-art deep neural networks for 3D\ndepth estimation with those for 3D object detection by converting 2D depth map\noutputs to 3D point cloud inputs. However, so far these two networks have to be\ntrained separately. In this paper, we introduce a new framework based on\ndifferentiable Change of Representation (CoR) modules that allow the entire PL\npipeline to be trained end-to-end. The resulting framework is compatible with\nmost state-of-the-art networks for both tasks and in combination with PointRCNN\nimproves over PL consistently across all benchmarks -- yielding the highest\nentry on the KITTI image-based 3D object detection leaderboard at the time of\nsubmission. Our code will be made available at\nhttps://github.com/mileyan/pseudo-LiDAR_e2e.",
          "arxiv_id": "2004.03080v2"
        }
      ],
      "5": [
        {
          "title": "Motion-Informed Deep Learning for Brain MR Image Reconstruction Framework",
          "year": "2024-05",
          "abstract": "Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the\nfrequently occurring artifacts due to patient movements during scanning. Motion\nis estimated to be present in approximately 30% of clinical MRI scans; however,\nmotion has not been explicitly modeled within deep learning image\nreconstruction models. Deep learning (DL) algorithms have been demonstrated to\nbe effective for both the image reconstruction task and the motion correction\ntask, but the two tasks are considered separately. The image reconstruction\ntask involves removing undersampling artifacts such as noise and aliasing\nartifacts, whereas motion correction involves removing artifacts including\nblurring, ghosting, and ringing. In this work, we propose a novel method to\nsimultaneously accelerate imaging and correct motion. This is achieved by\nintegrating a motion module into the deep learning-based MRI reconstruction\nprocess, enabling real-time detection and correction of motion. We model motion\nas a tightly integrated auxiliary layer in the deep learning model during\ntraining, making the deep learning model 'motion-informed'. During inference,\nimage reconstruction is performed from undersampled raw k-space data using a\ntrained motion-informed DL model. Experimental results demonstrate that the\nproposed motion-informed deep learning image reconstruction network\noutperformed the conventional image reconstruction network for motion-degraded\nMRI datasets.",
          "arxiv_id": "2405.17756v1"
        },
        {
          "title": "CL-MRI: Self-Supervised Contrastive Learning to Improve the Accuracy of Undersampled MRI Reconstruction",
          "year": "2023-06",
          "abstract": "In Magnetic Resonance Imaging (MRI), image acquisitions are often\nundersampled in the measurement domain to accelerate the scanning process, at\nthe expense of image quality. However, image quality is a crucial factor that\ninfluences the accuracy of clinical diagnosis; hence, high-quality image\nreconstruction from undersampled measurements has been a key area of research.\nRecently, deep learning (DL) methods have emerged as the state-of-the-art for\nMRI reconstruction, typically involving deep neural networks to transform\nundersampled MRI images into high-quality MRI images through data-driven\nprocesses. Nevertheless, there is clear and significant room for improvement in\nundersampled DL MRI reconstruction to meet the high standards required for\nclinical diagnosis, in terms of eliminating aliasing artifacts and reducing\nimage noise. In this paper, we introduce a self-supervised pretraining\nprocedure using contrastive learning to improve the accuracy of undersampled DL\nMRI reconstruction. We use contrastive learning to transform the MRI image\nrepresentations into a latent space that maximizes mutual information among\ndifferent undersampled representations and optimizes the information content at\nthe input of the downstream DL reconstruction models. Our experiments\ndemonstrate improved reconstruction accuracy across a range of acceleration\nfactors and datasets, both quantitatively and qualitatively. Furthermore, our\nextended experiments validate the proposed framework's robustness under\nadversarial conditions, such as measurement noise, different k-space sampling\npatterns, and pathological abnormalities, and also prove the transfer learning\ncapabilities on MRI datasets with completely different anatomy. Additionally,\nwe conducted experiments to visualize and analyze the properties of the\nproposed MRI contrastive learning latent space.",
          "arxiv_id": "2306.00530v3"
        },
        {
          "title": "Deep Cardiac MRI Reconstruction with ADMM",
          "year": "2023-10",
          "abstract": "Cardiac magnetic resonance imaging is a valuable non-invasive tool for\nidentifying cardiovascular diseases. For instance, Cine MRI is the benchmark\nmodality for assessing the cardiac function and anatomy. On the other hand,\nmulti-contrast (T1 and T2) mapping has the potential to assess pathologies and\nabnormalities in the myocardium and interstitium. However, voluntary\nbreath-holding and often arrhythmia, in combination with MRI's slow imaging\nspeed, can lead to motion artifacts, hindering real-time acquisition image\nquality. Although performing accelerated acquisitions can facilitate dynamic\nimaging, it induces aliasing, causing low reconstructed image quality in Cine\nMRI and inaccurate T1 and T2 mapping estimation. In this work, inspired by\nrelated work in accelerated MRI reconstruction, we present a deep learning\n(DL)-based method for accelerated cine and multi-contrast reconstruction in the\ncontext of dynamic cardiac imaging. We formulate the reconstruction problem as\na least squares regularized optimization task, and employ vSHARP, a\nstate-of-the-art DL-based inverse problem solver, which incorporates\nhalf-quadratic variable splitting and the alternating direction method of\nmultipliers with neural networks. We treat the problem in two setups; a 2D\nreconstruction and a 2D dynamic reconstruction task, and employ 2D and 3D deep\nlearning networks, respectively. Our method optimizes in both the image and\nk-space domains, allowing for high reconstruction fidelity. Although the target\ndata is undersampled with a Cartesian equispaced scheme, we train our model\nusing both Cartesian and simulated non-Cartesian undersampling schemes to\nenhance generalization of the model to unseen data. Furthermore, our model\nadopts a deep neural network to learn and refine the sensitivity maps of\nmulti-coil k-space data. Lastly, our method is jointly trained on both,\nundersampled cine and multi-contrast data.",
          "arxiv_id": "2310.06628v1"
        }
      ],
      "6": [
        {
          "title": "Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain Adaptation",
          "year": "2021-03",
          "abstract": "Despite the successes of deep neural networks on many challenging vision\ntasks, they often fail to generalize to new test domains that are not\ndistributed identically to the training data. The domain adaptation becomes\nmore challenging for cross-modality medical data with a notable domain shift.\nGiven that specific annotated imaging modalities may not be accessible nor\ncomplete. Our proposed solution is based on the cross-modality synthesis of\nmedical images to reduce the costly annotation burden by radiologists and\nbridge the domain gap in radiological images. We present a novel approach for\nimage-to-image translation in medical images, capable of supervised or\nunsupervised (unpaired image data) setups. Built upon adversarial training, we\npropose a learnable self-attentive spatial normalization of the deep\nconvolutional generator network's intermediate activations. Unlike previous\nattention-based image-to-image translation approaches, which are either\ndomain-specific or require distortion of the source domain's structures, we\nunearth the importance of the auxiliary semantic information to handle the\ngeometric changes and preserve anatomical structures during image translation.\nWe achieve superior results for cross-modality segmentation between unpaired\nMRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI\n(T1/T2) datasets compared to the state-of-the-art methods. We also observe\nencouraging results in cross-modality conversion for paired MRI and CT images\non a brain dataset. Furthermore, a detailed analysis of the cross-modality\nimage translation, thorough ablation studies confirm our proposed method's\nefficacy.",
          "arxiv_id": "2103.03781v1"
        },
        {
          "title": "SAG-GAN: Semi-Supervised Attention-Guided GANs for Data Augmentation on Medical Images",
          "year": "2020-11",
          "abstract": "Recently deep learning methods, in particular, convolutional neural networks\n(CNNs), have led to a massive breakthrough in the range of computer vision.\nAlso, the large-scale annotated dataset is the essential key to a successful\ntraining procedure. However, it is a huge challenge to get such datasets in the\nmedical domain. Towards this, we present a data augmentation method for\ngenerating synthetic medical images using cycle-consistency Generative\nAdversarial Networks (GANs). We add semi-supervised attention modules to\ngenerate images with convincing details. We treat tumor images and normal\nimages as two domains. The proposed GANs-based model can generate a tumor image\nfrom a normal image, and in turn, it can also generate a normal image from a\ntumor image. Furthermore, we show that generated medical images can be used for\nimproving the performance of ResNet18 for medical image classification. Our\nmodel is applied to three limited datasets of tumor MRI images. We first\ngenerate MRI images on limited datasets, then we trained three popular\nclassification models to get the best model for tumor classification. Finally,\nwe train the classification model using real images with classic data\naugmentation methods and classification models using synthetic images. The\nclassification results between those trained models showed that the proposed\nSAG-GAN data augmentation method can boost Accuracy and AUC compare with\nclassic data augmentation methods. We believe the proposed data augmentation\nmethod can apply to other medical image domains, and improve the accuracy of\ncomputer-assisted diagnosis.",
          "arxiv_id": "2011.07534v1"
        },
        {
          "title": "A 3D generative model of pathological multi-modal MR images and segmentations",
          "year": "2023-11",
          "abstract": "Generative modelling and synthetic data can be a surrogate for real medical\nimaging datasets, whose scarcity and difficulty to share can be a nuisance when\ndelivering accurate deep learning models for healthcare applications. In recent\nyears, there has been an increased interest in using these models for data\naugmentation and synthetic data sharing, using architectures such as generative\nadversarial networks (GANs) or diffusion models (DMs). Nonetheless, the\napplication of synthetic data to tasks such as 3D magnetic resonance imaging\n(MRI) segmentation remains limited due to the lack of labels associated with\nthe generated images. Moreover, many of the proposed generative MRI models lack\nthe ability to generate arbitrary modalities due to the absence of explicit\ncontrast conditioning. These limitations prevent the user from adjusting the\ncontrast and content of the images and obtaining more generalisable data for\ntraining task-specific models. In this work, we propose brainSPADE3D, a 3D\ngenerative model for brain MRI and associated segmentations, where the user can\ncondition on specific pathological phenotypes and contrasts. The proposed joint\nimaging-segmentation generative model is shown to generate high-fidelity\nsynthetic images and associated segmentations, with the ability to combine\npathologies. We demonstrate how the model can alleviate issues with\nsegmentation model performance when unexpected pathologies are present in the\ndata.",
          "arxiv_id": "2311.04552v1"
        }
      ],
      "7": [
        {
          "title": "BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery",
          "year": "2024-02",
          "abstract": "Satellites equipped with optical sensors capture high-resolution imagery,\nproviding valuable insights into various environmental phenomena. In recent\nyears, there has been a surge of research focused on addressing some challenges\nin remote sensing, ranging from water detection in diverse landscapes to the\nsegmentation of mountainous and terrains. Ongoing investigations goals to\nenhance the precision and efficiency of satellite imagery analysis. Especially,\nthere is a growing emphasis on developing methodologies for accurate water body\ndetection, snow and clouds, important for environmental monitoring, resource\nmanagement, and disaster response. Within this context, this paper focus on the\ncloud segmentation from remote sensing imagery. Accurate remote sensing data\nanalysis can be challenging due to the presence of clouds in optical\nsensor-based applications. The quality of resulting products such as\napplications and research is directly impacted by cloud detection, which plays\na key role in the remote sensing data processing pipeline. This paper examines\nseven cutting-edge semantic segmentation and detection algorithms applied to\nclouds identification, conducting a benchmark analysis to evaluate their\narchitectural approaches and identify the most performing ones. To increase the\nmodel's adaptability, critical elements including the type of imagery and the\namount of spectral bands used during training are analyzed. Additionally, this\nresearch tries to produce machine learning algorithms that can perform cloud\nsegmentation using only a few spectral bands, including RGB and RGBN-IR\ncombinations. The model's flexibility for a variety of applications and user\nscenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as\ndatasets. This benchmark can be reproduced using the material from this github\nlink: https://github.com/toelt-llc/cloud_segmentation_comparative.",
          "arxiv_id": "2402.13918v3"
        },
        {
          "title": "The QXS-SAROPT Dataset for Deep Learning in SAR-Optical Data Fusion",
          "year": "2021-03",
          "abstract": "Deep learning techniques have made an increasing impact on the field of\nremote sensing. However, deep neural networks based fusion of multimodal data\nfrom different remote sensors with heterogenous characteristics has not been\nfully explored, due to the lack of availability of big amounts of perfectly\naligned multi-sensor image data with diverse scenes of high resolutions,\nespecially for synthetic aperture radar (SAR) data and optical imagery. To\npromote the development of deep learning based SAR-optical fusion approaches,\nwe release the QXS-SAROPT dataset, which contains 20,000 pairs of SAR-optical\nimage patches. We obtain the SAR patches from SAR satellite GaoFen-3 images and\nthe optical patches from Google Earth images. These images cover three port\ncities: San Diego, Shanghai and Qingdao. Here, we present a detailed\nintroduction of the construction of the dataset, and show its two\nrepresentative exemplary applications, namely SAR-optical image matching and\nSAR ship detection boosted by cross-modal information from optical images. As a\nlarge open SAR-optical dataset with multiple scenes of a high resolution, we\nbelieve QXS-SAROPT will be of potential value for further research in\nSAR-optical data fusion technology based on deep learning.",
          "arxiv_id": "2103.08259v2"
        },
        {
          "title": "OpenEarthMap-SAR: A Benchmark Synthetic Aperture Radar Dataset for Global High-Resolution Land Cover Mapping",
          "year": "2025-01",
          "abstract": "High-resolution land cover mapping plays a crucial role in addressing a wide\nrange of global challenges, including urban planning, environmental monitoring,\ndisaster response, and sustainable development. However, creating accurate,\nlarge-scale land cover datasets remains a significant challenge due to the\ninherent complexities of geospatial data, such as diverse terrain, varying\nsensor modalities, and atmospheric conditions. Synthetic Aperture Radar (SAR)\nimagery, with its ability to penetrate clouds and capture data in all-weather,\nday-and-night conditions, offers unique advantages for land cover mapping.\nDespite these strengths, the lack of benchmark datasets tailored for SAR\nimagery has limited the development of robust models specifically designed for\nthis data modality. To bridge this gap and facilitate advancements in SAR-based\ngeospatial analysis, we introduce OpenEarthMap-SAR, a benchmark SAR dataset,\nfor global high-resolution land cover mapping. OpenEarthMap-SAR consists of 1.5\nmillion segments of 5033 aerial and satellite images with the size of\n1024$\\times$1024 pixels, covering 35 regions from Japan, France, and the USA,\nwith partially manually annotated and fully pseudo 8-class land cover labels at\na ground sampling distance of 0.15--0.5 m. We evaluated the performance of\nstate-of-the-art methods for semantic segmentation and present challenging\nproblem settings suitable for further technical development. The dataset also\nserves the official dataset for IEEE GRSS Data Fusion Contest Track I. The\ndataset has been made publicly available at\nhttps://zenodo.org/records/14622048.",
          "arxiv_id": "2501.10891v2"
        }
      ],
      "8": [
        {
          "title": "Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction",
          "year": "2025-06",
          "abstract": "The generalization of deep learning-based low-dose computed tomography (CT)\nreconstruction models to doses unseen in the training data is important and\nremains challenging. Previous efforts heavily rely on paired data to improve\nthe generalization performance and robustness through collecting either diverse\nCT data for re-training or a few test data for fine-tuning. Recently, diffusion\nmodels have shown promising and generalizable performance in low-dose CT (LDCT)\nreconstruction, however, they may produce unrealistic structures due to the CT\nimage noise deviating from Gaussian distribution and imprecise prior\ninformation from the guidance of noisy LDCT images. In this paper, we propose a\nnoise-inspired diffusion model for generalizable LDCT reconstruction, termed\nNEED, which tailors diffusion models for noise characteristics of each domain.\nFirst, we propose a novel shifted Poisson diffusion model to denoise projection\ndata, which aligns the diffusion process with the noise model in pre-log LDCT\nprojections. Second, we devise a doubly guided diffusion model to refine\nreconstructed images, which leverages LDCT images and initial reconstructions\nto more accurately locate prior information and enhance reconstruction\nfidelity. By cascading these two diffusion models for dual-domain\nreconstruction, our NEED requires only normal-dose data for training and can be\neffectively extended to various unseen dose levels during testing via a time\nstep matching strategy. Extensive qualitative, quantitative, and\nsegmentation-based evaluations on two datasets demonstrate that our NEED\nconsistently outperforms state-of-the-art methods in reconstruction and\ngeneralization performance. Source code is made available at\nhttps://github.com/qgao21/NEED.",
          "arxiv_id": "2506.22012v1"
        },
        {
          "title": "Diffusion Probabilistic Priors for Zero-Shot Low-Dose CT Image Denoising",
          "year": "2023-05",
          "abstract": "Denoising low-dose computed tomography (CT) images is a critical task in\nmedical image computing. Supervised deep learning-based approaches have made\nsignificant advancements in this area in recent years. However, these methods\ntypically require pairs of low-dose and normal-dose CT images for training,\nwhich are challenging to obtain in clinical settings. Existing unsupervised\ndeep learning-based methods often require training with a large number of\nlow-dose CT images or rely on specially designed data acquisition processes to\nobtain training data. To address these limitations, we propose a novel\nunsupervised method that only utilizes normal-dose CT images during training,\nenabling zero-shot denoising of low-dose CT images. Our method leverages the\ndiffusion model, a powerful generative model. We begin by training a cascaded\nunconditional diffusion model capable of generating high-quality normal-dose CT\nimages from low-resolution to high-resolution. The cascaded architecture makes\nthe training of high-resolution diffusion models more feasible. Subsequently,\nwe introduce low-dose CT images into the reverse process of the diffusion model\nas likelihood, combined with the priors provided by the diffusion model and\niteratively solve multiple maximum a posteriori (MAP) problems to achieve\ndenoising. Additionally, we propose methods to adaptively adjust the\ncoefficients that balance the likelihood and prior in MAP estimations, allowing\nfor adaptation to different noise levels in low-dose CT images. We test our\nmethod on low-dose CT datasets of different regions with varying dose levels.\nThe results demonstrate that our method outperforms the state-of-the-art\nunsupervised method and surpasses several supervised deep learning-based\nmethods. Codes are available in https://github.com/DeepXuan/Dn-Dp.",
          "arxiv_id": "2305.15887v2"
        },
        {
          "title": "End-to-End Deep Learning for Interior Tomography with Low-Dose X-ray CT",
          "year": "2025-01",
          "abstract": "Objective: There exist several X-ray computed tomography (CT) scanning\nstrategies to reduce a radiation dose, such as (1) sparse-view CT, (2) low-dose\nCT, and (3) region-of-interest (ROI) CT (called interior tomography). To\nfurther reduce the dose, the sparse-view and/or low-dose CT settings can be\napplied together with interior tomography. Interior tomography has various\nadvantages in terms of reducing the number of detectors and decreasing the\nX-ray radiation dose. However, a large patient or small field-of-view (FOV)\ndetector can cause truncated projections, and then the reconstructed images\nsuffer from severe cupping artifacts. In addition, although the low-dose CT can\nreduce the radiation exposure dose, analytic reconstruction algorithms produce\nimage noise. Recently, many researchers have utilized image-domain deep\nlearning (DL) approaches to remove each artifact and demonstrated impressive\nperformances, and the theory of deep convolutional framelets supports the\nreason for the performance improvement. Approach: In this paper, we found that\nthe image-domain convolutional neural network (CNN) is difficult to solve\ncoupled artifacts, based on deep convolutional framelets. Significance: To\naddress the coupled problem, we decouple it into two sub-problems: (i) image\ndomain noise reduction inside truncated projection to solve low-dose CT problem\nand (ii) extrapolation of projection outside truncated projection to solve the\nROI CT problem. The decoupled sub-problems are solved directly with a novel\nproposed end-to-end learning using dual-domain CNNs. Main results: We\ndemonstrate that the proposed method outperforms the conventional image-domain\ndeep learning methods, and a projection-domain CNN shows better performance\nthan the image-domain CNNs which are commonly used by many researchers.",
          "arxiv_id": "2501.05085v1"
        }
      ],
      "9": [
        {
          "title": "A Practical Approach for Rate-Distortion-Perception Analysis in Learned Image Compression",
          "year": "2021-04",
          "abstract": "Rate-distortion optimization (RDO) of codecs, where distortion is quantified\nby the mean-square error, has been a standard practice in image/video\ncompression over the years. RDO serves well for optimization of codec\nperformance for evaluation of the results in terms of PSNR. However, it is well\nknown that the PSNR does not correlate well with perceptual evaluation of\nimages; hence, RDO is not well suited for perceptual optimization of codecs.\nRecently, rate-distortion-perception trade-off has been formalized by taking\nthe Kullback-Leibner (KL) divergence between the distributions of the original\nand reconstructed images as a perception measure. Learned image compression\nmethods that simultaneously optimize rate, mean-square loss, VGG loss, and an\nadversarial loss were proposed. Yet, there exists no easy approach to fix the\nrate, distortion or perception at a desired level in a practical learned image\ncompression solution to perform an analysis of the trade-off between rate,\ndistortion and perception measures. In this paper, we propose a practical\napproach to fix the rate to carry out perception-distortion analysis at a fixed\nrate in order to perform perceptual evaluation of image compression results in\na principled manner. Experimental results provide several insights for\npractical rate-distortion-perception analysis in learned image compression.",
          "arxiv_id": "2104.14836v1"
        },
        {
          "title": "ANFIC: Image Compression Using Augmented Normalizing Flows",
          "year": "2021-07",
          "abstract": "This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.",
          "arxiv_id": "2107.08470v2"
        },
        {
          "title": "Learning End-to-End Lossy Image Compression: A Benchmark",
          "year": "2020-02",
          "abstract": "Image compression is one of the most fundamental techniques and commonly used\napplications in the image and video processing field. Earlier methods built a\nwell-designed pipeline, and efforts were made to improve all modules of the\npipeline by handcrafted tuning. Later, tremendous contributions were made,\nespecially when data-driven methods revitalized the domain with their excellent\nmodeling capacities and flexibility in incorporating newly designed modules and\nconstraints. Despite great progress, a systematic benchmark and comprehensive\nanalysis of end-to-end learned image compression methods are lacking. In this\npaper, we first conduct a comprehensive literature survey of learned image\ncompression methods. The literature is organized based on several aspects to\njointly optimize the rate-distortion performance with a neural network, i.e.,\nnetwork architecture, entropy model and rate control. We describe milestones in\ncutting-edge learned image-compression methods, review a broad range of\nexisting works, and provide insights into their historical development routes.\nWith this survey, the main challenges of image compression methods are\nrevealed, along with opportunities to address the related issues with recent\nadvanced learning methods. This analysis provides an opportunity to take a\nfurther step towards higher-efficiency image compression. By introducing a\ncoarse-to-fine hyperprior model for entropy estimation and signal\nreconstruction, we achieve improved rate-distortion performance, especially on\nhigh-resolution images. Extensive benchmark experiments demonstrate the\nsuperiority of our model in rate-distortion performance and time complexity on\nmulti-core CPUs and GPUs. Our project website is available at\nhttps://huzi96.github.io/compression-bench.html.",
          "arxiv_id": "2002.03711v4"
        }
      ],
      "10": [
        {
          "title": "Automatic Detection of Microaneurysms in OCT Images Using Bag of Features",
          "year": "2022-05",
          "abstract": "Diabetic Retinopathy (DR) caused by diabetes occurs as a result of changes in\nthe retinal vessels and causes visual impairment. Microaneurysms (MAs) are the\nearly clinical signs of DR, whose timely diagnosis can help detecting DR in the\nearly stages of its development. It has been observed that MAs are more common\nin the inner retinal layers compared to the outer retinal layers in eyes\nsuffering from DR. Optical Coherence Tomography (OCT) is a noninvasive imaging\ntechnique that provides a cross-sectional view of the retina and it has been\nused in recent years to diagnose many eye diseases. As a result, in this paper\nhas attempted to identify areas with MA from normal areas of the retina using\nOCT images. This work is done using the dataset collected from FA and OCT\nimages of 20 patients with DR. In this regard, firstly Fluorescein Angiography\n(FA) and OCT images were registered. Then the MA and normal areas were\nseparated and the features of each of these areas were extracted using the Bag\nof Features (BOF) approach with Speeded-Up Robust Feature (SURF) descriptor.\nFinally, the classification process was performed using a multilayer perceptron\nnetwork. For each of the criteria of accuracy, sensitivity, specificity, and\nprecision, the obtained results were 96.33%, 97.33%, 95.4%, and 95.28%,\nrespectively. Utilizing OCT images to detect MAsautomatically is a new idea and\nthe results obtained as preliminary research in this field are promising .",
          "arxiv_id": "2205.04695v1"
        },
        {
          "title": "Automated segmentation of retinal fluid volumes from structural and angiographic optical coherence tomography using deep learning",
          "year": "2020-06",
          "abstract": "Purpose: We proposed a deep convolutional neural network (CNN), named Retinal\nFluid Segmentation Network (ReF-Net) to segment volumetric retinal fluid on\noptical coherence tomography (OCT) volume. Methods: 3 x 3-mm OCT scans were\nacquired on one eye by a 70-kHz OCT commercial AngioVue system (RTVue-XR;\nOptovue, Inc.) from 51 participants in a clinical diabetic retinopathy (DR)\nstudy (45 with retinal edema and 6 healthy controls). A CNN with U-Net-like\narchitecture was constructed to detect and segment the retinal fluid.\nCross-sectional OCT and angiography (OCTA) scans were used for training and\ntesting ReF-Net. The effect of including OCTA data for retinal fluid\nsegmentation was investigated in this study. Volumetric retinal fluid can be\nconstructed using the output of ReF-Net.\nArea-under-Receiver-Operating-Characteristic-curve (AROC),\nintersection-over-union (IoU), and F1-score were calculated to evaluate the\nperformance of ReF-Net. Results: ReF-Net shows high accuracy (F1 = 0.864 +/-\n0.084) in retinal fluid segmentation. The performance can be further improved\n(F1 = 0.892 +/- 0.038) by including information from both OCTA and structural\nOCT. ReF-Net also shows strong robustness to shadow artifacts. Volumetric\nretinal fluid can provide more comprehensive information than the 2D area,\nwhether cross-sectional or en face projections. Conclusions: A\ndeep-learning-based method can accurately segment retinal fluid volumetrically\non OCT/OCTA scans with strong robustness to shadow artifacts. OCTA data can\nimprove retinal fluid segmentation. Volumetric representations of retinal fluid\nare superior to 2D projections. Translational Relevance: Using a deep learning\nmethod to segment retinal fluid volumetrically has the potential to improve the\ndiagnostic accuracy of diabetic macular edema by OCT systems.",
          "arxiv_id": "2006.02569v1"
        },
        {
          "title": "OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods",
          "year": "2023-12",
          "abstract": "Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.",
          "arxiv_id": "2312.08255v4"
        }
      ],
      "11": [
        {
          "title": "Joint Multi-Scale Tone Mapping and Denoising for HDR Image Enhancement",
          "year": "2023-03",
          "abstract": "An image processing unit (IPU), or image signal processor (ISP) for high\ndynamic range (HDR) imaging usually consists of demosaicing, white balancing,\nlens shading correction, color correction, denoising, and tone-mapping. Besides\nnoise from the imaging sensors, almost every step in the ISP introduces or\namplifies noise in different ways, and denoising operators are designed to\nreduce the noise from these sources. Designed for dynamic range compressing,\ntone-mapping operators in an ISP can significantly amplify the noise level,\nespecially for images captured in low-light conditions, making denoising very\ndifficult. Therefore, we propose a joint multi-scale denoising and tone-mapping\nframework that is designed with both operations in mind for HDR images. Our\njoint network is trained in an end-to-end format that optimizes both operators\ntogether, to prevent the tone-mapping operator from overwhelming the denoising\noperator. Our model outperforms existing HDR denoising and tone-mapping\noperators both quantitatively and qualitatively on most of our benchmarking\ndatasets.",
          "arxiv_id": "2303.09071v2"
        },
        {
          "title": "Cycle-Interactive Generative Adversarial Network for Robust Unsupervised Low-Light Enhancement",
          "year": "2022-07",
          "abstract": "Getting rid of the fundamental limitations in fitting to the paired training\ndata, recent unsupervised low-light enhancement methods excel in adjusting\nillumination and contrast of images. However, for unsupervised low light\nenhancement, the remaining noise suppression issue due to the lacking of\nsupervision of detailed signal largely impedes the wide deployment of these\nmethods in real-world applications. Herein, we propose a novel\nCycle-Interactive Generative Adversarial Network (CIGAN) for unsupervised\nlow-light image enhancement, which is capable of not only better transferring\nillumination distributions between low/normal-light images but also\nmanipulating detailed signals between two domains, e.g.,\nsuppressing/synthesizing realistic noise in the cyclic enhancement/degradation\nprocess. In particular, the proposed low-light guided transformation\nfeed-forwards the features of low-light images from the generator of\nenhancement GAN (eGAN) into the generator of degradation GAN (dGAN). With the\nlearned information of real low-light images, dGAN can synthesize more\nrealistic diverse illumination and contrast in low-light images. Moreover, the\nfeature randomized perturbation module in dGAN learns to increase the feature\nrandomness to produce diverse feature distributions, persuading the synthesized\nlow-light images to contain realistic noise. Extensive experiments demonstrate\nboth the superiority of the proposed method and the effectiveness of each\nmodule in CIGAN.",
          "arxiv_id": "2207.00965v1"
        },
        {
          "title": "Deep Bilateral Retinex for Low-Light Image Enhancement",
          "year": "2020-07",
          "abstract": "Low-light images, i.e. the images captured in low-light conditions, suffer\nfrom very poor visibility caused by low contrast, color distortion and\nsignificant measurement noise. Low-light image enhancement is about improving\nthe visibility of low-light images. As the measurement noise in low-light\nimages is usually significant yet complex with spatially-varying\ncharacteristic, how to handle the noise effectively is an important yet\nchallenging problem in low-light image enhancement. Based on the Retinex\ndecomposition of natural images, this paper proposes a deep learning method for\nlow-light image enhancement with a particular focus on handling the measurement\nnoise. The basic idea is to train a neural network to generate a set of\npixel-wise operators for simultaneously predicting the noise and the\nillumination layer, where the operators are defined in the bilateral space.\nSuch an integrated approach allows us to have an accurate prediction of the\nreflectance layer in the presence of significant spatially-varying measurement\nnoise. Extensive experiments on several benchmark datasets have shown that the\nproposed method is very competitive to the state-of-the-art methods, and has\nsignificant advantage over others when processing images captured in extremely\nlow lighting conditions.",
          "arxiv_id": "2007.02018v1"
        }
      ],
      "12": [
        {
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization",
          "year": "2021-07",
          "abstract": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "arxiv_id": "2107.13200v1"
        },
        {
          "title": "Brain-Aware Readout Layers in GNNs: Advancing Alzheimer's early Detection and Neuroimaging",
          "year": "2024-10",
          "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder characterized by\nprogressive memory and cognitive decline, affecting millions worldwide.\nDiagnosing AD is challenging due to its heterogeneous nature and variable\nprogression. This study introduces a novel brain-aware readout layer (BA\nreadout layer) for Graph Neural Networks (GNNs), designed to improve\ninterpretability and predictive accuracy in neuroimaging for early AD\ndiagnosis. By clustering brain regions based on functional connectivity and\nnode embedding, this layer improves the GNN's capability to capture complex\nbrain network characteristics. We analyzed neuroimaging data from 383\nparticipants, including both cognitively normal and preclinical AD individuals,\nusing T1-weighted MRI, resting-state fMRI, and FBB-PET to construct brain\ngraphs. Our results show that GNNs with the BA readout layer significantly\noutperform traditional models in predicting the Preclinical Alzheimer's\nCognitive Composite (PACC) score, demonstrating higher robustness and\nstability. The adaptive BA readout layer also offers enhanced interpretability\nby highlighting task-specific brain regions critical to cognitive functions\nimpacted by AD. These findings suggest that our approach provides a valuable\ntool for the early diagnosis and analysis of Alzheimer's disease.",
          "arxiv_id": "2410.14683v1"
        },
        {
          "title": "Predicting conversion of mild cognitive impairment to Alzheimer's disease",
          "year": "2022-03",
          "abstract": "Alzheimer's disease (AD) is the most common age-related dementia. Mild\ncognitive impairment (MCI) is the early stage of cognitive decline before AD.\nIt is crucial to predict the MCI-to-AD conversion for precise management, which\nremains challenging due to the diversity of patients. Previous evidence shows\nthat the brain network generated from diffusion MRI promises to classify\ndementia using deep learning. However, the limited availability of diffusion\nMRI challenges the model training. In this study, we develop a self-supervised\ncontrastive learning approach to generate structural brain networks from\nroutine anatomical MRI under the guidance of diffusion MRI. The generated brain\nnetworks are applied to train a learning framework for predicting the MCI-to-AD\nconversion. Instead of directly modelling the AD brain networks, we train a\ngraph encoder and a variational autoencoder to model the healthy ageing\ntrajectories from brain networks of healthy controls. To predict the MCI-to-AD\nconversion, we further design a recurrent neural networks based approach to\nmodel the longitudinal deviation of patients' brain networks from the healthy\nageing trajectory. Numerical results show that the proposed methods outperform\nthe benchmarks in the prediction task. We also visualize the model\ninterpretation to explain the prediction and identify abnormal changes of white\nmatter tracts.",
          "arxiv_id": "2203.04725v1"
        }
      ],
      "13": [
        {
          "title": "A Lightweight Image Super-Resolution Transformer Trained on Low-Resolution Images Only",
          "year": "2025-03",
          "abstract": "Transformer architectures prominently lead single-image super-resolution\n(SISR) benchmarks, reconstructing high-resolution (HR) images from their\nlow-resolution (LR) counterparts. Their strong representative power, however,\ncomes with a higher demand for training data compared to convolutional neural\nnetworks (CNNs). For many real-world SR applications, the availability of\nhigh-quality HR training images is not given, sparking interest in LR-only\ntraining methods. The LR-only SISR benchmark mimics this condition by allowing\nonly low-resolution (LR) images for model training. For a 4x super-resolution,\nthis effectively reduces the amount of available training data to 6.25% of the\nHR image pixels, which puts the employment of a data-hungry transformer model\ninto question. In this work, we are the first to utilize a lightweight vision\ntransformer model with LR-only training methods addressing the unsupervised\nSISR LR-only benchmark. We adopt and configure a recent LR-only training method\nfrom microscopy image super-resolution to macroscopic real-world data,\nresulting in our multi-scale training method for bicubic degradation (MSTbic).\nFurthermore, we compare it with reference methods and prove its effectiveness\nboth for a transformer and a CNN model. We evaluate on the classic SR benchmark\ndatasets Set5, Set14, BSD100, Urban100, and Manga109, and show superior\nperformance over state-of-the-art (so far: CNN-based) LR-only SISR methods. The\ncode is available on GitHub:\nhttps://github.com/ifnspaml/SuperResolutionMultiscaleTraining.",
          "arxiv_id": "2503.23265v1"
        },
        {
          "title": "Deep Cyclic Generative Adversarial Residual Convolutional Networks for Real Image Super-Resolution",
          "year": "2020-09",
          "abstract": "Recent deep learning based single image super-resolution (SISR) methods\nmostly train their models in a clean data domain where the low-resolution (LR)\nand the high-resolution (HR) images come from noise-free settings (same domain)\ndue to the bicubic down-sampling assumption. However, such degradation process\nis not available in real-world settings. We consider a deep cyclic network\nstructure to maintain the domain consistency between the LR and HR data\ndistributions, which is inspired by the recent success of CycleGAN in the\nimage-to-image translation applications. We propose the Super-Resolution\nResidual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a\ngenerative adversarial network (GAN) framework for the LR to HR domain\ntranslation in an end-to-end manner. We demonstrate our proposed approach in\nthe quantitative and qualitative experiments that generalize well to the real\nimage super-resolution and it is easy to deploy for the mobile/embedded\ndevices. In addition, our SR results on the AIM 2020 Real Image SR Challenge\ndatasets demonstrate that the proposed SR approach achieves comparable results\nas the other state-of-art methods.",
          "arxiv_id": "2009.03693v1"
        },
        {
          "title": "ACDMSR: Accelerated Conditional Diffusion Models for Single Image Super-Resolution",
          "year": "2023-07",
          "abstract": "Diffusion models have gained significant popularity in the field of\nimage-to-image translation. Previous efforts applying diffusion models to image\nsuper-resolution (SR) have demonstrated that iteratively refining pure Gaussian\nnoise using a U-Net architecture trained on denoising at various noise levels\ncan yield satisfactory high-resolution images from low-resolution inputs.\nHowever, this iterative refinement process comes with the drawback of low\ninference speed, which strongly limits its applications. To speed up inference\nand further enhance the performance, our research revisits diffusion models in\nimage super-resolution and proposes a straightforward yet significant diffusion\nmodel-based super-resolution method called ACDMSR (accelerated conditional\ndiffusion model for image super-resolution). Specifically, our method adapts\nthe standard diffusion model to perform super-resolution through a\ndeterministic iterative denoising process. Our study also highlights the\neffectiveness of using a pre-trained SR model to provide the conditional image\nof the given low-resolution (LR) image to achieve superior high-resolution\nresults. We demonstrate that our method surpasses previous attempts in\nqualitative and quantitative results through extensive experiments conducted on\nbenchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109.\nMoreover, our approach generates more visually realistic counterparts for\nlow-resolution images, emphasizing its effectiveness in practical scenarios.",
          "arxiv_id": "2307.00781v1"
        }
      ],
      "14": [
        {
          "title": "Unsupervised Hyperspectral and Multispectral Image Blind Fusion Based on Deep Tucker Decomposition Network with Spatial-Spectral Manifold Learning",
          "year": "2024-09",
          "abstract": "Hyperspectral and multispectral image fusion aims to generate high spectral\nand spatial resolution hyperspectral images (HR-HSI) by fusing high-resolution\nmultispectral images (HR-MSI) and low-resolution hyperspectral images (LR-HSI).\nHowever, existing fusion methods encounter challenges such as unknown\ndegradation parameters, incomplete exploitation of the correlation between\nhigh-dimensional structures and deep image features. To overcome these issues,\nin this article, an unsupervised blind fusion method for hyperspectral and\nmultispectral images based on Tucker decomposition and spatial spectral\nmanifold learning (DTDNML) is proposed. We design a novel deep Tucker\ndecomposition network that maps LR-HSI and HR-MSI into a consistent feature\nspace, achieving reconstruction through decoders with shared parameter. To\nbetter exploit and fuse spatial-spectral features in the data, we design a core\ntensor fusion network that incorporates a spatial spectral attention mechanism\nfor aligning and fusing features at different scales. Furthermore, to enhance\nthe capacity in capturing global information, a Laplacian-based\nspatial-spectral manifold constraints is introduced in shared-decoders.\nSufficient experiments have validated that this method enhances the accuracy\nand efficiency of hyperspectral and multispectral fusion on different remote\nsensing datasets. The source code is available at\nhttps://github.com/Shawn-H-Wang/DTDNML.",
          "arxiv_id": "2409.09670v2"
        },
        {
          "title": "Learning Spatial-Spectral Prior for Super-Resolution of Hyperspectral Imagery",
          "year": "2020-05",
          "abstract": "Recently, single gray/RGB image super-resolution reconstruction task has been\nextensively studied and made significant progress by leveraging the advanced\nmachine learning techniques based on deep convolutional neural networks\n(DCNNs). However, there has been limited technical development focusing on\nsingle hyperspectral image super-resolution due to the high-dimensional and\ncomplex spectral patterns in hyperspectral image. In this paper, we make a step\nforward by investigating how to adapt state-of-the-art residual learning based\nsingle gray/RGB image super-resolution approaches for computationally efficient\nsingle hyperspectral image super-resolution, referred as SSPSR. Specifically,\nwe introduce a spatial-spectral prior network (SSPN) to fully exploit the\nspatial information and the correlation between the spectra of the\nhyperspectral data. Considering that the hyperspectral training samples are\nscarce and the spectral dimension of hyperspectral image data is very high, it\nis nontrivial to train a stable and effective deep network. Therefore, a group\nconvolution (with shared network parameters) and progressive upsampling\nframework is proposed. This will not only alleviate the difficulty in feature\nextraction due to high-dimension of the hyperspectral data, but also make the\ntraining process more stable. To exploit the spatial and spectral prior, we\ndesign a spatial-spectral block (SSB), which consists of a spatial residual\nmodule and a spectral attention residual module. Experimental results on some\nhyperspectral images demonstrate that the proposed SSPSR method enhances the\ndetails of the recovered high-resolution hyperspectral images, and outperforms\nstate-of-the-arts. The source code is available at\n\\url{https://github.com/junjun-jiang/SSPSR",
          "arxiv_id": "2005.08752v2"
        },
        {
          "title": "Cross-Scope Spatial-Spectral Information Aggregation for Hyperspectral Image Super-Resolution",
          "year": "2023-11",
          "abstract": "Hyperspectral image super-resolution has attained widespread prominence to\nenhance the spatial resolution of hyperspectral images. However,\nconvolution-based methods have encountered challenges in harnessing the global\nspatial-spectral information. The prevailing transformer-based methods have not\nadequately captured the long-range dependencies in both spectral and spatial\ndimensions. To alleviate this issue, we propose a novel cross-scope\nspatial-spectral Transformer (CST) to efficiently investigate long-range\nspatial and spectral similarities for single hyperspectral image\nsuper-resolution. Specifically, we devise cross-attention mechanisms in spatial\nand spectral dimensions to comprehensively model the long-range\nspatial-spectral characteristics. By integrating global information into the\nrectangle-window self-attention, we first design a cross-scope spatial\nself-attention to facilitate long-range spatial interactions. Then, by\nleveraging appropriately characteristic spatial-spectral features, we construct\na cross-scope spectral self-attention to effectively capture the intrinsic\ncorrelations among global spectral bands. Finally, we elaborate a concise\nfeed-forward neural network to enhance the feature representation capacity in\nthe Transformer structure. Extensive experiments over three hyperspectral\ndatasets demonstrate that the proposed CST is superior to other\nstate-of-the-art methods both quantitatively and visually. The code is\navailable at \\url{https://github.com/Tomchenshi/CST.git}.",
          "arxiv_id": "2311.17340v1"
        }
      ],
      "15": [
        {
          "title": "Context-Aware Self-Supervised Learning of Whole Slide Images",
          "year": "2023-06",
          "abstract": "Presenting whole slide images (WSIs) as graph will enable a more efficient\nand accurate learning framework for cancer diagnosis. Due to the fact that a\nsingle WSI consists of billions of pixels and there is a lack of vast annotated\ndatasets required for computational pathology, the problem of learning from\nWSIs using typical deep learning approaches such as convolutional neural\nnetwork (CNN) is challenging. Additionally, WSIs down-sampling may lead to the\nloss of data that is essential for cancer detection. A novel two-stage learning\ntechnique is presented in this work. Since context, such as topological\nfeatures in the tumor surroundings, may hold important information for cancer\ngrading and diagnosis, a graph representation capturing all dependencies among\nregions in the WSI is very intuitive. Graph convolutional network (GCN) is\ndeployed to include context from the tumor and adjacent tissues, and\nself-supervised learning is used to enhance training through unlabeled data.\nMore specifically, the entire slide is presented as a graph, where the nodes\ncorrespond to the patches from the WSI. The proposed framework is then tested\nusing WSIs from prostate and kidney cancers. To assess the performance\nimprovement through self-supervised mechanism, the proposed context-aware model\nis tested with and without use of pre-trained self-supervised layer. The\noverall model is also compared with multi-instance learning (MIL) based and\nother existing approaches.",
          "arxiv_id": "2306.04763v1"
        },
        {
          "title": "LESS: Label-efficient Multi-scale Learning for Cytological Whole Slide Image Screening",
          "year": "2023-06",
          "abstract": "In computational pathology, multiple instance learning (MIL) is widely used\nto circumvent the computational impasse in giga-pixel whole slide image (WSI)\nanalysis. It usually consists of two stages: patch-level feature extraction and\nslide-level aggregation. Recently, pretrained models or self-supervised\nlearning have been used to extract patch features, but they suffer from low\neffectiveness or inefficiency due to overlooking the task-specific supervision\nprovided by slide labels. Here we propose a weakly-supervised Label-Efficient\nWSI Screening method, dubbed LESS, for cytological WSI analysis with only\nslide-level labels, which can be effectively applied to small datasets. First,\nwe suggest using variational positive-unlabeled (VPU) learning to uncover\nhidden labels of both benign and malignant patches. We provide appropriate\nsupervision by using slide-level labels to improve the learning of patch-level\nfeatures. Next, we take into account the sparse and random arrangement of cells\nin cytological WSIs. To address this, we propose a strategy to crop patches at\nmultiple scales and utilize a cross-attention vision transformer (CrossViT) to\ncombine information from different scales for WSI classification. The\ncombination of our two steps achieves task-alignment, improving effectiveness\nand efficiency. We validate the proposed label-efficient method on a urine\ncytology WSI dataset encompassing 130 samples (13,000 patches) and FNAC 2019\ndataset with 212 samples (21,200 patches). The experiment shows that the\nproposed LESS reaches 84.79%, 85.43%, 91.79% and 78.30% on a urine cytology WSI\ndataset, and 96.88%, 96.86%, 98.95%, 97.06% on FNAC 2019 dataset in terms of\naccuracy, AUC, sensitivity and specificity. It outperforms state-of-the-art MIL\nmethods on pathology WSIs and realizes automatic cytological WSI cancer\nscreening.",
          "arxiv_id": "2306.03407v2"
        },
        {
          "title": "Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images",
          "year": "2020-04",
          "abstract": "The rapidly emerging field of computational pathology has the potential to\nenable objective diagnosis, therapeutic response prediction and identification\nof new morphological features of clinical relevance. However, deep\nlearning-based computational pathology approaches either require manual\nannotation of gigapixel whole slide images (WSIs) in fully-supervised settings\nor thousands of WSIs with slide-level labels in a weakly-supervised setting.\nMoreover, whole slide level computational pathology methods also suffer from\ndomain adaptation and interpretability issues. These challenges have prevented\nthe broad adaptation of computational pathology for clinical and research\npurposes. Here we present CLAM - Clustering-constrained attention multiple\ninstance learning, an easy-to-use, high-throughput, and interpretable WSI-level\nprocessing and learning method that only requires slide-level labels while\nbeing data efficient, adaptable and capable of handling multi-class subtyping\nproblems. CLAM is a deep-learning-based weakly-supervised method that uses\nattention-based learning to automatically identify sub-regions of high\ndiagnostic value in order to accurately classify the whole slide, while also\nutilizing instance-level clustering over the representative regions identified\nto constrain and refine the feature space. In three separate analyses, we\ndemonstrate the data efficiency and adaptability of CLAM and its superior\nperformance over standard weakly-supervised classification. We demonstrate that\nCLAM models are interpretable and can be used to identify well-known and new\nmorphological features. We further show that models trained using CLAM are\nadaptable to independent test cohorts, cell phone microscopy images, and\nbiopsies. CLAM is a general-purpose and adaptable method that can be used for a\nvariety of different computational pathology tasks in both clinical and\nresearch settings.",
          "arxiv_id": "2004.09666v2"
        }
      ],
      "16": [
        {
          "title": "Hybrid Multihead Attentive Unet-3D for Brain Tumor Segmentation",
          "year": "2024-05",
          "abstract": "Brain tumor segmentation is a critical task in medical image analysis, aiding\nin the diagnosis and treatment planning of brain tumor patients. The importance\nof automated and accurate brain tumor segmentation cannot be overstated. It\nenables medical professionals to precisely delineate tumor regions, assess\ntumor growth or regression, and plan targeted treatments. Various deep\nlearning-based techniques proposed in the literature have made significant\nprogress in this field, however, they still face limitations in terms of\naccuracy due to the complex and variable nature of brain tumor morphology. In\nthis research paper, we propose a novel Hybrid Multihead Attentive U-Net\narchitecture, to address the challenges in accurate brain tumor segmentation,\nand to capture complex spatial relationships and subtle tumor boundaries. The\nU-Net architecture has proven effective in capturing contextual information and\nfeature representations, while attention mechanisms enhance the model's ability\nto focus on informative regions and refine the segmentation boundaries. By\nintegrating these two components, our proposed architecture improves accuracy\nin brain tumor segmentation. We test our proposed model on the BraTS 2020\nbenchmark dataset and compare its performance with the state-of-the-art\nwell-known SegNet, FCN-8s, and Dense121 U-Net architectures. The results show\nthat our proposed model outperforms the others in terms of the evaluated\nperformance metrics.",
          "arxiv_id": "2405.13304v1"
        },
        {
          "title": "QuickTumorNet: Fast Automatic Multi-Class Segmentation of Brain Tumors",
          "year": "2020-12",
          "abstract": "Non-invasive techniques such as magnetic resonance imaging (MRI) are widely\nemployed in brain tumor diagnostics. However, manual segmentation of brain\ntumors from 3D MRI volumes is a time-consuming task that requires trained\nexpert radiologists. Due to the subjectivity of manual segmentation, there is\nlow inter-rater reliability which can result in diagnostic discrepancies. As\nthe success of many brain tumor treatments depends on early intervention, early\ndetection is paramount. In this context, a fully automated segmentation method\nfor brain tumor segmentation is necessary as an efficient and reliable method\nfor brain tumor detection and quantification. In this study, we propose an\nend-to-end approach for brain tumor segmentation, capitalizing on a modified\nversion of QuickNAT, a brain tissue type segmentation deep convolutional neural\nnetwork (CNN). Our method was evaluated on a data set of 233 patient's T1\nweighted images containing three tumor type classes annotated (meningioma,\nglioma, and pituitary). Our model, QuickTumorNet, demonstrated fast, reliable,\nand accurate brain tumor segmentation that can be utilized to assist clinicians\nin diagnosis and treatment.",
          "arxiv_id": "2012.12410v1"
        },
        {
          "title": "HI-Net: Hyperdense Inception 3D UNet for Brain Tumor Segmentation",
          "year": "2020-12",
          "abstract": "The brain tumor segmentation task aims to classify tissue into the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) classes using multimodel\nMRI images. Quantitative analysis of brain tumors is critical for clinical\ndecision making. While manual segmentation is tedious, time-consuming, and\nsubjective, this task is at the same time very challenging to automatic\nsegmentation methods. Thanks to the powerful learning ability, convolutional\nneural networks (CNNs), mainly fully convolutional networks, have shown\npromising brain tumor segmentation. This paper further boosts the performance\nof brain tumor segmentation by proposing hyperdense inception 3D UNet (HI-Net),\nwhich captures multi-scale information by stacking factorization of 3D weighted\nconvolutional layers in the residual inception block. We use hyper dense\nconnections among factorized convolutional layers to extract more contexual\ninformation, with the help of features reusability. We use a dice loss function\nto cope with class imbalances. We validate the proposed architecture on the\nmulti-modal brain tumor segmentation challenges (BRATS) 2020 testing dataset.\nPreliminary results on the BRATS 2020 testing set show that achieved by our\nproposed approach, the dice (DSC) scores of ET, WT, and TC are 0.79457,\n0.87494, and 0.83712, respectively.",
          "arxiv_id": "2012.06760v1"
        }
      ],
      "17": [
        {
          "title": "Spatio-Temporal Pruning and Quantization for Low-latency Spiking Neural Networks",
          "year": "2021-04",
          "abstract": "Spiking Neural Networks (SNNs) are a promising alternative to traditional\ndeep learning methods since they perform event-driven information processing.\nHowever, a major drawback of SNNs is high inference latency. The efficiency of\nSNNs could be enhanced using compression methods such as pruning and\nquantization. Notably, SNNs, unlike their non-spiking counterparts, consist of\na temporal dimension, the compression of which can lead to latency reduction.\nIn this paper, we propose spatial and temporal pruning of SNNs. First,\nstructured spatial pruning is performed by determining the layer-wise\nsignificant dimensions using principal component analysis of the average\naccumulated membrane potential of the neurons. This step leads to 10-14X model\ncompression. Additionally, it enables inference with lower latency and\ndecreases the spike count per inference. To further reduce latency, temporal\npruning is performed by gradually reducing the timesteps while training. The\nnetworks are trained using surrogate gradient descent based backpropagation and\nwe validate the results on CIFAR10 and CIFAR100, using VGG architectures. The\nspatiotemporally pruned SNNs achieve 89.04% and 66.4% accuracy on CIFAR10 and\nCIFAR100, respectively, while performing inference with 3-30X reduced latency\ncompared to state-of-the-art SNNs. Moreover, they require 8-14X lesser compute\nenergy compared to their unpruned standard deep learning counterparts. The\nenergy numbers are obtained by multiplying the number of operations with energy\nper operation. These SNNs also provide 1-4% higher robustness against Gaussian\nnoise corrupted inputs. Furthermore, we perform weight quantization and find\nthat performance remains reasonably stable up to 5-bit quantization.",
          "arxiv_id": "2104.12528v2"
        },
        {
          "title": "Hardware-Algorithm Co-design Enabling Processing-in-Pixel-in-Memory (P2M) for Neuromorphic Vision Sensors",
          "year": "2023-10",
          "abstract": "The high volume of data transmission between the edge sensor and the cloud\nprocessor leads to energy and throughput bottlenecks for resource-constrained\nedge devices focused on computer vision. Hence, researchers are investigating\ndifferent approaches (e.g., near-sensor processing, in-sensor processing,\nin-pixel processing) by executing computations closer to the sensor to reduce\nthe transmission bandwidth. Specifically, in-pixel processing for neuromorphic\nvision sensors (e.g., dynamic vision sensors (DVS)) involves incorporating\nasynchronous multiply-accumulate (MAC) operations within the pixel array,\nresulting in improved energy efficiency. In a CMOS implementation, low overhead\nenergy-efficient analog MAC accumulates charges on a passive capacitor;\nhowever, the capacitor's limited charge retention time affects the algorithmic\nintegration time choices, impacting the algorithmic accuracy, bandwidth,\nenergy, and training efficiency. Consequently, this results in a design\ntrade-off on the hardware aspect-creating a need for a low-leakage compute unit\nwhile maintaining the area and energy benefits. In this work, we present a\nholistic analysis of the hardware-algorithm co-design trade-off based on the\nlimited integration time posed by the hardware and techniques to improve the\nleakage performance of the in-pixel analog MAC operations.",
          "arxiv_id": "2310.16844v1"
        },
        {
          "title": "Embedded event based object detection with spiking neural network",
          "year": "2024-06",
          "abstract": "The complexity of event-based object detection (OD) poses considerable\nchallenges. Spiking Neural Networks (SNNs) show promising results and pave the\nway for efficient event-based OD. Despite this success, the path to efficient\nSNNs on embedded devices remains a challenge. This is due to the size of the\nnetworks required to accomplish the task and the ability of devices to take\nadvantage of SNNs benefits. Even when \"edge\" devices are considered, they\ntypically use embedded GPUs that consume tens of watts. In response to these\nchallenges, our research introduces an embedded neuromorphic testbench that\nutilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.\nUsing an extended version of the Qualia framework, we can train, evaluate,\nquantize, and deploy spiking neural networks on an FPGA implementation of\nSPLEAT. We used this testbench to load a state-of-the-art SNN solution,\nestimate the performance loss associated with deploying the network on\ndedicated hardware, and run real-world event-based OD on neuromorphic hardware\nspecifically designed for low-power spiking neural networks. Remarkably, our\nembedded spiking solution, which includes a model with 1.08 million parameters,\noperates efficiently with 490 mJ per prediction.",
          "arxiv_id": "2406.17617v1"
        }
      ],
      "18": [
        {
          "title": "Segment Anything in Defect Detection",
          "year": "2023-11",
          "abstract": "Defect detection plays a crucial role in infrared non-destructive testing\nsystems, offering non-contact, safe, and efficient inspection capabilities.\nHowever, challenges such as low resolution, high noise, and uneven heating in\ninfrared thermal images hinder comprehensive and accurate defect detection. In\nthis study, we propose DefectSAM, a novel approach for segmenting defects on\nhighly noisy thermal images based on the widely adopted model, Segment Anything\n(SAM)\\cite{kirillov2023segany}. Harnessing the power of a meticulously curated\ndataset generated through labor-intensive lab experiments and valuable prompts\nfrom experienced experts, DefectSAM surpasses existing state-of-the-art\nsegmentation algorithms and achieves significant improvements in defect\ndetection rates. Notably, DefectSAM excels in detecting weaker and smaller\ndefects on complex and irregular surfaces, reducing the occurrence of missed\ndetections and providing more accurate defect size estimations. Experimental\nstudies conducted on various materials have validated the effectiveness of our\nsolutions in defect detection, which hold significant potential to expedite the\nevolution of defect detection tools, enabling enhanced inspection capabilities\nand accuracy in defect identification.",
          "arxiv_id": "2311.10245v1"
        },
        {
          "title": "A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization",
          "year": "2025-07",
          "abstract": "Surface defect detection of steel, especially the recognition of multi-scale\ndefects, has always been a major challenge in industrial manufacturing. Steel\nsurfaces not only have defects of various sizes and shapes, which limit the\naccuracy of traditional image processing and detection methods in complex\nenvironments. However, traditional defect detection methods face issues of\ninsufficient accuracy and high miss-detection rates when dealing with small\ntarget defects. To address this issue, this study proposes a detection\nframework based on deep learning, specifically YOLOv9s, combined with the\nC3Ghost module, SCConv module, and CARAFE upsampling operator, to improve\ndetection accuracy and model performance. First, the SCConv module is used to\nreduce feature redundancy and optimize feature representation by reconstructing\nthe spatial and channel dimensions. Second, the C3Ghost module is introduced to\nenhance the model's feature extraction ability by reducing redundant\ncomputations and parameter volume, thereby improving model efficiency. Finally,\nthe CARAFE upsampling operator, which can more finely reorganize feature maps\nin a content-aware manner, optimizes the upsampling process and ensures\ndetailed restoration of high-resolution defect regions. Experimental results\ndemonstrate that the proposed model achieves higher accuracy and robustness in\nsteel surface defect detection tasks compared to other methods, effectively\naddressing defect detection problems.",
          "arxiv_id": "2507.15476v2"
        },
        {
          "title": "A Novel Approach for Defect Detection of Wind Turbine Blade Using Virtual Reality and Deep Learning",
          "year": "2023-12",
          "abstract": "Wind turbines are subjected to continuous rotational stresses and unusual\nexternal forces such as storms, lightning, strikes by flying objects, etc.,\nwhich may cause defects in turbine blades. Hence, it requires a periodical\ninspection to ensure proper functionality and avoid catastrophic failure. The\ntask of inspection is challenging due to the remote location and inconvenient\nreachability by human inspection. Researchers used images with cropped defects\nfrom the wind turbine in the literature. They neglected possible background\nbiases, which may hinder real-time and autonomous defect detection using aerial\nvehicles such as drones or others. To overcome such challenges, in this paper,\nwe experiment with defect detection accuracy by having the defects with the\nbackground using a two-step deep-learning methodology. In the first step, we\ndevelop virtual models of wind turbines to synthesize the near-reality images\nfor four types of common defects - cracks, leading edge erosion, bending, and\nlight striking damage. The Unity perception package is used to generate wind\nturbine blade defects images with variations in background, randomness, camera\nangle, and light effects. In the second step, a customized U-Net architecture\nis trained to classify and segment the defect in turbine blades. The outcomes\nof U-Net architecture have been thoroughly tested and compared with 5-fold\nvalidation datasets. The proposed methodology provides reasonable defect\ndetection accuracy, making it suitable for autonomous and remote inspection\nthrough aerial vehicles.",
          "arxiv_id": "2401.00237v1"
        }
      ],
      "19": [
        {
          "title": "Automatic extraction of coronary arteries using deep learning in invasive coronary angiograms",
          "year": "2022-06",
          "abstract": "Accurate extraction of coronary arteries from invasive coronary angiography\n(ICA) is important in clinical decision-making for the diagnosis and risk\nstratification of coronary artery disease (CAD). In this study, we develop a\nmethod using deep learning to automatically extract the coronary artery lumen.\nMethods. A deep learning model U-Net 3+, which incorporates the full-scale skip\nconnections and deep supervisions, was proposed for automatic extraction of\ncoronary arteries from ICAs. Transfer learning and a hybrid loss function were\nemployed in this novel coronary artery extraction framework. Results. A data\nset containing 616 ICAs obtained from 210 patients was used. In the technical\nevaluation, the U-Net 3+ achieved a Dice score of 0.8942 and a sensitivity of\n0.8735, which is higher than U-Net ++ (Dice score: 0.8814, the sensitivity of\n0.8331) and U-net (Dice score: 0.8799, the sensitivity of 0.8305). Conclusion.\nOur study demonstrates that the U-Net 3+ is superior to other segmentation\nframeworks for the automatic extraction of the coronary arteries from ICAs.\nThis result suggests great promise for clinical use.",
          "arxiv_id": "2206.12300v1"
        },
        {
          "title": "A Deep Learning Model for Coronary Artery Segmentation and Quantitative Stenosis Detection in Angiographic Images",
          "year": "2024-06",
          "abstract": "Coronary artery disease (CAD) is a leading cause of cardiovascular-related\nmortality, and accurate stenosis detection is crucial for effective clinical\ndecision-making. Coronary angiography remains the gold standard for diagnosing\nCAD, but manual analysis of angiograms is prone to errors and subjectivity.\nThis study aims to develop a deep learning-based approach for the automatic\nsegmentation of coronary arteries from angiographic images and the quantitative\ndetection of stenosis, thereby improving the accuracy and efficiency of CAD\ndiagnosis. We propose a novel deep learning-based method for the automatic\nsegmentation of coronary arteries in angiographic images, coupled with a\ndynamic cohort method for stenosis detection. The segmentation model combines\nthe MedSAM and VM-UNet architectures to achieve high-performance results. After\nsegmentation, the vascular centerline is extracted, vessel diameter is\ncomputed, and the degree of stenosis is measured with high precision, enabling\naccurate identification of arterial stenosis. On the mixed dataset (including\nthe ARCADE, DCA1, and GH datasets), the model achieved an average IoU of\n0.6308, with sensitivity and specificity of 0.9772 and 0.9903, respectively. On\nthe ARCADE dataset, the average IoU was 0.6303, with sensitivity of 0.9832 and\nspecificity of 0.9933. Additionally, the stenosis detection algorithm achieved\na true positive rate (TPR) of 0.5867 and a positive predictive value (PPV) of\n0.5911, demonstrating the effectiveness of our model in analyzing coronary\nangiography images. SAM-VMNet offers a promising tool for the automated\nsegmentation and detection of coronary artery stenosis. The model's high\naccuracy and robustness provide significant clinical value for the early\ndiagnosis and treatment planning of CAD. The code and examples are available at\nhttps://github.com/qimingfan10/SAM-VMNet.",
          "arxiv_id": "2406.00492v2"
        },
        {
          "title": "Segmentation and Vascular Vectorization for Coronary Artery by Geometry-based Cascaded Neural Network",
          "year": "2023-05",
          "abstract": "Segmentation of the coronary artery is an important task for the quantitative\nanalysis of coronary computed tomography angiography (CCTA) images and is being\nstimulated by the field of deep learning. However, the complex structures with\ntiny and narrow branches of the coronary artery bring it a great challenge.\nCoupled with the medical image limitations of low resolution and poor contrast,\nfragmentations of segmented vessels frequently occur in the prediction.\nTherefore, a geometry-based cascaded segmentation method is proposed for the\ncoronary artery, which has the following innovations: 1) Integrating geometric\ndeformation networks, we design a cascaded network for segmenting the coronary\nartery and vectorizing results. The generated meshes of the coronary artery are\ncontinuous and accurate for twisted and sophisticated coronary artery\nstructures, without fragmentations. 2) Different from mesh annotations\ngenerated by the traditional marching cube method from voxel-based labels, a\nfiner vectorized mesh of the coronary artery is reconstructed with the\nregularized morphology. The novel mesh annotation benefits the geometry-based\nsegmentation network, avoiding bifurcation adhesion and point cloud dispersion\nin intricate branches. 3) A dataset named CCA-200 is collected, consisting of\n200 CCTA images with coronary artery disease. The ground truths of 200 cases\nare coronary internal diameter annotations by professional radiologists.\nExtensive experiments verify our method on our collected dataset CCA-200 and\npublic ASOCA dataset, with a Dice of 0.778 on CCA-200 and 0.895 on ASOCA,\nshowing superior results. Especially, our geometry-based model generates an\naccurate, intact and smooth coronary artery, devoid of any fragmentations of\nsegmented vessels.",
          "arxiv_id": "2305.04208v1"
        }
      ],
      "20": [
        {
          "title": "CNN-based Cardiac Motion Extraction to Generate Deformable Geometric Left Ventricle Myocardial Models from Cine MRI",
          "year": "2021-03",
          "abstract": "Patient-specific left ventricle (LV) myocardial models have the potential to\nbe used in a variety of clinical scenarios for improved diagnosis and treatment\nplans. Cine cardiac magnetic resonance (MR) imaging provides high resolution\nimages to reconstruct patient-specific geometric models of the LV myocardium.\nWith the advent of deep learning, accurate segmentation of cardiac chambers\nfrom cine cardiac MR images and unsupervised learning for image registration\nfor cardiac motion estimation on a large number of image datasets is\nattainable. Here, we propose a deep leaning-based framework for the development\nof patient-specific geometric models of LV myocardium from cine cardiac MR\nimages, using the Automated Cardiac Diagnosis Challenge (ACDC) dataset. We use\nthe deformation field estimated from the VoxelMorph-based convolutional neural\nnetwork (CNN) to propagate the isosurface mesh and volume mesh of the\nend-diastole (ED) frame to the subsequent frames of the cardiac cycle. We\nassess the CNN-based propagated models against segmented models at each cardiac\nphase, as well as models propagated using another traditional nonrigid image\nregistration technique.",
          "arxiv_id": "2103.16695v1"
        },
        {
          "title": "Multi-Modality Cardiac Image Analysis with Deep Learning",
          "year": "2021-11",
          "abstract": "Accurate cardiac computing, analysis and modeling from multi-modality images\nare important for the diagnosis and treatment of cardiac disease. Late\ngadolinium enhancement magnetic resonance imaging (LGE MRI) is a promising\ntechnique to visualize and quantify myocardial infarction (MI) and atrial\nscars. Automating quantification of MI and atrial scars can be challenging due\nto the low image quality and complex enhancement patterns of LGE MRI. Moreover,\ncompared with the other sequences LGE MRIs with gold standard labels are\nparticularly limited, which represents another obstacle for developing novel\nalgorithms for automatic segmentation and quantification of LGE MRIs. This\nchapter aims to summarize the state-of-the-art and our recent advanced\ncontributions on deep learning based multi-modality cardiac image analysis.\nFirstly, we introduce two benchmark works for multi-sequence cardiac MRI based\nmyocardial and pathology segmentation. Secondly, two novel frameworks for left\natrial scar segmentation and quantification from LGE MRI were presented.\nThirdly, we present three unsupervised domain adaptation techniques for\ncross-modality cardiac image segmentation.",
          "arxiv_id": "2111.04736v1"
        },
        {
          "title": "Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark Study from Multi-Sequence Cardiac MR Segmentation Challenge",
          "year": "2020-06",
          "abstract": "Accurate computing, analysis and modeling of the ventricles and myocardium\nfrom medical images are important, especially in the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). Late\ngadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an\nimportant protocol to visualize MI. However, automated segmentation of LGE CMR\nis still challenging, due to the indistinguishable boundaries, heterogeneous\nintensity distribution and complex enhancement patterns of pathological\nmyocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR\nimages with gold standard labels are particularly limited, which represents\nanother obstacle for developing novel algorithms for automatic segmentation of\nLGE CMR. This paper presents the selective results from the Multi-Sequence\nCardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.\nThe challenge offered a data set of paired MS-CMR images, including auxiliary\nCMR sequences as well as LGE CMR, from 45 patients who underwent\ncardiomyopathy. It was aimed to develop new algorithms, as well as benchmark\nexisting ones for LGE CMR segmentation and compare them objectively. In\naddition, the paired MS-CMR images could enable algorithms to combine the\ncomplementary information from the other sequences for the segmentation of LGE\nCMR. Nine representative works were selected for evaluation and comparisons,\namong which three methods are unsupervised methods and the other six are\nsupervised. The results showed that the average performance of the nine methods\nwas comparable to the inter-observer variations. The success of these methods\nwas mainly attributed to the inclusion of the auxiliary sequences from the\nMS-CMR images, which provide important label information for the training of\ndeep neural networks.",
          "arxiv_id": "2006.12434v2"
        }
      ],
      "21": [
        {
          "title": "Joint 2D-3D Breast Cancer Classification",
          "year": "2020-02",
          "abstract": "Breast cancer is the malignant tumor that causes the highest number of cancer\ndeaths in females. Digital mammograms (DM or 2D mammogram) and digital breast\ntomosynthesis (DBT or 3D mammogram) are the two types of mammography imagery\nthat are used in clinical practice for breast cancer detection and diagnosis.\nRadiologists usually read both imaging modalities in combination; however,\nexisting computer-aided diagnosis tools are designed using only one imaging\nmodality. Inspired by clinical practice, we propose an innovative convolutional\nneural network (CNN) architecture for breast cancer classification, which uses\nboth 2D and 3D mammograms, simultaneously. Our experiment shows that the\nproposed method significantly improves the performance of breast cancer\nclassification. By assembling three CNN classifiers, the proposed model\nachieves 0.97 AUC, which is 34.72% higher than the methods using only one\nimaging modality.",
          "arxiv_id": "2002.12392v1"
        },
        {
          "title": "Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment",
          "year": "2020-11",
          "abstract": "Breast density is an important risk factor for breast cancer that also\naffects the specificity and sensitivity of screening mammography. Current\nfederal legislation mandates reporting of breast density for all women\nundergoing breast screening. Clinically, breast density is assessed visually\nusing the American College of Radiology Breast Imaging Reporting And Data\nSystem (BI-RADS) scale. Here, we introduce an artificial intelligence (AI)\nmethod to estimate breast percentage density (PD) from digital mammograms. Our\nmethod leverages deep learning (DL) using two convolutional neural network\narchitectures to accurately segment the breast area. A machine-learning\nalgorithm combining superpixel generation, texture feature analysis, and\nsupport vector machine is then applied to differentiate dense from non-dense\ntissue regions, from which PD is estimated. Our method has been trained and\nvalidated on a multi-ethnic, multi-institutional dataset of 15,661 images\n(4,437 women), and then tested on an independent dataset of 6,368 digital\nmammograms (1,702 women; cases=414) for both PD estimation and discrimination\nof breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and\nan expert reader were strongly correlated (Spearman correlation coefficient =\n0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination\nperformance (area under the ROC curve, AUC = 0.611 [95% confidence interval\n(CI): 0.583, 0.639]) compared to four other widely-used research and commercial\nPD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong\nagreement of PD estimates between Deep-LIBRA and gold-standard assessment by an\nexpert reader, as well as improved performance in breast cancer risk assessment\nover state-of-the-art open-source and commercial methods.",
          "arxiv_id": "2011.08001v3"
        },
        {
          "title": "Deep Learning Predicts Mammographic Breast Density in Clinical Breast Ultrasound Images",
          "year": "2024-10",
          "abstract": "Background: Breast density, as derived from mammographic images and defined\nby the American College of Radiology's Breast Imaging Reporting and Data System\n(BI-RADS), is one of the strongest risk factors for breast cancer. Breast\nultrasound (BUS) is an alternative breast cancer screening modality,\nparticularly useful for early detection in low-resource, rural contexts. The\npurpose of this study was to explore an artificial intelligence (AI) model to\npredict BI-RADS mammographic breast density category from clinical, handheld\nBUS imaging. Methods: All data are sourced from the Hawaii and Pacific Islands\nMammography Registry. We compared deep learning methods from BUS imaging, as\nwell as machine learning models from image statistics alone. The use of\nAI-derived BUS density as a risk factor for breast cancer was then compared to\nclinical BI-RADS breast density while adjusting for age. The BUS data were\nsplit by individual into 70/20/10% groups for training, validation, and\ntesting. Results: 405,120 clinical BUS images from 14.066 women were selected\nfor inclusion in this study, resulting in 9.846 women for training (302,574\nimages), 2,813 for validation (11,223 images), and 1,406 for testing (4,042\nimages). On the held-out testing set, the strongest AI model achieves AUROC\n0.854 predicting BI-RADS mammographic breast density from BUS imaging and\noutperforms all shallow machine learning methods based on image statistics. In\ncancer risk prediction, age-adjusted AI BUS breast density predicted 5-year\nbreast cancer risk with 0.633 AUROC, as compared to 0.637 AUROC from\nage-adjusted clinical breast density. Conclusions: BI-RADS mammographic breast\ndensity can be estimated from BUS imaging with high accuracy using a deep\nlearning model. Furthermore, we demonstrate that AI-derived BUS breast density\nis predictive of 5-year breast cancer risk in our population.",
          "arxiv_id": "2411.00891v2"
        }
      ],
      "22": [
        {
          "title": "multiGradICON: A Foundation Model for Multimodal Medical Image Registration",
          "year": "2024-08",
          "abstract": "Modern medical image registration approaches predict deformations using deep\nnetworks. These approaches achieve state-of-the-art (SOTA) registration\naccuracy and are generally fast. However, deep learning (DL) approaches are, in\ncontrast to conventional non-deep-learning-based approaches, anatomy-specific.\nRecently, a universal deep registration approach, uniGradICON, has been\nproposed. However, uniGradICON focuses on monomodal image registration. In this\nwork, we therefore develop multiGradICON as a first step towards universal\n*multimodal* medical image registration. Specifically, we show that 1) we can\ntrain a DL registration model that is suitable for monomodal *and* multimodal\nregistration; 2) loss function randomization can increase multimodal\nregistration accuracy; and 3) training a model with multimodal data helps\nmultimodal generalization. Our code and the multiGradICON model are available\nat https://github.com/uncbiag/uniGradICON.",
          "arxiv_id": "2408.00221v2"
        },
        {
          "title": "Deep Learning for Medical Image Registration: A Comprehensive Review",
          "year": "2022-04",
          "abstract": "Image registration is a critical component in the applications of various\nmedical image analyses. In recent years, there has been a tremendous surge in\nthe development of deep learning (DL)-based medical image registration models.\nThis paper provides a comprehensive review of medical image registration.\nFirstly, a discussion is provided for supervised registration categories, for\nexample, fully supervised, dual supervised, and weakly supervised registration.\nNext, similarity-based as well as generative adversarial network (GAN)-based\nregistration are presented as part of unsupervised registration. Deep iterative\nregistration is then described with emphasis on deep similarity-based and\nreinforcement learning-based registration. Moreover, the application areas of\nmedical image registration are reviewed. This review focuses on monomodal and\nmultimodal registration and associated imaging, for instance, X-ray, CT scan,\nultrasound, and MRI. The existing challenges are highlighted in this review,\nwhere it is shown that a major challenge is the absence of a training dataset\nwith known transformations. Finally, a discussion is provided on the promising\nfuture research areas in the field of DL-based medical image registration.",
          "arxiv_id": "2204.11341v1"
        },
        {
          "title": "A survey on deep learning in medical image registration: new technologies, uncertainty, evaluation metrics, and beyond",
          "year": "2023-07",
          "abstract": "Deep learning technologies have dramatically reshaped the field of medical\nimage registration over the past decade. The initial developments, such as\nregression-based and U-Net-based networks, established the foundation for deep\nlearning in image registration. Subsequent progress has been made in various\naspects of deep learning-based registration, including similarity measures,\ndeformation regularizations, network architectures, and uncertainty estimation.\nThese advancements have not only enriched the field of image registration but\nhave also facilitated its application in a wide range of tasks, including atlas\nconstruction, multi-atlas segmentation, motion estimation, and 2D-3D\nregistration. In this paper, we present a comprehensive overview of the most\nrecent advancements in deep learning-based image registration. We begin with a\nconcise introduction to the core concepts of deep learning-based image\nregistration. Then, we delve into innovative network architectures, loss\nfunctions specific to registration, and methods for estimating registration\nuncertainty. Additionally, this paper explores appropriate evaluation metrics\nfor assessing the performance of deep learning models in registration tasks.\nFinally, we highlight the practical applications of these novel techniques in\nmedical imaging and discuss the future prospects of deep learning-based image\nregistration.",
          "arxiv_id": "2307.15615v4"
        }
      ],
      "23": [
        {
          "title": "Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists",
          "year": "2022-07",
          "abstract": "A fully-automated deep learning algorithm matched performance of radiologists\nin assessment of knee osteoarthritis severity in radiographs using the\nKellgren-Lawrence grading system.\n  To develop an automated deep learning-based algorithm that jointly uses\nPosterior-Anterior (PA) and Lateral (LAT) views of knee radiographs to assess\nknee osteoarthritis severity according to the Kellgren-Lawrence grading system.\n  We used a dataset of 9739 exams from 2802 patients from Multicenter\nOsteoarthritis Study (MOST). The dataset was divided into a training set of\n2040 patients, a validation set of 259 patients and a test set of 503 patients.\nA novel deep learning-based method was utilized for assessment of knee OA in\ntwo steps: (1) localization of knee joints in the images, (2) classification\naccording to the KL grading system. Our method used both PA and LAT views as\nthe input to the model. The scores generated by the algorithm were compared to\nthe grades provided in the MOST dataset for the entire test set as well as\ngrades provided by 5 radiologists at our institution for a subset of the test\nset.\n  The model obtained a multi-class accuracy of 71.90% on the entire test set\nwhen compared to the ratings provided in the MOST dataset. The quadratic\nweighted Kappa coefficient for this set was 0.9066. The average quadratic\nweighted Kappa between all pairs of radiologists from our institution who took\na part of study was 0.748. The average quadratic-weighted Kappa between the\nalgorithm and the radiologists at our institution was 0.769.\n  The proposed model performed demonstrated equivalency of KL classification to\nMSK radiologists, but clearly superior reproducibility. Our model also agreed\nwith radiologists at our institution to the same extent as the radiologists\nwith each other. The algorithm could be used to provide reproducible assessment\nof knee osteoarthritis severity.",
          "arxiv_id": "2207.12521v1"
        },
        {
          "title": "A Lightweight CNN and Joint Shape-Joint Space (JS2) Descriptor for Radiological Osteoarthritis Detection",
          "year": "2020-05",
          "abstract": "Knee osteoarthritis (OA) is very common progressive and degenerative\nmusculoskeletal disease worldwide creates a heavy burden on patients with\nreduced quality of life and also on society due to financial impact. Therefore,\nany attempt to reduce the burden of the disease could help both patients and\nsociety. In this study, we propose a fully automated novel method, based on\ncombination of joint shape and convolutional neural network (CNN) based bone\ntexture features, to distinguish between the knee radiographs with and without\nradiographic osteoarthritis. Moreover, we report the first attempt at\ndescribing the bone texture using CNN. Knee radiographs from Osteoarthritis\nInitiative (OAI) and Multicenter Osteoarthritis (MOST) studies were used in the\nexperiments. Our models were trained on 8953 knee radiographs from OAI and\nevaluated on 3445 knee radiographs from MOST. Our results demonstrate that\nfusing the proposed shape and texture parameters achieves the state-of-the art\nperformance in radiographic OA detection yielding area under the ROC curve\n(AUC) of 95.21%",
          "arxiv_id": "2005.11715v1"
        },
        {
          "title": "Automated anomaly-aware 3D segmentation of bones and cartilages in knee MR images from the Osteoarthritis Initiative",
          "year": "2022-11",
          "abstract": "In medical image analysis, automated segmentation of multi-component\nanatomical structures, which often have a spectrum of potential anomalies and\npathologies, is a challenging task. In this work, we develop a multi-step\napproach using U-Net-based neural networks to initially detect anomalies (bone\nmarrow lesions, bone cysts) in the distal femur, proximal tibia and patella\nfrom 3D magnetic resonance (MR) images of the knee in individuals with varying\ngrades of osteoarthritis. Subsequently, the extracted data are used for\ndownstream tasks involving semantic segmentation of individual bone and\ncartilage volumes as well as bone anomalies. For anomaly detection, the\nU-Net-based models were developed to reconstruct the bone profiles of the femur\nand tibia in images via inpainting so anomalous bone regions could be replaced\nwith close to normal appearances. The reconstruction error was used to detect\nbone anomalies. A second anomaly-aware network, which was compared to\nanomaly-na\\\"ive segmentation networks, was used to provide a final automated\nsegmentation of the femoral, tibial and patellar bones and cartilages from the\nknee MR images containing a spectrum of bone anomalies. The anomaly-aware\nsegmentation approach provided up to 58% reduction in Hausdorff distances for\nbone segmentations compared to the results from the anomaly-na\\\"ive\nsegmentation networks. In addition, the anomaly-aware networks were able to\ndetect bone lesions in the MR images with greater sensitivity and specificity\n(area under the receiver operating characteristic curve [AUC] up to 0.896)\ncompared to the anomaly-na\\\"ive segmentation networks (AUC up to 0.874).",
          "arxiv_id": "2211.16696v2"
        }
      ],
      "24": [
        {
          "title": "Inverse Problem Approach to Aberration Correction for in vivo Transcranial Imaging Based on a Sparse Representation of Contrast-enhanced Ultrasound Data",
          "year": "2024-01",
          "abstract": "Transcranial ultrasound imaging is currently limited by attenuation and\naberration induced by the skull. First used in contrast-enhanced ultrasound\n(CEUS), highly echoic microbubbles allowed for the development of novel imaging\nmodalities such as ultrasound localization microscopy (ULM). Herein, we develop\nan inverse problem approach to aberration correction (IPAC) that leverages the\nsparsity of microbubble signals. We propose to use the \\textit{a priori}\nknowledge of the medium based upon microbubble localization and wave\npropagation to build a forward model to link the measured signals directly to\nthe aberration function. A standard least-squares inversion is then used to\nretrieve the aberration function. We first validated IPAC on simulated data of\na vascular network using plane wave as well as divergent wave emissions. We\nthen evaluated the reproducibility of IPAC \\textit{in vivo} in 5 mouse brains.\nWe showed that aberration correction improved the contrast of CEUS images by\n4.6 dB. For ULM images, IPAC yielded sharper vessels, reduced vessel\nduplications, and improved the resolution from 21.1 $\\mu$m to 18.3 $\\mu$m.\nAberration correction also improved hemodynamic quantification for velocity\nmagnitude and flow direction.",
          "arxiv_id": "2401.10389v3"
        },
        {
          "title": "Denoising convolutional neural networks for photoacoustic microscopy",
          "year": "2020-09",
          "abstract": "Photoacoustic imaging is a new imaging technology in recent years, which\ncombines the advantages of high resolution and rich contrast of optical imaging\nwith the advantages of high penetration depth of acoustic imaging.\nPhotoacoustic imaging has been widely used in biomedical fields, such as brain\nimaging, tumor detection and so on. The signal-to-noise ratio (SNR) of image\nsignals in photoacoustic imaging is generally low due to the limitation of\nlaser pulse energy, electromagnetic interference in the external environment\nand system noise. In order to solve the problem of low SNR of photoacoustic\nimages, we use feedforward denoising convolutional neural network to further\nprocess the obtained images, so as to obtain higher SNR images and improve\nimage quality. We use Python language to manage the referenced Python external\nlibrary through Anaconda, and build a feedforward noise-reducing convolutional\nneural network on Pycharm platform.We first processed and segmated a training\nset containing 400 images, and then used it for network training. Finally, we\ntested it with a series of cerebrovascular photoacoustic microscopy images.The\nresults show that the peak signal-to-noise ratio (PSNR) of the image increases\nsignificantly before and after denoising.The experimental results verify that\nthe feed-forward noise reduction convolutional neural network can effectively\nimprove the quality of photoacoustic microscopic images, which provides a good\nfoundation for the subsequent biomedical research.",
          "arxiv_id": "2009.13913v1"
        },
        {
          "title": "Super-Resolution Ultrasound Localization Microscopy Based on a High Frame-rate Clinical Ultrasound Scanner: An In-human Feasibility Study",
          "year": "2020-09",
          "abstract": "Non-invasive detection of microvascular alterations in deep tissues in vivo\nprovides critical information for clinical diagnosis and evaluation of a\nbroad-spectrum of pathologies. Recently, the emergence of super-resolution\nultrasound localization microscopy (ULM) offers new possibilities for clinical\nimaging of microvasculature at capillary level. Currently, the clinical utility\nof ULM on clinical ultrasound scanners is hindered by the technical\nlimitations, such as long data acquisition time, and compromised tracking\nperformance associated with low imaging frame-rate. Here we present an in-human\nULM on a high frame-rate (HFR) clinical ultrasound scanner to achieve\nsuper-resolution microvessel imaging using a short acquisition time (<10s).\nUltrasound MB data were acquired from different human tissues, (liver, kidney,\npancreatic, and breast tumor) using an HFR clinical scanner. By leveraging the\nHFR and advanced processing techniques including sub-pixel motion registration,\nMB signal separation, and Kalman filter-based tracking, MBs can be robustly\nlocalized and tracked for successful ULM under the circumstances of relatively\nhigh MB concentration and limited data acquisition time in humans. Subtle\nmorphological and hemodynamic information were demonstrated on data acquired\nwith single breath-hold and free-hand scanning. Compared with contrast-enhanced\npower Doppler generated based on the same MB dataset, ULM showed a 5.7-fold\nresolution improvement in a vessel, and provided a wide-range flow speed\nmeasurement that is Doppler angle-independent. This study demonstrated the\nfeasibility of ultrafast in-human ULM in various human tissues based on a\nclinical scanner that supports HFR imaging, and showed a great potential for\nthe implementation of super-resolution ultrasound microvessel imaging in a\nmyriad of clinical applications involving microvascular abnormalities and\npathologies.",
          "arxiv_id": "2009.13477v1"
        }
      ],
      "25": [
        {
          "title": "Meta-Analysis of Transfer Learning for Segmentation of Brain Lesions",
          "year": "2023-06",
          "abstract": "A major challenge in stroke research and stroke recovery predictions is the\ndetermination of a stroke lesion's extent and its impact on relevant brain\nsystems. Manual segmentation of stroke lesions from 3D magnetic resonance (MR)\nimaging volumes, the current gold standard, is not only very time-consuming,\nbut its accuracy highly depends on the operator's experience. As a result,\nthere is a need for a fully automated segmentation method that can efficiently\nand objectively measure lesion extent and the impact of each lesion to predict\nimpairment and recovery potential which might be beneficial for clinical,\ntranslational, and research settings. We have implemented and tested a fully\nautomatic method for stroke lesion segmentation which was developed using eight\ndifferent 2D-model architectures trained via transfer learning (TL) and mixed\ndata approaches. Additionally, the final prediction was made using a novel\nensemble method involving stacking and agreement window. Our novel method was\nevaluated in a novel in-house dataset containing 22 T1w brain MR images, which\nwere challenging in various perspectives, but mostly because they included T1w\nMR images from the subacute (which typically less well defined T1 lesions) and\nchronic stroke phase (which typically means well defined T1-lesions).\nCross-validation results indicate that our new method can efficiently and\nautomatically segment lesions fast and with high accuracy compared to ground\ntruth. In addition to segmentation, we provide lesion volume and weighted\nlesion load of relevant brain systems based on the lesions' overlap with a\ncanonical structural motor system that stretches from the cortical motor region\nto the lowest end of the brain stem.",
          "arxiv_id": "2306.11714v1"
        },
        {
          "title": "CPAISD: Core-penumbra acute ischemic stroke dataset",
          "year": "2024-04",
          "abstract": "We introduce the CPAISD: Core-Penumbra Acute Ischemic Stroke Dataset, aimed\nat enhancing the early detection and segmentation of ischemic stroke using\nNon-Contrast Computed Tomography (NCCT) scans. Addressing the challenges in\ndiagnosing acute ischemic stroke during its early stages due to often\nnon-revealing native CT findings, the dataset provides a collection of\nsegmented NCCT images. These include annotations of ischemic core and penumbra\nregions, critical for developing machine learning models for rapid stroke\nidentification and assessment. By offering a carefully collected and annotated\ndataset, we aim to facilitate the development of advanced diagnostic tools,\ncontributing to improved patient care and outcomes in stroke management. Our\ndataset's uniqueness lies in its focus on the acute phase of ischemic stroke,\nwith non-informative native CT scans, and includes a baseline model to\ndemonstrate the dataset's application, encouraging further research and\ninnovation in the field of medical imaging and stroke diagnosis.",
          "arxiv_id": "2404.02518v1"
        },
        {
          "title": "APIS: A paired CT-MRI dataset for ischemic stroke segmentation challenge",
          "year": "2023-09",
          "abstract": "Stroke is the second leading cause of mortality worldwide. Immediate\nattention and diagnosis play a crucial role regarding patient prognosis. The\nkey to diagnosis consists in localizing and delineating brain lesions. Standard\nstroke examination protocols include the initial evaluation from a non-contrast\nCT scan to discriminate between hemorrhage and ischemia. However, non-contrast\nCTs may lack sensitivity in detecting subtle ischemic changes in the acute\nphase. As a result, complementary diffusion-weighted MRI studies are captured\nto provide valuable insights, allowing to recover and quantify stroke lesions.\nThis work introduced APIS, the first paired public dataset with NCCT and ADC\nstudies of acute ischemic stroke patients. APIS was presented as a challenge at\nthe 20th IEEE International Symposium on Biomedical Imaging 2023, where\nresearchers were invited to propose new computational strategies that leverage\npaired data and deal with lesion segmentation over CT sequences. Despite all\nthe teams employing specialized deep learning tools, the results suggest that\nthe ischemic stroke segmentation task from NCCT remains challenging. The\nannotated dataset remains accessible to the public upon registration, inviting\nthe scientific community to deal with stroke characterization from NCCT but\nguided with paired DWI information.",
          "arxiv_id": "2309.15243v1"
        }
      ],
      "26": [
        {
          "title": "Analysis of skin lesion images with deep learning",
          "year": "2021-01",
          "abstract": "Skin cancer is the most common cancer worldwide, with melanoma being the\ndeadliest form. Dermoscopy is a skin imaging modality that has shown an\nimprovement in the diagnosis of skin cancer compared to visual examination\nwithout support. We evaluate the current state of the art in the classification\nof dermoscopic images based on the ISIC-2019 Challenge for the classification\nof skin lesions and current literature. Various deep neural network\narchitectures pre-trained on the ImageNet data set are adapted to a combined\ntraining data set comprised of publicly available dermoscopic and clinical\nimages of skin lesions using transfer learning and model fine-tuning. The\nperformance and applicability of these models for the detection of eight\nclasses of skin lesions are examined. Real-time data augmentation, which uses\nrandom rotation, translation, shear, and zoom within specified bounds is used\nto increase the number of available training samples. Model predictions are\nmultiplied by inverse class frequencies and normalized to better approximate\nactual probability distributions. Overall prediction accuracy is further\nincreased by using the arithmetic mean of the predictions of several\nindependently trained models. The best single model has been published as a web\nservice.",
          "arxiv_id": "2101.03814v1"
        },
        {
          "title": "Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis",
          "year": "2023-01",
          "abstract": "Automatic lesion analysis is critical in skin cancer diagnosis and ensures\neffective treatment. The computer aided diagnosis of such skin cancer in\ndermoscopic images can significantly reduce the clinicians workload and help\nimprove diagnostic accuracy. Although researchers are working extensively to\naddress this problem, early detection and accurate identification of skin\nlesions remain challenging. This research focuses on a two step framework for\nskin lesion segmentation followed by classification for lesion analysis. We\nexplored the effectiveness of deep convolutional neural network based\narchitectures by designing an encoder-decoder architecture for skin lesion\nsegmentation and CNN based classification network. The proposed approaches are\nevaluated quantitatively in terms of the Accuracy, mean Intersection over Union\nand Dice Similarity Coefficient. Our cascaded end to end deep learning based\napproach is the first of its kind, where the classification accuracy of the\nlesion is significantly improved because of prior segmentation.",
          "arxiv_id": "2301.06226v1"
        },
        {
          "title": "PAD-UFES-20: a skin lesion dataset composed of patient data and clinical images collected from smartphones",
          "year": "2020-07",
          "abstract": "Over the past few years, different computer-aided diagnosis (CAD) systems\nhave been proposed to tackle skin lesion analysis. Most of these systems work\nonly for dermoscopy images since there is a strong lack of public clinical\nimages archive available to design them. To fill this gap, we release a skin\nlesion benchmark composed of clinical images collected from smartphone devices\nand a set of patient clinical data containing up to 22 features. The dataset\nconsists of 1,373 patients, 1,641 skin lesions, and 2,298 images for six\ndifferent diagnostics: three skin diseases and three skin cancers. In total,\n58.4% of the skin lesions are biopsy-proven, including 100% of the skin\ncancers. By releasing this benchmark, we aim to aid future research and the\ndevelopment of new tools to assist clinicians to detect skin cancer.",
          "arxiv_id": "2007.00478v2"
        }
      ],
      "27": [
        {
          "title": "Analysis of the performance of U-Net neural networks for the segmentation of living cells",
          "year": "2022-10",
          "abstract": "The automated analysis of microscopy images is a challenge in the context of\nsingle-cell tracking and quantification. This work has as goals the study of\nthe performance of deep learning for segmenting microscopy images and the\nimprovement of the previously available pipeline for tracking single cells.\nDeep learning techniques, mainly convolutional neural networks, have been\napplied to cell segmentation problems and have shown high accuracy and fast\nperformance. To perform the image segmentation, an analysis of hyperparameters\nwas done in order to implement a convolutional neural network with U-Net\narchitecture. Furthermore, different models were built in order to optimize the\nsize of the network and the number of learnable parameters. The trained network\nis then used in the pipeline that localizes the traps in a microfluidic device,\nperforms the image segmentation on trap images, and evaluates the fluorescence\nintensity and the area of single cells over time. The tracking of the cells\nduring an experiment is performed by image processing algorithms, such as\ncentroid estimation and watershed. Finally, with all improvements in the neural\nnetwork to segment single cells and in the pipeline, quasi-real-time image\nanalysis was enabled, where 6.20GB of data was processed in 4 minutes.",
          "arxiv_id": "2210.01538v1"
        },
        {
          "title": "Advanced Multi-Microscopic Views Cell Semi-supervised Segmentation",
          "year": "2023-03",
          "abstract": "Although deep learning (DL) shows powerful potential in cell segmentation\ntasks, it suffers from poor generalization as DL-based methods originally\nsimplified cell segmentation in detecting cell membrane boundary, lacking\nprominent cellular structures to position overall differentiating. Moreover,\nthe scarcity of annotated cell images limits the performance of DL models.\nSegmentation limitations of a single category of cell make massive practice\ndifficult, much less, with varied modalities. In this paper, we introduce a\nnovel semi-supervised cell segmentation method called Multi-Microscopic-view\nCell semi-supervised Segmentation (MMCS), which can train cell segmentation\nmodels utilizing less labeled multi-posture cell images with different\nmicroscopy well. Technically, MMCS consists of Nucleus-assisted global\nrecognition, Self-adaptive diameter filter, and Temporal-ensembling models.\nNucleus-assisted global recognition adds additional cell nucleus channel to\nimprove the global distinguishing performance of fuzzy cell membrane boundaries\neven when cells aggregate. Besides, self-adapted cell diameter filter can help\nseparate multi-resolution cells with different morphology properly. It further\nleverages the temporal-ensembling models to improve the semi-supervised\ntraining process, achieving effective training with less labeled data.\nAdditionally, optimizing the weight of unlabeled loss contributed to total loss\nalso improve the model performance. Evaluated on the Tuning Set of NeurIPS 2022\nCell Segmentation Challenge (NeurIPS CellSeg), MMCS achieves an F1-score of\n0.8239 and the running time for all cases is within the time tolerance.",
          "arxiv_id": "2303.11661v1"
        },
        {
          "title": "Generative modeling of living cells with SO(3)-equivariant implicit neural representations",
          "year": "2023-04",
          "abstract": "Data-driven cell tracking and segmentation methods in biomedical imaging\nrequire diverse and information-rich training data. In cases where the number\nof training samples is limited, synthetic computer-generated data sets can be\nused to improve these methods. This requires the synthesis of cell shapes as\nwell as corresponding microscopy images using generative models. To synthesize\nrealistic living cell shapes, the shape representation used by the generative\nmodel should be able to accurately represent fine details and changes in\ntopology, which are common in cells. These requirements are not met by 3D voxel\nmasks, which are restricted in resolution, and polygon meshes, which do not\neasily model processes like cell growth and mitosis. In this work, we propose\nto represent living cell shapes as level sets of signed distance functions\n(SDFs) which are estimated by neural networks. We optimize a fully-connected\nneural network to provide an implicit representation of the SDF value at any\npoint in a 3D+time domain, conditioned on a learned latent code that is\ndisentangled from the rotation of the cell shape. We demonstrate the\neffectiveness of this approach on cells that exhibit rapid deformations\n(Platynereis dumerilii), cells that grow and divide (C. elegans), and cells\nthat have growing and branching filopodial protrusions (A549 human lung\ncarcinoma cells). A quantitative evaluation using shape features and Dice\nsimilarity coefficients of real and synthetic cell shapes shows that our model\ncan generate topologically plausible complex cell shapes in 3D+time with high\nsimilarity to real living cell shapes. Finally, we show how microscopy images\nof living cells that correspond to our generated cell shapes can be synthesized\nusing an image-to-image model.",
          "arxiv_id": "2304.08960v2"
        }
      ],
      "28": [
        {
          "title": "User-generated Video Quality Assessment: A Subjective and Objective Study",
          "year": "2020-05",
          "abstract": "Recently, we have observed an exponential increase of user-generated content\n(UGC) videos. The distinguished characteristic of UGC videos originates from\nthe video production and delivery chain, as they are usually acquired and\nprocessed by non-professional users before uploading to the hosting platforms\nfor sharing. As such, these videos usually undergo multiple distortion stages\nthat may affect visual quality before ultimately being viewed. Inspired by the\nincreasing consensus that the optimization of the video coding and processing\nshall be fully driven by the perceptual quality, in this paper, we propose to\nstudy the quality of the UGC videos from both objective and subjective\nperspectives. We first construct a UGC video quality assessment (VQA) database,\naiming to provide useful guidance for the UGC video coding and processing in\nthe hosting platform. The database contains source UGC videos uploaded to the\nplatform and their transcoded versions that are ultimately enjoyed by\nend-users, along with their subjective scores. Furthermore, we develop an\nobjective quality assessment algorithm that automatically evaluates the quality\nof the transcoded videos based on the corrupted reference, which is in\naccordance with the application scenarios of UGC video sharing in the hosting\nplatforms. The information from the corrupted reference is well leveraged and\nthe quality is predicted based on the inferred quality maps with deep neural\nnetworks (DNN). Experimental results show that the proposed method yields\nsuperior performance. Both subjective and objective evaluations of the UGC\nvideos also shed lights on the design of perceptual UGC video coding.",
          "arxiv_id": "2005.08527v1"
        },
        {
          "title": "UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content",
          "year": "2020-05",
          "abstract": "Recent years have witnessed an explosion of user-generated content (UGC)\nvideos shared and streamed over the Internet, thanks to the evolution of\naffordable and reliable consumer capture devices, and the tremendous popularity\nof social media platforms. Accordingly, there is a great need for accurate\nvideo quality assessment (VQA) models for UGC/consumer videos to monitor,\ncontrol, and optimize this vast content. Blind quality prediction of\nin-the-wild videos is quite challenging, since the quality degradations of UGC\ncontent are unpredictable, complicated, and often commingled. Here we\ncontribute to advancing the UGC-VQA problem by conducting a comprehensive\nevaluation of leading no-reference/blind VQA (BVQA) features and models on a\nfixed evaluation architecture, yielding new empirical insights on both\nsubjective video quality studies and VQA model design. By employing a feature\nselection strategy on top of leading VQA model features, we are able to extract\n60 of the 763 statistical features used by the leading models to create a new\nfusion-based BVQA model, which we dub the \\textbf{VID}eo quality\n\\textbf{EVAL}uator (VIDEVAL), that effectively balances the trade-off between\nVQA performance and efficiency. Our experimental results show that VIDEVAL\nachieves state-of-the-art performance at considerably lower computational cost\nthan other leading models. Our study protocol also defines a reliable benchmark\nfor the UGC-VQA problem, which we believe will facilitate further research on\ndeep learning-based VQA modeling, as well as perceptually-optimized efficient\nUGC video processing, transcoding, and streaming. To promote reproducible\nresearch and public evaluation, an implementation of VIDEVAL has been made\navailable online: \\url{https://github.com/tu184044109/VIDEVAL_release}.",
          "arxiv_id": "2005.14354v2"
        },
        {
          "title": "FineVQ: Fine-Grained User Generated Content Video Quality Assessment",
          "year": "2024-12",
          "abstract": "The rapid growth of user-generated content (UGC) videos has produced an\nurgent need for effective video quality assessment (VQA) algorithms to monitor\nvideo quality and guide optimization and recommendation procedures. However,\ncurrent VQA models generally only give an overall rating for a UGC video, which\nlacks fine-grained labels for serving video processing and recommendation\napplications. To address the challenges and promote the development of UGC\nvideos, we establish the first large-scale Fine-grained Video quality\nassessment Database, termed FineVD, which comprises 6104 UGC videos with\nfine-grained quality scores and descriptions across multiple dimensions. Based\non this database, we propose a Fine-grained Video Quality assessment (FineVQ)\nmodel to learn the fine-grained quality of UGC videos, with the capabilities of\nquality rating, quality scoring, and quality attribution. Extensive\nexperimental results demonstrate that our proposed FineVQ can produce\nfine-grained video-quality results and achieve state-of-the-art performance on\nFineVD and other commonly used UGC-VQA datasets.",
          "arxiv_id": "2412.19238v2"
        }
      ],
      "29": [
        {
          "title": "Federated learning: Applications, challenges and future directions",
          "year": "2022-05",
          "abstract": "Federated learning (FL) is a system in which a central aggregator coordinates\nthe efforts of multiple clients to solve machine learning problems. This\nsetting allows training data to be dispersed in order to protect privacy. The\npurpose of this paper is to provide an overview of FL systems with a focus on\nhealthcare. FL is evaluated here based on its frameworks, architectures, and\napplications. It is shown here that FL solves the preceding issues with a\nshared global deep learning (DL) model via a central aggregator server. This\npaper examines recent developments and provides a comprehensive list of\nunresolved issues, inspired by the rapid growth of FL research. In the context\nof FL, several privacy methods are described, including secure multiparty\ncomputation, homomorphic encryption, differential privacy, and stochastic\ngradient descent. Furthermore, a review of various FL classes, such as\nhorizontal and vertical FL and federated transfer learning, is provided. FL has\napplications in wireless communication, service recommendation, intelligent\nmedical diagnosis systems, and healthcare, all of which are discussed in this\npaper. We also present a thorough review of existing FL challenges, such as\nprivacy protection, communication cost, system heterogeneity, and unreliable\nmodel upload, followed by future research directions.",
          "arxiv_id": "2205.09513v2"
        },
        {
          "title": "Future-Proofing Medical Imaging with Privacy-Preserving Federated Learning and Uncertainty Quantification: A Review",
          "year": "2024-09",
          "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in\nautomating various medical imaging tasks, which could soon become routine in\nclinical practice for disease diagnosis, prognosis, treatment planning, and\npost-treatment surveillance. However, the privacy concerns surrounding patient\ndata present a major barrier to the widespread adoption of AI in medical\nimaging, as large, diverse training datasets are essential for developing\naccurate, generalizable, and robust Artificial intelligence models. Federated\nLearning (FL) offers a solution that enables organizations to train AI models\ncollaboratively without sharing sensitive data. federated learning exchanges\nmodel training information, such as gradients, between the participating sites.\nDespite its promise, federated learning is still in its developmental stages\nand faces several challenges. Notably, sensitive information can still be\ninferred from the gradients shared during model training. Quantifying AI\nmodels' uncertainty is vital due to potential data distribution shifts\npost-deployment, which can affect model performance. Uncertainty quantification\n(UQ) in FL is particularly challenging due to data heterogeneity across\nparticipating sites. This review provides a comprehensive examination of FL,\nprivacy-preserving FL (PPFL), and UQ in FL. We identify key gaps in current FL\nmethodologies and propose future research directions to enhance data privacy\nand trustworthiness in medical imaging applications.",
          "arxiv_id": "2409.16340v1"
        },
        {
          "title": "ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation",
          "year": "2022-03",
          "abstract": "Chest Computational Tomography (CT) scans present low cost, speed and\nobjectivity for COVID-19 diagnosis and deep learning methods have shown great\npromise in assisting the analysis and interpretation of these images. Most\nhospitals or countries can train their own models using in-house data, however\nempirical evidence shows that those models perform poorly when tested on new\nunseen cases, surfacing the need for coordinated global collaboration. Due to\nprivacy regulations, medical data sharing between hospitals and nations is\nextremely difficult. We propose a GAN-augmented federated learning model,\ndubbed ST-FL (Style Transfer Federated Learning), for COVID-19 image\nsegmentation. Federated learning (FL) permits a centralised model to be learned\nin a secure manner from heterogeneous datasets located in disparate private\ndata silos. We demonstrate that the widely varying data quality on FL client\nnodes leads to a sub-optimal centralised FL model for COVID-19 chest CT image\nsegmentation. ST-FL is a novel FL framework that is robust in the face of\nhighly variable data quality at client nodes. The robustness is achieved by a\ndenoising CycleGAN model at each client of the federation that maps arbitrary\nquality images into the same target quality, counteracting the severe data\nvariability evident in real-world FL use-cases. Each client is provided with\nthe target style, which is the same for all clients, and trains their own\ndenoiser. Our qualitative and quantitative results suggest that this FL model\nperforms comparably to, and in some cases better than, a model that has\ncentralised access to all the training data.",
          "arxiv_id": "2203.13680v1"
        }
      ],
      "30": [
        {
          "title": "Comprehensive study of good model training for prostate segmentation in volumetric MRI",
          "year": "2022-08",
          "abstract": "Prostate cancer was the third most common cancer in 2020 internationally,\ncoming after breast cancer and lung cancer. Furthermore, in recent years\nprostate cancer has shown an increasing trend. According to clinical\nexperience, if this problem is detected and treated early, there can be a high\nchance of survival for the patient. One task that helps diagnose prostate\ncancer is prostate segmentation from magnetic resonance imaging. Manual\nsegmentation performed by clinical experts has its drawbacks such as: the high\ntime and concentration required from observers; and inter- and intra-observer\nvariability. This is why in recent years automatic approaches to segment a\nprostate based on convolutional neural networks have emerged. Many of them have\nnovel proposed architectures. In this paper I make an exhaustive study of\nseveral deep learning models by adjusting them to the task of prostate\nprediction. I do not use novel architectures, but focus my work more on how to\ntrain the networks. My approach is based on a ResNext101 3D encoder and a\nUnet3D decoder. I provide a study of the importance of resolutions in\nresampling data, something that no one else has done before.",
          "arxiv_id": "2208.13671v1"
        },
        {
          "title": "Mask Enhanced Deeply Supervised Prostate Cancer Detection on B-mode Micro-Ultrasound",
          "year": "2024-12",
          "abstract": "Prostate cancer is a leading cause of cancer-related deaths among men. The\nrecent development of high frequency, micro-ultrasound imaging offers improved\nresolution compared to conventional ultrasound and potentially a better ability\nto differentiate clinically significant cancer from normal tissue. However, the\nfeatures of prostate cancer remain subtle, with ambiguous borders with normal\ntissue and large variations in appearance, making it challenging for both\nmachine learning and humans to localize it on micro-ultrasound images.\n  We propose a novel Mask Enhanced Deeply-supervised Micro-US network, termed\nMedMusNet, to automatically and more accurately segment prostate cancer to be\nused as potential targets for biopsy procedures. MedMusNet leverages predicted\nmasks of prostate cancer to enforce the learned features layer-wisely within\nthe network, reducing the influence of noise and improving overall consistency\nacross frames.\n  MedMusNet successfully detected 76% of clinically significant cancer with a\nDice Similarity Coefficient of 0.365, significantly outperforming the baseline\nSwin-M2F in specificity and accuracy (Wilcoxon test, Bonferroni correction,\np-value<0.05). While the lesion-level and patient-level analyses showed\nimproved performance compared to human experts and different baseline, the\nimprovements did not reach statistical significance, likely on account of the\nsmall cohort.\n  We have presented a novel approach to automatically detect and segment\nclinically significant prostate cancer on B-mode micro-ultrasound images. Our\nMedMusNet model outperformed other models, surpassing even human experts. These\npreliminary results suggest the potential for aiding urologists in prostate\ncancer diagnosis via biopsy and treatment decision-making.",
          "arxiv_id": "2412.10997v1"
        },
        {
          "title": "Using deep learning to detect patients at risk for prostate cancer despite benign biopsies",
          "year": "2021-06",
          "abstract": "Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.",
          "arxiv_id": "2106.14256v3"
        }
      ],
      "31": [
        {
          "title": "FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model",
          "year": "2024-03",
          "abstract": "The quality of fetal MRI is significantly affected by unpredictable and\nsubstantial fetal motion, leading to the introduction of artifacts even when\nfast acquisition sequences are employed. The development of 3D real-time fetal\npose estimation approaches on volumetric EPI fetal MRI opens up a promising\navenue for fetal motion monitoring and prediction. Challenges arise in fetal\npose estimation due to limited number of real scanned fetal MR training images,\nhindering model generalization when the acquired fetal MRI lacks adequate pose.\n  In this study, we introduce FetalDiffusion, a novel approach utilizing a\nconditional diffusion model to generate 3D synthetic fetal MRI with\ncontrollable pose. Additionally, an auxiliary pose-level loss is adopted to\nenhance model performance. Our work demonstrates the success of this proposed\nmodel by producing high-quality synthetic fetal MRI images with accurate and\nrecognizable fetal poses, comparing favorably with in-vivo real fetal MRI.\nFurthermore, we show that the integration of synthetic fetal MR images enhances\nthe fetal pose estimation model's performance, particularly when the number of\navailable real scanned data is limited resulting in 15.4% increase in PCK and\n50.2% reduced in mean error. All experiments are done on a single 32GB V100\nGPU. Our method holds promise for improving real-time tracking models, thereby\naddressing fetal motion issues more effectively.",
          "arxiv_id": "2404.00132v1"
        },
        {
          "title": "Fetal-BET: Brain Extraction Tool for Fetal MRI",
          "year": "2023-10",
          "abstract": "Fetal brain extraction is a necessary first step in most computational fetal\nbrain MRI pipelines. However, it has been a very challenging task due to\nnon-standard fetal head pose, fetal movements during examination, and vastly\nheterogeneous appearance of the developing fetal brain and the neighboring\nfetal and maternal anatomy across various sequences and scanning conditions.\nDevelopment of a machine learning method to effectively address this task\nrequires a large and rich labeled dataset that has not been previously\navailable. As a result, there is currently no method for accurate fetal brain\nextraction on various fetal MRI sequences. In this work, we first built a large\nannotated dataset of approximately 72,000 2D fetal brain MRI images. Our\ndataset covers the three common MRI sequences including T2-weighted,\ndiffusion-weighted, and functional MRI acquired with different scanners.\nMoreover, it includes normal and pathological brains. Using this dataset, we\ndeveloped and validated deep learning methods, by exploiting the power of the\nU-Net style architectures, the attention mechanism, multi-contrast feature\nlearning, and data augmentation for fast, accurate, and generalizable automatic\nfetal brain extraction. Our approach leverages the rich information from\nmulti-contrast (multi-sequence) fetal MRI data, enabling precise delineation of\nthe fetal brain structures. Evaluations on independent test data show that our\nmethod achieves accurate brain extraction on heterogeneous test data acquired\nwith different scanners, on pathological brains, and at various gestational\nstages. This robustness underscores the potential utility of our deep learning\nmodel for fetal brain imaging and image analysis.",
          "arxiv_id": "2310.01523v2"
        },
        {
          "title": "Automatic linear measurements of the fetal brain on MRI with deep neural networks",
          "year": "2021-06",
          "abstract": "Timely, accurate and reliable assessment of fetal brain development is\nessential to reduce short and long-term risks to fetus and mother. Fetal MRI is\nincreasingly used for fetal brain assessment. Three key biometric linear\nmeasurements important for fetal brain evaluation are Cerebral Biparietal\nDiameter (CBD), Bone Biparietal Diameter (BBD), and Trans-Cerebellum Diameter\n(TCD), obtained manually by expert radiologists on reference slices, which is\ntime consuming and prone to human error. The aim of this study was to develop a\nfully automatic method computing the CBD, BBD and TCD measurements from fetal\nbrain MRI. The input is fetal brain MRI volumes which may include the fetal\nbody and the mother's abdomen. The outputs are the measurement values and\nreference slices on which the measurements were computed. The method, which\nfollows the manual measurements principle, consists of five stages: 1)\ncomputation of a Region Of Interest that includes the fetal brain with an\nanisotropic 3D U-Net classifier; 2) reference slice selection with a\nConvolutional Neural Network; 3) slice-wise fetal brain structures segmentation\nwith a multiclass U-Net classifier; 4) computation of the fetal brain\nmidsagittal line and fetal brain orientation, and; 5) computation of the\nmeasurements. Experimental results on 214 volumes for CBD, BBD and TCD\nmeasurements yielded a mean $L_1$ difference of 1.55mm, 1.45mm and 1.23mm\nrespectively, and a Bland-Altman 95% confidence interval ($CI_{95}$) of 3.92mm,\n3.98mm and 2.25mm respectively. These results are similar to the manual\ninter-observer variability. The proposed automatic method for computing\nbiometric linear measurements of the fetal brain from MR imaging achieves human\nlevel performance. It has the potential of being a useful method for the\nassessment of fetal brain biometry in normal and pathological cases, and of\nimproving routine clinical practice.",
          "arxiv_id": "2106.08174v1"
        }
      ],
      "32": [
        {
          "title": "PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression",
          "year": "2024-02",
          "abstract": "The universality of the point cloud format enables many 3D applications,\nmaking the compression of point clouds a critical phase in practice. Sampled as\ndiscrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D\nwith a finite bit-depth. However, the point distribution of a practical point\ncloud changes drastically as its bit-depth increases, requiring different\nmethodologies for effective consumption/analysis. In this regard, a\nheterogeneous point cloud compression (PCC) framework is proposed. We unify\ntypical point cloud representations -- point-based, voxel-based, and tree-based\nrepresentations -- and their associated backbones under a learning-based\nframework to compress an input point cloud at different bit-depth levels.\nHaving recognized the importance of voxel-domain processing, we augment the\nframework with a proposed context-aware upsampling for decoding and an enhanced\nvoxel transformer for feature aggregation. Extensive experimentation\ndemonstrates the state-of-the-art performance of our proposal on a wide range\nof point clouds.",
          "arxiv_id": "2402.07243v1"
        },
        {
          "title": "Lossless Point Cloud Geometry and Attribute Compression Using a Learned Conditional Probability Model",
          "year": "2023-03",
          "abstract": "In recent years, we have witnessed the presence of point cloud data in many\naspects of our life, from immersive media, autonomous driving to healthcare,\nalthough at the cost of a tremendous amount of data. In this paper, we present\nan efficient lossless point cloud compression method that uses sparse\ntensor-based deep neural networks to learn point cloud geometry and color\nprobability distributions. Our method represents a point cloud with both\noccupancy feature and three attribute features at different bit depths in a\nunified sparse representation. This allows us to efficiently exploit\nfeature-wise and point-wise dependencies within point clouds using a sparse\ntensor-based neural network and thus build an accurate auto-regressive context\nmodel for an arithmetic coder. To the best of our knowledge, this is the first\nlearning-based lossless point cloud geometry and attribute compression\napproach. Compared with the-state-of-the-art lossless point cloud compression\nmethod from Moving Picture Experts Group (MPEG), our method achieves 22.6%\nreduction in total bitrate on a diverse set of test point clouds while having\n49.0% and 18.3% rate reduction on geometry and color attribute component,\nrespectively.",
          "arxiv_id": "2303.06519v2"
        },
        {
          "title": "Performance analysis of Deep Learning-based Lossy Point Cloud Geometry Compression Coding Solutions",
          "year": "2024-02",
          "abstract": "The quality evaluation of three deep learning-based coding solutions for\npoint cloud geometry, notably ADLPCC, PCC GEO CNNv2, and PCGCv2, is presented.\nThe MPEG G-PCC was used as an anchor. Furthermore, LUT SR, which uses\nmulti-resolution Look-Up tables, was also considered. A set of six point clouds\nrepresenting landscapes and objects were used. As point cloud texture has a\ngreat influence on the perceived quality, two different subjective studies that\ndiffer in the texture addition model are reported and statistically compared.\nIn the first experiment, the dataset was first encoded with the identified\ncodecs. Then, the texture of the original point cloud was mapped to the decoded\npoint cloud using the Meshlab software, resulting in a point cloud with both\ngeometry and texture information. Finally, the resulting point cloud was\nencoded with G-PCC using the lossless-geometry-lossy-atts mode, while in the\nsecond experiment the texture was mapped directly onto the distorted geometry.\nMoreover, both subjective evaluations were used to benchmark a set of objective\npoint cloud quality metrics. The two experiments were shown to be statistically\ndifferent, and the tested metrics revealed quite different behaviors for the\ntwo sets of data. The results reveal that the preferred method of evaluation is\nthe encoding of texture information with G-PCC after mapping the texture of the\noriginal point cloud to the distorted point cloud. The results suggest that\ncurrent objective metrics are not suitable to evaluate distortions created by\nmachine learning-based codecs.",
          "arxiv_id": "2402.05192v1"
        }
      ],
      "33": [
        {
          "title": "Comprehensive evaluation of no-reference image quality assessment algorithms on authentic distortions",
          "year": "2020-10",
          "abstract": "Objective image quality assessment deals with the prediction of digital\nimages' perceptual quality. No-reference image quality assessment predicts the\nquality of a given input image without any knowledge or information about its\npristine (distortion free) counterpart. Machine learning algorithms are heavily\nused in no-reference image quality assessment because it is very complicated to\nmodel the human visual system's quality perception. Moreover, no-reference\nimage quality assessment algorithms are evaluated on publicly available\nbenchmark databases. These databases contain images with their corresponding\nquality scores. In this study, we evaluate several machine learning based\nNR-IQA methods and one opinion unaware method on databases consisting of\nauthentic distortions. Specifically, LIVE In the Wild and KonIQ-10k databases\nwere applied to evaluate the state-of-the-art. For machine learning based\nmethods, appx. 80% were used for training and the remaining 20% were used for\ntesting. Furthermore, average PLCC, SROCC, and KROCC values were reported over\n100 random train-test splits. The statistics of PLCC, SROCC, and KROCC values\nwere also published using boxplots. Our evaluation results may be helpful to\nobtain a clear understanding about the status of state-of-the-art no-reference\nimage quality assessment methods.",
          "arxiv_id": "2011.07950v1"
        },
        {
          "title": "Perceptual Image Quality Assessment with Transformers",
          "year": "2021-04",
          "abstract": "In this paper, we propose an image quality transformer (IQT) that\nsuccessfully applies a transformer architecture to a perceptual full-reference\nimage quality assessment (IQA) task. Perceptual representation becomes more\nimportant in image quality assessment. In this context, we extract the\nperceptual feature representations from each of input images using a\nconvolutional neural network (CNN) backbone. The extracted feature maps are fed\ninto the transformer encoder and decoder in order to compare a reference and\ndistorted images. Following an approach of the transformer-based vision models,\nwe use extra learnable quality embedding and position embedding. The output of\nthe transformer is passed to a prediction head in order to predict a final\nquality score. The experimental results show that our proposed model has an\noutstanding performance for the standard IQA datasets. For a large-scale IQA\ndataset containing output images of generative model, our model also shows the\npromising results. The proposed IQT was ranked first among 13 participants in\nthe NTIRE 2021 perceptual image quality assessment challenge. Our work will be\nan opportunity to further expand the approach for the perceptual IQA task.",
          "arxiv_id": "2104.14730v2"
        },
        {
          "title": "Cross-IQA: Unsupervised Learning for Image Quality Assessment",
          "year": "2024-05",
          "abstract": "Automatic perception of image quality is a challenging problem that impacts\nbillions of Internet and social media users daily. To advance research in this\nfield, we propose a no-reference image quality assessment (NR-IQA) method\ntermed Cross-IQA based on vision transformer(ViT) model. The proposed Cross-IQA\nmethod can learn image quality features from unlabeled image data. We construct\nthe pretext task of synthesized image reconstruction to unsupervised extract\nthe image quality information based ViT block. The pretrained encoder of\nCross-IQA is used to fine-tune a linear regression model for score prediction.\nExperimental results show that Cross-IQA can achieve state-of-the-art\nperformance in assessing the low-frequency degradation information (e.g., color\nchange, blurring, etc.) of images compared with the classical full-reference\nIQA and NR-IQA under the same datasets.",
          "arxiv_id": "2405.04311v1"
        }
      ],
      "34": [
        {
          "title": "Automatic Polyp Segmentation using Fully Convolutional Neural Network",
          "year": "2021-01",
          "abstract": "Colorectal cancer is one of fatal cancer worldwide. Colonoscopy is the\nstandard treatment for examination, localization, and removal of colorectal\npolyps. However, it has been shown that the miss-rate of colorectal polyps\nduring colonoscopy is between 6 to 27%. The use of an automated, accurate, and\nreal-time polyp segmentation during colonoscopy examinations can help the\nclinicians to eliminate missing lesions and prevent further progression of\ncolorectal cancer. The ``Medico automatic polyp segmentation challenge''\nprovides an opportunity to study polyp segmentation and build a fast\nsegmentation model. The challenge organizers provide a Kvasir-SEG dataset to\ntrain the model. Then it is tested on a separate unseen dataset to validate the\nefficiency and speed of the segmentation model. The experiments demonstrate\nthat the model trained on the Kvasir-SEG dataset and tested on an unseen\ndataset achieves a dice coefficient of 0.7801, mIoU of 0.6847, recall of\n0.8077, and precision of 0.8126, demonstrating the generalization ability of\nour model. The model has achieved 80.60 FPS on the unseen dataset with an image\nresolution of $512 \\times 512$.",
          "arxiv_id": "2101.04001v1"
        },
        {
          "title": "TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation",
          "year": "2022-06",
          "abstract": "Colorectal cancer (CRC) is one of the most common causes of cancer and\ncancer-related mortality worldwide. Performing colon cancer screening in a\ntimely fashion is the key to early detection. Colonoscopy is the primary\nmodality used to diagnose colon cancer. However, the miss rate of polyps,\nadenomas and advanced adenomas remains significantly high. Early detection of\npolyps at the precancerous stage can help reduce the mortality rate and the\neconomic burden associated with colorectal cancer. Deep learning-based\ncomputer-aided diagnosis (CADx) system may help gastroenterologists to identify\npolyps that may otherwise be missed, thereby improving the polyp detection\nrate. Additionally, CADx system could prove to be a cost-effective system that\nimproves long-term colorectal cancer prevention. In this study, we proposed a\ndeep learning-based architecture for automatic polyp segmentation, called\nTransformer ResU-Net (TransResU-Net). Our proposed architecture is built upon\nresidual blocks with ResNet-50 as the backbone and takes the advantage of\ntransformer self-attention mechanism as well as dilated convolution(s). Our\nexperimental results on two publicly available polyp segmentation benchmark\ndatasets showed that TransResU-Net obtained a highly promising dice score and a\nreal-time speed. With high efficacy in our performance metrics, we concluded\nthat TransResU-Net could be a strong benchmark for building a real-time polyp\ndetection system for the early diagnosis, treatment, and prevention of\ncolorectal cancer. The source code of the proposed TransResU-Net is publicly\navailable at https://github.com/nikhilroxtomar/TransResUNet.",
          "arxiv_id": "2206.08985v1"
        },
        {
          "title": "Automatic Polyp Segmentation using U-Net-ResNet50",
          "year": "2020-12",
          "abstract": "Polyps are the predecessors to colorectal cancer which is considered as one\nof the leading causes of cancer-related deaths worldwide. Colonoscopy is the\nstandard procedure for the identification, localization, and removal of\ncolorectal polyps. Due to variability in shape, size, and surrounding tissue\nsimilarity, colorectal polyps are often missed by the clinicians during\ncolonoscopy. With the use of an automatic, accurate, and fast polyp\nsegmentation method during the colonoscopy, many colorectal polyps can be\neasily detected and removed. The ``Medico automatic polyp segmentation\nchallenge'' provides an opportunity to study polyp segmentation and build an\nefficient and accurate segmentation algorithm. We use the U-Net with\npre-trained ResNet50 as the encoder for the polyp segmentation. The model is\ntrained on Kvasir-SEG dataset provided for the challenge and tested on the\norganizer's dataset and achieves a dice coefficient of 0.8154, Jaccard of\n0.7396, recall of 0.8533, precision of 0.8532, accuracy of 0.9506, and F2 score\nof 0.8272, demonstrating the generalization ability of our model.",
          "arxiv_id": "2012.15247v1"
        }
      ],
      "35": [
        {
          "title": "Learning Kernel-Modulated Neural Representation for Efficient Light Field Compression",
          "year": "2023-07",
          "abstract": "Light field is a type of image data that captures the 3D scene information by\nrecording light rays emitted from a scene at various orientations. It offers a\nmore immersive perception than classic 2D images but at the cost of huge data\nvolume. In this paper, we draw inspiration from the visual characteristics of\nSub-Aperture Images (SAIs) of light field and design a compact neural network\nrepresentation for the light field compression task. The network backbone takes\nrandomly initialized noise as input and is supervised on the SAIs of the target\nlight field. It is composed of two types of complementary kernels: descriptive\nkernels (descriptors) that store scene description information learned during\ntraining, and modulatory kernels (modulators) that control the rendering of\ndifferent SAIs from the queried perspectives. To further enhance compactness of\nthe network meanwhile retain high quality of the decoded light field, we\naccordingly introduce modulator allocation and kernel tensor decomposition\nmechanisms, followed by non-uniform quantization and lossless entropy coding\ntechniques, to finally form an efficient compression pipeline. Extensive\nexperiments demonstrate that our method outperforms other state-of-the-art\n(SOTA) methods by a significant margin in the light field compression task.\nMoreover, after aligning descriptors, the modulators learned from one light\nfield can be transferred to new light fields for rendering dense views,\nindicating a potential solution for view synthesis task.",
          "arxiv_id": "2307.06143v1"
        },
        {
          "title": "Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues",
          "year": "2024-03",
          "abstract": "Light field cameras and multi-camera arrays have emerged as promising\nsolutions for accurately estimating depth by passively capturing light\ninformation. This is possible because the 3D information of a scene is embedded\nin the 4D light field geometry. Commonly, depth estimation methods extract this\ninformation relying on gradient information, heuristic-based optimisation\nmodels, or learning-based approaches. This paper focuses mainly on explicitly\nunderstanding and exploiting 4D geometrical cues for light field depth\nestimation. Thus, a novel method is proposed, based on a non-learning-based\noptimisation approach for depth estimation that explicitly considers surface\nnormal accuracy and occlusion regions by utilising a fully explainable 4D\ngeometric model of the light field. The 4D model performs depth/disparity\nestimation by determining the orientations and analysing the intersections of\nkey 2D planes in 4D space, which are the images of 3D-space points in the 4D\nlight field. Experimental results show that the proposed method outperforms\nboth learning-based and non-learning-based state-of-the-art methods in terms of\nsurface normal angle accuracy, achieving a Median Angle Error on planar\nsurfaces, on average, 26.3$\\%$ lower than the state-of-the-art, and still being\ncompetitive with state-of-the-art methods in terms of MSE ${\\times}$ 100 and\nBadpix 0.07.",
          "arxiv_id": "2403.02043v2"
        },
        {
          "title": "Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses",
          "year": "2024-02",
          "abstract": "In this paper, we design a beam splitter-based hybrid light field imaging\nprototype to record 4D light field image and high-resolution 2D image\nsimultaneously, and make a hybrid light field dataset. The 2D image could be\nconsidered as the high-resolution ground truth corresponding to the\nlow-resolution central sub-aperture image of 4D light field image.\nSubsequently, we propose an unsupervised learning-based super-resolution\nframework with the hybrid light field dataset, which adaptively settles the\nlight field spatial super-resolution problem with a complex degradation model.\nSpecifically, we design two loss functions based on pre-trained models that\nenable the super-resolution network to learn the detailed features and light\nfield parallax structure with only one ground truth. Extensive experiments\ndemonstrate the same superiority of our approach with supervised learning-based\nstate-of-the-art ones. To our knowledge, it is the first end-to-end\nunsupervised learning-based spatial super-resolution approach in light field\nimaging research, whose input is available from our beam splitter-based hybrid\nlight field system. The hardware and software together may help promote the\napplication of light field super-resolution to a great extent.",
          "arxiv_id": "2402.19020v1"
        }
      ],
      "36": [
        {
          "title": "Virtual staining of defocused autofluorescence images of unlabeled tissue using deep neural networks",
          "year": "2022-07",
          "abstract": "Deep learning-based virtual staining was developed to introduce image\ncontrast to label-free tissue sections, digitally matching the histological\nstaining, which is time-consuming, labor-intensive, and destructive to tissue.\nStandard virtual staining requires high autofocusing precision during the whole\nslide imaging of label-free tissue, which consumes a significant portion of the\ntotal imaging time and can lead to tissue photodamage. Here, we introduce a\nfast virtual staining framework that can stain defocused autofluorescence\nimages of unlabeled tissue, achieving equivalent performance to virtual\nstaining of in-focus label-free images, also saving significant imaging time by\nlowering the microscope's autofocusing precision. This framework incorporates a\nvirtual-autofocusing neural network to digitally refocus the defocused images\nand then transforms the refocused images into virtually stained images using a\nsuccessive network. These cascaded networks form a collaborative inference\nscheme: the virtual staining model regularizes the virtual-autofocusing network\nthrough a style loss during the training. To demonstrate the efficacy of this\nframework, we trained and blindly tested these networks using human lung\ntissue. Using 4x fewer focus points with 2x lower focusing precision, we\nsuccessfully transformed the coarsely-focused autofluorescence images into\nhigh-quality virtually stained H&E images, matching the standard virtual\nstaining framework that used finely-focused autofluorescence input images.\nWithout sacrificing the staining quality, this framework decreases the total\nimage acquisition time needed for virtual staining of a label-free whole-slide\nimage (WSI) by ~32%, together with a ~89% decrease in the autofocusing time,\nand has the potential to eliminate the laborious and costly histochemical\nstaining process in pathology.",
          "arxiv_id": "2207.02946v1"
        },
        {
          "title": "Digital synthesis of histological stains using micro-structured and multiplexed virtual staining of label-free tissue",
          "year": "2020-01",
          "abstract": "Histological staining is a vital step used to diagnose various diseases and\nhas been used for more than a century to provide contrast to tissue sections,\nrendering the tissue constituents visible for microscopic analysis by medical\nexperts. However, this process is time-consuming, labor-intensive, expensive\nand destructive to the specimen. Recently, the ability to virtually-stain\nunlabeled tissue sections, entirely avoiding the histochemical staining step,\nhas been demonstrated using tissue-stain specific deep neural networks. Here,\nwe present a new deep learning-based framework which generates\nvirtually-stained images using label-free tissue, where different stains are\nmerged following a micro-structure map defined by the user. This approach uses\na single deep neural network that receives two different sources of information\nat its input: (1) autofluorescence images of the label-free tissue sample, and\n(2) a digital staining matrix which represents the desired microscopic map of\ndifferent stains to be virtually generated at the same tissue section. This\ndigital staining matrix is also used to virtually blend existing stains,\ndigitally synthesizing new histological stains. We trained and blindly tested\nthis virtual-staining network using unlabeled kidney tissue sections to\ngenerate micro-structured combinations of Hematoxylin and Eosin (H&E), Jones\nsilver stain, and Masson's Trichrome stain. Using a single network, this\napproach multiplexes virtual staining of label-free tissue with multiple types\nof stains and paves the way for synthesizing new digital histological stains\nthat can be created on the same tissue cross-section, which is currently not\nfeasible with standard histochemical staining methods.",
          "arxiv_id": "2001.07267v1"
        },
        {
          "title": "Virtual stain transfer in histology via cascaded deep neural networks",
          "year": "2022-07",
          "abstract": "Pathological diagnosis relies on the visual inspection of histologically\nstained thin tissue specimens, where different types of stains are applied to\nbring contrast to and highlight various desired histological features. However,\nthe destructive histochemical staining procedures are usually irreversible,\nmaking it very difficult to obtain multiple stains on the same tissue section.\nHere, we demonstrate a virtual stain transfer framework via a cascaded deep\nneural network (C-DNN) to digitally transform hematoxylin and eosin (H&E)\nstained tissue images into other types of histological stains. Unlike a single\nneural network structure which only takes one stain type as input to digitally\noutput images of another stain type, C-DNN first uses virtual staining to\ntransform autofluorescence microscopy images into H&E and then performs stain\ntransfer from H&E to the domain of the other stain in a cascaded manner. This\ncascaded structure in the training phase allows the model to directly exploit\nhistochemically stained image data on both H&E and the target special stain of\ninterest. This advantage alleviates the challenge of paired data acquisition\nand improves the image quality and color accuracy of the virtual stain transfer\nfrom H&E to another stain. We validated the superior performance of this C-DNN\napproach using kidney needle core biopsy tissue sections and successfully\ntransferred the H&E-stained tissue images into virtual PAS (periodic\nacid-Schiff) stain. This method provides high-quality virtual images of special\nstains using existing, histochemically stained slides and creates new\nopportunities in digital pathology by performing highly accurate stain-to-stain\ntransformations.",
          "arxiv_id": "2207.06578v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T19:46:32Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}