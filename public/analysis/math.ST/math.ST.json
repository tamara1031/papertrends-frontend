{
  "topics": {
    "data": {
      "0": {
        "name": "0_test_distribution_tests_distributions",
        "keywords": [
          [
            "test",
            0.01875402917396214
          ],
          [
            "distribution",
            0.018606646391446758
          ],
          [
            "tests",
            0.01507726039896231
          ],
          [
            "distributions",
            0.014389248926047276
          ],
          [
            "dependence",
            0.014302383082861487
          ],
          [
            "testing",
            0.013040639565134682
          ],
          [
            "multivariate",
            0.012656381570525852
          ],
          [
            "statistics",
            0.011048087066666318
          ],
          [
            "data",
            0.011025390427183894
          ],
          [
            "values",
            0.01100957842067777
          ]
        ],
        "count": 874
      },
      "1": {
        "name": "1_treatment_causal_effect_effects",
        "keywords": [
          [
            "treatment",
            0.03984920905382455
          ],
          [
            "causal",
            0.02469824114719586
          ],
          [
            "effect",
            0.024255147706512135
          ],
          [
            "effects",
            0.02101610372664524
          ],
          [
            "outcome",
            0.015863155864986166
          ],
          [
            "inference",
            0.014594751658775764
          ],
          [
            "treatment effect",
            0.014532925281276279
          ],
          [
            "estimators",
            0.013283821679223755
          ],
          [
            "estimator",
            0.012422493885622266
          ],
          [
            "outcomes",
            0.012021912667075588
          ]
        ],
        "count": 609
      },
      "2": {
        "name": "2_graph_network_graphs_model",
        "keywords": [
          [
            "graph",
            0.028616837834173573
          ],
          [
            "network",
            0.0245772537528683
          ],
          [
            "graphs",
            0.02164711206421508
          ],
          [
            "model",
            0.017785103550747982
          ],
          [
            "community",
            0.016167158552963922
          ],
          [
            "networks",
            0.016060773109348614
          ],
          [
            "random",
            0.014811061346119214
          ],
          [
            "degree",
            0.012530577370267927
          ],
          [
            "edge",
            0.011915496227033966
          ],
          [
            "vertices",
            0.011562379200015712
          ]
        ],
        "count": 496
      },
      "3": {
        "name": "3_change_time_series_time series",
        "keywords": [
          [
            "change",
            0.03396453393310347
          ],
          [
            "time",
            0.02974132293552044
          ],
          [
            "series",
            0.027292609520571064
          ],
          [
            "time series",
            0.025912663627461287
          ],
          [
            "detection",
            0.020077021839175837
          ],
          [
            "change point",
            0.016986198361334085
          ],
          [
            "point",
            0.015072225728343611
          ],
          [
            "stationary",
            0.01293729359291851
          ],
          [
            "data",
            0.012623762660629437
          ],
          [
            "test",
            0.012584487497804492
          ]
        ],
        "count": 481
      },
      "4": {
        "name": "4_Wasserstein_transport_distance_metric",
        "keywords": [
          [
            "Wasserstein",
            0.01969288952008333
          ],
          [
            "transport",
            0.018702760521586335
          ],
          [
            "distance",
            0.016752634035079764
          ],
          [
            "metric",
            0.01613981617884994
          ],
          [
            "optimal transport",
            0.014760237081613448
          ],
          [
            "manifold",
            0.014744864866619497
          ],
          [
            "space",
            0.014231360849970722
          ],
          [
            "optimal",
            0.013986620246554442
          ],
          [
            "measures",
            0.012617171445578085
          ],
          [
            "data",
            0.012317884633937366
          ]
        ],
        "count": 436
      },
      "5": {
        "name": "5_neural_networks_neural networks_learning",
        "keywords": [
          [
            "neural",
            0.031207612053351992
          ],
          [
            "networks",
            0.02652514090729082
          ],
          [
            "neural networks",
            0.023848482749901687
          ],
          [
            "learning",
            0.01846281425786905
          ],
          [
            "deep",
            0.01768815321132203
          ],
          [
            "training",
            0.014363022676595203
          ],
          [
            "network",
            0.01420025043946807
          ],
          [
            "generalization",
            0.013956473562115813
          ],
          [
            "data",
            0.012983210442338687
          ],
          [
            "descent",
            0.012545437316697244
          ]
        ],
        "count": 399
      },
      "6": {
        "name": "6_matrix_tensor_rank_low",
        "keywords": [
          [
            "matrix",
            0.0254668935152626
          ],
          [
            "tensor",
            0.025137365733084713
          ],
          [
            "rank",
            0.02509015709659617
          ],
          [
            "low",
            0.016005357193419747
          ],
          [
            "low rank",
            0.015080278165784836
          ],
          [
            "noise",
            0.014144116845031675
          ],
          [
            "signal",
            0.013697188067684735
          ],
          [
            "algorithm",
            0.01217476753422384
          ],
          [
            "problem",
            0.011254514974724942
          ],
          [
            "estimation",
            0.010939693328542914
          ]
        ],
        "count": 338
      },
      "7": {
        "name": "7_regression_dimensional_high_linear",
        "keywords": [
          [
            "regression",
            0.02236010216483472
          ],
          [
            "dimensional",
            0.016793483495048712
          ],
          [
            "high",
            0.01635555769491137
          ],
          [
            "linear",
            0.014413329457104435
          ],
          [
            "model",
            0.013851242064425144
          ],
          [
            "Lasso",
            0.013696604044551763
          ],
          [
            "selection",
            0.013382095594933663
          ],
          [
            "sparse",
            0.01278030132654972
          ],
          [
            "estimator",
            0.012208941343408662
          ],
          [
            "models",
            0.011238245803309675
          ]
        ],
        "count": 335
      },
      "8": {
        "name": "8_sampling_Langevin_diffusion_convergence",
        "keywords": [
          [
            "sampling",
            0.024154931298264376
          ],
          [
            "Langevin",
            0.021865172081947465
          ],
          [
            "diffusion",
            0.018795007620146992
          ],
          [
            "convergence",
            0.017625550565577408
          ],
          [
            "target",
            0.016380044092607127
          ],
          [
            "Carlo",
            0.015666202660057933
          ],
          [
            "Monte",
            0.01566197634730362
          ],
          [
            "Markov",
            0.01541402637731613
          ],
          [
            "distribution",
            0.0144135948441913
          ],
          [
            "Metropolis",
            0.013652053792945333
          ]
        ],
        "count": 324
      },
      "9": {
        "name": "9_Gaussian_kernel_processes_regression",
        "keywords": [
          [
            "Gaussian",
            0.02122034281218865
          ],
          [
            "kernel",
            0.019906782137975757
          ],
          [
            "processes",
            0.017676736783137234
          ],
          [
            "regression",
            0.016348625762022753
          ],
          [
            "function",
            0.014048916276491318
          ],
          [
            "spatial",
            0.01385891640958322
          ],
          [
            "process",
            0.013744212328674042
          ],
          [
            "covariance",
            0.01229833651041697
          ],
          [
            "Gaussian process",
            0.0117622955145799
          ],
          [
            "functions",
            0.011381088791499896
          ]
        ],
        "count": 293
      },
      "10": {
        "name": "10_policy_regret_learning_bandits",
        "keywords": [
          [
            "policy",
            0.03463119455045439
          ],
          [
            "regret",
            0.03114654863908132
          ],
          [
            "learning",
            0.021448198907862287
          ],
          [
            "bandits",
            0.017668953075025744
          ],
          [
            "bandit",
            0.01736250528917258
          ],
          [
            "RL",
            0.016934442055494128
          ],
          [
            "optimal",
            0.016696716330052683
          ],
          [
            "reward",
            0.01649866903062029
          ],
          [
            "algorithm",
            0.016061037342958627
          ],
          [
            "arm",
            0.014674424423286913
          ]
        ],
        "count": 273
      },
      "11": {
        "name": "11_mixture_Bayesian_models_posterior",
        "keywords": [
          [
            "mixture",
            0.023057326709846376
          ],
          [
            "Bayesian",
            0.0205948907865812
          ],
          [
            "models",
            0.018965055445064435
          ],
          [
            "posterior",
            0.016750376637639498
          ],
          [
            "model",
            0.016649286649134634
          ],
          [
            "distribution",
            0.016440425050007045
          ],
          [
            "Dirichlet",
            0.01602258083288646
          ],
          [
            "divergence",
            0.014703312231097767
          ],
          [
            "distributions",
            0.012753982676194616
          ],
          [
            "likelihood",
            0.01257610463017027
          ]
        ],
        "count": 248
      },
      "12": {
        "name": "12_process_processes_fractional_drift",
        "keywords": [
          [
            "process",
            0.028517724256742816
          ],
          [
            "processes",
            0.027572380433672897
          ],
          [
            "fractional",
            0.02634769788159543
          ],
          [
            "drift",
            0.026228040092956525
          ],
          [
            "diffusion",
            0.025671322046968523
          ],
          [
            "Brownian",
            0.024490415089631096
          ],
          [
            "motion",
            0.020977106531233053
          ],
          [
            "Brownian motion",
            0.020311262770525577
          ],
          [
            "estimator",
            0.018934232816712235
          ],
          [
            "estimators",
            0.01850259277118022
          ]
        ],
        "count": 215
      },
      "13": {
        "name": "13_privacy_private_differential privacy_differential",
        "keywords": [
          [
            "privacy",
            0.07894832126536883
          ],
          [
            "private",
            0.039457849082335544
          ],
          [
            "differential privacy",
            0.03340234360588501
          ],
          [
            "differential",
            0.027898766933053022
          ],
          [
            "data",
            0.021098754504018484
          ],
          [
            "DP",
            0.020367493663786897
          ],
          [
            "mechanism",
            0.01592054317706872
          ],
          [
            "estimation",
            0.014049399053550116
          ],
          [
            "Privacy",
            0.013734794872551673
          ],
          [
            "optimal",
            0.011612088392141657
          ]
        ],
        "count": 195
      },
      "14": {
        "name": "14_random_inequalities_bounds_inequality",
        "keywords": [
          [
            "random",
            0.028828222358193673
          ],
          [
            "inequalities",
            0.023684246532956775
          ],
          [
            "bounds",
            0.02096124773568924
          ],
          [
            "inequality",
            0.020848527240839878
          ],
          [
            "concentration",
            0.01688449417648835
          ],
          [
            "random variables",
            0.01508452152983121
          ],
          [
            "variables",
            0.013830863894351533
          ],
          [
            "results",
            0.01376208277336503
          ],
          [
            "statistics",
            0.013665180075590437
          ],
          [
            "Gaussian",
            0.013363805037265
          ]
        ],
        "count": 193
      },
      "15": {
        "name": "15_prediction_conformal_conformal prediction_learning",
        "keywords": [
          [
            "prediction",
            0.039924615375608805
          ],
          [
            "conformal",
            0.034172205127421015
          ],
          [
            "conformal prediction",
            0.022137818968893896
          ],
          [
            "learning",
            0.021132356461902285
          ],
          [
            "coverage",
            0.02085481532857826
          ],
          [
            "calibration",
            0.017326134745424285
          ],
          [
            "data",
            0.015251090376380025
          ],
          [
            "classification",
            0.013951531193897705
          ],
          [
            "PAC",
            0.012570400454473675
          ],
          [
            "sets",
            0.01207424530001757
          ]
        ],
        "count": 190
      },
      "16": {
        "name": "16_matrix_matrices_eigenvalues_covariance",
        "keywords": [
          [
            "matrix",
            0.0442571112675382
          ],
          [
            "matrices",
            0.03854857490665189
          ],
          [
            "eigenvalues",
            0.03355812178422479
          ],
          [
            "covariance",
            0.029465012060980095
          ],
          [
            "sample",
            0.02476288466028113
          ],
          [
            "spectral",
            0.023911815191936577
          ],
          [
            "Wishart",
            0.023735712503528583
          ],
          [
            "spiked",
            0.020491336316643103
          ],
          [
            "eigenvalue",
            0.018835349692035694
          ],
          [
            "random",
            0.018273913463264645
          ]
        ],
        "count": 169
      },
      "17": {
        "name": "17_inverse_inverse problems_problems_Bayesian",
        "keywords": [
          [
            "inverse",
            0.03881834418374452
          ],
          [
            "inverse problems",
            0.02879946459143526
          ],
          [
            "problems",
            0.028137768945383966
          ],
          [
            "Bayesian",
            0.025186028108752003
          ],
          [
            "posterior",
            0.02468740935266957
          ],
          [
            "problem",
            0.019587361990939404
          ],
          [
            "prior",
            0.015354836352207135
          ],
          [
            "inverse problem",
            0.014049508866429593
          ],
          [
            "Gaussian",
            0.013532294628261868
          ],
          [
            "priors",
            0.012898043247882051
          ]
        ],
        "count": 148
      },
      "18": {
        "name": "18_causal_variables_DAG_graph",
        "keywords": [
          [
            "causal",
            0.05061143610564053
          ],
          [
            "variables",
            0.02649230106385837
          ],
          [
            "DAG",
            0.024756260614296952
          ],
          [
            "graph",
            0.024559989339062373
          ],
          [
            "models",
            0.023873694883876626
          ],
          [
            "graphs",
            0.0236403823746978
          ],
          [
            "acyclic",
            0.022763414562846987
          ],
          [
            "equivalence",
            0.021225884538882314
          ],
          [
            "graphical",
            0.01812147629018249
          ],
          [
            "causal discovery",
            0.017368292895708615
          ]
        ],
        "count": 111
      }
    },
    "correlations": [
      [
        1.0,
        -0.7270037233761395,
        -0.6765549271231066,
        -0.7026246280885154,
        -0.6882494305155122,
        -0.7430926182721311,
        -0.7307215598143264,
        -0.7028400572933224,
        -0.7096132090553453,
        -0.7311063829688957,
        -0.7395747651605495,
        -0.6816959636529472,
        -0.7318901224266181,
        -0.755341535322265,
        -0.676195035864922,
        -0.7357552721010313,
        -0.6850268888698865,
        -0.7058408241723317,
        -0.7175835721842645
      ],
      [
        -0.7270037233761395,
        1.0,
        -0.7265597841637291,
        -0.7354506396109123,
        -0.7473058085603823,
        -0.7446721252207833,
        -0.7483671094660949,
        -0.7200442214579821,
        -0.7546506790146069,
        -0.7407512845074747,
        -0.7264212670693796,
        -0.7277638537523339,
        -0.7525011866494422,
        -0.7558659187831556,
        -0.7407595674695957,
        -0.7389352636223755,
        -0.7371880057135227,
        -0.7322211288380134,
        -0.5203142244350082
      ],
      [
        -0.6765549271231066,
        -0.7265597841637291,
        1.0,
        -0.7088813723507683,
        -0.7136356359211209,
        -0.6673283399057199,
        -0.7136727248977339,
        -0.6963931276479856,
        -0.7333064624461096,
        -0.7263857917522694,
        -0.7371740008345652,
        -0.3815340658204554,
        -0.7300297555082215,
        -0.7553226649285341,
        -0.6922285341293017,
        -0.7305056265949449,
        -0.7037404953584934,
        -0.7004324329007124,
        -0.572711469341338
      ],
      [
        -0.7026246280885154,
        -0.7354506396109123,
        -0.7088813723507683,
        1.0,
        -0.7322071109059554,
        -0.7369064405289572,
        -0.7355073327319159,
        -0.7108140534341338,
        -0.7321524365448874,
        -0.7224496690825462,
        -0.7396335544245364,
        -0.7026535098883493,
        -0.6924834663721057,
        -0.7555277116942205,
        -0.7240656372201096,
        -0.7237906708005046,
        -0.7172036465243243,
        -0.7229914776661335,
        -0.7083542703477412
      ],
      [
        -0.6882494305155122,
        -0.7473058085603823,
        -0.7136356359211209,
        -0.7322071109059554,
        1.0,
        -0.733558079556523,
        -0.7482688493532901,
        -0.7309367020204434,
        -0.7105680277141235,
        -0.7234654274447957,
        -0.7443854679750017,
        -0.7246078270101037,
        -0.7444893994917214,
        -0.7524779331479212,
        -0.6827619408076806,
        -0.7373310757078693,
        -0.7323434248936486,
        -0.7149551854617074,
        -0.7453450221662523
      ],
      [
        -0.7430926182721311,
        -0.7446721252207833,
        -0.6673283399057199,
        -0.7369064405289572,
        -0.733558079556523,
        1.0,
        -0.7364369758372677,
        -0.718822902488199,
        -0.735924123657323,
        -0.7242494886570539,
        -0.7110791522712422,
        -0.7157007140774412,
        -0.7498988148280347,
        -0.7528200781809011,
        -0.7422238415429384,
        -0.679266396459171,
        -0.7355288031267524,
        -0.725483199053675,
        -0.7265851957079319
      ],
      [
        -0.7307215598143264,
        -0.7483671094660949,
        -0.7136727248977339,
        -0.7355073327319159,
        -0.7482688493532901,
        -0.7364369758372677,
        1.0,
        -0.7051767175153618,
        -0.7518711124107418,
        -0.7415520823526991,
        -0.7417974357809254,
        -0.7199557374625287,
        -0.7524593902854608,
        -0.761186738035815,
        -0.7295802601113885,
        -0.7418515901986797,
        -0.48145137601262233,
        -0.7148522655888597,
        -0.735787850273778
      ],
      [
        -0.7028400572933224,
        -0.7200442214579821,
        -0.6963931276479856,
        -0.7108140534341338,
        -0.7309367020204434,
        -0.718822902488199,
        -0.7051767175153618,
        1.0,
        -0.7305336823441873,
        -0.6681421106771468,
        -0.7400033018941474,
        -0.6633592687017034,
        -0.7410258006164128,
        -0.7491685096256211,
        -0.7247958508588166,
        -0.7041116901310824,
        -0.5856449122281018,
        -0.6809017695865197,
        -0.7039880700673822
      ],
      [
        -0.7096132090553453,
        -0.7546506790146069,
        -0.7333064624461096,
        -0.7321524365448874,
        -0.7105680277141235,
        -0.735924123657323,
        -0.7518711124107418,
        -0.7305336823441873,
        1.0,
        -0.7398166305353145,
        -0.7421191118583818,
        -0.7019547648626736,
        -0.704380649433558,
        -0.7592269513400534,
        -0.7325382362295411,
        -0.7472741445502579,
        -0.7275649632871624,
        -0.7131697283315617,
        -0.7425808265578643
      ],
      [
        -0.7311063829688957,
        -0.7407512845074747,
        -0.7263857917522694,
        -0.7224496690825462,
        -0.7234654274447957,
        -0.7242494886570539,
        -0.7415520823526991,
        -0.6681421106771468,
        -0.7398166305353145,
        1.0,
        -0.7479511064983317,
        -0.7103889644254037,
        -0.6226886777868461,
        -0.7594015403175975,
        -0.7195068384272498,
        -0.7196972953813655,
        -0.7161790377449109,
        -0.6999756894319202,
        -0.7321955238089795
      ],
      [
        -0.7395747651605495,
        -0.7264212670693796,
        -0.7371740008345652,
        -0.7396335544245364,
        -0.7443854679750017,
        -0.7110791522712422,
        -0.7417974357809254,
        -0.7400033018941474,
        -0.7421191118583818,
        -0.7479511064983317,
        1.0,
        -0.7377505439711569,
        -0.747669111500486,
        -0.7468419439486852,
        -0.7360816537888688,
        -0.7039098754705724,
        -0.7417207231847669,
        -0.7114635612992122,
        -0.7409382637234994
      ],
      [
        -0.6816959636529472,
        -0.7277638537523339,
        -0.3815340658204554,
        -0.7026535098883493,
        -0.7246078270101037,
        -0.7157007140774412,
        -0.7199557374625287,
        -0.6633592687017034,
        -0.7019547648626736,
        -0.7103889644254037,
        -0.7377505439711569,
        1.0,
        -0.7218133271982323,
        -0.7584591058953201,
        -0.7260720259794129,
        -0.7177922117529008,
        -0.70539472730516,
        -0.5664863558120454,
        -0.48780557854140005
      ],
      [
        -0.7318901224266181,
        -0.7525011866494422,
        -0.7300297555082215,
        -0.6924834663721057,
        -0.7444893994917214,
        -0.7498988148280347,
        -0.7524593902854608,
        -0.7410258006164128,
        -0.704380649433558,
        -0.6226886777868461,
        -0.747669111500486,
        -0.7218133271982323,
        1.0,
        -0.7592160862355724,
        -0.7371424492437515,
        -0.7466603694352427,
        -0.7341848341687096,
        -0.7315070727128825,
        -0.735369169858187
      ],
      [
        -0.755341535322265,
        -0.7558659187831556,
        -0.7553226649285341,
        -0.7555277116942205,
        -0.7524779331479212,
        -0.7528200781809011,
        -0.761186738035815,
        -0.7491685096256211,
        -0.7592269513400534,
        -0.7594015403175975,
        -0.7468419439486852,
        -0.7584591058953201,
        -0.7592160862355724,
        1.0,
        -0.757961883162384,
        -0.7535946029802469,
        -0.7532091271381636,
        -0.7525764487223658,
        -0.7609519986527751
      ],
      [
        -0.676195035864922,
        -0.7407595674695957,
        -0.6922285341293017,
        -0.7240656372201096,
        -0.6827619408076806,
        -0.7422238415429384,
        -0.7295802601113885,
        -0.7247958508588166,
        -0.7325382362295411,
        -0.7195068384272498,
        -0.7360816537888688,
        -0.7260720259794129,
        -0.7371424492437515,
        -0.757961883162384,
        1.0,
        -0.73880832072687,
        -0.6923798867839512,
        -0.7262868322302161,
        -0.7046545369291658
      ],
      [
        -0.7357552721010313,
        -0.7389352636223755,
        -0.7305056265949449,
        -0.7237906708005046,
        -0.7373310757078693,
        -0.679266396459171,
        -0.7418515901986797,
        -0.7041116901310824,
        -0.7472741445502579,
        -0.7196972953813655,
        -0.7039098754705724,
        -0.7177922117529008,
        -0.7466603694352427,
        -0.7535946029802469,
        -0.73880832072687,
        1.0,
        -0.7294479451430198,
        -0.721324247910389,
        -0.7321515148285925
      ],
      [
        -0.6850268888698865,
        -0.7371880057135227,
        -0.7037404953584934,
        -0.7172036465243243,
        -0.7323434248936486,
        -0.7355288031267524,
        -0.48145137601262233,
        -0.5856449122281018,
        -0.7275649632871624,
        -0.7161790377449109,
        -0.7417207231847669,
        -0.70539472730516,
        -0.7341848341687096,
        -0.7532091271381636,
        -0.6923798867839512,
        -0.7294479451430198,
        1.0,
        -0.6894766153355423,
        -0.7133584375927389
      ],
      [
        -0.7058408241723317,
        -0.7322211288380134,
        -0.7004324329007124,
        -0.7229914776661335,
        -0.7149551854617074,
        -0.725483199053675,
        -0.7148522655888597,
        -0.6809017695865197,
        -0.7131697283315617,
        -0.6999756894319202,
        -0.7114635612992122,
        -0.5664863558120454,
        -0.7315070727128825,
        -0.7525764487223658,
        -0.7262868322302161,
        -0.721324247910389,
        -0.6894766153355423,
        1.0,
        -0.711423596054344
      ],
      [
        -0.7175835721842645,
        -0.5203142244350082,
        -0.572711469341338,
        -0.7083542703477412,
        -0.7453450221662523,
        -0.7265851957079319,
        -0.735787850273778,
        -0.7039880700673822,
        -0.7425808265578643,
        -0.7321955238089795,
        -0.7409382637234994,
        -0.48780557854140005,
        -0.735369169858187,
        -0.7609519986527751,
        -0.7046545369291658,
        -0.7321515148285925,
        -0.7133584375927389,
        -0.711423596054344,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        16,
        6,
        5,
        11,
        6,
        3,
        6,
        4,
        2,
        10,
        3,
        7,
        8,
        0,
        9,
        5,
        12,
        2,
        3
      ],
      "2020-02": [
        42,
        7,
        10,
        18,
        4,
        7,
        6,
        12,
        7,
        8,
        9,
        5,
        5,
        5,
        7,
        8,
        9,
        12,
        5
      ],
      "2020-03": [
        38,
        2,
        5,
        20,
        3,
        8,
        1,
        12,
        2,
        2,
        6,
        8,
        9,
        3,
        11,
        1,
        8,
        12,
        2
      ],
      "2020-04": [
        38,
        7,
        9,
        15,
        4,
        5,
        3,
        10,
        7,
        5,
        1,
        6,
        8,
        0,
        6,
        2,
        8,
        9,
        6
      ],
      "2020-05": [
        39,
        2,
        6,
        12,
        2,
        8,
        8,
        6,
        5,
        2,
        5,
        6,
        7,
        2,
        8,
        4,
        15,
        6,
        2
      ],
      "2020-06": [
        39,
        6,
        8,
        16,
        9,
        15,
        5,
        17,
        7,
        11,
        7,
        7,
        4,
        3,
        12,
        9,
        15,
        11,
        6
      ],
      "2020-07": [
        36,
        6,
        4,
        13,
        8,
        7,
        2,
        20,
        5,
        5,
        4,
        8,
        12,
        1,
        7,
        5,
        13,
        10,
        4
      ],
      "2020-08": [
        30,
        3,
        11,
        9,
        2,
        3,
        6,
        7,
        4,
        4,
        3,
        8,
        6,
        2,
        5,
        1,
        17,
        8,
        6
      ],
      "2020-09": [
        38,
        10,
        6,
        10,
        2,
        5,
        6,
        12,
        5,
        8,
        0,
        6,
        10,
        1,
        11,
        3,
        6,
        11,
        1
      ],
      "2020-10": [
        50,
        6,
        6,
        14,
        7,
        12,
        3,
        15,
        12,
        4,
        5,
        3,
        8,
        4,
        3,
        4,
        14,
        9,
        5
      ],
      "2020-11": [
        50,
        9,
        8,
        11,
        8,
        8,
        3,
        10,
        3,
        6,
        9,
        6,
        9,
        5,
        5,
        3,
        7,
        7,
        1
      ],
      "2020-12": [
        37,
        4,
        8,
        11,
        8,
        5,
        11,
        8,
        10,
        8,
        2,
        7,
        5,
        0,
        10,
        6,
        16,
        6,
        2
      ],
      "2021-01": [
        34,
        4,
        4,
        12,
        6,
        6,
        4,
        6,
        10,
        7,
        6,
        3,
        4,
        3,
        9,
        1,
        15,
        5,
        4
      ],
      "2021-02": [
        36,
        9,
        8,
        19,
        5,
        7,
        1,
        8,
        5,
        1,
        7,
        5,
        12,
        1,
        12,
        3,
        9,
        8,
        3
      ],
      "2021-03": [
        43,
        7,
        3,
        9,
        3,
        9,
        4,
        6,
        5,
        3,
        2,
        10,
        7,
        2,
        11,
        5,
        18,
        7,
        5
      ],
      "2021-04": [
        34,
        12,
        5,
        15,
        5,
        4,
        1,
        2,
        8,
        5,
        1,
        7,
        4,
        2,
        8,
        5,
        18,
        6,
        5
      ],
      "2021-05": [
        29,
        10,
        11,
        17,
        7,
        5,
        7,
        6,
        7,
        5,
        4,
        6,
        8,
        3,
        2,
        4,
        12,
        8,
        5
      ],
      "2021-06": [
        39,
        5,
        5,
        15,
        9,
        9,
        5,
        16,
        7,
        4,
        3,
        5,
        6,
        1,
        7,
        8,
        15,
        12,
        6
      ],
      "2021-07": [
        47,
        9,
        9,
        16,
        10,
        12,
        3,
        9,
        1,
        2,
        5,
        5,
        4,
        5,
        6,
        3,
        13,
        11,
        4
      ],
      "2021-08": [
        29,
        4,
        12,
        11,
        2,
        7,
        3,
        9,
        2,
        4,
        3,
        5,
        5,
        4,
        1,
        2,
        18,
        8,
        2
      ],
      "2021-09": [
        32,
        9,
        5,
        16,
        4,
        3,
        1,
        9,
        3,
        7,
        6,
        5,
        4,
        3,
        6,
        3,
        17,
        7,
        5
      ],
      "2021-10": [
        41,
        7,
        16,
        16,
        7,
        12,
        2,
        17,
        10,
        9,
        14,
        10,
        6,
        3,
        2,
        5,
        15,
        10,
        3
      ],
      "2021-11": [
        30,
        13,
        8,
        4,
        6,
        3,
        6,
        5,
        5,
        1,
        4,
        1,
        9,
        4,
        7,
        6,
        8,
        9,
        2
      ],
      "2021-12": [
        33,
        8,
        8,
        20,
        7,
        11,
        2,
        10,
        6,
        5,
        6,
        7,
        5,
        1,
        11,
        5,
        16,
        11,
        3
      ],
      "2022-01": [
        31,
        8,
        3,
        3,
        5,
        3,
        5,
        8,
        3,
        4,
        3,
        5,
        6,
        4,
        3,
        0,
        7,
        8,
        3
      ],
      "2022-02": [
        29,
        7,
        8,
        8,
        6,
        9,
        2,
        9,
        10,
        3,
        6,
        6,
        3,
        2,
        11,
        6,
        14,
        8,
        3
      ],
      "2022-03": [
        28,
        7,
        6,
        8,
        6,
        4,
        3,
        13,
        6,
        4,
        4,
        13,
        8,
        2,
        5,
        6,
        14,
        7,
        4
      ],
      "2022-04": [
        35,
        5,
        9,
        8,
        3,
        5,
        3,
        9,
        6,
        2,
        3,
        2,
        8,
        2,
        4,
        2,
        6,
        8,
        4
      ],
      "2022-05": [
        42,
        6,
        4,
        9,
        3,
        8,
        2,
        12,
        8,
        5,
        5,
        15,
        6,
        5,
        7,
        6,
        7,
        6,
        6
      ],
      "2022-06": [
        56,
        10,
        9,
        18,
        11,
        14,
        6,
        9,
        9,
        8,
        4,
        7,
        8,
        3,
        8,
        5,
        8,
        14,
        3
      ],
      "2022-07": [
        46,
        3,
        9,
        9,
        4,
        6,
        4,
        9,
        5,
        9,
        1,
        6,
        8,
        2,
        8,
        1,
        11,
        10,
        3
      ],
      "2022-08": [
        43,
        7,
        5,
        13,
        5,
        3,
        5,
        12,
        7,
        7,
        4,
        2,
        5,
        6,
        7,
        8,
        8,
        7,
        3
      ],
      "2022-09": [
        35,
        7,
        10,
        14,
        10,
        1,
        1,
        16,
        9,
        4,
        10,
        7,
        8,
        2,
        11,
        7,
        18,
        14,
        5
      ],
      "2022-10": [
        44,
        13,
        6,
        12,
        3,
        5,
        6,
        23,
        10,
        5,
        7,
        6,
        9,
        4,
        9,
        12,
        15,
        4,
        6
      ],
      "2022-11": [
        44,
        6,
        9,
        21,
        4,
        1,
        8,
        8,
        4,
        6,
        10,
        7,
        8,
        1,
        11,
        5,
        13,
        15,
        1
      ],
      "2022-12": [
        37,
        14,
        5,
        17,
        6,
        5,
        3,
        12,
        5,
        7,
        7,
        2,
        4,
        0,
        9,
        3,
        14,
        11,
        2
      ],
      "2023-01": [
        36,
        11,
        9,
        14,
        2,
        4,
        2,
        13,
        5,
        4,
        4,
        3,
        9,
        7,
        5,
        2,
        10,
        10,
        1
      ],
      "2023-02": [
        32,
        7,
        11,
        11,
        7,
        7,
        7,
        6,
        4,
        5,
        10,
        5,
        5,
        4,
        7,
        6,
        7,
        4,
        3
      ],
      "2023-03": [
        44,
        4,
        9,
        17,
        4,
        8,
        3,
        8,
        8,
        8,
        2,
        5,
        8,
        3,
        10,
        7,
        11,
        12,
        1
      ],
      "2023-04": [
        25,
        6,
        7,
        6,
        5,
        3,
        7,
        5,
        2,
        5,
        3,
        5,
        7,
        0,
        9,
        3,
        12,
        11,
        3
      ],
      "2023-05": [
        36,
        9,
        10,
        13,
        8,
        10,
        7,
        13,
        13,
        7,
        14,
        11,
        6,
        5,
        5,
        8,
        15,
        9,
        5
      ],
      "2023-06": [
        35,
        8,
        4,
        11,
        3,
        10,
        3,
        18,
        4,
        4,
        7,
        10,
        6,
        2,
        11,
        7,
        16,
        9,
        9
      ],
      "2023-07": [
        20,
        10,
        10,
        14,
        15,
        7,
        2,
        10,
        7,
        6,
        7,
        4,
        13,
        1,
        9,
        3,
        11,
        15,
        6
      ],
      "2023-08": [
        33,
        6,
        7,
        8,
        2,
        6,
        3,
        5,
        3,
        1,
        2,
        5,
        1,
        1,
        8,
        2,
        13,
        7,
        5
      ],
      "2023-09": [
        39,
        7,
        5,
        11,
        4,
        12,
        1,
        14,
        4,
        7,
        3,
        4,
        3,
        3,
        5,
        7,
        9,
        5,
        5
      ],
      "2023-10": [
        50,
        14,
        10,
        20,
        5,
        8,
        3,
        17,
        9,
        8,
        5,
        7,
        8,
        4,
        10,
        6,
        9,
        12,
        3
      ],
      "2023-11": [
        36,
        8,
        10,
        12,
        9,
        12,
        5,
        12,
        3,
        8,
        3,
        12,
        2,
        2,
        6,
        3,
        11,
        16,
        8
      ],
      "2023-12": [
        32,
        9,
        3,
        6,
        6,
        6,
        4,
        12,
        4,
        8,
        3,
        5,
        6,
        3,
        9,
        6,
        8,
        5,
        4
      ],
      "2024-01": [
        31,
        8,
        15,
        7,
        5,
        6,
        4,
        6,
        7,
        3,
        4,
        4,
        10,
        9,
        7,
        2,
        11,
        8,
        7
      ],
      "2024-02": [
        51,
        12,
        13,
        12,
        6,
        8,
        4,
        14,
        12,
        4,
        6,
        9,
        11,
        5,
        15,
        9,
        15,
        16,
        5
      ],
      "2024-03": [
        38,
        8,
        10,
        16,
        6,
        6,
        4,
        13,
        8,
        9,
        5,
        6,
        10,
        5,
        9,
        7,
        13,
        10,
        6
      ],
      "2024-04": [
        46,
        4,
        4,
        12,
        6,
        5,
        8,
        9,
        7,
        6,
        4,
        3,
        5,
        1,
        8,
        6,
        15,
        8,
        4
      ],
      "2024-05": [
        51,
        11,
        9,
        20,
        4,
        10,
        6,
        14,
        11,
        2,
        5,
        5,
        7,
        5,
        10,
        7,
        20,
        9,
        3
      ],
      "2024-06": [
        41,
        8,
        13,
        10,
        3,
        10,
        3,
        13,
        8,
        9,
        6,
        5,
        5,
        11,
        5,
        4,
        17,
        10,
        9
      ],
      "2024-07": [
        48,
        10,
        8,
        11,
        7,
        6,
        2,
        7,
        6,
        6,
        5,
        7,
        6,
        3,
        6,
        7,
        17,
        9,
        3
      ],
      "2024-08": [
        30,
        3,
        5,
        10,
        2,
        4,
        4,
        9,
        6,
        7,
        1,
        6,
        12,
        4,
        7,
        2,
        13,
        3,
        6
      ],
      "2024-09": [
        39,
        9,
        4,
        13,
        7,
        7,
        1,
        14,
        14,
        5,
        3,
        5,
        6,
        4,
        8,
        6,
        16,
        10,
        1
      ],
      "2024-10": [
        48,
        12,
        11,
        18,
        10,
        9,
        6,
        19,
        14,
        5,
        9,
        4,
        8,
        1,
        5,
        6,
        16,
        9,
        9
      ],
      "2024-11": [
        36,
        19,
        5,
        10,
        3,
        8,
        3,
        13,
        6,
        4,
        2,
        5,
        7,
        3,
        7,
        7,
        15,
        6,
        3
      ],
      "2024-12": [
        32,
        10,
        9,
        14,
        3,
        8,
        5,
        13,
        9,
        1,
        5,
        6,
        9,
        3,
        6,
        9,
        7,
        4,
        3
      ],
      "2025-01": [
        34,
        7,
        5,
        12,
        3,
        7,
        9,
        15,
        11,
        5,
        6,
        5,
        10,
        6,
        12,
        5,
        11,
        6,
        4
      ],
      "2025-02": [
        48,
        8,
        10,
        15,
        6,
        8,
        4,
        17,
        10,
        5,
        7,
        7,
        8,
        5,
        10,
        14,
        17,
        10,
        8
      ],
      "2025-03": [
        40,
        10,
        14,
        12,
        4,
        3,
        5,
        12,
        5,
        4,
        5,
        4,
        12,
        3,
        11,
        6,
        11,
        11,
        6
      ],
      "2025-04": [
        36,
        15,
        9,
        12,
        3,
        10,
        1,
        10,
        10,
        4,
        4,
        11,
        5,
        5,
        12,
        9,
        7,
        11,
        5
      ],
      "2025-05": [
        45,
        7,
        11,
        17,
        11,
        16,
        6,
        20,
        7,
        6,
        10,
        5,
        7,
        4,
        13,
        10,
        18,
        13,
        2
      ],
      "2025-06": [
        43,
        17,
        12,
        15,
        11,
        6,
        6,
        12,
        14,
        6,
        2,
        7,
        8,
        5,
        9,
        6,
        12,
        8,
        7
      ],
      "2025-07": [
        44,
        5,
        7,
        14,
        9,
        3,
        5,
        5,
        8,
        10,
        4,
        9,
        3,
        7,
        4,
        10,
        15,
        9,
        10
      ],
      "2025-08": [
        38,
        12,
        3,
        12,
        5,
        3,
        2,
        4,
        8,
        11,
        7,
        7,
        6,
        5,
        11,
        7,
        8,
        10,
        9
      ],
      "2025-09": [
        28,
        4,
        3,
        8,
        0,
        2,
        3,
        2,
        4,
        4,
        4,
        5,
        2,
        1,
        5,
        6,
        8,
        6,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "A new class of copulas having dependence range larger than FGM-type copulas",
          "year": "2023-04",
          "abstract": "We propose a new bivariate symmetric copula with positive and negative\ndependence properties. The main features of the proposed copula are its simple\nmathematical structure, wider dependence range compared to FGM copula and its\ngeneralizations, and no lower and upper tail dependence. The maximum range of\nSpearman's Rho of the proposed copula is [-0.5866,0.5866], which improves the\ndependence range of the FGM copula and its various generalizations. A new\nbivariate Rayleigh distribution is developed using the proposed copula, and\nsome statistical properties have been studied. A real data set is analyzed to\nillustrate the proposed bivariate distribution's relevance in practical\ncontexts.",
          "arxiv_id": "2304.02231v1"
        },
        {
          "title": "A new goodness of fit test for normal distribution based on Stein's characterization",
          "year": "2020-01",
          "abstract": "In this paper, we develop a simple non-parametric test for testing normal\ndistribution based on the distance between empirical zero-bias transformation\nand empirical distribution. The asymptotic properties of the test statistic are\nstudied. The finite sample performance of the proposed test is evaluated\nthrough a Monte Carlo simulation study. The power of our test is compared with\nseveral other tests for normality. We illustrate the test procedure using two\nreal data sets. We also develop a jackknife empirical likelihood ratio test for\nstandard normal distribution.",
          "arxiv_id": "2001.07932v4"
        },
        {
          "title": "Family of mean-mixtures of multivariate normal distributions: properties, inference and assessment of multivariate skewness",
          "year": "2020-06",
          "abstract": "In this paper, a new mixture family of multivariate normal distributions,\nformed by mixing multivariate normal distribution and skewed distribution, is\nconstructed. Some properties of this family, such as characteristic function,\nmoment generating function, and the first four moments are derived. The\ndistributions of affine transformations and canonical forms of the model are\nalso derived. An EM type algorithm is developed for the maximum likelihood\nestimation of model parameters. We have considered in detail, some special\ncases of the family, using standard gamma and standard exponential mixture\ndistributions, denoted by MMNG and MMNE, respectively. For the proposed family\nof distributions, different multivariate measures of skewness are computed. In\norder to examine the performance of the developed estimation method, some\nsimulation studies are carried out to show that the maximum likelihood\nestimates based on the EM type algorithm do provide good performance. For\ndifferent choices of parameters of MMNE distribution, several multivariate\nmeasures of skewness are computed and compared. Because some measures of\nskewness are scalar and some are vectors, in order to evaluate them properly,\nwe have carried out a simulation study to determine the power of tests, based\non sample versions of skewness measures as test statistics to test the fit of\nthe MMNE distribution. Finally, two real data sets are used to illustrate the\nusefulness of the proposed family of distributions and the associated\ninferential method.",
          "arxiv_id": "2006.10018v2"
        }
      ],
      "1": [
        {
          "title": "Treatment Effect Estimation with Observational Network Data using Machine Learning",
          "year": "2022-06",
          "abstract": "Causal inference methods for treatment effect estimation usually assume\nindependent units. However, this assumption is often questionable because units\nmay interact, resulting in spillover effects between them. We develop augmented\ninverse probability weighting (AIPW) for estimation and inference of the\nexpected average treatment effect (EATE) with observational data from a single\n(social) network with spillover effects. In contrast to overall effects such as\nthe global average treatment effect (GATE), the EATE measures, in expectation\nand on average over all units, how the outcome of a unit is causally affected\nby its own treatment, marginalizing over the spillover effects from other\nunits. We develop cross-fitting theory with plugin machine learning to obtain a\nsemiparametric treatment effect estimator that converges at the parametric rate\nand asymptotically follows a Gaussian distribution. The asymptotics are\ndeveloped using the dependency graph rather than the network graph, which makes\nexplicit that we allow for spillover effects beyond immediate neighbors in the\nnetwork. We apply our AIPW method to the Swiss StudentLife Study data to\ninvestigate the effect of hours spent studying on exam performance accounting\nfor the students' social network.",
          "arxiv_id": "2206.14591v4"
        },
        {
          "title": "Randomization-based Z-estimation for evaluating average and individual treatment effects",
          "year": "2024-11",
          "abstract": "Randomized experiments have been the gold standard for drawing causal\ninference. The conventional model-based approach has been one of the most\npopular ways for analyzing treatment effects from randomized experiments, which\nis often carried through inference for certain model parameters. In this paper,\nwe provide a systematic investigation of model-based analyses for treatment\neffects under the randomization-based inference framework. This framework does\nnot impose any distributional assumptions on the outcomes, covariates and their\ndependence, and utilizes only randomization as the \"reasoned basis\". We first\nderive the asymptotic theory for Z-estimation in completely randomized\nexperiments, and propose sandwich-type conservative covariance estimation. We\nthen apply the developed theory to analyze both average and individual\ntreatment effects in randomized experiments. For the average treatment effect,\nwe consider three estimation strategies: model-based, model-imputed, and\nmodel-assisted, where the first two can be sensitive to model misspecification\nor require specific ways for parameter estimation. The model-assisted approach\nis robust to arbitrary model misspecification and always provides consistent\naverage treatment effect estimation. We propose optimal ways to conduct\nmodel-assisted estimation using generally nonlinear least squares for parameter\nestimation. For the individual treatment effects, we propose to directly model\nthe relationship between individual effects and covariates, and discuss the\nmodel's identifiability, inference and interpretation allowing model\nmisspecification.",
          "arxiv_id": "2411.11737v1"
        },
        {
          "title": "The CATT SATT on the MATT: semiparametric inference for sample treatment effects on the treated",
          "year": "2024-02",
          "abstract": "We study variants of the average treatment effect on the treated with\npopulation parameters replaced by their sample counterparts. For each estimand,\nwe derive the limiting distribution with respect to a semiparametric efficient\nestimator of the population effect and provide guidance on variance estimation.\nIncluded in our analysis is the well-known sample average treatment effect on\nthe treated, for which we obtain some unexpected results. Unlike the ordinary\nsample average treatment effect, we find that the asymptotic variance for the\nsample average treatment effect on the treated is point-identified and\nconsistently estimable, but it potentially exceeds that of the population\nestimand. To address this shortcoming, we propose a modification that yields a\nnew estimand, the mixed average treatment effect on the treated, which is\nalways estimated more precisely than both the population and sample effects. We\nalso introduce a second new estimand that arises from an alternative\ninterpretation of the treatment effect on the treated with which all\nindividuals are weighted by the propensity score.",
          "arxiv_id": "2402.05844v2"
        }
      ],
      "2": [
        {
          "title": "The Power of Two Matrices in Spectral Algorithms for Community Recovery",
          "year": "2022-10",
          "abstract": "Spectral algorithms are some of the main tools in optimization and inference\nproblems on graphs. Typically, the graph is encoded as a matrix and\neigenvectors and eigenvalues of the matrix are then used to solve the given\ngraph problem. Spectral algorithms have been successfully used for graph\npartitioning, hidden clique recovery and graph coloring. In this paper, we\nstudy the power of spectral algorithms using two matrices in a graph\npartitioning problem. We use two different matrices resulting from two\ndifferent encodings of the same graph and then combine the spectral information\ncoming from these two matrices.\n  We analyze a two-matrix spectral algorithm for the problem of identifying\nlatent community structure in large random graphs. In particular, we consider\nthe problem of recovering community assignments exactly in the censored\nstochastic block model, where each edge status is revealed independently with\nsome probability. We show that spectral algorithms based on two matrices are\noptimal and succeed in recovering communities up to the information theoretic\nthreshold. Further, we show that for most choices of the parameters, any\nspectral algorithm based on one matrix is suboptimal. The latter observation is\nin contrast to our prior works (2022a, 2022b) which showed that for the\nsymmetric Stochastic Block Model and the Planted Dense Subgraph problem, a\nspectral algorithm based on one matrix achieves the information theoretic\nthreshold. We additionally provide more general geometric conditions for the\n(sub)-optimality of spectral algorithms.",
          "arxiv_id": "2210.05893v3"
        },
        {
          "title": "Exact Community Recovery in Correlated Stochastic Block Models",
          "year": "2022-03",
          "abstract": "We consider the problem of learning latent community structure from multiple\ncorrelated networks. We study edge-correlated stochastic block models with two\nbalanced communities, focusing on the regime where the average degree is\nlogarithmic in the number of vertices. Our main result derives the precise\ninformation-theoretic threshold for exact community recovery using multiple\ncorrelated graphs. This threshold captures the interplay between the community\nrecovery and graph matching tasks. In particular, we uncover and characterize a\nregion of the parameter space where exact community recovery is possible using\nmultiple correlated graphs, even though (1) this is information-theoretically\nimpossible using a single graph and (2) exact graph matching is also\ninformation-theoretically impossible. In this regime, we develop a novel\nalgorithm that carefully synthesizes algorithms from the community recovery and\ngraph matching literatures.",
          "arxiv_id": "2203.15736v1"
        },
        {
          "title": "Weak recovery, hypothesis testing, and mutual information in stochastic block models and planted factor graphs",
          "year": "2024-06",
          "abstract": "The stochastic block model is a canonical model of communities in random\ngraphs. It was introduced in the social sciences and statistics as a model of\ncommunities, and in theoretical computer science as an average case model for\ngraph partitioning problems under the name of the ``planted partition model.''\nGiven a sparse stochastic block model, the two standard inference tasks are:\n(i) Weak recovery: can we estimate the communities with non trivial overlap\nwith the true communities? (ii) Detection/Hypothesis testing: can we\ndistinguish if the sample was drawn from the block model or from a random graph\nwith no community structure with probability tending to $1$ as the graph size\ntends to infinity?\n  In this work, we show that for sparse stochastic block models, the two\ninference tasks are equivalent except at a critical point. That is, weak\nrecovery is information theoretically possible if and only if detection is\npossible. We thus find a strong connection between these two notions of\ninference for the model. We further prove that when detection is impossible, an\nexplicit hypothesis test based on low degree polynomials in the adjacency\nmatrix of the observed graph achieves the optimal statistical power. This low\ndegree test is efficient as opposed to the likelihood ratio test, which is not\nknown to be efficient. Moreover, we prove that the asymptotic mutual\ninformation between the observed network and the community structure exhibits a\nphase transition at the weak recovery threshold.\n  Our results are proven in much broader settings including the hypergraph\nstochastic block models and general planted factor graphs. In these settings we\nprove that the impossibility of weak recovery implies contiguity and provide a\ncondition which guarantees the equivalence of weak recovery and detection.",
          "arxiv_id": "2406.15957v2"
        }
      ],
      "3": [
        {
          "title": "Adaptive Matrix Change Point Detection: Leveraging Structured Mean Shifts",
          "year": "2024-01",
          "abstract": "In high-dimensional time series, the component processes are often assembled\ninto a matrix to display their interrelationship. We focus on detecting mean\nshifts with unknown change point locations in these matrix time series. Series\nthat are activated by a change may cluster along certain rows (columns), which\nforms mode-specific change point alignment. Leveraging mode-specific change\npoint alignments may substantially enhance the power for change point\ndetection. Yet, there may be no mode-specific alignments in the change point\nstructure. We propose a powerful test to detect mode-specific change points,\nyet robust to non-mode-specific changes. We show the validity of using the\nmultiplier bootstrap to compute the p-value of the proposed methods, and derive\nnon-asymptotic bounds on the size and power of the tests. We also propose a\nparallel bootstrap, a computationally efficient approach for computing the\np-value of the proposed adaptive test. In particular, we show the consistency\nof the proposed test, under mild regularity conditions. To obtain the\ntheoretical results, we derive new, sharp bounds on Gaussian approximation and\nmultiplier bootstrap approximation, which are of independent interest for high\ndimensional problems with diverging sparsity.",
          "arxiv_id": "2401.17473v2"
        },
        {
          "title": "Optimal Change-Point Detection and Localization",
          "year": "2020-10",
          "abstract": "Given a times series ${\\bf Y}$ in $\\mathbb{R}^n$, with a piece-wise contant\nmean and independent components, the twin problems of change-point detection\nand change-point localization respectively amount to detecting the existence of\ntimes where the mean varies and estimating the positions of those\nchange-points. In this work, we tightly characterize optimal rates for both\nproblems and uncover the phase transition phenomenon from a global testing\nproblem to a local estimation problem. Introducing a suitable definition of the\nenergy of a change-point, we first establish in the single change-point setting\nthat the optimal detection threshold is $\\sqrt{2\\log\\log(n)}$. When the energy\nis just above the detection threshold, then the problem of localizing the\nchange-point becomes purely parametric: it only depends on the difference in\nmeans and not on the position of the change-point anymore. Interestingly, for\nmost change-point positions, it is possible to detect and localize them at a\nmuch smaller energy level. In the multiple change-point setting, we establish\nthe energy detection threshold and show similarly that the optimal localization\nerror of a specific change-point becomes purely parametric. Along the way,\ntight optimal rates for Hausdorff and $l_1$ estimation losses of the vector of\nall change-points positions are also established. Two procedures achieving\nthese optimal rates are introduced. The first one is a least-squares estimator\nwith a new multiscale penalty that favours well spread change-points. The\nsecond one is a two-step multiscale post-processing procedure whose\ncomputational complexity can be as low as $O(n\\log(n))$. Notably, these two\nprocedures accommodate with the presence of possibly many low-energy and\ntherefore undetectable change-points and are still able to detect and localize\nhigh-energy change-points even with the presence of those nuisance parameters.",
          "arxiv_id": "2010.11470v2"
        },
        {
          "title": "An Encoding Approach for Stable Change Point Detection",
          "year": "2021-05",
          "abstract": "Without imposing prior distributional knowledge underlying multivariate time\nseries of interest, we propose a nonparametric change-point detection approach\nto estimate the number of change points and their locations along the temporal\naxis. We develop a structural subsampling procedure such that the observations\nare encoded into multiple sequences of Bernoulli variables. A maximum\nlikelihood approach in conjunction with a newly developed searching algorithm\nis implemented to detect change points on each Bernoulli process separately.\nThen, aggregation statistics are proposed to collectively synthesize\nchange-point results from all individual univariate time series into consistent\nand stable location estimations. We also study a weighting strategy to measure\nthe degree of relevance for different subsampled groups. Simulation studies are\nconducted and shown that the proposed change-point methodology for multivariate\ntime series has favorable performance comparing with currently popular\nnonparametric methods under various settings with different degrees of\ncomplexity. Real data analyses are finally performed on categorical, ordinal,\nand continuous time series taken from fields of genetics, climate, and finance.",
          "arxiv_id": "2105.05341v1"
        }
      ],
      "4": [
        {
          "title": "Minimax Rates of Estimation for Optimal Transport Map between Infinite-Dimensional Spaces",
          "year": "2025-05",
          "abstract": "We investigate the estimation of an optimal transport map between probability\nmeasures on an infinite-dimensional space and reveal its minimax optimal rate.\nOptimal transport theory defines distances within a space of probability\nmeasures, utilizing an optimal transport map as its key component. Estimating\nthe optimal transport map from samples finds several applications, such as\nsimulating dynamics between probability measures and functional data analysis.\nHowever, some transport maps on infinite-dimensional spaces require\nexponential-order data for estimation, which undermines their applicability. In\nthis paper, we investigate the estimation of an optimal transport map between\ninfinite-dimensional spaces, focusing on optimal transport maps characterized\nby the notion of $\\gamma$-smoothness. Consequently, we show that the order of\nthe minimax risk is polynomial rate in the sample size even in the\ninfinite-dimensional setup. We also develop an estimator whose estimation error\nmatches the minimax optimal rate. With these results, we obtain a class of\nreasonably estimable optimal transport maps on infinite-dimensional spaces and\na method for their estimation. Our experiments validate the theory and\npractical utility of our approach with application to functional data analysis.",
          "arxiv_id": "2505.13570v2"
        },
        {
          "title": "Plugin Estimation of Smooth Optimal Transport Maps",
          "year": "2021-07",
          "abstract": "We analyze a number of natural estimators for the optimal transport map\nbetween two distributions and show that they are minimax optimal. We adopt the\nplugin approach: our estimators are simply optimal couplings between measures\nderived from our observations, appropriately extended so that they define\nfunctions on $\\mathbb{R}^d$. When the underlying map is assumed to be\nLipschitz, we show that computing the optimal coupling between the empirical\nmeasures, and extending it using linear smoothers, already gives a minimax\noptimal estimator. When the underlying map enjoys higher regularity, we show\nthat the optimal coupling between appropriate nonparametric density estimates\nyields faster rates. Our work also provides new bounds on the risk of\ncorresponding plugin estimators for the quadratic Wasserstein distance, and we\nshow how this problem relates to that of estimating optimal transport maps\nusing stability arguments for smooth and strongly convex Brenier potentials. As\nan application of our results, we derive central limit theorems for plugin\nestimators of the squared Wasserstein distance, which are centered at their\npopulation counterpart when the underlying distributions have sufficiently\nsmooth densities. In contrast to known central limit theorems for empirical\nestimators, this result easily lends itself to statistical inference for the\nquadratic Wasserstein distance.",
          "arxiv_id": "2107.12364v3"
        },
        {
          "title": "Gromov-Wasserstein Distances: Entropic Regularization, Duality, and Sample Complexity",
          "year": "2022-12",
          "abstract": "The Gromov-Wasserstein (GW) distance, rooted in optimal transport (OT)\ntheory, quantifies dissimilarity between metric measure spaces and provides a\nframework for aligning heterogeneous datasets. While computational aspects of\nthe GW problem have been widely studied, a duality theory and fundamental\nstatistical questions concerning empirical convergence rates remained obscure.\nThis work closes these gaps for the quadratic GW distance over Euclidean spaces\nof different dimensions $d_x$ and $d_y$. We treat both the standard and the\nentropically regularized GW distance, and derive dual forms that represent them\nin terms of the well-understood OT and entropic OT (EOT) problems,\nrespectively. This enables employing proof techniques from statistical OT based\non regularity analysis of dual potentials and empirical process theory, using\nwhich we establish the first GW empirical convergence rates. The derived\ntwo-sample rates are $n^{-2/\\max\\{\\min\\{d_x,d_y\\},4\\}}$ (up to a log factor\nwhen $\\min\\{d_x,d_y\\}=4$) for standard GW and $n^{-1/2}$ for EGW, which matches\nthe corresponding rates for standard and entropic OT. The parametric rate for\nEGW is evidently optimal, while for standard GW we provide matching lower\nbounds, which establish sharpness of the derived rates. We also study stability\nof EGW in the entropic regularization parameter and prove approximation and\ncontinuity results for the cost and optimal couplings. Lastly, the duality is\nleveraged to shed new light on the open problem of the one-dimensional GW\ndistance between uniform distributions on $n$ points, illuminating why the\nidentity and anti-identity permutations may not be optimal. Our results serve\nas a first step towards a comprehensive statistical theory as well as\ncomputational advancements for GW distances, based on the discovered dual\nformulations.",
          "arxiv_id": "2212.12848v3"
        }
      ],
      "5": [
        {
          "title": "A Near Complete Nonasymptotic Generalization Theory For Multilayer Neural Networks: Beyond the Bias-Variance Tradeoff",
          "year": "2025-03",
          "abstract": "We propose a first near complete (that will make explicit sense in the main\ntext) nonasymptotic generalization theory for multilayer neural networks with\narbitrary Lipschitz activations and general Lipschitz loss functions (with some\nvery mild conditions). In particular, it doens't require the boundness of loss\nfunction, as commonly assumed in the literature. Our theory goes beyond the\nbias-variance tradeoff, aligned with phenomenon typically encountered in deep\nlearning. It is therefore sharp different with other existing nonasymptotic\ngeneralization error bounds for neural networks. More explicitly, we propose an\nexplicit generalization error upper bound for multilayer neural networks with\narbitrary Lipschitz activations $\\sigma$ with $\\sigma(0)=0$ and broad enough\nLipschitz loss functions, without requiring either the width, depth or other\nhyperparameters of the neural network approaching infinity, a specific neural\nnetwork architect (e.g. sparsity, boundness of some norms), a particular\nactivation function, a particular optimization algorithm or boundness of the\nloss function, and with taking the approximation error into consideration.\nGeneral Lipschitz activation can also be accommodated into our framework. A\nfeature of our theory is that it also considers approximation errors.\nFurthermore, we show the near minimax optimality of our theory for multilayer\nReLU networks for regression problems. Notably, our upper bound exhibits the\nfamous double descent phenomenon for such networks, which is the most\ndistinguished characteristic compared with other existing results. This work\nemphasizes a view that many classical results should be improved to embrace the\nunintuitive characteristics of deep learning to get a better understanding of\nit.",
          "arxiv_id": "2503.02129v1"
        },
        {
          "title": "Mathematical Models of Overparameterized Neural Networks",
          "year": "2020-12",
          "abstract": "Deep learning has received considerable empirical successes in recent years.\nHowever, while many ad hoc tricks have been discovered by practitioners, until\nrecently, there has been a lack of theoretical understanding for tricks\ninvented in the deep learning literature. Known by practitioners that\noverparameterized neural networks are easy to learn, in the past few years\nthere have been important theoretical developments in the analysis of\noverparameterized neural networks. In particular, it was shown that such\nsystems behave like convex systems under various restricted settings, such as\nfor two-layer NNs, and when learning is restricted locally in the so-called\nneural tangent kernel space around specialized initializations. This paper\ndiscusses some of these recent progresses leading to significant better\nunderstanding of neural networks. We will focus on the analysis of two-layer\nneural networks, and explain the key mathematical models, with their\nalgorithmic implications. We will then discuss challenges in understanding deep\nneural networks and some current research directions.",
          "arxiv_id": "2012.13982v1"
        },
        {
          "title": "Deep Nonparametric Regression on Approximate Manifolds: Non-Asymptotic Error Bounds with Polynomial Prefactors",
          "year": "2021-04",
          "abstract": "We study the properties of nonparametric least squares regression using deep\nneural networks. We derive non-asymptotic upper bounds for the prediction error\nof the empirical risk minimizer of feedforward deep neural regression. Our\nerror bounds achieve minimax optimal rate and significantly improve over the\nexisting ones in the sense that they depend polynomially on the dimension of\nthe predictor, instead of exponentially on dimension. We show that the neural\nregression estimator can circumvent the curse of dimensionality under the\nassumption that the predictor is supported on an approximate low-dimensional\nmanifold or a set with low Minkowski dimension. We also establish the optimal\nconvergence rate under the exact manifold support assumption. We investigate\nhow the prediction error of the neural regression estimator depends on the\nstructure of neural networks and propose a notion of network relative\nefficiency between two types of neural networks, which provides a quantitative\nmeasure for evaluating the relative merits of different network structures. To\nestablish these results, we derive a novel approximation error bound for the\nH\\\"older smooth functions with a positive smoothness index using ReLU activated\nneural networks, which may be of independent interest. Our results are derived\nunder weaker assumptions on the data distribution and the neural network\nstructure than those in the existing literature.",
          "arxiv_id": "2104.06708v6"
        }
      ],
      "6": [
        {
          "title": "Low solution rank of the matrix LASSO under RIP with consequences for rank-constrained algorithms",
          "year": "2024-04",
          "abstract": "We show that solutions to the popular convex matrix LASSO problem\n(nuclear-norm--penalized linear least-squares) have low rank under similar\nassumptions as required by classical low-rank matrix sensing error bounds.\nAlthough the purpose of the nuclear norm penalty is to promote low solution\nrank, a proof has not yet (to our knowledge) been provided outside very\nspecific circumstances. Furthermore, we show that this result has significant\ntheoretical consequences for nonconvex rank-constrained optimization\napproaches. Specifically, we show that if (a) the ground truth matrix has low\nrank, (b) the (linear) measurement operator has the matrix restricted isometry\nproperty (RIP), and (c) the measurement error is small enough relative to the\nnuclear norm penalty, then the (unique) LASSO solution has rank (approximately)\nbounded by that of the ground truth. From this, we show (a) that a\nlow-rank--projected proximal gradient descent algorithm will converge linearly\nto the LASSO solution from any initialization, and (b) that the nonconvex\nlandscape of the low-rank Burer-Monteiro--factored problem formulation is\nbenign in the sense that all second-order critical points are globally optimal\nand yield the LASSO solution.",
          "arxiv_id": "2404.12828v2"
        },
        {
          "title": "A non-backtracking method for long matrix and tensor completion",
          "year": "2023-04",
          "abstract": "We consider the problem of low-rank rectangular matrix completion in the\nregime where the matrix $M$ of size $n\\times m$ is ``long\", i.e., the aspect\nratio $m/n$ diverges to infinity. Such matrices are of particular interest in\nthe study of tensor completion, where they arise from the unfolding of a\nlow-rank tensor. In the case where the sampling probability is\n$\\frac{d}{\\sqrt{mn}}$, we propose a new spectral algorithm for recovering the\nsingular values and left singular vectors of the original matrix $M$ based on a\nvariant of the standard non-backtracking operator of a suitably defined\nbipartite weighted random graph, which we call a \\textit{non-backtracking wedge\noperator}. When $d$ is above a Kesten-Stigum-type sampling threshold, our\nalgorithm recovers a correlated version of the singular value decomposition of\n$M$ with quantifiable error bounds. This is the first result in the regime of\nbounded $d$ for weak recovery and the first for weak consistency when\n$d\\to\\infty$ arbitrarily slowly without any polylog factors. As an application,\nfor low-CP-rank orthogonal $k$-tensor completion, we efficiently achieve weak\nrecovery with sample size $O(n^{k/2})$ and weak consistency with sample size\n$\\omega(n^{k/2})$. A similar result is obtained for low-multilinear-rank tensor\ncompletion with $O(n^{k/2})$ many samples.",
          "arxiv_id": "2304.02077v5"
        },
        {
          "title": "Approximately low-rank recovery from noisy and local measurements by convex program",
          "year": "2021-10",
          "abstract": "Low-rank matrix models have been universally useful for numerous\napplications, from classical system identification to more modern matrix\ncompletion in signal processing and statistics. The nuclear norm has been\nemployed as a convex surrogate of the low-rankness since it induces a low-rank\nsolution to inverse problems. While the nuclear norm for low rankness has an\nexcellent analogy with the $\\ell_1$ norm for sparsity through the singular\nvalue decomposition, other matrix norms also induce low-rankness. Particularly\nas one interprets a matrix as a linear operator between Banach spaces, various\ntensor product norms generalize the role of the nuclear norm. We provide a\ntensor-norm-constrained estimator for the recovery of approximately low-rank\nmatrices from local measurements corrupted with noise. A tensor-norm\nregularizer is designed to adapt to the local structure. We derive statistical\nanalysis of the estimator over matrix completion and decentralized sketching by\napplying Maurey's empirical method to tensor products of Banach spaces. The\nestimator provides a near-optimal error bound in a minimax sense and admits a\npolynomial-time algorithm for these applications.",
          "arxiv_id": "2110.15205v2"
        }
      ],
      "7": [
        {
          "title": "Scaled minimax optimality in high-dimensional linear regression: A non-convex algorithmic regularization approach",
          "year": "2020-08",
          "abstract": "The question of fast convergence in the classical problem of high dimensional\nlinear regression has been extensively studied. Arguably, one of the fastest\nprocedures in practice is Iterative Hard Thresholding (IHT). Still, IHT relies\nstrongly on the knowledge of the true sparsity parameter $s$. In this paper, we\npresent a novel fast procedure for estimation in the high dimensional linear\nregression. Taking advantage of the interplay between estimation, support\nrecovery and optimization we achieve both optimal statistical accuracy and fast\nconvergence. The main advantage of our procedure is that it is fully adaptive,\nmaking it more practical than state of the art IHT methods. Our procedure\nachieves optimal statistical accuracy faster than, for instance, classical\nalgorithms for the Lasso. Moreover, we establish sharp optimal results for both\nestimation and support recovery. As a consequence, we present a new iterative\nhard thresholding algorithm for high dimensional linear regression that is\nscaled minimax optimal (achieves the estimation error of the oracle that knows\nthe sparsity pattern if possible), fast and adaptive.",
          "arxiv_id": "2008.12236v1"
        },
        {
          "title": "Empirical Bayes inference in sparse high-dimensional generalized linear models",
          "year": "2023-03",
          "abstract": "High-dimensional linear models have been widely studied, but the developments\nin high-dimensional generalized linear models, or GLMs, have been slower. In\nthis paper, we propose an empirical or data-driven prior leading to an\nempirical Bayes posterior distribution which can be used for estimation of and\ninference on the coefficient vector in a high-dimensional GLM, as well as for\nvariable selection. We prove that our proposed posterior concentrates around\nthe true/sparse coefficient vector at the optimal rate, provide conditions\nunder which the posterior can achieve variable selection consistency, and prove\na Bernstein--von Mises theorem that implies asymptotically valid uncertainty\nquantification. Computation of the proposed empirical Bayes posterior is simple\nand efficient, and is shown to perform well in simulations compared to existing\nBayesian and non-Bayesian methods in terms of estimation and variable\nselection.",
          "arxiv_id": "2303.07854v2"
        },
        {
          "title": "Canonical thresholding for non-sparse high-dimensional linear regression",
          "year": "2020-07",
          "abstract": "We consider a high-dimensional linear regression problem. Unlike many papers\non the topic, we do not require sparsity of the regression coefficients;\ninstead, our main structural assumption is a decay of eigenvalues of the\ncovariance matrix of the data. We propose a new family of estimators, called\nthe canonical thresholding estimators, which pick largest regression\ncoefficients in the canonical form. The estimators admit an explicit form and\ncan be linked to LASSO and Principal Component Regression (PCR). A theoretical\nanalysis for both fixed design and random design settings is provided. Obtained\nbounds on the mean squared error and the prediction error of a specific\nestimator from the family allow to clearly state sufficient conditions on the\ndecay of eigenvalues to ensure convergence. In addition, we promote the use of\nthe relative errors, strongly linked with the out-of-sample $R^2$. The study of\nthese relative errors leads to a new concept of joint effective dimension,\nwhich incorporates the covariance of the data and the regression coefficients\nsimultaneously, and describes the complexity of a linear regression problem.\nSome minimax lower bounds are established to showcase the optimality of our\nprocedure. Numerical simulations confirm good performance of the proposed\nestimators compared to the previously developed methods.",
          "arxiv_id": "2007.12313v2"
        }
      ],
      "8": [
        {
          "title": "Improved Discretization Analysis for Underdamped Langevin Monte Carlo",
          "year": "2023-02",
          "abstract": "Underdamped Langevin Monte Carlo (ULMC) is an algorithm used to sample from\nunnormalized densities by leveraging the momentum of a particle moving in a\npotential well. We provide a novel analysis of ULMC, motivated by two central\nquestions: (1) Can we obtain improved sampling guarantees beyond strong\nlog-concavity? (2) Can we achieve acceleration for sampling?\n  For (1), prior results for ULMC only hold under a log-Sobolev inequality\ntogether with a restrictive Hessian smoothness condition. Here, we relax these\nassumptions by removing the Hessian smoothness condition and by considering\ndistributions satisfying a Poincar\\'e inequality. Our analysis achieves the\nstate of art dimension dependence, and is also flexible enough to handle weakly\nsmooth potentials. As a byproduct, we also obtain the first KL divergence\nguarantees for ULMC without Hessian smoothness under strong log-concavity,\nwhich is based on a new result on the log-Sobolev constant along the\nunderdamped Langevin diffusion.\n  For (2), the recent breakthrough of Cao, Lu, and Wang (2020) established the\nfirst accelerated result for sampling in continuous time via PDE methods. Our\ndiscretization analysis translates their result into an algorithmic guarantee,\nwhich indeed enjoys better condition number dependence than prior works on\nULMC, although we leave open the question of full acceleration in discrete\ntime.\n  Both (1) and (2) necessitate R\\'enyi discretization bounds, which are more\nchallenging than the typically used Wasserstein coupling arguments. We address\nthis using a flexible discretization analysis based on Girsanov's theorem that\neasily extends to more general settings.",
          "arxiv_id": "2302.08049v1"
        },
        {
          "title": "High-accuracy sampling from constrained spaces with the Metropolis-adjusted Preconditioned Langevin Algorithm",
          "year": "2024-12",
          "abstract": "In this work, we propose a first-order sampling method called the\nMetropolis-adjusted Preconditioned Langevin Algorithm for approximate sampling\nfrom a target distribution whose support is a proper convex subset of\n$\\mathbb{R}^{d}$. Our proposed method is the result of applying a\nMetropolis-Hastings filter to the Markov chain formed by a single step of the\npreconditioned Langevin algorithm with a metric $\\mathscr{G}$, and is motivated\nby the natural gradient descent algorithm for optimisation. We derive\nnon-asymptotic upper bounds for the mixing time of this method for sampling\nfrom target distributions whose potentials are bounded relative to\n$\\mathscr{G}$, and for exponential distributions restricted to the support. Our\nanalysis suggests that if $\\mathscr{G}$ satisfies stronger notions of\nself-concordance introduced in Kook and Vempala (2024), then these mixing time\nupper bounds have a strictly better dependence on the dimension than when is\nmerely self-concordant. We also provide numerical experiments that demonstrates\nthe practicality of our proposed method. Our method is a high-accuracy sampler\ndue to the polylogarithmic dependence on the error tolerance in our mixing time\nupper bounds.",
          "arxiv_id": "2412.18701v3"
        },
        {
          "title": "Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling",
          "year": "2024-07",
          "abstract": "We consider the outstanding problem of sampling from an unnormalized density\nthat may be non-log-concave and multimodal. To enhance the performance of\nsimple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type\nhave been widely used. However, quantitative theoretical guarantees of these\ntechniques are under-explored. This study takes a first step toward providing a\nnon-asymptotic analysis of annealed MCMC. Specifically, we establish, for the\nfirst time, an oracle complexity of $\\widetilde{O}\\left(\\frac{d\\beta^2{\\cal\nA}^2}{\\varepsilon^6}\\right)$ for the simple annealed Langevin Monte Carlo\nalgorithm to achieve $\\varepsilon^2$ accuracy in Kullback-Leibler divergence to\nthe target distribution $\\pi\\propto{\\rm e}^{-V}$ on $\\mathbb{R}^d$ with\n$\\beta$-smooth potential $V$. Here, ${\\cal A}$ represents the action of a curve\nof probability measures interpolating the target distribution $\\pi$ and a\nreadily sampleable distribution.",
          "arxiv_id": "2407.16936v2"
        }
      ],
      "9": [
        {
          "title": "Smoothness Estimation for Whittle-Matérn Processes on Closed Riemannian Manifolds",
          "year": "2023-12",
          "abstract": "The family of Mat\\'ern kernels are often used in spatial statistics, function\napproximation and Gaussian process methods in machine learning. One reason for\ntheir popularity is the presence of a smoothness parameter that controls, for\nexample, optimal error bounds for kriging and posterior contraction rates in\nGaussian process regression. On closed Riemannian manifolds, we show that the\nsmoothness parameter can be consistently estimated from the maximizer(s) of the\nGaussian likelihood when the underlying data are from point evaluations of a\nGaussian process and, perhaps surprisingly, even when the data comprise\nevaluations of a non-Gaussian process. The points at which the process is\nobserved need not have any particular spatial structure beyond\nquasi-uniformity. Our methods are based on results from approximation theory\nfor the Sobolev scale of Hilbert spaces. Moreover, we generalize a well-known\nequivalence of measures phenomenon related to Mat\\'ern kernels to the\nnon-Gaussian case by using Kakutani's theorem.",
          "arxiv_id": "2401.00510v3"
        },
        {
          "title": "Posterior Concentration for Gaussian Process Priors under Rescaled and Hierarchical Matérn and Confluent Hypergeometric Covariance Functions",
          "year": "2023-12",
          "abstract": "In nonparameteric Bayesian approaches, Gaussian stochastic processes can\nserve as priors on real-valued function spaces. Existing literature on the\nposterior convergence rates under Gaussian process priors shows that it is\npossible to achieve optimal or near-optimal posterior contraction rates if the\nsmoothness of the Gaussian process matches that of the target function. Among\nthose priors, Gaussian processes with a parametric Mat\\'ern covariance function\nis particularly notable in that its degree of smoothness can be determined by a\ndedicated smoothness parameter. \\citet{ma2022beyond} recently introduced a new\nfamily of covariance functions called the Confluent Hypergeometric (CH) class\nthat simultaneously possess two parameters: one controls the tail index of the\npolynomially decaying covariance function, and the other parameter controls the\ndegree of mean-squared smoothness analogous to the Mat\\'ern class. In this\npaper, we show that with proper choice of rescaling parameters in the Mat\\'ern\nand CH covariance functions, it is possible to obtain the minimax optimal\nposterior contraction rate for $\\eta$-regular functions for nonparametric\nregression model with fixed design. Unlike the previous results for unrescaled\ncases, the smoothness parameter of the covariance function need not equal\n$\\eta$ for achieving the optimal minimax rate, for either rescaled Mat\\'ern or\nrescaled CH covariances, illustrating a key benefit for rescaling. We also\nconsider a fully Bayesian treatment of the rescaling parameters and show the\nresulting posterior distributions still contract at the minimax-optimal rate.\nThe resultant hierarchical Bayesian procedure is fully adaptive to the unknown\ntrue smoothness.",
          "arxiv_id": "2312.07502v3"
        },
        {
          "title": "Convergence of Gaussian process regression: Optimality, robustness, and relationship with kernel ridge regression",
          "year": "2021-04",
          "abstract": "In this work, we investigate Gaussian process regression used to recover a\nfunction based on noisy observations. We derive upper and lower error bounds\nfor Gaussian process regression with possibly misspecified correlation\nfunctions. The optimal convergence rate can be attained even if the smoothness\nof the imposed correlation function exceeds that of the true correlation\nfunction and the sampling scheme is quasi-uniform. As byproducts, we also\nobtain convergence rates of kernel ridge regression with misspecified kernel\nfunction, where the underlying truth is a deterministic function. The\nconvergence rates of Gaussian process regression and kernel ridge regression\nare closely connected, which is aligned with the relationship between sample\npaths of Gaussian process and the corresponding reproducing kernel Hilbert\nspace.",
          "arxiv_id": "2104.09778v2"
        }
      ],
      "10": [
        {
          "title": "Robust Batch Policy Learning in Markov Decision Processes",
          "year": "2020-11",
          "abstract": "We study the offline data-driven sequential decision making problem in the\nframework of Markov decision process (MDP). In order to enhance the\ngeneralizability and adaptivity of the learned policy, we propose to evaluate\neach policy by a set of the average rewards with respect to distributions\ncentered at the policy induced stationary distribution. Given a pre-collected\ndataset of multiple trajectories generated by some behavior policy, our goal is\nto learn a robust policy in a pre-specified policy class that can maximize the\nsmallest value of this set. Leveraging the theory of semi-parametric\nstatistics, we develop a statistically efficient policy learning method for\nestimating the de ned robust optimal policy. A rate-optimal regret bound up to\na logarithmic factor is established in terms of total decision points in the\ndataset.",
          "arxiv_id": "2011.04185v4"
        },
        {
          "title": "A Simple and Optimal Policy Design with Safety against Heavy-Tailed Risk for Stochastic Bandits",
          "year": "2022-06",
          "abstract": "We study the stochastic multi-armed bandit problem and design new policies\nthat enjoy both worst-case optimality for expected regret and light-tailed risk\nfor regret distribution. Specifically, our policy design (i) enjoys the\nworst-case optimality for the expected regret at order $O(\\sqrt{KT\\ln T})$ and\n(ii) has the worst-case tail probability of incurring a regret larger than any\n$x>0$ being upper bounded by $\\exp(-\\Omega(x/\\sqrt{KT}))$, a rate that we prove\nto be best achievable with respect to $T$ for all worst-case optimal policies.\nOur proposed policy achieves a delicate balance between doing more exploration\nat the beginning of the time horizon and doing more exploitation when\napproaching the end, compared to standard confidence-bound-based policies. We\nalso enhance the policy design to accommodate the \"any-time\" setting where $T$\nis unknown a priori, and prove equivalently desired policy performances as\ncompared to the \"fixed-time\" setting with known $T$. Numerical experiments are\nconducted to illustrate the theoretical findings. We find that from a\nmanagerial perspective, our new policy design yields better tail distributions\nand is preferable than celebrated policies especially when (i) there is a risk\nof under-estimating the volatility profile, or (ii) there is a challenge of\ntuning policy hyper-parameters. We conclude by extending our proposed policy\ndesign to the stochastic linear bandit setting that leads to both worst-case\noptimality in terms of expected regret and light-tailed risk on the regret\ndistribution.",
          "arxiv_id": "2206.02969v6"
        },
        {
          "title": "Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective",
          "year": "2020-10",
          "abstract": "In the classical multi-armed bandit problem, instance-dependent algorithms\nattain improved performance on \"easy\" problems with a gap between the best and\nsecond-best arm. Are similar guarantees possible for contextual bandits? While\npositive results are known for certain special cases, there is no general\ntheory characterizing when and how instance-dependent regret bounds for\ncontextual bandits can be achieved for rich, general classes of policies. We\nintroduce a family of complexity measures that are both sufficient and\nnecessary to obtain instance-dependent regret bounds. We then introduce new\noracle-efficient algorithms which adapt to the gap whenever possible, while\nalso attaining the minimax rate in the worst case. Finally, we provide\nstructural results that tie together a number of complexity measures previously\nproposed throughout contextual bandits, reinforcement learning, and active\nlearning and elucidate their role in determining the optimal instance-dependent\nregret. In a large-scale empirical evaluation, we find that our approach often\ngives superior results for challenging exploration problems.\n  Turning our focus to reinforcement learning with function approximation, we\ndevelop new oracle-efficient algorithms for reinforcement learning with rich\nobservations that obtain optimal gap-dependent sample complexity.",
          "arxiv_id": "2010.03104v1"
        }
      ],
      "11": [
        {
          "title": "Clustering consistency with Dirichlet process mixtures",
          "year": "2022-05",
          "abstract": "Dirichlet process mixtures are flexible non-parametric models, particularly\nsuited to density estimation and probabilistic clustering. In this work we\nstudy the posterior distribution induced by Dirichlet process mixtures as the\nsample size increases, and more specifically focus on consistency for the\nunknown number of clusters when the observed data are generated from a finite\nmixture. Crucially, we consider the situation where a prior is placed on the\nconcentration parameter of the underlying Dirichlet process. Previous findings\nin the literature suggest that Dirichlet process mixtures are typically not\nconsistent for the number of clusters if the concentration parameter is held\nfixed and data come from a finite mixture. Here we show that consistency for\nthe number of clusters can be achieved if the concentration parameter is\nadapted in a fully Bayesian way, as commonly done in practice. Our results are\nderived for data coming from a class of finite mixtures, with mild assumptions\non the prior for the concentration parameter and for a variety of choices of\nlikelihood kernels for the mixture.",
          "arxiv_id": "2205.12924v1"
        },
        {
          "title": "Optimal Bayesian estimation of Gaussian mixtures with growing number of components",
          "year": "2020-07",
          "abstract": "We study Bayesian estimation of finite mixture models in a general setup\nwhere the number of components is unknown and allowed to grow with the sample\nsize. An assumption on growing number of components is a natural one as the\ndegree of heterogeneity present in the sample can grow and new components can\narise as sample size increases, allowing full flexibility in modeling the\ncomplexity of data. This however will lead to a high-dimensional model which\nposes great challenges for estimation. We novelly employ the idea of a sample\nsize dependent prior in a Bayesian model and establish a number of important\ntheoretical results. We first show that under mild conditions on the prior, the\nposterior distribution concentrates around the true mixing distribution at a\nnear optimal rate with respect to the Wasserstein distance. Under a separation\ncondition on the true mixing distribution, we further show that a better and\nadaptive convergence rate can be achieved, and the number of components can be\nconsistently estimated. Furthermore, we derive optimal convergence rates for\nthe higher-order mixture models where the number of components diverges\narbitrarily fast. In addition, we suggest a simple recipe for using Dirichlet\nprocess (DP) mixture prior for estimating the finite mixture models and provide\ntheoretical guarantees. In particular, we provide a novel solution for adopting\nthe number of clusters in a DP mixture model as an estimate of the number of\ncomponents in a finite mixture model. Simulation study and real data\napplications are carried out demonstrating the utilities of our method.",
          "arxiv_id": "2007.09284v2"
        },
        {
          "title": "Bayesian mixture models (in)consistency for the number of clusters",
          "year": "2022-10",
          "abstract": "Bayesian nonparametric mixture models are common for modeling complex data.\nWhile these models are well-suited for density estimation, recent results\nproved posterior inconsistency of the number of clusters when the true number\nof components is finite, for the Dirichlet process and Pitman--Yor process\nmixture models. We extend these results to additional Bayesian nonparametric\npriors such as Gibbs-type processes and finite-dimensional representations\nthereof. The latter include the Dirichlet multinomial process, the recently\nproposed Pitman-Yor, and normalized generalized gamma multinomial processes. We\nshow that mixture models based on these processes are also inconsistent in the\nnumber of clusters and discuss possible solutions. Notably, we show that a\npost-processing algorithm introduced for the Dirichlet process can be extended\nto more general models and provides a consistent method to estimate the number\nof components.",
          "arxiv_id": "2210.14201v3"
        }
      ],
      "12": [
        {
          "title": "Increasing Domain Infill Asymptotics for Stochastic Differential Equations Driven by Fractional Brownian Motion",
          "year": "2020-05",
          "abstract": "Although statistical inference in stochastic differential equations (SDEs)\ndriven by Wiener process has received significant attention in the literature,\ninference in those driven by fractional Brownian motion seem to have seen much\nless development in comparison, despite their importance in modeling long range\ndependence. In this article, we consider both classical and Bayesian inference\nin such fractional Brownian motion based SDEs. In particular, we consider\nasymptotic inference for two parameters in this regard; a multiplicative\nparameter associated with the drift function, and the so-called \"Hurst\nparameter\" of the fractional Brownian motion, when the time domain tends to\ninfinity. For unknown Hurst parameter, the likelihood does not lend itself\namenable to the popular Girsanov form, rendering usual asymptotic development\ndifficult. As such, we develop increasing domain infill asymptotic theory, by\ndiscretizing the SDE. In this setup, we establish consistency and asymptotic\nnormality of the maximum likelihood estimators, as well as consistency and\nasymptotic normality of the Bayesian posterior distributions. However,\nclassical or Bayesian asymptotic normality with respect to the Hurst parameter\ncould not be established. We supplement our theoretical investigations with\nsimulation studies in a non-asymptotic setup, prescribing suitable\nmethodologies for classical and Bayesian analyses of SDEs driven by fractional\nBrownian motion. Applications to a real, close price data, along with\ncomparison with standard SDE driven by Wiener process, is also considered. As\nexpected, it turned out that our Bayesian fractional SDE triumphed over the\nother model and methods, in both simulated and real data applications.",
          "arxiv_id": "2005.09577v4"
        },
        {
          "title": "Parameter estimation of stochastic differential equation driven by small fractional noise",
          "year": "2022-01",
          "abstract": "We study the problem of parametric estimation for continuously observed\nstochastic processes driven by additive small fractional Brownian motion with\nHurst index 0<H<1/2 and 1/2<H<1. Under some assumptions on the drift\ncoefficient, we obtain the asymptotic normality and moment convergence of\nmaximum likelihood estimator of the drift parameter .",
          "arxiv_id": "2201.00372v1"
        },
        {
          "title": "The maximum likelihood type estimator of SDEs with fractional Brownian motion under small noise asymptotics in the rough case",
          "year": "2024-06",
          "abstract": "We study the problem of parametric estimation for continuously observed\nstochastic differential equation driven by fractional Brownian motion. Under\nsome assumptions on drift and diffusion coefficients, we construct maximum\nlikelihood estimator and establish its the asymptotic normality and moment\nconvergence of the drift parameter when a small dispersion coefficient\nvanishes.",
          "arxiv_id": "2406.07804v3"
        }
      ],
      "13": [
        {
          "title": "Federated Transfer Learning with Differential Privacy",
          "year": "2024-03",
          "abstract": "Federated learning has emerged as a powerful framework for analysing\ndistributed data, yet two challenges remain pivotal: heterogeneity across sites\nand privacy of local data. In this paper, we address both challenges within a\nfederated transfer learning framework, aiming to enhance learning on a target\ndata set by leveraging information from multiple heterogeneous source data sets\nwhile adhering to privacy constraints. We rigorously formulate the notion of\nfederated differential privacy, which offers privacy guarantees for each data\nset without assuming a trusted central server. Under this privacy model, we\nstudy three classical statistical problems: univariate mean estimation,\nlow-dimensional linear regression, and high-dimensional linear regression. By\ninvestigating the minimax rates and quantifying the cost of privacy in each\nproblem, we show that federated differential privacy is an intermediate privacy\nmodel between the well-established local and central models of differential\nprivacy. Our analyses account for data heterogeneity and privacy, highlighting\nthe fundamental costs associated with each factor and the benefits of knowledge\ntransfer in federated learning.",
          "arxiv_id": "2403.11343v3"
        },
        {
          "title": "Private sampling: a noiseless approach for generating differentially private synthetic data",
          "year": "2021-09",
          "abstract": "In a world where artificial intelligence and data science become omnipresent,\ndata sharing is increasingly locking horns with data-privacy concerns.\nDifferential privacy has emerged as a rigorous framework for protecting\nindividual privacy in a statistical database, while releasing useful\nstatistical information about the database. The standard way to implement\ndifferential privacy is to inject a sufficient amount of noise into the data.\nHowever, in addition to other limitations of differential privacy, this process\nof adding noise will affect data accuracy and utility. Another approach to\nenable privacy in data sharing is based on the concept of synthetic data. The\ngoal of synthetic data is to create an as-realistic-as-possible dataset, one\nthat not only maintains the nuances of the original data, but does so without\nrisk of exposing sensitive information. The combination of differential privacy\nwith synthetic data has been suggested as a best-of-both-worlds solutions. In\nthis work, we propose the first noisefree method to construct differentially\nprivate synthetic data; we do this through a mechanism called \"private\nsampling\". Using the Boolean cube as benchmark data model, we derive explicit\nbounds on accuracy and privacy of the constructed synthetic data. The key\nmathematical tools are hypercontractivity, duality, and empirical processes. A\ncore ingredient of our private sampling mechanism is a rigorous \"marginal\ncorrection\" method, which has the remarkable property that importance\nreweighting can be utilized to exactly match the marginals of the sample to the\nmarginals of the population.",
          "arxiv_id": "2109.14839v1"
        },
        {
          "title": "A Statistical Viewpoint on Differential Privacy: Hypothesis Testing, Representation and Blackwell's Theorem",
          "year": "2024-09",
          "abstract": "Differential privacy is widely considered the formal privacy for\nprivacy-preserving data analysis due to its robust and rigorous guarantees,\nwith increasingly broad adoption in public services, academia, and industry.\nDespite originating in the cryptographic context, in this review paper we argue\nthat, fundamentally, differential privacy can be considered a \\textit{pure}\nstatistical concept. By leveraging David Blackwell's informativeness theorem,\nour focus is to demonstrate based on prior work that all definitions of\ndifferential privacy can be formally motivated from a hypothesis testing\nperspective, thereby showing that hypothesis testing is not merely convenient\nbut also the right language for reasoning about differential privacy. This\ninsight leads to the definition of $f$-differential privacy, which extends\nother differential privacy definitions through a representation theorem. We\nreview techniques that render $f$-differential privacy a unified framework for\nanalyzing privacy bounds in data analysis and machine learning. Applications of\nthis differential privacy definition to private deep learning, private convex\noptimization, shuffled mechanisms, and U.S.\\ Census data are discussed to\nhighlight the benefits of analyzing privacy bounds under this framework\ncompared to existing alternatives.",
          "arxiv_id": "2409.09558v2"
        }
      ],
      "14": [
        {
          "title": "Sparse Hanson-Wright Inequalities with Applications",
          "year": "2024-10",
          "abstract": "We derive new Hanson-Wright-type inequalities tailored to the quadratic forms\nof random vectors with sparse independent components. Specifically, we consider\ncases where the components of the random vector are sparse\n$\\alpha$-subexponential random variables with $\\alpha>0$. When $\\alpha=\\infty$,\nthese inequalities can be seen as quadratic generalizations of the classical\nBernstein and Bennett inequalities for sparse bounded random vectors. To\nestablish this quadratic generalization, we also develop new Bersntein-type and\nBennett-type inequalities for linear forms of sparse $\\alpha$-subexponential\nrandom variables that go beyond the bounded case $(\\alpha=\\infty)$. Our proof\nrelies on a novel combinatorial method for estimating the moments of both\nrandom linear forms and quadratic forms.\n  We present two key applications of these new sparse Hanson-Wright\ninequalities: (1) A local law and complete eigenvector delocalization for\nsparse $\\alpha$-subexponential Hermitian random matrices, generalizing the\nresult of He et al. (2019) beyond sparse Bernoulli random matrices. To the best\nof our knowledge, this is the first local law and complete delocalization\nresult for sparse $\\alpha$-subexponeitial random matrices down to the\nnear-optimal sparsity $p\\geq \\frac{\\mathrm{polylog}(n)}{n}$ when $\\alpha\\in\n(0,2)$ as well as for unbounded sparse sub-gaussian random matrices down to the\noptimal sparsity $p\\gtrsim \\frac{\\log n}{n}.$ (2) Concentration of the\nEuclidean norm for the linear transformation of a sparse\n$\\alpha$-subexponential random vector, improving on the results of G\\\"otze et\nal. (2021) for sparse sub-exponential random vectors.",
          "arxiv_id": "2410.15652v3"
        },
        {
          "title": "On the links between Stein transforms and concentration inequalities for dependent random variables",
          "year": "2022-11",
          "abstract": "In this paper, we explore some links between transforms derived by Stein's\nmethod and concentration inequalities. In particular, we show that the\nstochastic domination of the zero bias transform of a random variable is\nequivalent to sub-Gaussian concentration. For this purpose a new stochastic\norder is considered. In a second time, we study the case of functions of\nslightly dependent light-tailed random variables. We are able to recover the\nfamous McDiarmid type of concentration inequality for functions with the\nbounded difference property. Additionally, we obtain new concentration bounds\nwhen we authorize a light dependence between the random variables. Finally, we\ngive a analogous result for another type of Stein's transform, the so-called\nsize bias transform.",
          "arxiv_id": "2211.13211v1"
        },
        {
          "title": "Tight Concentration Inequality for Sub-Weibull Random Variables with Generalized Bernstien Orlicz norm",
          "year": "2023-02",
          "abstract": "Recent development in high-dimensional statistical inference has necessitated\nconcentration inequalities for a broader range of random variables. We focus on\nsub-Weibull random variables, which extend sub-Gaussian or sub-exponential\nrandom variables to allow heavy-tailed distributions. This paper presents\nconcentration inequalities for independent sub-Weibull random variables with\nfinite Generalized Bernstein-Orlicz norms, providing generalized Bernstein's\ninequalities and Rosenthal-type moment bounds. The tightness of the proposed\nbounds is shown through lower bounds of the concentration inequalities obtained\nvia the Paley-Zygmund inequality. The results are applied to a graphical model\ninference problem, improving previous sample complexity bounds.",
          "arxiv_id": "2302.03850v2"
        }
      ],
      "15": [
        {
          "title": "Localized Conformal Prediction: A Generalized Inference Framework for Conformal Prediction",
          "year": "2021-06",
          "abstract": "We propose a new inference framework called localized conformal prediction.\nIt generalizes the framework of conformal prediction by offering a\nsingle-test-sample adaptive construction that emphasizes a local region around\nthis test sample, and can be combined with different conformal score\nconstructions. The proposed framework enjoys an assumption-free finite sample\nmarginal coverage guarantee, and it also offers additional local coverage\nguarantees under suitable assumptions. We demonstrate how to change from\nconformal prediction to localized conformal prediction using several conformal\nscores, and we illustrate a potential gain via numerical examples.",
          "arxiv_id": "2106.08460v2"
        },
        {
          "title": "Efficient and Differentiable Conformal Prediction with General Function Classes",
          "year": "2022-02",
          "abstract": "Quantifying the data uncertainty in learning tasks is often done by learning\na prediction interval or prediction set of the label given the input. Two\ncommonly desired properties for learned prediction sets are \\emph{valid\ncoverage} and \\emph{good efficiency} (such as low length or low cardinality).\nConformal prediction is a powerful technique for learning prediction sets with\nvalid coverage, yet by default its conformalization step only learns a single\nparameter, and does not optimize the efficiency over more expressive function\nclasses.\n  In this paper, we propose a generalization of conformal prediction to\nmultiple learnable parameters, by considering the constrained empirical risk\nminimization (ERM) problem of finding the most efficient prediction set subject\nto valid empirical coverage. This meta-algorithm generalizes existing conformal\nprediction algorithms, and we show that it achieves approximate valid\npopulation coverage and near-optimal efficiency within class, whenever the\nfunction class in the conformalization step is low-capacity in a certain sense.\nNext, this ERM problem is challenging to optimize as it involves a\nnon-differentiable coverage constraint. We develop a gradient-based algorithm\nfor it by approximating the original constrained ERM using differentiable\nsurrogate losses and Lagrangians. Experiments show that our algorithm is able\nto learn valid prediction sets and improve the efficiency significantly over\nexisting approaches in several applications such as prediction intervals with\nimproved length, minimum-volume prediction sets for multi-output regression,\nand label prediction sets for image classification.",
          "arxiv_id": "2202.11091v2"
        },
        {
          "title": "Conditional validity and a fast approximation formula of full conformal prediction sets",
          "year": "2025-08",
          "abstract": "Prediction sets based on full conformal prediction have seen an increasing\ninterest in statistical learning due to their universal marginal coverage\nguarantees. However, practitioners have refrained from using it in applications\nfor two reasons: Firstly, it comes at very high computational costs, exceeding\neven that of cross-validation. Secondly, an applicant is typically not\ninterested in a marginal coverage guarantee which averages over all possible\n(but not available) training data sets, but rather in a guarantee conditional\non the specific training data. This work tackles these problems by, firstly,\nshowing that full conformal prediction sets are conditionally conservative\ngiven the training data if the conformity score is stochastically bounded and\nsatisfies a stability condition. Secondly, we propose an approximation for the\nfull conformal prediction set that has asymptotically the same training\nconditional coverage as full conformal prediction under the stability\nassumption derived before, and can be computed more easily. Furthermore, we\nshow that under the stability assumption, $n$-fold cross-conformal prediction\nalso has the same asymptotic training conditional coverage guarantees as full\nconformal prediction. If the conformity score is defined as the out-of-sample\nprediction error, our approximation of the full conformal set coincides with\nthe symmetrized Jackknife. We conclude that for this conformity score, if based\non a stable prediction algorithm, full-conformal, $n$-fold cross-conformal, the\nJackknife+, our approximation formula, and hence also the Jackknife, all yield\nthe same asymptotic training conditional coverage guarantees.",
          "arxiv_id": "2508.05272v1"
        }
      ],
      "16": [
        {
          "title": "A CLT for the LSS of large dimensional sample covariance matrices with diverging spikes",
          "year": "2022-12",
          "abstract": "In this paper, we establish the central limit theorem (CLT) for linear\nspectral statistics (LSSs) of a large-dimensional sample covariance matrix when\nthe population covariance matrices are involved with diverging spikes. This\nconstitutes a nontrivial extension of the Bai-Silverstein theorem (BST) (Ann\nProbab 32(1):553--605, 2004), a theorem that has strongly influenced the\ndevelopment of high-dimensional statistics, especially in the applications of\nrandom matrix theory to statistics. Recently, there has been a growing\nrealization that the assumption of uniform boundedness of the population\ncovariance matrices in the BST is not satisfied in some fields, such as\neconomics, where the variances of principal components may diverge as the\ndimension tends to infinity. Therefore, in this paper, we aim to eliminate this\nobstacle to applications of the BST. Our new CLT accommodates spiked\neigenvalues, which may either be bounded or tend to infinity. A distinguishing\nfeature of our result is that the variance in the new CLT is related to both\nspiked eigenvalues and bulk eigenvalues, with dominance being determined by the\ndivergence rate of the largest spiked eigenvalues. The new CLT for LSS is then\napplied to test the hypothesis that the population covariance matrix is the\nidentity matrix or a generalized spiked model. The asymptotic distributions of\nthe corrected likelihood ratio test statistic and the corrected Nagao's trace\ntest statistic are derived under the alternative hypothesis. Moreover, we\npresent power comparisons between these two LSSs and Roy's largest root test.\nIn particular, we demonstrate that except for the case in which the number of\nspikes is equal to one, the LSSs could exhibit higher asymptotic power than\nRoy's largest root test.",
          "arxiv_id": "2212.05896v4"
        },
        {
          "title": "Large sample correlation matrices: a comparison theorem and its applications",
          "year": "2022-01",
          "abstract": "In this paper, we show that the diagonal of a high-dimensional sample\ncovariance matrix stemming from $n$ independent observations of a\n$p$-dimensional time series with finite fourth moments can be approximated in\nspectral norm by the diagonal of the population covariance matrix. We assume\nthat $n,p\\to \\infty$ with $p/n$ tending to a constant which might be positive\nor zero. As applications, we provide an approximation of the sample correlation\nmatrix ${\\mathbf R}$ and derive a variety of results for its eigenvalues. We\nidentify the limiting spectral distribution of ${\\mathbf R}$ and construct an\nestimator for the population correlation matrix and its eigenvalues. Finally,\nthe almost sure limits of the extreme eigenvalues of ${\\mathbf R}$ in a\ngeneralized spiked correlation model are analyzed.",
          "arxiv_id": "2201.00916v1"
        },
        {
          "title": "A CLT for the LSS of large dimensional sample covariance matrices with unbounded dispersions",
          "year": "2022-05",
          "abstract": "In this paper, we establish the central limit theorem (CLT) for linear\nspectral statistics (LSS) of large-dimensional sample covariance matrix when\nthe population covariance matrices are not uniformly bounded, which is a\nnontrivial extension of the Bai-Silverstein theorem (BST) (2004). The latter\nhas strongly stimulated the development of high-dimensional statistics,\nespecially the application of random matrix theory to statistics. However, the\nassumption of uniform boundedness of the population covariance matrices is\nfound strongly limited to the applications of BST. The aim of this paper is to\nremove the blockages to the applications of BST. The new CLT, allows the spiked\neigenvalues to exist and tend to infinity. It is interesting to note that the\nroles of either spiked eigenvalues or the bulk eigenvalues or both of the two\nare dominating in the CLT.\n  Moreover, the results are checked by simulation studies with various\npopulation settings. The CLT for LSS is then applied for testing the hypothesis\nthat a covariance matrix $ \\bSi $ is equal to an identity matrix. For this, the\nasymptotic distributions for the corrected likelihood ratio test (LRT) and\nNagao's trace test (NT) under alternative are derived, and we also propose the\nasymptotic power of LRT and NT under certain alternatives.",
          "arxiv_id": "2205.07280v1"
        }
      ],
      "17": [
        {
          "title": "The Bayesian approach to inverse Robin problems",
          "year": "2023-11",
          "abstract": "In this paper we investigate the Bayesian approach to inverse Robin problems.\nThese are problems for certain elliptic boundary value problems of determining\na Robin coefficient on a hidden part of the boundary from Cauchy data on the\nobservable part. Such a nonlinear inverse problem arises naturally in the\ninitialisation of large-scale ice sheet models that are crucial in climate and\nsea-level predictions. We motivate the Bayesian approach for a prototypical\nRobin inverse problem by showing that the posterior mean converges in\nprobability to the data-generating ground truth as the number of observations\nincreases. Related to the stability theory for inverse Robin problems, we\nestablish a logarithmic convergence rate for Sobolev-regular Robin\ncoefficients, whereas for analytic coefficients we can attain an algebraic\nrate. The use of rescaled analytic Gaussian priors in posterior consistency for\nnonlinear inverse problems is new and may be of separate interest in other\ninverse problems. Our numerical results illustrate the convergence property in\ntwo observation settings.",
          "arxiv_id": "2311.17542v1"
        },
        {
          "title": "Designing truncated priors for direct and inverse Bayesian problems",
          "year": "2021-05",
          "abstract": "The Bayesian approach to inverse problems with functional unknowns, has\nreceived significant attention in recent years. An important component of the\ndeveloping theory is the study of the asymptotic performance of the posterior\ndistribution in the frequentist setting. The present paper contributes to the\narea of Bayesian inverse problems by formulating a posterior contraction theory\nfor linear inverse problems, with truncated Gaussian series priors, and under\ngeneral smoothness assumptions. Emphasis is on the intrinsic role of the\ntruncation point both for the direct as well as for the inverse problem, which\nare related through the modulus of continuity as this was recently highlighted\nby Knapik and Salomond (2018).",
          "arxiv_id": "2105.10254v2"
        },
        {
          "title": "Variational Gaussian Processes For Linear Inverse Problems",
          "year": "2023-11",
          "abstract": "By now Bayesian methods are routinely used in practice for solving inverse\nproblems. In inverse problems the parameter or signal of interest is observed\nonly indirectly, as an image of a given map, and the observations are typically\nfurther corrupted with noise. Bayes offers a natural way to regularize these\nproblems via the prior distribution and provides a probabilistic solution,\nquantifying the remaining uncertainty in the problem. However, the\ncomputational costs of standard, sampling based Bayesian approaches can be\noverly large in such complex models. Therefore, in practice variational Bayes\nis becoming increasingly popular. Nevertheless, the theoretical understanding\nof these methods is still relatively limited, especially in context of inverse\nproblems. In our analysis we investigate variational Bayesian methods for\nGaussian process priors to solve linear inverse problems. We consider both\nmildly and severely ill-posed inverse problems and work with the popular\ninducing variables variational Bayes approach proposed by Titsias in 2009. We\nderive posterior contraction rates for the variational posterior in general\nsettings and show that the minimax estimation rate can be attained by correctly\ntunned procedures. As specific examples we consider a collection of inverse\nproblems including the heat equation, Volterra operator and Radon transform and\ninducing variable methods based on population and empirical spectral features.",
          "arxiv_id": "2311.00663v1"
        }
      ],
      "18": [
        {
          "title": "Consistent DAG selection for Bayesian causal discovery under general error distributions",
          "year": "2025-08",
          "abstract": "We consider the problem of learning the underlying causal structure among a\nset of variables, which are assumed to follow a Bayesian network or, more\nspecifically, a linear recursive structural equation model (SEM) with the\nassociated errors being independent and allowed to be non-Gaussian. A Bayesian\nhierarchical model is proposed to identify the true data-generating directed\nacyclic graph (DAG) structure where the nodes and edges represent the variables\nand the direct causal effects, respectively. Moreover, incorporating the\ninformation of non-Gaussian errors, we characterize the distribution\nequivalence class of the true DAG, which specifies the best possible extent to\nwhich the DAG can be identified based on purely observational data.\nFurthermore, under the consideration that the errors are distributed as some\nscale mixture of Gaussian, where the mixing distribution is unspecified, and\nmild distributional assumptions, we establish that by employing a non-standard\nDAG prior, the posterior probability of the distribution equivalence class of\nthe true DAG converges to unity as the sample size grows. This shows that the\nproposed method achieves the posterior DAG selection consistency, which is\nfurther illustrated with examples and simulation studies.",
          "arxiv_id": "2508.00993v1"
        },
        {
          "title": "On the number and size of Markov equivalence classes of random directed acyclic graphs",
          "year": "2022-09",
          "abstract": "In causal inference on directed acyclic graphs, the orientation of edges is\nin general only recovered up to Markov equivalence classes. We study Markov\nequivalence classes of uniformly random directed acyclic graphs. Using a tower\ndecomposition, we show that the ratio between the number of Markov equivalence\nclasses and directed acyclic graphs approaches a positive constant when the\nnumber of sites goes to infinity. For a typical directed acyclic graph, the\nexpected number of elements in its Markov equivalence class remains bounded.\nMore precisely, we prove that for a uniformly chosen directed acyclic graph,\nthe size of its Markov equivalence class has super-polynomial tails.",
          "arxiv_id": "2209.04395v2"
        },
        {
          "title": "Partial Homoscedasticity in Causal Discovery with Linear Models",
          "year": "2023-08",
          "abstract": "Recursive linear structural equation models and the associated directed\nacyclic graphs (DAGs) play an important role in causal discovery. The classic\nidentifiability result for this class of models states that when only\nobservational data is available, each DAG can be identified only up to a Markov\nequivalence class. In contrast, recent work has shown that the DAG can be\nuniquely identified if the errors in the model are homoscedastic, i.e., all\nhave the same variance. This equal variance assumption yields methods that, if\nappropriate, are highly scalable and also sheds light on fundamental\ninformation-theoretic limits and optimality in causal discovery. In this paper,\nwe fill the gap that exists between the two previously considered cases, which\nassume the error variances to be either arbitrary or all equal. Specifically,\nwe formulate a framework of partial homoscedasticity, in which the variables\nare partitioned into blocks and each block shares the same error variance. For\nany such groupwise equal variances assumption, we characterize when two DAGs\ngive rise to identical Gaussian linear structural equation models. Furthermore,\nwe show how the resulting distributional equivalence classes may be represented\nusing a completed partially directed acyclic graph (CPDAG), and we give an\nalgorithm to efficiently construct this CPDAG. In a simulation study, we\ndemonstrate that greedy search provides an effective way to learn the CPDAG and\nexploit partial knowledge about homoscedasticity of errors in structural\nequation models.",
          "arxiv_id": "2308.08959v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T19:58:20Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}