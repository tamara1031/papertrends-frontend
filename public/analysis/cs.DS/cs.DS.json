{
  "topics": {
    "data": {
      "0": {
        "name": "0_graph_graphs_time_problem",
        "keywords": [
          [
            "graph",
            0.02341819681453893
          ],
          [
            "graphs",
            0.02221966553206485
          ],
          [
            "time",
            0.017675527456231077
          ],
          [
            "problem",
            0.01674437269465821
          ],
          [
            "algorithm",
            0.0160050167846094
          ],
          [
            "vertex",
            0.013743433020421785
          ],
          [
            "edge",
            0.012401434472625565
          ],
          [
            "vertices",
            0.011121861309124539
          ],
          [
            "algorithms",
            0.0104407516213424
          ],
          [
            "approximation",
            0.009617345841604522
          ]
        ],
        "count": 2745
      },
      "1": {
        "name": "1_problem_online_algorithm_competitive",
        "keywords": [
          [
            "problem",
            0.01969642275363778
          ],
          [
            "online",
            0.0185171190807911
          ],
          [
            "algorithm",
            0.016892853214378913
          ],
          [
            "competitive",
            0.014876473779719597
          ],
          [
            "algorithms",
            0.01209766601645856
          ],
          [
            "ratio",
            0.011976131572036123
          ],
          [
            "approximation",
            0.01181445858588548
          ],
          [
            "time",
            0.011268794262859906
          ],
          [
            "optimal",
            0.010788338611459792
          ],
          [
            "matching",
            0.009588413708585672
          ]
        ],
        "count": 1975
      },
      "2": {
        "name": "2_string_time_strings_length",
        "keywords": [
          [
            "string",
            0.03064753068352475
          ],
          [
            "time",
            0.024912704552832187
          ],
          [
            "strings",
            0.022382842917042494
          ],
          [
            "length",
            0.02231140586890548
          ],
          [
            "pattern",
            0.016321908406046432
          ],
          [
            "space",
            0.015213608070284561
          ],
          [
            "distance",
            0.014315237113008153
          ],
          [
            "algorithm",
            0.014195966687818898
          ],
          [
            "edit",
            0.014053065588088498
          ],
          [
            "problem",
            0.0135427135290038
          ]
        ],
        "count": 683
      },
      "3": {
        "name": "3_clustering_approximation_Clustering_algorithm",
        "keywords": [
          [
            "clustering",
            0.03093776432998332
          ],
          [
            "approximation",
            0.02068387633712161
          ],
          [
            "Clustering",
            0.015900288416793657
          ],
          [
            "algorithm",
            0.015900006489097975
          ],
          [
            "points",
            0.015256687701318486
          ],
          [
            "data",
            0.014664365769202652
          ],
          [
            "means",
            0.01421781161420899
          ],
          [
            "problem",
            0.013507749244159806
          ],
          [
            "algorithms",
            0.012183593554625755
          ],
          [
            "time",
            0.01163753227069417
          ]
        ],
        "count": 548
      },
      "4": {
        "name": "4_data_sorting_tree_time",
        "keywords": [
          [
            "data",
            0.02504811314369484
          ],
          [
            "sorting",
            0.019091569313198522
          ],
          [
            "tree",
            0.014787366743230536
          ],
          [
            "time",
            0.013897661373433243
          ],
          [
            "trees",
            0.013608041292459941
          ],
          [
            "queries",
            0.013058839183520498
          ],
          [
            "structures",
            0.012926050783964802
          ],
          [
            "structure",
            0.012548417532310912
          ],
          [
            "data structures",
            0.012096659738645045
          ],
          [
            "hash",
            0.012046655454380877
          ]
        ],
        "count": 515
      },
      "5": {
        "name": "5_quantum_Quantum_classical_algorithm",
        "keywords": [
          [
            "quantum",
            0.08596607135896978
          ],
          [
            "Quantum",
            0.026706407076731366
          ],
          [
            "classical",
            0.02320518149257864
          ],
          [
            "algorithm",
            0.017507060282314673
          ],
          [
            "state",
            0.017485539036867222
          ],
          [
            "algorithms",
            0.015083236755697149
          ],
          [
            "states",
            0.014479807983474964
          ],
          [
            "quantum algorithms",
            0.012841633541786981
          ],
          [
            "quantum algorithm",
            0.012762579797049704
          ],
          [
            "complexity",
            0.012159378381271386
          ]
        ],
        "count": 497
      },
      "6": {
        "name": "6_matrix_algorithm_rank_regression",
        "keywords": [
          [
            "matrix",
            0.022595277757763276
          ],
          [
            "algorithm",
            0.013624188622479407
          ],
          [
            "rank",
            0.013119916386439303
          ],
          [
            "regression",
            0.012674239605126597
          ],
          [
            "Gaussian",
            0.011198861075917171
          ],
          [
            "algorithms",
            0.01100736175993563
          ],
          [
            "low",
            0.01087712607052889
          ],
          [
            "linear",
            0.010767381791034461
          ],
          [
            "sparse",
            0.01036172951842898
          ],
          [
            "matrices",
            0.010202361913032816
          ]
        ],
        "count": 464
      },
      "7": {
        "name": "7_random_graphs_graph_degree",
        "keywords": [
          [
            "random",
            0.020955341818609738
          ],
          [
            "graphs",
            0.01868976191820986
          ],
          [
            "graph",
            0.017118014885647924
          ],
          [
            "degree",
            0.017041118369556702
          ],
          [
            "spectral",
            0.016238251019984364
          ],
          [
            "mixing",
            0.01534498415366924
          ],
          [
            "model",
            0.014149784773081847
          ],
          [
            "sampling",
            0.013221274434260048
          ],
          [
            "algorithm",
            0.012896117821801877
          ],
          [
            "dynamics",
            0.012841976219282817
          ]
        ],
        "count": 406
      },
      "8": {
        "name": "8_privacy_private_DP_differential privacy",
        "keywords": [
          [
            "privacy",
            0.062065576696886865
          ],
          [
            "private",
            0.037240067378630046
          ],
          [
            "DP",
            0.027947978046142567
          ],
          [
            "differential privacy",
            0.02630449093671979
          ],
          [
            "differential",
            0.025863431821257662
          ],
          [
            "data",
            0.01955727543229386
          ],
          [
            "Private",
            0.0175377553629555
          ],
          [
            "mechanism",
            0.017062400998610856
          ],
          [
            "error",
            0.013919805598585761
          ],
          [
            "Privacy",
            0.013820449576311145
          ]
        ],
        "count": 331
      },
      "9": {
        "name": "9_learning_testing_distribution_sample",
        "keywords": [
          [
            "learning",
            0.03755255935017679
          ],
          [
            "testing",
            0.023947711660008492
          ],
          [
            "distribution",
            0.023736047656753377
          ],
          [
            "sample",
            0.017370623644226882
          ],
          [
            "complexity",
            0.0168673279155499
          ],
          [
            "samples",
            0.014920890810500092
          ],
          [
            "algorithm",
            0.0140790056500077
          ],
          [
            "distributions",
            0.013809482220522163
          ],
          [
            "sample complexity",
            0.01290101526287527
          ],
          [
            "PAC",
            0.012386700327869165
          ]
        ],
        "count": 309
      },
      "10": {
        "name": "10_graph_graphs_algorithm_algorithms",
        "keywords": [
          [
            "graph",
            0.029999284590505878
          ],
          [
            "graphs",
            0.019296808037153078
          ],
          [
            "algorithm",
            0.015609773533092012
          ],
          [
            "algorithms",
            0.014980988451536234
          ],
          [
            "subgraph",
            0.012975721714967116
          ],
          [
            "networks",
            0.012899122286317562
          ],
          [
            "real",
            0.012568920011156772
          ],
          [
            "parallel",
            0.011642595873646296
          ],
          [
            "large",
            0.01115633316888054
          ],
          [
            "world",
            0.010864140292274975
          ]
        ],
        "count": 297
      },
      "11": {
        "name": "11_algorithm_time_Subset_Sum",
        "keywords": [
          [
            "algorithm",
            0.02412849613685199
          ],
          [
            "time",
            0.020557291375533936
          ],
          [
            "Subset",
            0.015916447942297754
          ],
          [
            "Sum",
            0.015083104839414411
          ],
          [
            "problem",
            0.012694137158617048
          ],
          [
            "algorithms",
            0.01182784179481262
          ],
          [
            "polynomial",
            0.011634452917854561
          ],
          [
            "matrix",
            0.011454985669202153
          ],
          [
            "integers",
            0.009728142990470133
          ],
          [
            "integer",
            0.009437922964125864
          ]
        ],
        "count": 267
      },
      "12": {
        "name": "12_stream_data_streaming_space",
        "keywords": [
          [
            "stream",
            0.028012240465860573
          ],
          [
            "data",
            0.025413578289668712
          ],
          [
            "streaming",
            0.02249490782719249
          ],
          [
            "space",
            0.021307932189374042
          ],
          [
            "sketch",
            0.01863766122190203
          ],
          [
            "estimation",
            0.01780423961487649
          ],
          [
            "sketches",
            0.015448127462963033
          ],
          [
            "streams",
            0.014916649900309331
          ],
          [
            "algorithms",
            0.01451731216207608
          ],
          [
            "algorithm",
            0.014319912820975594
          ]
        ],
        "count": 187
      },
      "13": {
        "name": "13_SAT_CSPs_variables_problems",
        "keywords": [
          [
            "SAT",
            0.04513586083367299
          ],
          [
            "CSPs",
            0.021049408413028506
          ],
          [
            "variables",
            0.018663772445619436
          ],
          [
            "problems",
            0.016558465825225757
          ],
          [
            "Boolean",
            0.015306142835204609
          ],
          [
            "constraint",
            0.015210351582413623
          ],
          [
            "formula",
            0.0150429328584297
          ],
          [
            "problem",
            0.01381753449147725
          ],
          [
            "constraints",
            0.01375604520557912
          ],
          [
            "CNF",
            0.013097313688660154
          ]
        ],
        "count": 166
      },
      "14": {
        "name": "14_temporal_time_Temporal_temporal graphs",
        "keywords": [
          [
            "temporal",
            0.06899233114154667
          ],
          [
            "time",
            0.02332283289912019
          ],
          [
            "Temporal",
            0.02204353212298052
          ],
          [
            "temporal graphs",
            0.020711900113077344
          ],
          [
            "network",
            0.020335387764596168
          ],
          [
            "graph",
            0.01999734137432604
          ],
          [
            "graphs",
            0.019735189914232662
          ],
          [
            "temporal graph",
            0.018776161734881842
          ],
          [
            "problem",
            0.01729003244539166
          ],
          [
            "networks",
            0.013823200065713973
          ]
        ],
        "count": 159
      },
      "15": {
        "name": "15_robots_agent_agents_robot",
        "keywords": [
          [
            "robots",
            0.030332331528362125
          ],
          [
            "agent",
            0.027386229547824124
          ],
          [
            "agents",
            0.021149010998268457
          ],
          [
            "robot",
            0.02085532937626208
          ],
          [
            "time",
            0.019299489466068416
          ],
          [
            "algorithm",
            0.017327453941169593
          ],
          [
            "node",
            0.016077743310170586
          ],
          [
            "problem",
            0.015446138340913856
          ],
          [
            "leader",
            0.014970456930601736
          ],
          [
            "nodes",
            0.013523994965775513
          ]
        ],
        "count": 152
      },
      "16": {
        "name": "16_points_time_set_problem",
        "keywords": [
          [
            "points",
            0.030000771309225953
          ],
          [
            "time",
            0.029184444249806523
          ],
          [
            "set",
            0.026238892442802253
          ],
          [
            "problem",
            0.024109814280965283
          ],
          [
            "plane",
            0.02018497107276946
          ],
          [
            "algorithm",
            0.01822220558742647
          ],
          [
            "point",
            0.016456009342805673
          ],
          [
            "disks",
            0.014855961986094167
          ],
          [
            "disk",
            0.01236053304418786
          ],
          [
            "approximation",
            0.011237144403428562
          ]
        ],
        "count": 134
      },
      "17": {
        "name": "17_streaming_space_pass_algorithms",
        "keywords": [
          [
            "streaming",
            0.0494555341907154
          ],
          [
            "space",
            0.032710308159103925
          ],
          [
            "pass",
            0.027967927617136422
          ],
          [
            "algorithms",
            0.020802554933880654
          ],
          [
            "passes",
            0.020749093303724677
          ],
          [
            "graph",
            0.018949102795451096
          ],
          [
            "semi",
            0.018220332232241534
          ],
          [
            "algorithm",
            0.01749540743133255
          ],
          [
            "matching",
            0.016523963880562004
          ],
          [
            "streaming model",
            0.016136797823053634
          ]
        ],
        "count": 113
      }
    },
    "correlations": [
      [
        1.0,
        -0.047205840678336844,
        -0.49468438524797664,
        -0.3557806309052304,
        -0.6661807412003933,
        -0.7199396504296995,
        -0.6798685695973918,
        -0.2310667884750649,
        -0.7347304320107702,
        -0.6239450459474225,
        -0.18133947695623223,
        0.12522596510073034,
        -0.6884735477800228,
        -0.7299804312574654,
        -0.5624912565272517,
        -0.5430438234475545,
        -0.07736895047130453,
        -0.6234209834571095
      ],
      [
        -0.047205840678336844,
        1.0,
        -0.6133650433440876,
        -0.40215661185848994,
        -0.6710435644484576,
        -0.7209675774295174,
        -0.6671144731609029,
        -0.5824077359870701,
        -0.7300623767756997,
        -0.5003655843650666,
        -0.6163631450782088,
        -0.26498953668535985,
        -0.6855367506660375,
        -0.7346786727024986,
        -0.6829819644935888,
        -0.6344778382330523,
        -0.23050504023498208,
        -0.6191352776493347
      ],
      [
        -0.49468438524797664,
        -0.6133650433440876,
        1.0,
        -0.6710711602226818,
        -0.6573557012868716,
        -0.7357889468337921,
        -0.7394275497410182,
        -0.6940216949849535,
        -0.7512990064194139,
        -0.7220647703615625,
        -0.6977619622988405,
        -0.4480974536052406,
        -0.6994482799522249,
        -0.7515274922230037,
        -0.4869693718218301,
        -0.4921917957688066,
        -0.4387355337766563,
        -0.6833721820060322
      ],
      [
        -0.3557806309052304,
        -0.40215661185848994,
        -0.6710711602226818,
        1.0,
        -0.6981717305359805,
        -0.7370650442956042,
        -0.7169763362938968,
        -0.6749540552995308,
        -0.7372569638272477,
        -0.6764740913277043,
        -0.664645793352409,
        -0.2578054985310297,
        -0.6907824864640404,
        -0.7490929381175249,
        -0.7072305710626201,
        -0.6921817250663969,
        -0.5341057813871375,
        -0.644160520185775
      ],
      [
        -0.6661807412003933,
        -0.6710435644484576,
        -0.6573557012868716,
        -0.6981717305359805,
        1.0,
        -0.7496729865429984,
        -0.7392159538203289,
        -0.7211364183368971,
        -0.7372649074598236,
        -0.7147017751305555,
        -0.7093410642629413,
        -0.6779083066415381,
        -0.4523993685082348,
        -0.762971395421122,
        -0.7107408884890297,
        -0.717351139798649,
        -0.5419666738880307,
        -0.6805579488689002
      ],
      [
        -0.7199396504296995,
        -0.7209675774295174,
        -0.7357889468337921,
        -0.7370650442956042,
        -0.7496729865429984,
        1.0,
        -0.7356230904974741,
        -0.7421349473700636,
        -0.7583342781808936,
        -0.7239099363421476,
        -0.746083829831711,
        -0.7200798594643245,
        -0.757898293178366,
        -0.74606701740569,
        -0.7528701462679356,
        -0.7496926133444277,
        -0.7366288156284734,
        -0.7427915915861314
      ],
      [
        -0.6798685695973918,
        -0.6671144731609029,
        -0.7394275497410182,
        -0.7169763362938968,
        -0.7392159538203289,
        -0.7356230904974741,
        1.0,
        -0.6718592348312058,
        -0.7481490079731825,
        -0.6533766941192389,
        -0.7412579574988664,
        -0.6586628681869877,
        -0.7304175497818042,
        -0.750985358385466,
        -0.7473159774545206,
        -0.7456367995201121,
        -0.7188511072857959,
        -0.7295692043387814
      ],
      [
        -0.2310667884750649,
        -0.5824077359870701,
        -0.6940216949849535,
        -0.6749540552995308,
        -0.7211364183368971,
        -0.7421349473700636,
        -0.6718592348312058,
        1.0,
        -0.7517254720826422,
        -0.6660220184485881,
        -0.06861778451163038,
        -0.5965047651200894,
        -0.7325050728952792,
        -0.750248175268617,
        -0.7088841683294712,
        -0.7019798583339772,
        -0.6151759233547845,
        -0.6987007137529611
      ],
      [
        -0.7347304320107702,
        -0.7300623767756997,
        -0.7512990064194139,
        -0.7372569638272477,
        -0.7372649074598236,
        -0.7583342781808936,
        -0.7481490079731825,
        -0.7517254720826422,
        1.0,
        -0.7016646888871797,
        -0.7491150888046938,
        -0.7299298425593665,
        -0.6955053772811495,
        -0.7638818880235348,
        -0.7528614088910528,
        -0.7469064254112483,
        -0.7412814665450534,
        -0.7376665692347871
      ],
      [
        -0.6239450459474225,
        -0.5003655843650666,
        -0.7220647703615625,
        -0.6764740913277043,
        -0.7147017751305555,
        -0.7239099363421476,
        -0.6533766941192389,
        -0.6660220184485881,
        -0.7016646888871797,
        1.0,
        -0.7251397608279039,
        -0.6240370607055787,
        -0.7171365859533949,
        -0.7586316423622885,
        -0.7384090691967313,
        -0.7207963708411416,
        -0.6665518275359591,
        -0.6876317791354585
      ],
      [
        -0.18133947695623223,
        -0.6163631450782088,
        -0.6977619622988405,
        -0.664645793352409,
        -0.7093410642629413,
        -0.746083829831711,
        -0.7412579574988664,
        -0.06861778451163038,
        -0.7491150888046938,
        -0.7251397608279039,
        1.0,
        -0.6328412779504319,
        -0.7258257466276261,
        -0.7539683624264384,
        -0.6895683765275931,
        -0.7043591294500025,
        -0.6244982935476997,
        -0.6834988512049317
      ],
      [
        0.12522596510073034,
        -0.26498953668535985,
        -0.4480974536052406,
        -0.2578054985310297,
        -0.6779083066415381,
        -0.7200798594643245,
        -0.6586628681869877,
        -0.5965047651200894,
        -0.7299298425593665,
        -0.6240370607055787,
        -0.6328412779504319,
        1.0,
        -0.6921753610960082,
        -0.7307372523752574,
        -0.5210705537824891,
        -0.5009603969890276,
        -0.3936111839769985,
        -0.6526320921165325
      ],
      [
        -0.6884735477800228,
        -0.6855367506660375,
        -0.6994482799522249,
        -0.6907824864640404,
        -0.4523993685082348,
        -0.757898293178366,
        -0.7304175497818042,
        -0.7325050728952792,
        -0.6955053772811495,
        -0.7171365859533949,
        -0.7258257466276261,
        -0.6921753610960082,
        1.0,
        -0.7563371802530054,
        -0.7290511601504698,
        -0.7385137711174037,
        -0.6862443575189383,
        -0.3972012864801669
      ],
      [
        -0.7299804312574654,
        -0.7346786727024986,
        -0.7515274922230037,
        -0.7490929381175249,
        -0.762971395421122,
        -0.74606701740569,
        -0.750985358385466,
        -0.750248175268617,
        -0.7638818880235348,
        -0.7586316423622885,
        -0.7539683624264384,
        -0.7307372523752574,
        -0.7563371802530054,
        1.0,
        -0.7559159024970883,
        -0.756617654792113,
        -0.735935012383695,
        -0.7380922784168522
      ],
      [
        -0.5624912565272517,
        -0.6829819644935888,
        -0.4869693718218301,
        -0.7072305710626201,
        -0.7107408884890297,
        -0.7528701462679356,
        -0.7473159774545206,
        -0.7088841683294712,
        -0.7528614088910528,
        -0.7384090691967313,
        -0.6895683765275931,
        -0.5210705537824891,
        -0.7290511601504698,
        -0.7559159024970883,
        1.0,
        -0.4242835786143371,
        -0.5455524271768908,
        -0.7190478372296958
      ],
      [
        -0.5430438234475545,
        -0.6344778382330523,
        -0.4921917957688066,
        -0.6921817250663969,
        -0.717351139798649,
        -0.7496926133444277,
        -0.7456367995201121,
        -0.7019798583339772,
        -0.7469064254112483,
        -0.7207963708411416,
        -0.7043591294500025,
        -0.5009603969890276,
        -0.7385137711174037,
        -0.756617654792113,
        -0.4242835786143371,
        1.0,
        -0.5275125433122523,
        -0.7209755125273742
      ],
      [
        -0.07736895047130453,
        -0.23050504023498208,
        -0.4387355337766563,
        -0.5341057813871375,
        -0.5419666738880307,
        -0.7366288156284734,
        -0.7188511072857959,
        -0.6151759233547845,
        -0.7412814665450534,
        -0.6665518275359591,
        -0.6244982935476997,
        -0.3936111839769985,
        -0.6862443575189383,
        -0.735935012383695,
        -0.5455524271768908,
        -0.5275125433122523,
        1.0,
        -0.637907756548452
      ],
      [
        -0.6234209834571095,
        -0.6191352776493347,
        -0.6833721820060322,
        -0.644160520185775,
        -0.6805579488689002,
        -0.7427915915861314,
        -0.7295692043387814,
        -0.6987007137529611,
        -0.7376665692347871,
        -0.6876317791354585,
        -0.6834988512049317,
        -0.6526320921165325,
        -0.3972012864801669,
        -0.7380922784168522,
        -0.7190478372296958,
        -0.7209755125273742,
        -0.637907756548452,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        45,
        18,
        5,
        2,
        10,
        6,
        0,
        2,
        2,
        4,
        3,
        10,
        0,
        1,
        4,
        1,
        11,
        6
      ],
      "2020-02": [
        53,
        46,
        11,
        13,
        15,
        4,
        4,
        3,
        6,
        11,
        5,
        12,
        0,
        2,
        1,
        4,
        23,
        18
      ],
      "2020-03": [
        26,
        26,
        9,
        6,
        7,
        3,
        1,
        1,
        4,
        5,
        6,
        14,
        0,
        1,
        2,
        3,
        20,
        9
      ],
      "2020-04": [
        56,
        41,
        11,
        10,
        9,
        8,
        2,
        6,
        3,
        9,
        4,
        17,
        5,
        3,
        5,
        2,
        19,
        8
      ],
      "2020-05": [
        30,
        39,
        10,
        8,
        10,
        6,
        2,
        3,
        3,
        6,
        5,
        11,
        1,
        2,
        0,
        6,
        15,
        8
      ],
      "2020-06": [
        44,
        42,
        10,
        18,
        7,
        3,
        1,
        4,
        7,
        19,
        12,
        18,
        0,
        0,
        2,
        2,
        12,
        6
      ],
      "2020-07": [
        53,
        43,
        11,
        14,
        10,
        7,
        2,
        6,
        4,
        9,
        10,
        22,
        3,
        2,
        5,
        3,
        25,
        9
      ],
      "2020-08": [
        44,
        31,
        4,
        10,
        7,
        1,
        1,
        4,
        3,
        5,
        12,
        12,
        2,
        2,
        1,
        2,
        17,
        5
      ],
      "2020-09": [
        24,
        39,
        4,
        12,
        10,
        8,
        3,
        2,
        8,
        4,
        8,
        11,
        0,
        2,
        2,
        2,
        13,
        16
      ],
      "2020-10": [
        37,
        38,
        5,
        5,
        16,
        5,
        3,
        4,
        14,
        10,
        6,
        15,
        1,
        1,
        5,
        2,
        14,
        15
      ],
      "2020-11": [
        38,
        51,
        8,
        14,
        8,
        7,
        3,
        2,
        4,
        9,
        5,
        25,
        4,
        5,
        1,
        2,
        23,
        12
      ],
      "2020-12": [
        32,
        42,
        7,
        8,
        9,
        5,
        0,
        5,
        5,
        8,
        5,
        10,
        1,
        1,
        1,
        1,
        11,
        5
      ],
      "2021-01": [
        35,
        19,
        1,
        4,
        6,
        2,
        2,
        1,
        1,
        4,
        3,
        5,
        1,
        1,
        3,
        0,
        18,
        6
      ],
      "2021-02": [
        40,
        41,
        5,
        10,
        6,
        7,
        3,
        6,
        7,
        14,
        7,
        9,
        0,
        3,
        3,
        4,
        14,
        12
      ],
      "2021-03": [
        35,
        25,
        4,
        5,
        4,
        10,
        1,
        3,
        4,
        4,
        1,
        9,
        0,
        0,
        1,
        1,
        10,
        4
      ],
      "2021-04": [
        27,
        34,
        1,
        9,
        4,
        0,
        0,
        3,
        3,
        2,
        5,
        9,
        0,
        0,
        1,
        0,
        10,
        5
      ],
      "2021-05": [
        60,
        29,
        14,
        14,
        8,
        6,
        1,
        5,
        3,
        6,
        2,
        15,
        0,
        1,
        4,
        6,
        15,
        13
      ],
      "2021-06": [
        37,
        36,
        9,
        20,
        7,
        6,
        2,
        6,
        14,
        12,
        3,
        18,
        3,
        1,
        2,
        3,
        23,
        19
      ],
      "2021-07": [
        39,
        41,
        6,
        13,
        11,
        5,
        5,
        1,
        1,
        14,
        3,
        16,
        2,
        5,
        1,
        1,
        20,
        17
      ],
      "2021-08": [
        27,
        25,
        6,
        6,
        7,
        10,
        2,
        2,
        5,
        6,
        2,
        9,
        1,
        2,
        1,
        4,
        10,
        8
      ],
      "2021-09": [
        26,
        29,
        3,
        3,
        7,
        5,
        0,
        3,
        2,
        1,
        3,
        9,
        1,
        2,
        2,
        4,
        12,
        9
      ],
      "2021-10": [
        32,
        34,
        3,
        11,
        8,
        10,
        2,
        5,
        8,
        7,
        5,
        17,
        0,
        1,
        4,
        2,
        14,
        6
      ],
      "2021-11": [
        53,
        47,
        5,
        17,
        9,
        4,
        4,
        7,
        9,
        8,
        6,
        12,
        1,
        2,
        2,
        4,
        26,
        15
      ],
      "2021-12": [
        31,
        33,
        6,
        14,
        3,
        14,
        1,
        2,
        5,
        9,
        4,
        12,
        1,
        2,
        4,
        2,
        19,
        6
      ],
      "2022-01": [
        43,
        23,
        6,
        2,
        8,
        8,
        2,
        0,
        5,
        5,
        3,
        8,
        1,
        2,
        5,
        1,
        7,
        7
      ],
      "2022-02": [
        27,
        34,
        9,
        10,
        4,
        14,
        3,
        1,
        3,
        6,
        2,
        12,
        2,
        1,
        6,
        4,
        13,
        7
      ],
      "2022-03": [
        29,
        33,
        4,
        8,
        7,
        11,
        1,
        5,
        10,
        4,
        6,
        7,
        4,
        1,
        2,
        0,
        19,
        8
      ],
      "2022-04": [
        30,
        36,
        8,
        6,
        2,
        5,
        3,
        3,
        2,
        10,
        6,
        14,
        3,
        2,
        5,
        4,
        17,
        14
      ],
      "2022-05": [
        35,
        35,
        4,
        11,
        4,
        4,
        2,
        4,
        7,
        8,
        2,
        10,
        2,
        4,
        2,
        2,
        16,
        11
      ],
      "2022-06": [
        31,
        26,
        9,
        7,
        10,
        9,
        2,
        3,
        9,
        12,
        7,
        12,
        0,
        2,
        3,
        1,
        18,
        8
      ],
      "2022-07": [
        35,
        30,
        7,
        8,
        6,
        7,
        1,
        3,
        7,
        8,
        3,
        16,
        0,
        7,
        0,
        1,
        17,
        17
      ],
      "2022-08": [
        39,
        36,
        7,
        14,
        9,
        4,
        3,
        2,
        5,
        6,
        5,
        11,
        3,
        3,
        1,
        0,
        23,
        7
      ],
      "2022-09": [
        35,
        42,
        7,
        8,
        8,
        11,
        3,
        4,
        1,
        5,
        7,
        14,
        0,
        0,
        1,
        4,
        18,
        13
      ],
      "2022-10": [
        43,
        32,
        7,
        12,
        7,
        11,
        2,
        1,
        5,
        7,
        6,
        13,
        2,
        2,
        1,
        3,
        11,
        7
      ],
      "2022-11": [
        47,
        43,
        9,
        10,
        14,
        9,
        5,
        7,
        12,
        15,
        8,
        18,
        0,
        4,
        2,
        2,
        27,
        15
      ],
      "2022-12": [
        36,
        19,
        5,
        3,
        8,
        8,
        2,
        3,
        4,
        7,
        1,
        10,
        0,
        2,
        1,
        1,
        10,
        6
      ],
      "2023-01": [
        34,
        18,
        4,
        12,
        6,
        7,
        1,
        1,
        10,
        8,
        4,
        11,
        2,
        1,
        3,
        4,
        9,
        8
      ],
      "2023-02": [
        34,
        36,
        6,
        6,
        7,
        7,
        1,
        4,
        9,
        13,
        9,
        24,
        1,
        2,
        4,
        4,
        20,
        7
      ],
      "2023-03": [
        34,
        35,
        7,
        7,
        6,
        8,
        2,
        2,
        4,
        5,
        6,
        10,
        2,
        1,
        3,
        2,
        11,
        4
      ],
      "2023-04": [
        36,
        43,
        5,
        10,
        11,
        4,
        6,
        6,
        7,
        4,
        6,
        10,
        1,
        0,
        5,
        1,
        12,
        10
      ],
      "2023-05": [
        41,
        39,
        10,
        15,
        12,
        9,
        1,
        5,
        9,
        8,
        3,
        13,
        3,
        0,
        3,
        4,
        16,
        19
      ],
      "2023-06": [
        36,
        35,
        12,
        8,
        11,
        9,
        3,
        1,
        13,
        11,
        8,
        13,
        2,
        3,
        4,
        1,
        20,
        7
      ],
      "2023-07": [
        57,
        53,
        8,
        12,
        9,
        9,
        4,
        11,
        7,
        7,
        10,
        14,
        0,
        4,
        2,
        2,
        20,
        19
      ],
      "2023-08": [
        42,
        30,
        8,
        21,
        4,
        9,
        3,
        5,
        7,
        4,
        5,
        11,
        0,
        1,
        1,
        0,
        22,
        16
      ],
      "2023-09": [
        32,
        34,
        6,
        11,
        6,
        4,
        4,
        1,
        5,
        8,
        5,
        8,
        1,
        3,
        3,
        0,
        13,
        9
      ],
      "2023-10": [
        46,
        32,
        5,
        12,
        13,
        6,
        0,
        3,
        3,
        7,
        6,
        20,
        2,
        4,
        2,
        3,
        17,
        9
      ],
      "2023-11": [
        48,
        37,
        3,
        11,
        13,
        18,
        6,
        4,
        5,
        11,
        6,
        17,
        2,
        2,
        0,
        4,
        16,
        11
      ],
      "2023-12": [
        37,
        31,
        5,
        9,
        7,
        3,
        1,
        2,
        5,
        6,
        4,
        10,
        1,
        2,
        3,
        2,
        11,
        8
      ],
      "2024-01": [
        28,
        21,
        2,
        9,
        6,
        7,
        0,
        3,
        2,
        7,
        3,
        10,
        2,
        3,
        2,
        1,
        9,
        6
      ],
      "2024-02": [
        51,
        46,
        7,
        9,
        9,
        14,
        2,
        4,
        6,
        10,
        12,
        17,
        1,
        2,
        3,
        7,
        20,
        15
      ],
      "2024-03": [
        27,
        30,
        7,
        7,
        7,
        5,
        3,
        3,
        11,
        10,
        6,
        18,
        2,
        4,
        4,
        6,
        16,
        8
      ],
      "2024-04": [
        49,
        36,
        12,
        9,
        7,
        8,
        3,
        4,
        6,
        11,
        8,
        16,
        2,
        0,
        2,
        3,
        16,
        5
      ],
      "2024-05": [
        36,
        42,
        5,
        9,
        10,
        7,
        1,
        5,
        8,
        7,
        7,
        15,
        1,
        2,
        2,
        4,
        26,
        13
      ],
      "2024-06": [
        43,
        26,
        7,
        7,
        13,
        10,
        4,
        4,
        14,
        9,
        5,
        9,
        3,
        2,
        1,
        4,
        16,
        9
      ],
      "2024-07": [
        47,
        31,
        13,
        9,
        9,
        9,
        3,
        6,
        8,
        9,
        7,
        12,
        3,
        0,
        2,
        3,
        20,
        7
      ],
      "2024-08": [
        33,
        37,
        6,
        1,
        4,
        14,
        2,
        5,
        9,
        5,
        4,
        8,
        2,
        1,
        1,
        0,
        20,
        4
      ],
      "2024-09": [
        37,
        33,
        11,
        7,
        11,
        7,
        0,
        4,
        5,
        9,
        7,
        12,
        3,
        2,
        2,
        4,
        18,
        3
      ],
      "2024-10": [
        42,
        47,
        11,
        14,
        7,
        24,
        3,
        8,
        5,
        15,
        7,
        22,
        1,
        2,
        0,
        5,
        17,
        13
      ],
      "2024-11": [
        37,
        39,
        14,
        18,
        11,
        16,
        1,
        7,
        9,
        21,
        4,
        11,
        6,
        5,
        3,
        1,
        14,
        10
      ],
      "2024-12": [
        35,
        26,
        3,
        13,
        7,
        5,
        1,
        0,
        13,
        7,
        5,
        17,
        1,
        3,
        3,
        1,
        13,
        9
      ],
      "2025-01": [
        30,
        25,
        0,
        10,
        4,
        3,
        1,
        2,
        4,
        7,
        1,
        7,
        0,
        2,
        4,
        2,
        10,
        6
      ],
      "2025-02": [
        45,
        34,
        7,
        7,
        10,
        5,
        2,
        4,
        7,
        19,
        12,
        13,
        2,
        1,
        3,
        4,
        18,
        16
      ],
      "2025-03": [
        34,
        32,
        7,
        13,
        6,
        11,
        2,
        1,
        5,
        5,
        3,
        15,
        1,
        0,
        2,
        6,
        16,
        12
      ],
      "2025-04": [
        47,
        47,
        9,
        15,
        6,
        12,
        1,
        3,
        7,
        12,
        8,
        15,
        1,
        2,
        9,
        0,
        20,
        22
      ],
      "2025-05": [
        42,
        35,
        7,
        10,
        5,
        9,
        6,
        4,
        12,
        13,
        7,
        8,
        1,
        1,
        2,
        4,
        15,
        8
      ],
      "2025-06": [
        49,
        42,
        9,
        5,
        7,
        4,
        2,
        3,
        7,
        14,
        4,
        13,
        2,
        3,
        2,
        3,
        17,
        6
      ],
      "2025-07": [
        53,
        52,
        9,
        16,
        7,
        9,
        2,
        2,
        5,
        10,
        10,
        11,
        2,
        3,
        5,
        5,
        23,
        12
      ],
      "2025-08": [
        42,
        30,
        6,
        11,
        4,
        9,
        3,
        3,
        3,
        4,
        6,
        18,
        1,
        2,
        2,
        2,
        15,
        12
      ],
      "2025-09": [
        19,
        17,
        5,
        2,
        6,
        6,
        0,
        1,
        2,
        2,
        3,
        10,
        2,
        1,
        1,
        0,
        9,
        7
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Induced-Minor-Free Graphs: Separator Theorem, Subexponential Algorithms, and Improved Hardness of Recognition",
          "year": "2023-08",
          "abstract": "A graph $G$ contains a graph $H$ as an induced minor if $H$ can be obtained\nfrom $G$ by vertex deletions and edge contractions. The class of\n$H$-induced-minor-free graphs generalizes the class of $H$-minor-free graphs,\nbut unlike $H$-minor-free graphs, it can contain dense graphs. We show that if\nan $n$-vertex $m$-edge graph $G$ does not contain a graph $H$ as an induced\nminor, then it has a balanced vertex separator of size $O_{H}(\\sqrt{m})$, where\nthe $O_{H}(\\cdot)$-notation hides factors depending on $H$. More precisely, our\nupper bound for the size of the balanced separator is $O(\\min(|V(H)|^2, \\log n)\n\\cdot \\sqrt{|V(H)|+|E(H)|} \\cdot \\sqrt{m})$. We give an algorithm for finding\neither an induced minor model of $H$ in $G$ or such a separator in randomized\npolynomial-time. We apply this to obtain subexponential $2^{O_{H}(n^{2/3} \\log\nn)}$ time algorithms on $H$-induced-minor-free graphs for a large class of\nproblems including maximum independent set, minimum feedback vertex set,\n3-coloring, and planarization.\n  For graphs $H$ where every edge is incident to a vertex of degree at most 2,\nour results imply a $2^{O_{H}(n^{2/3} \\log n)}$ time algorithm for testing if\n$G$ contains $H$ as an induced minor. Our second main result is that there\nexists a fixed tree $T$, so that there is no $2^{o(n/\\log^3 n)}$ time algorithm\nfor testing if a given $n$-vertex graph contains $T$ as an induced minor unless\nthe Exponential Time Hypothesis (ETH) fails. Our reduction also gives\nNP-hardness, which solves an open problem asked by Fellows, Kratochv\\'il,\nMiddendorf, and Pfeiffer [Algorithmica, 1995], who asked if there exists a\nfixed planar graph $H$ so that testing for $H$ as an induced minor is NP-hard.",
          "arxiv_id": "2308.04795v1"
        },
        {
          "title": "On the (In)Approximability of the Monitoring Edge Geodetic Set Problem",
          "year": "2025-07",
          "abstract": "We study the minimum \\emph{Monitoring Edge Geodetic Set} (\\megset) problem\nintroduced in [Foucaud et al., CALDAM'23]: given a graph $G$, we say that an\nedge is monitored by a pair $u,v$ of vertices if \\emph{all} shortest paths\nbetween $u$ and $v$ traverse $e$; the goal of the problem consists in finding a\nsubset $M$ of vertices of $G$ such that each edge of $G$ is monitored by at\nleast one pair of vertices in $M$, and $|M|$ is minimized.\n  In this paper, we prove that all polynomial-time approximation algorithms for\nthe minimum \\megset problem must have an approximation ratio of $\\Omega(\\log\nn)$, unless \\p = \\np. To the best of our knowledge, this is the first\nnon-constant inapproximability result known for this problem. We also\nstrengthen the known \\np-hardness of the problem on $2$-apex graphs by showing\nthat the same result holds for $1$-apex graphs. This leaves open the problem of\ndetermining whether the problem remains \\np-hard on planar (i.e., $0$-apex)\ngraphs.\n  On the positive side, we design an algorithm that computes good approximate\nsolutions for hereditary graph classes that admit efficiently computable\nbalanced separators of truly sublinear size. This immediately results in\npolynomial-time approximation algorithms achieving an approximation ratio of\n$O(n^{\\frac{1}{4}} \\sqrt{\\log n})$ on planar graphs, graphs with bounded genus,\nand $k$-apex graphs with $k=O(n^{\\frac{1}{4}})$. On graphs with bounded\ntreewidth, we obtain an approximation ratio of $O(\\log^{3/2} n)$ for any\nconstant $\\varepsilon > 0$. This compares favorably with the best-known\napproximation algorithm for general graphs, which achieves an approximation\nratio of $O(\\sqrt{n \\log n})$ via a simple reduction to the \\textsc{Set Cover}\nproblem.",
          "arxiv_id": "2507.00708v2"
        },
        {
          "title": "The Even-Path Problem in Directed Single-Crossing-Minor-Free Graphs",
          "year": "2024-06",
          "abstract": "Finding a simple path of even length between two designated vertices in a\ndirected graph is a fundamental NP-complete problem known as the EvenPath\nproblem. Nedev proved in 1999, that for directed planar graphs, the problem can\nbe solved in polynomial time. More than two decades since then, we make the\nfirst progress in extending the tractable classes of graphs for this problem.\nWe give a polynomial time algorithm to solve the EvenPath problem for classes\nof H-minor-free directed graphs,1 where H is a single-crossing graph. We make\ntwo new technical contributions along the way, that might be of independent\ninterest. The first, and perhaps our main, contribution is the construction of\nsmall, planar, parity-mimicking networks. These are graphs that mimic parities\nof all possible paths between a designated set of terminals of the original\ngraph. Finding vertex disjoint paths between given source-destination pairs of\nvertices is another fundamental problem, known to be NP-complete in directed\ngraphs, though known to be tractable in planar directed graphs. We encounter a\nnatural variant of this problem, that of finding disjoint paths between given\npairs of vertices, but with constraints on parity of the total length of paths.\nThe other significant contribution of our paper is to give a polynomial time\nalgorithm for the 3-disjoint paths with total parity problem, in directed\nplanar graphs with some restrictions (and also in directed graphs of bounded\ntreewidth).",
          "arxiv_id": "2407.00237v1"
        }
      ],
      "1": [
        {
          "title": "Removable Online Knapsack and Advice",
          "year": "2020-05",
          "abstract": "In the knapsack problem, we are given a knapsack of some capacity and a set\nof items, each with a size and a value. The goal is to pack a selection of\nthese items fitting the knapsack that maximizes the total value. The online\nversion of this problem reveals the items one by one. For each item, the\nalgorithm must decide immediately whether to pack it or not. We consider a\nnatural variant of this problem, coined removable online knapsack. It differs\nfrom the classical variant by allowing the removal of packed items. Repacking\nis impossible, however: Once an item is removed, it is gone for good.\n  We analyze the advice complexity of this problem. It measures how many advice\nbits an omniscient oracle needs to provide for an online algorithm to reach any\ngiven competitive ratio, which is, understood in its strict sense, just the\napproximation factor. We show that the competitive ratio jumps from unbounded\nwithout advice to near-optimal with just constantly many advice bits, a\nbehavior unique among all problems examined so far. We also examine algorithms\nwith barely any advice, for example just a single bit, and analyze the special\ncase of the proportional knapsack problem, where an item's size always equals\nits value.\n  We show that advice algorithms have various concrete applications and that\nlower bounds on the advice complexity of any problem are exceptionally strong.\nOur results improve some of the best known lower bounds on the competitive\nratio for randomized algorithms and even for deterministic deterministic\nalgorithms in established models such as knapsack with a resource buffer and\nvarious problems with multiple knapsacks. The seminal paper introducing\nknapsack with removability proposed such a problem for which we can even\nestablish a one-to-one correspondence with the advice model; this paper\ntherefore also provides a comprehensive analysis for this neglected problem.",
          "arxiv_id": "2005.01867v2"
        },
        {
          "title": "The Online Submodular Assignment Problem",
          "year": "2024-01",
          "abstract": "Online resource allocation is a rich and varied field. One of the most\nwell-known problems in this area is online bipartite matching, introduced in\n1990 by Karp, Vazirani, and Vazirani [KVV90]. Since then, many variants have\nbeen studied, including AdWords, the generalized assignment problem (GAP), and\nonline submodular welfare maximization.\n  In this paper, we introduce a generalization of GAP which we call the\nsubmodular assignment problem (SAP). This generalization captures many online\nassignment problems, including all classical online bipartite matching problems\nas well as broader online combinatorial optimization problems such as online\narboricity, flow scheduling, and laminar restricted allocations. We present a\nfractional algorithm for online SAP that is $(1-\\frac{1}{e})$-competitive.\n  Additionally, we study several integral special cases of the problem. In\nparticular, we provide a $(1-\\frac{1}{e}-\\epsilon)$-competitive integral\nalgorithm under a small-bids assumption, and a $(1-\\frac{1}{e})$-competitive\nintegral algorithm for online submodular welfare maximization where the utility\nfunctions are given by rank functions of matroids.\n  The key new ingredient for our results is the construction and structural\nanalysis of a \"water level\" vector for polymatroids, which allows us to\ngeneralize the classic water-filling paradigm used in online matching problems.\nThis construction reveals connections to submodular utility allocation markets\nand principal partition sequences of matroids.",
          "arxiv_id": "2401.06981v2"
        },
        {
          "title": "Cardinality Constrained Scheduling in Online Models",
          "year": "2022-01",
          "abstract": "Makespan minimization on parallel identical machines is a classical and\nintensively studied problem in scheduling, and a classic example for online\nalgorithm analysis with Graham's famous list scheduling algorithm dating back\nto the 1960s. In this problem, jobs arrive over a list and upon an arrival, the\nalgorithm needs to assign the job to a machine. The goal is to minimize the\nmakespan, that is, the maximum machine load. In this paper, we consider the\nvariant with an additional cardinality constraint: The algorithm may assign at\nmost $k$ jobs to each machine where $k$ is part of the input. While the offline\n(strongly NP-hard) variant of cardinality constrained scheduling is well\nunderstood and an EPTAS exists here, no non-trivial results are known for the\nonline variant. We fill this gap by making a comprehensive study of various\ndifferent online models. First, we show that there is a constant competitive\nalgorithm for the problem and further, present a lower bound of $2$ on the\ncompetitive ratio of any online algorithm. Motivated by the lower bound, we\nconsider a semi-online variant where upon arrival of a job of size $p$, we are\nallowed to migrate jobs of total size at most a constant times $p$. This\nconstant is called the migration factor of the algorithm. Algorithms with small\nmigration factors are a common approach to bridge the performance of online\nalgorithms and offline algorithms. One can obtain algorithms with a constant\nmigration factor by rounding the size of each incoming job and then applying an\nordinal algorithm to the resulting rounded instance. With this in mind, we also\nconsider the framework of ordinal algorithms and characterize the competitive\nratio that can be achieved using the aforementioned approaches.",
          "arxiv_id": "2201.05113v1"
        }
      ],
      "2": [
        {
          "title": "Position Heaps for Cartesian-tree Matching on Strings and Tries",
          "year": "2021-06",
          "abstract": "The Cartesian-tree pattern matching is a recently introduced scheme of\npattern matching that detects fragments in a sequential data stream which have\na similar structure as a query pattern. Formally, Cartesian-tree pattern\nmatching seeks all substrings $S'$ of the text string $S$ such that the\nCartesian tree of $S'$ and that of a query pattern $P$ coincide. In this paper,\nwe present a new indexing structure for this problem called the Cartesian-tree\nPosition Heap (CPH). Let $n$ be the length of the input text string $S$, $m$\nthe length of a query pattern $P$, and $\\sigma$ the alphabet size. We show that\nthe CPH of $S$, denoted $\\mathsf{CPH}(S)$, supports pattern matching queries in\n$O(m (\\sigma + \\log (\\min\\{h, m\\})) + occ)$ time with $O(n)$ space, where $h$\nis the height of the CPH and $occ$ is the number of pattern occurrences. We\nshow how to build $\\mathsf{CPH}(S)$ in $O(n \\log \\sigma)$ time with $O(n)$\nworking space. Further, we extend the problem to the case where the text is a\nlabeled tree (i.e. a trie). Given a trie $T$ with $N$ nodes, we show that the\nCPH of $T$, denoted $\\mathsf{CPH}(T)$, supports pattern matching queries on the\ntrie in $O(m (\\sigma^2 + \\log (\\min\\{h, m\\})) + occ)$ time with $O(N \\sigma)$\nspace. We also show a construction algorithm for $\\mathsf{CPH}(T)$ running in\n$O(N \\sigma)$ time and $O(N \\sigma)$ working space.",
          "arxiv_id": "2106.01595v2"
        },
        {
          "title": "Pattern Matching on Grammar-Compressed Strings in Linear Time",
          "year": "2021-11",
          "abstract": "The most fundamental problem considered in algorithms for text processing is\npattern matching: given a pattern $p$ of length $m$ and a text $t$ of length\n$n$, does $p$ occur in $t$? Multiple versions of this basic question have been\nconsidered, and by now we know algorithms that are fast both in practice and in\ntheory. However, the rapid increase in the amount of generated and stored data\nbrings the need of designing algorithms that operate directly on compressed\nrepresentations of data. In the compressed pattern matching problem we are\ngiven a compressed representation of the text, with $n$ being the length of the\ncompressed representation and $N$ being the length of the text, and an\nuncompressed pattern of length $m$. The most challenging (and yet relevant when\nworking with highly repetitive data, say biological information) scenario is\nwhen the chosen compression method is capable of describing a string of\nexponential length (in the size of its representation). An elegant formalism\nfor such a compression method is that of straight-line programs, which are\nsimply context-free grammars describing exactly one string. While it has been\nknown that compressed pattern matching problem can be solved in $O(m+n\\log N)$\ntime for this compression method, designing a linear-time algorithm remained\nopen. We resolve this open question by presenting an $O(n+m)$ time algorithm\nthat, given a context-free grammar of size $n$ that produces a single string\n$t$ and a pattern $p$ of length $m$, decides whether $p$ occurs in $t$ as a\nsubstring. To this end, we devise improved solutions for the weighted ancestor\nproblem and the substring concatenation problem.",
          "arxiv_id": "2111.05016v1"
        },
        {
          "title": "Text Indexing and Pattern Matching with Ephemeral Edits",
          "year": "2025-08",
          "abstract": "A sequence $e_0,e_1,\\ldots$ of edit operations in a string $T$ is called\nephemeral if operation $e_i$ constructing string $T^i$, for all $i=2k$ with\n$k\\in\\mathbb{N}$, is reverted by operation $e_{i+1}$ that reconstructs $T$.\nSuch a sequence arises when processing a stream of independent edits or testing\nhypothetical edits.\n  We introduce text indexing with ephemeral substring edits, a new version of\ntext indexing. Our goal is to design a data structure over a given text that\nsupports subsequent pattern matching queries with ephemeral substring\ninsertions, deletions, or substitutions in the text; we require insertions and\nsubstitutions to be of constant length. In particular, we preprocess a text\n$T=T[0\\mathinner{.\\,.} n)$ over an integer alphabet $\\Sigma=[0,\\sigma)$ with\n$\\sigma=n^{\\mathcal{O}(1)}$ in $\\mathcal{O}(n)$ time. Then, we can preprocess\nany arbitrary pattern $P=P[0\\mathinner{.\\,.} m)$ given online in\n$\\mathcal{O}(m\\log\\log m)$ time and $\\mathcal{O}(m)$ space and allow any\nephemeral sequence of edit operations in $T$. Before reverting the $i$th\noperation, we report all Occ occurrences of $P$ in $T^i$ in\n$\\mathcal{O}(\\log\\log n + \\text{Occ})$ time.\n  We also introduce pattern matching with ephemeral edits. In particular, we\npreprocess two strings $T$ and $P$, each of length at most $n$, over an integer\nalphabet $\\Sigma=[0,\\sigma)$ with $\\sigma=n^{\\mathcal{O}(1)}$ in\n$\\mathcal{O}(n)$ time. Then, we allow any ephemeral sequence of edit operations\nin $T$. Before reverting the $i$th operation, we report all Occ occurrences of\n$P$ in $T^i$ in the optimal $\\mathcal{O}(\\text{Occ})$ time. Along our way to\nthis result, we also give an optimal solution for pattern matching with\nephemeral block deletions.",
          "arxiv_id": "2508.05124v1"
        }
      ],
      "3": [
        {
          "title": "Improved Approximation Algorithms for Relational Clustering",
          "year": "2024-09",
          "abstract": "Clustering plays a crucial role in computer science, facilitating data\nanalysis and problem-solving across numerous fields. By partitioning large\ndatasets into meaningful groups, clustering reveals hidden structures and\nrelationships within the data, aiding tasks such as unsupervised learning,\nclassification, anomaly detection, and recommendation systems. Particularly in\nrelational databases, where data is distributed across multiple tables,\nefficient clustering is essential yet challenging due to the computational\ncomplexity of joining tables. This paper addresses this challenge by\nintroducing efficient algorithms for $k$-median and $k$-means clustering on\nrelational data without the need for pre-computing the join query results. For\nthe relational $k$-median clustering, we propose the first efficient relative\napproximation algorithm. For the relational $k$-means clustering, our algorithm\nsignificantly improves both the approximation factor and the running time of\nthe known relational $k$-means clustering algorithms, which suffer either from\nlarge constant approximation factors, or expensive running time. Given a join\nquery $Q$ and a database instance $D$ of $O(N)$ tuples, for both $k$-median and\n$k$-means clustering on the results of $Q$ on $D$, we propose randomized\n$(1+\\varepsilon)\\gamma$-approximation algorithms that run in roughly\n$O(k^2N^{\\mathsf{fhw}})+T_\\gamma(k^2)$ time, where $\\varepsilon\\in (0,1)$ is a\nconstant parameter decided by the user, $\\mathsf{fhw}$ is the fractional\nhyper-tree width of $Q$, while $\\gamma$ and $T_\\gamma(x)$ are respectively the\napproximation factor and the running time of a traditional clustering algorithm\nin the standard computational setting over $x$ points.",
          "arxiv_id": "2409.18498v1"
        },
        {
          "title": "Achieving anonymity via weak lower bound constraints for k-median and k-means",
          "year": "2020-09",
          "abstract": "We study $k$-clustering problems with lower bounds, including $k$-median and\n$k$-means clustering with lower bounds. In addition to the point set $P$ and\nthe number of centers $k$, a $k$-clustering problem with (uniform) lower bounds\ngets a number $B$. The solution space is restricted to clusterings where every\ncluster has at least $B$ points. We demonstrate how to approximate $k$-median\nwith lower bounds via a reduction to facility location with lower bounds, for\nwhich $O(1)$-approximation algorithms are known.\n  Then we propose a new constrained clustering problem with lower bounds where\nwe allow points to be assigned multiple times (to different centers). This\nmeans that for every point, the clustering specifies a set of centers to which\nit is assigned. We call this clustering with weak lower bounds. We give a\n$(6.5+\\epsilon)$-approximation for $k$-median clustering with weak lower bounds\nand an $O(1)$-approximation for $k$-means with weak lower bounds.\n  We conclude by showing that at a constant increase in the approximation\nfactor, we can restrict the number of assignments of every point to $2$ (or, if\nwe allow fractional assignments, to $1+\\epsilon$). This also leads to the first\nbicritera approximation algorithm for $k$-means with (standard) lower bounds\nwhere bicriteria is interpreted in the sense that the lower bounds are violated\nby a constant factor.\n  All algorithms in this paper run in time that is polynomial in $n$ and $k$\n(and $d$ for the Euclidean variants considered).",
          "arxiv_id": "2009.03078v3"
        },
        {
          "title": "FPT Approximations for Capacitated/Fair Clustering with Outliers",
          "year": "2023-05",
          "abstract": "Clustering problems such as $k$-Median, and $k$-Means, are motivated from\napplications such as location planning, unsupervised learning among others. In\nsuch applications, it is important to find the clustering of points that is not\n``skewed'' in terms of the number of points, i.e., no cluster should contain\ntoo many points. This is modeled by capacity constraints on the sizes of\nclusters. In an orthogonal direction, another important consideration in\nclustering is how to handle the presence of outliers in the data. Indeed, these\nclustering problems have been generalized in the literature to separately\nhandle capacity constraints and outliers. To the best of our knowledge, there\nhas been very little work on studying the approximability of clustering\nproblems that can simultaneously handle both capacities and outliers.\n  We initiate the study of the Capacitated $k$-Median with Outliers (C$k$MO)\nproblem. Here, we want to cluster all except $m$ outlier points into at most\n$k$ clusters, such that (i) the clusters respect the capacity constraints, and\n(ii) the cost of clustering, defined as the sum of distances of each\nnon-outlier point to its assigned cluster-center, is minimized.\n  We design the first constant-factor approximation algorithms for C$k$MO. In\nparticular, our algorithm returns a (3+\\epsilon)-approximation for C$k$MO in\ngeneral metric spaces, and a (1+\\epsilon)-approximation in Euclidean spaces of\nconstant dimension, that runs in time in time $f(k, m, \\epsilon) \\cdot\n|I_m|^{O(1)}$, where $|I_m|$ denotes the input size. We can also extend these\nresults to a broader class of problems, including Capacitated\nk-Means/k-Facility Location with Outliers, and Size-Balanced Fair Clustering\nproblems with Outliers. For each of these problems, we obtain an approximation\nratio that matches the best known guarantee of the corresponding outlier-free\nproblem.",
          "arxiv_id": "2305.01471v1"
        }
      ],
      "4": [
        {
          "title": "Lock-Free Augmented Trees",
          "year": "2024-05",
          "abstract": "Augmenting an existing sequential data structure with extra information to\nsupport greater functionality is a widely used technique. For example, search\ntrees are augmented to build sequential data structures like order-statistic\ntrees, interval trees, tango trees, link/cut trees and many others.\n  We study how to design concurrent augmented tree data structures. We present\na new, general technique that can augment a lock-free tree to add any new\nfields to each tree node, provided the new fields' values can be computed from\ninformation in the node and its children. This enables the design of lock-free,\nlinearizable analogues of a wide variety of classical augmented data\nstructures. As a first example, we give a wait-free trie that stores a set $S$\nof elements drawn from $\\{1,\\ldots,N\\}$ and supports linearizable\norder-statistic queries such as finding the $k$th smallest element of $S$.\nUpdates and queries take $O(\\log N)$ steps. We also apply our technique to a\nlock-free binary search tree (BST), where changes to the structure of the tree\nmake the linearization argument more challenging. Our augmented BST supports\norder statistic queries in $O(h)$ steps on a tree of height $h$. The\naugmentation does not affect the asymptotic running time of the updates.\n  For both our trie and BST, we give an alternative augmentation to improve\nsearches and order-statistic queries to run in $O(\\log |S|)$ steps (with a\nsmall increase in step complexity of updates). As an added bonus, our technique\nsupports arbitrary multi-point queries (such as range queries) with the same\ntime complexity as they would have in the corresponding sequential data\nstructure.",
          "arxiv_id": "2405.10506v1"
        },
        {
          "title": "Hypersuccinct Trees -- New universal tree source codes for optimal compressed tree data structures and range minima",
          "year": "2021-04",
          "abstract": "We present a new universal source code for distributions of unlabeled binary\nand ordinal trees that achieves optimal compression to within lower order terms\nfor all tree sources covered by existing universal codes. At the same time, it\nsupports answering many navigational queries on the compressed representation\nin constant time on the word-RAM; this is not known to be possible for any\nexisting tree compression method. The resulting data structures, \"hypersuccinct\ntrees\", hence combine the compression achieved by the best known universal\ncodes with the operation support of the best succinct tree data structures. We\napply hypersuccinct trees to obtain a universal compressed data structure for\nrange-minimum queries. It has constant query time and the optimal worst-case\nspace usage of $2n+o(n)$ bits, but the space drops to $1.736n + o(n)$ bits on\naverage for random permutations of $n$ elements, and $2\\lg\\binom nr + o(n)$ for\narrays with $r$ increasing runs, respectively. Both results are optimal; the\nformer answers an open problem of Davoodi et al. (2014) and Golin et al.\n(2016). Compared to prior work on succinct data structures, we do not have to\ntailor our data structure to specific applications; hypersuccinct trees\nautomatically adapt to the trees at hand. We show that they simultaneously\nachieve the optimal space usage to within lower order terms for a wide range of\ndistributions over tree shapes, including: binary search trees (BSTs) generated\nby insertions in random order / Cartesian trees of random arrays, random\nfringe-balanced BSTs, binary trees with a given number of binary/unary/leaf\nnodes, random binary tries generated from memoryless sources, full binary\ntrees, unary paths, as well as uniformly chosen weight-balanced BSTs, AVL\ntrees, and left-leaning red-black trees.",
          "arxiv_id": "2104.13457v2"
        },
        {
          "title": "Lazy Search Trees",
          "year": "2020-10",
          "abstract": "We introduce the lazy search tree data structure. The lazy search tree is a\ncomparison-based data structure on the pointer machine that supports\norder-based operations such as rank, select, membership, predecessor,\nsuccessor, minimum, and maximum while providing dynamic operations insert,\ndelete, change-key, split, and merge. We analyze the performance of our data\nstructure based on a partition of current elements into a set of gaps\n$\\{\\Delta_i\\}$ based on rank. A query falls into a particular gap and splits\nthe gap into two new gaps at a rank $r$ associated with the query operation. If\nwe define $B = \\sum_i |\\Delta_i| \\log_2(n/|\\Delta_i|)$, our performance over a\nsequence of $n$ insertions and $q$ distinct queries is $O(B + \\min(n \\log \\log\nn, n \\log q))$. We show $B$ is a lower bound.\n  Effectively, we reduce the insertion time of binary search trees from\n$\\Theta(\\log n)$ to $O(\\min(\\log(n/|\\Delta_i|) + \\log \\log |\\Delta_i|, \\; \\log\nq))$, where $\\Delta_i$ is the gap in which the inserted element falls. Over a\nsequence of $n$ insertions and $q$ queries, a time bound of $O(n \\log q + q\n\\log n)$ holds; better bounds are possible when queries are non-uniformly\ndistributed. As an extreme case of non-uniformity, if all queries are for the\nminimum element, the lazy search tree performs as a priority queue with $O(\\log\n\\log n)$ time insert and decrease-key operations. The same data structure\nsupports queries for any rank, interpolating between binary search trees and\nefficient priority queues.\n  Lazy search trees can be implemented to operate mostly on arrays, requiring\nonly $O(\\min(q, n))$ pointers. Via direct reduction, our data structure also\nsupports the efficient access theorems of the splay tree, providing a powerful\ndata structure for non-uniform element access, both when the number of accesses\nis small and large.",
          "arxiv_id": "2010.08840v1"
        }
      ],
      "5": [
        {
          "title": "Schrdinger as a Quantum Programmer: Estimating Entanglement via Steering",
          "year": "2023-03",
          "abstract": "Quantifying entanglement is an important task by which the resourcefulness of\na quantum state can be measured. Here, we develop a quantum algorithm that\ntests for and quantifies the separability of a general bipartite state by using\nthe quantum steering effect, the latter initially discovered by Schr\\\"odinger.\nOur separability test consists of a distributed quantum computation involving\ntwo parties: a computationally limited client, who prepares a purification of\nthe state of interest, and a computationally unbounded server, who tries to\nsteer the reduced systems to a probabilistic ensemble of pure product states.\nTo design a practical algorithm, we replace the role of the server with a\ncombination of parameterized unitary circuits and classical optimization\ntechniques to perform the necessary computation. The result is a variational\nquantum steering algorithm (VQSA), a modified separability test that is\nimplementable on quantum computers that are available today. We then simulate\nour VQSA on noisy quantum simulators and find favorable convergence properties\non the examples tested. We also develop semidefinite programs, executable on\nclassical computers, that benchmark the results obtained from our VQSA. Thus,\nour findings provide a meaningful connection between steering, entanglement,\nquantum algorithms, and quantum computational complexity theory. They also\ndemonstrate the value of a parameterized mid-circuit measurement in a VQSA.",
          "arxiv_id": "2303.07911v4"
        },
        {
          "title": "Learning k-qubit Quantum Operators via Pauli Decomposition",
          "year": "2021-02",
          "abstract": "Motivated by the limited qubit capacity of current quantum systems, we study\nthe quantum sample complexity of $k$-qubit quantum operators, i.e., operations\napplicable on only $k$ out of $d$ qubits. The problem is studied according to\nthe quantum probably approximately correct (QPAC) model abiding by quantum\nmechanical laws such as no-cloning, state collapse, and measurement\nincompatibility. With the delicacy of quantum samples and the richness of\nquantum operations, one expects a significantly larger quantum sample\ncomplexity.\n  This paper proves the contrary. We show that the quantum sample complexity of\n$k$-qubit quantum operations is comparable to the classical sample complexity\nof their counterparts (juntas), at least when $\\frac{k}{d}\\ll 1$. This is\nsurprising, especially since sample duplication is prohibited, and measurement\nincompatibility would lead to an exponentially larger sample complexity with\nstandard methods. Our approach is based on the Pauli decomposition of quantum\noperators and a technique that we name Quantum Shadow Sampling (QSS) to reduce\nthe sample complexity exponentially. The results are proved by developing (i) a\nconnection between the learning loss and the Pauli decomposition; (ii) a\nscalable QSS circuit for estimating the Pauli coefficients; and (iii) a quantum\nalgorithm for learning $k$-qubit operators with sample complexity\n$O(\\frac{k4^k}{\\epsilon^2}\\log d)$.",
          "arxiv_id": "2102.05209v4"
        },
        {
          "title": "Probing Quantum Telecloning on Superconducting Quantum Processors",
          "year": "2023-08",
          "abstract": "Quantum information can not be perfectly cloned, but approximate copies of\nquantum information can be generated. Quantum telecloning combines approximate\nquantum cloning, more typically referred as quantum cloning, and quantum\nteleportation. Quantum telecloning allows approximate copies of quantum\ninformation to be constructed by separate parties, using the classical results\nof a Bell measurement made on a prepared quantum telecloning state. Quantum\ntelecloning can be implemented as a circuit on quantum computers using a\nclassical co-processor to compute classical feed forward instructions using if\nstatements based on the results of a mid-circuit Bell measurement in real time.\nWe present universal, symmetric, optimal $1 \\rightarrow M$ telecloning\ncircuits, and experimentally demonstrate these quantum telecloning circuits for\n$M=2$ up to $M=10$, natively executed with real time classical control systems\non IBM Quantum superconducting processors, known as dynamic circuits. We\nperform the cloning procedure on many different message states across the Bloch\nsphere, on $7$ IBM Quantum processors, optionally using the error suppression\ntechnique X-X sequence digital dynamical decoupling. Two circuit optimizations\nare utilized, one which removes ancilla qubits for $M=2, 3$, and one which\nreduces the total number of gates in the circuit but still uses ancilla qubits.\nParallel single qubit tomography with MLE density matrix reconstruction is used\nin order to compute the mixed state density matrices of the clone qubits, and\nclone quality is measured using quantum fidelity. These results present one of\nthe largest and most comprehensive NISQ computer experimental analyses on\n(single qubit) quantum telecloning to date. The clone fidelity sharply\ndecreases to $0.5$ for $M > 5$, but for $M=2$ we are able to achieve a mean\nclone fidelity of up to $0.79$ using dynamical decoupling.",
          "arxiv_id": "2308.15579v3"
        }
      ],
      "6": [
        {
          "title": "Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products",
          "year": "2023-11",
          "abstract": "Inspired by fast algorithms in natural language processing, we study low rank\napproximation in the entrywise transformed setting where we want to find a good\nrank $k$ approximation to $f(U \\cdot V)$, where $U, V^\\top \\in \\mathbb{R}^{n\n\\times r}$ are given, $r = O(\\log(n))$, and $f(x)$ is a general scalar\nfunction. Previous work in sublinear low rank approximation has shown that if\nboth (1) $U = V^\\top$ and (2) $f(x)$ is a PSD kernel function, then there is an\n$O(nk^{\\omega-1})$ time constant relative error approximation algorithm, where\n$\\omega \\approx 2.376$ is the exponent of matrix multiplication. We give the\nfirst conditional time hardness results for this problem, demonstrating that\nboth conditions (1) and (2) are in fact necessary for getting better than\n$n^{2-o(1)}$ time for a relative error low rank approximation for a wide class\nof functions. We give novel reductions from the Strong Exponential Time\nHypothesis (SETH) that rely on lower bounding the leverage scores of flat\nsparse vectors and hold even when the rank of the transformed matrix $f(UV)$\nand the target rank are $n^{o(1)}$, and when $U = V^\\top$. Furthermore, even\nwhen $f(x) = x^p$ is a simple polynomial, we give runtime lower bounds in the\ncase when $U \\neq V^\\top$ of the form $\\Omega(\\min(n^{2-o(1)}, \\Omega(2^p)))$.\nLastly, we demonstrate that our lower bounds are tight by giving an $O(n \\cdot\n\\text{poly}(k, 2^p, 1/\\epsilon))$ time relative error approximation algorithm\nand a fast $O(n \\cdot \\text{poly}(k, p, 1/\\epsilon))$ additive error\napproximation using fast tensor-based sketching. Additionally, since our low\nrank algorithms rely on matrix-vector product subroutines, our lower bounds\nextend to show that computing $f(UV)W$, for even a small matrix $W$, requires\n$\\Omega(n^{2-o(1)})$ time.",
          "arxiv_id": "2311.01960v1"
        },
        {
          "title": "Near-optimal hierarchical matrix approximation from matrix-vector products",
          "year": "2024-07",
          "abstract": "We describe a randomized algorithm for producing a near-optimal hierarchical\noff-diagonal low-rank (HODLR) approximation to an $n\\times n$ matrix\n$\\mathbf{A}$, accessible only though matrix-vector products with $\\mathbf{A}$\nand $\\mathbf{A}^{\\mathsf{T}}$. We prove that, for the rank-$k$ HODLR\napproximation problem, our method achieves a $(1+\\beta)^{\\log(n)}$-optimal\napproximation in expected Frobenius norm using $O(k\\log(n)/\\beta^3)$\nmatrix-vector products. In particular, the algorithm obtains a\n$(1+\\varepsilon)$-optimal approximation with $O(k\\log^4(n)/\\varepsilon^3)$\nmatrix-vector products, and for any constant $c$, an $n^c$-optimal\napproximation with $O(k \\log(n))$ matrix-vector products. Apart from\nmatrix-vector products, the additional computational cost of our method is just\n$O(n \\operatorname{poly}(\\log(n), k, \\beta))$. We complement the upper bound\nwith a lower bound, which shows that any matrix-vector query algorithm requires\nat least $\\Omega(k\\log(n) + k/\\varepsilon)$ queries to obtain a\n$(1+\\varepsilon)$-optimal approximation.\n  Our algorithm can be viewed as a robust version of widely used \"peeling\"\nmethods for recovering HODLR matrices and is, to the best of our knowledge, the\nfirst matrix-vector query algorithm to enjoy theoretical worst-case guarantees\nfor approximation by any hierarchical matrix class. To control the propagation\nof error between levels of hierarchical approximation, we introduce a new\nperturbation bound for low-rank approximation, which shows that the widely used\nGeneralized Nystr\\\"om method enjoys inherent stability when implemented with\nnoisy matrix-vector products. We also introduce a novel randomly perforated\nmatrix sketching method to further control the error in the peeling algorithm.",
          "arxiv_id": "2407.04686v2"
        },
        {
          "title": "Reduced-Rank Regression with Operator Norm Error",
          "year": "2020-11",
          "abstract": "A common data analysis task is the reduced-rank regression problem:\n$$\\min_{\\textrm{rank-}k \\ X} \\|AX-B\\|,$$ where $A \\in \\mathbb{R}^{n \\times c}$\nand $B \\in \\mathbb{R}^{n \\times d}$ are given large matrices and $\\|\\cdot\\|$ is\nsome norm. Here the unknown matrix $X \\in \\mathbb{R}^{c \\times d}$ is\nconstrained to be of rank $k$ as it results in a significant parameter\nreduction of the solution when $c$ and $d$ are large. In the case of Frobenius\nnorm error, there is a standard closed form solution to this problem and a fast\nalgorithm to find a $(1+\\varepsilon)$-approximate solution. However, for the\nimportant case of operator norm error, no closed form solution is known and the\nfastest known algorithms take singular value decomposition time.\n  We give the first randomized algorithms for this problem running in time\n$$(nnz{(A)} + nnz{(B)} + c^2) \\cdot k/\\varepsilon^{1.5} + (n+d)k^2/\\epsilon +\nc^{\\omega},$$ up to a polylogarithmic factor involving condition numbers,\nmatrix dimensions, and dependence on $1/\\varepsilon$. Here $nnz{(M)}$ denotes\nthe number of non-zero entries of a matrix $M$, and $\\omega$ is the exponent of\nmatrix multiplication. As both (1) spectral low rank approximation ($A = B$)\nand (2) linear system solving ($n = c$ and $d = 1$) are special cases, our time\ncannot be improved by more than a $1/\\varepsilon$ factor (up to polylogarithmic\nfactors) without a major breakthrough in linear algebra. Interestingly, known\ntechniques for low rank approximation, such as alternating minimization or\nsketch-and-solve, provably fail for this problem. Instead, our algorithm uses\nan existential characterization of a solution, together with Krylov methods,\nlow degree polynomial approximation, and sketching-based preconditioning.",
          "arxiv_id": "2011.04564v2"
        }
      ],
      "7": [
        {
          "title": "Rapid mixing of Glauber dynamics via spectral independence for all degrees",
          "year": "2021-05",
          "abstract": "We prove an optimal $\\Omega(n^{-1})$ lower bound on the spectral gap of\nGlauber dynamics for anti-ferromagnetic two-spin systems with $n$ vertices in\nthe tree uniqueness regime. This spectral gap holds for all, including\nunbounded, maximum degree $\\Delta$. Consequently, we have the following mixing\ntime bounds for the models satisfying the uniqueness condition with a slack\n$\\delta\\in(0,1)$:\n  $\\bullet$ $C(\\delta) n^2\\log n$ mixing time for the hardcore model with\nfugacity $\\lambda\\le (1-\\delta)\\lambda_c(\\Delta)= (1-\\delta)\\frac{(\\Delta -\n1)^{\\Delta - 1}}{(\\Delta - 2)^\\Delta}$;\n  $\\bullet$ $C(\\delta) n^2$ mixing time for the Ising model with edge activity\n$\\beta\\in\\left[\\frac{\\Delta-2+\\delta}{\\Delta-\\delta},\\frac{\\Delta-\\delta}{\\Delta-2+\\delta}\\right]$;\nwhere the maximum degree $\\Delta$ may depend on the number of vertices $n$, and\n$C(\\delta)$ depends only on $\\delta$.\n  Our proof is built upon the recently developed connections between the\nGlauber dynamics for spin systems and the high-dimensional expander walks. In\nparticular, we prove a stronger notion of spectral independence, called the\ncomplete spectral independence, and use a novel Markov chain called the field\ndynamics to connect this stronger spectral independence to the rapid mixing of\nGlauber dynamics for all degrees.",
          "arxiv_id": "2105.15005v3"
        },
        {
          "title": "Rapid Mixing on Random Regular Graphs beyond Uniqueness",
          "year": "2025-04",
          "abstract": "The hardcore model is a fundamental probabilistic model extensively studied\nin statistical physics, probability theory, and computer science. For graphs of\nmaximum degree $\\Delta$, a well-known computational phase transition occurs at\nthe tree-uniqueness threshold $\\lambda_c(\\Delta) =\n\\frac{(\\Delta-1)^{\\Delta-1}}{(\\Delta-2)^\\Delta}$, where the mixing behavior of\nthe Glauber dynamics (a simple Markov chain) undergoes a sharp transition.\n  It is conjectured that random regular graphs exhibit different mixing\nbehavior, with the slowdown occurring far beyond the uniqueness threshold. We\nconfirm this conjecture by showing that, for the hardcore model on random\n$\\Delta$-regular graphs, the Glauber dynamics mixes rapidly with high\nprobability when $\\lambda = O(1/\\sqrt{\\Delta})$, which is significantly beyond\nthe uniqueness threshold $\\lambda_c(\\Delta) \\approx e/\\Delta$. Our result\nestablishes a sharp distinction between the hardcore model on worst-case and\nbeyond-worst-case instances, showing that the worst-case and average-case\ncomplexities of sampling and counting are fundamentally different.\n  This result of rapid mixing on random instances follows from a new criterion\nwe establish for rapid mixing of Glauber dynamics for any distribution\nsupported on a downward closed set family. Our criterion is simple, general,\nand easy to check. In addition to proving new mixing conditions for the\nhardcore model, we also establish improved mixing time bounds for sampling\nuniform matchings or $b$ matchings on graphs, the random cluster model on\nmatroids with $q \\in [0,1)$, and the determinantal point process. Our proof of\nthis new criterion for rapid mixing combines and generalizes several recent\ntools in a novel way, including a trickle down theorem for field dynamics,\nspectral/entropic stability, and a new comparison result between field dynamics\nand Glauber dynamics.",
          "arxiv_id": "2504.03406v1"
        },
        {
          "title": "Optimal Mixing of Glauber Dynamics: Entropy Factorization via High-Dimensional Expansion",
          "year": "2020-11",
          "abstract": "We prove an optimal mixing time bound on the single-site update Markov chain\nknown as the Glauber dynamics or Gibbs sampling in a variety of settings. Our\nwork presents an improved version of the spectral independence approach of\nAnari et al. (2020) and shows $O(n\\log{n})$ mixing time on any $n$-vertex graph\nof bounded degree when the maximum eigenvalue of an associated influence matrix\nis bounded. As an application of our results, for the hard-core model on\nindependent sets weighted by a fugacity $\\lambda$, we establish $O(n\\log{n})$\nmixing time for the Glauber dynamics on any $n$-vertex graph of constant\nmaximum degree $\\Delta$ when $\\lambda<\\lambda_c(\\Delta)$ where\n$\\lambda_c(\\Delta)$ is the critical point for the uniqueness/non-uniqueness\nphase transition on the $\\Delta$-regular tree. More generally, for any\nantiferromagnetic 2-spin system we prove $O(n\\log{n})$ mixing time of the\nGlauber dynamics on any bounded degree graph in the corresponding tree\nuniqueness region. Our results apply more broadly; for example, we also obtain\n$O(n\\log{n})$ mixing for $q$-colorings of triangle-free graphs of maximum\ndegree $\\Delta$ when the number of colors satisfies $q > \\alpha \\Delta$ where\n$\\alpha \\approx 1.763$, and $O(m\\log{n})$ mixing for generating random\nmatchings of any graph with bounded degree and $m$ edges.",
          "arxiv_id": "2011.02075v4"
        }
      ],
      "8": [
        {
          "title": "Meeting Utility Constraints in Differential Privacy: A Privacy-Boosting Approach",
          "year": "2024-12",
          "abstract": "Data engineering often requires accuracy (utility) constraints on results,\nposing significant challenges in designing differentially private (DP)\nmechanisms, particularly under stringent privacy parameter $\\epsilon$. In this\npaper, we propose a privacy-boosting framework that is compatible with most\nnoise-adding DP mechanisms. Our framework enhances the likelihood of outputs\nfalling within a preferred subset of the support to meet utility requirements\nwhile enlarging the overall variance to reduce privacy leakage. We characterize\nthe privacy loss distribution of our framework and present the privacy profile\nformulation for $(\\epsilon,\\delta)$-DP and R\\'enyi DP (RDP) guarantees. We\nstudy special cases involving data-dependent and data-independent utility\nformulations. Through extensive experiments, we demonstrate that our framework\nachieves lower privacy loss than standard DP mechanisms under utility\nconstraints. Notably, our approach is particularly effective in reducing\nprivacy loss with large query sensitivity relative to the true answer, offering\na more practical and flexible approach to designing differentially private\nmechanisms that meet specific utility constraints.",
          "arxiv_id": "2412.10612v1"
        },
        {
          "title": "What Do Our Choices Say About Our Preferences?",
          "year": "2020-05",
          "abstract": "Taking online decisions is a part of everyday life. Think of buying a house,\nparking a car or taking part in an auction. We often take those decisions\npublicly, which may breach our privacy - a party observing our choices may\nlearn a lot about our preferences. In this paper we investigate the online\nstopping algorithms from the privacy preserving perspective, using a\nmathematically rigorous differential privacy notion. In differentially private\nalgorithms there is usually an issue of balancing the privacy and utility. In\nthis regime, in most cases, having both optimality and high level of privacy at\nthe same time is impossible. We propose a natural mechanism to achieve a\ncontrollable trade-off, quantified by a parameter, between the accuracy of the\nonline algorithm and its privacy. Depending on the parameter, our mechanism can\nbe optimal with weaker differential privacy or suboptimal, yet more\nprivacy-preserving. We conduct a detailed accuracy and privacy analysis of our\nmechanism applied to the optimal algorithm for the classical secretary problem.\nThereby the classical notions from two distinct areas - optimal stopping and\ndifferential privacy - meet for the first time.",
          "arxiv_id": "2005.01586v4"
        },
        {
          "title": "A Randomized Approach for Tight Privacy Accounting",
          "year": "2023-04",
          "abstract": "Bounding privacy leakage over compositions, i.e., privacy accounting, is a\nkey challenge in differential privacy (DP). The privacy parameter ($\\eps$ or\n$\\delta$) is often easy to estimate but hard to bound. In this paper, we\npropose a new differential privacy paradigm called estimate-verify-release\n(EVR), which addresses the challenges of providing a strict upper bound for\nprivacy parameter in DP compositions by converting an estimate of privacy\nparameter into a formal guarantee. The EVR paradigm first estimates the privacy\nparameter of a mechanism, then verifies whether it meets this guarantee, and\nfinally releases the query output based on the verification result. The core\ncomponent of the EVR is privacy verification. We develop a randomized privacy\nverifier using Monte Carlo (MC) technique. Furthermore, we propose an MC-based\nDP accountant that outperforms existing DP accounting techniques in terms of\naccuracy and efficiency. Our empirical evaluation shows the newly proposed EVR\nparadigm improves the utility-privacy tradeoff for privacy-preserving machine\nlearning.",
          "arxiv_id": "2304.07927v2"
        }
      ],
      "9": [
        {
          "title": "A Theory of Universal Learning",
          "year": "2020-11",
          "abstract": "How quickly can a given class of concepts be learned from examples? It is\ncommon to measure the performance of a supervised machine learning algorithm by\nplotting its \"learning curve\", that is, the decay of the error rate as a\nfunction of the number of training examples. However, the classical theoretical\nframework for understanding learnability, the PAC model of Vapnik-Chervonenkis\nand Valiant, does not explain the behavior of learning curves: the\ndistribution-free PAC model of learning can only bound the upper envelope of\nthe learning curves over all possible data distributions. This does not match\nthe practice of machine learning, where the data source is typically fixed in\nany given scenario, while the learner may choose the number of training\nexamples on the basis of factors such as computational resources and desired\naccuracy.\n  In this paper, we study an alternative learning model that better captures\nsuch practical aspects of machine learning, but still gives rise to a complete\ntheory of the learnable in the spirit of the PAC model. More precisely, we\nconsider the problem of universal learning, which aims to understand the\nperformance of learning algorithms on every data distribution, but without\nrequiring uniformity over the distribution. The main result of this paper is a\nremarkable trichotomy: there are only three possible rates of universal\nlearning. More precisely, we show that the learning curves of any given concept\nclass decay either at an exponential, linear, or arbitrarily slow rates.\nMoreover, each of these cases is completely characterized by appropriate\ncombinatorial parameters, and we exhibit optimal learning algorithms that\nachieve the best possible rate in each case.\n  For concreteness, we consider in this paper only the realizable case, though\nanalogous results are expected to extend to more general learning scenarios.",
          "arxiv_id": "2011.04483v1"
        },
        {
          "title": "Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds",
          "year": "2024-04",
          "abstract": "Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of\ntestable learning with distribution shift (TDS learning), where a learner is\ngiven labeled samples from training distribution $\\mathcal{D}$, unlabeled\nsamples from test distribution $\\mathcal{D}'$, and the goal is to output a\nclassifier with low error on $\\mathcal{D}'$ whenever the training samples pass\na corresponding test. Their model deviates from all prior work in that no\nassumptions are made on $\\mathcal{D}'$. Instead, the test must accept (with\nhigh probability) when the marginals of the training and test distributions are\nequal.\n  Here we focus on the fundamental case of intersections of halfspaces with\nrespect to Gaussian training distributions and prove a variety of new upper\nbounds including a $2^{(k/\\epsilon)^{O(1)}} \\mathsf{poly}(d)$-time algorithm\nfor TDS learning intersections of $k$ homogeneous halfspaces to accuracy\n$\\epsilon$ (prior work achieved $d^{(k/\\epsilon)^{O(1)}}$). We work under the\nmild assumption that the Gaussian training distribution contains at least an\n$\\epsilon$ fraction of both positive and negative examples\n($\\epsilon$-balanced). We also prove the first set of SQ lower-bounds for any\nTDS learning problem and show (1) the $\\epsilon$-balanced assumption is\nnecessary for $\\mathsf{poly}(d,1/\\epsilon)$-time TDS learning for a single\nhalfspace and (2) a $d^{\\tilde{\\Omega}(\\log 1/\\epsilon)}$ lower bound for the\nintersection of two general halfspaces, even with the $\\epsilon$-balanced\nassumption.\n  Our techniques significantly expand the toolkit for TDS learning. We use\ndimension reduction and coverings to give efficient algorithms for computing a\nlocalized version of discrepancy distance, a key metric from the domain\nadaptation literature.",
          "arxiv_id": "2404.02364v2"
        },
        {
          "title": "VC Dimension and Distribution-Free Sample-Based Testing",
          "year": "2020-12",
          "abstract": "We consider the problem of determining which classes of functions can be\ntested more efficiently than they can be learned, in the distribution-free\nsample-based model that corresponds to the standard PAC learning setting. Our\nmain result shows that while VC dimension by itself does not always provide\ntight bounds on the number of samples required to test a class of functions in\nthis model, it can be combined with a closely-related variant that we call\n\"lower VC\" (or LVC) dimension to obtain strong lower bounds on this sample\ncomplexity.\n  We use this result to obtain strong and in many cases nearly optimal lower\nbounds on the sample complexity for testing unions of intervals, halfspaces,\nintersections of halfspaces, polynomial threshold functions, and decision\ntrees. Conversely, we show that two natural classes of functions, juntas and\nmonotone functions, can be tested with a number of samples that is polynomially\nsmaller than the number of samples required for PAC learning.\n  Finally, we also use the connection between VC dimension and property testing\nto establish new lower bounds for testing radius clusterability and testing\nfeasibility of linear constraint systems.",
          "arxiv_id": "2012.03923v1"
        }
      ],
      "10": [
        {
          "title": "Accelerating Clique Counting in Sparse Real-World Graphs via Communication-Reducing Optimizations",
          "year": "2021-12",
          "abstract": "Counting instances of specific subgraphs in a larger graph is an important\nproblem in graph mining. Finding cliques of size k (k-cliques) is one example\nof this NP-hard problem. Different algorithms for clique counting avoid\ncounting the same clique multiple times by pivoting or ordering the graph.\nOrdering-based algorithms include an ordering step to direct the edges in the\ninput graph, and a counting step, which is dominated by building node or\nedge-induced subgraphs. Of the ordering-based algorithms, kClist is the\nstate-of-the art algorithm designed to work on sparse real-world graphs.\nDespite its leading overall performance, kClist's vertex-parallel\nimplementation does not scale well in practice on graphs with a few million\nvertices.\n  We present CITRON (Clique counting with Traffic Reducing Optimizations) to\nimprove the parallel scalability and thus overall performance of clique\ncounting. We accelerate the ordering phase by abandoning kClist's sequential\ncore ordering and using a parallelized degree ordering. We accelerate the\ncounting phase with our reorganized subgraph data structures that reduce memory\ntraffic to improve scaling bottlenecks. Our sorted, compact neighbor lists\nimprove locality and communication efficiency which results in near-linear\nparallel scaling. CITRON significantly outperforms kClist while counting\nmoderately sized cliques, and thus increases the size of graph practical for\nclique counting.\n  We have recently become aware of ArbCount (arXiv:2002.10047), which often\noutperforms us. However, we believe that the analysis included in this paper\nwill be helpful for anyone who wishes to understand the performance\ncharacteristics of k-clique counting.",
          "arxiv_id": "2112.10913v2"
        },
        {
          "title": "BFS based distributed algorithm for parallel local directed sub-graph enumeration",
          "year": "2022-01",
          "abstract": "Estimating the frequency of sub-graphs is of importance for many tasks,\nincluding sub-graph isomorphism, kernel-based anomaly detection, and network\nstructure analysis. While multiple algorithms were proposed for full\nenumeration or sampling-based estimates, these methods fail in very large\ngraphs. Recent advances in parallelization allow for estimates of total\nsub-graphs counts in very large graphs. The task of counting the frequency of\neach sub-graph associated with each vertex also received excellent solutions\nfor undirected graphs. However, there is currently no good solution for very\nlarge directed graphs.\n  We here propose VDMC (Vertex specific Distributed Motif Counting) -- a fully\ndistributed algorithm to optimally count all the 3 and 4 vertices connected\ndirected graphs (sub-graph motifs) associated with each vertex of a graph. VDMC\ncounts each motif only once and its efficacy is linear in the number of counted\nmotifs. It is fully parallelized to be efficient in GPU-based computation. VDMC\nis based on three main elements: 1) Ordering the vertices and only counting\nmotifs containing increasing order vertices, 2) sub-ordering motifs based on\nthe average length of the BFS composing the motif, and 3) removing isomorphisms\nonly once for the entire graph. We here compare VDMC to analytical estimates of\nthe expected number of motifs and show its accuracy. VDMC is available as a\nhighly efficient CPU and GPU code with a novel data structure for efficient\ngraph manipulation. We show the efficacy of VDMC and real-world graphs. VDMC\nallows for the precise analysis of sub-graph frequency around each vertex in\nlarge graphs and opens the way for the extension of methods until now limited\nto graphs of thousands of edges to graphs with millions of edges and above.\n  GIT: https://github.com/louzounlab/graph-measures",
          "arxiv_id": "2201.11655v1"
        },
        {
          "title": "Scaling Up Maximal k-plex Enumeration",
          "year": "2022-03",
          "abstract": "Finding all maximal $k$-plexes on networks is a fundamental research problem\nin graph analysis due to many important applications, such as community\ndetection, biological graph analysis, and so on. A $k$-plex is a subgraph in\nwhich every vertex is adjacent to all but at most $k$ vertices within the\nsubgraph. In this paper, we study the problem of enumerating all large maximal\n$k$-plexes of a graph and develop several new and efficient techniques to solve\nthe problem. Specifically, we first propose several novel upper-bounding\ntechniques to prune unnecessary computations during the enumeration procedure.\nWe show that the proposed upper bounds can be computed in linear time. Then, we\ndevelop a new branch-and-bound algorithm with a carefully-designed pivot\nre-selection strategy to enumerate all $k$-plexes, which outputs all $k$-plexes\nin $O(n^2\\gamma_k^n)$ time theoretically, where $n$ is the number of vertices\nof the graph and $\\gamma_k$ is strictly smaller than 2. In addition, a parallel\nversion of the proposed algorithm is further developed to scale up to process\nlarge real-world graphs. Finally, extensive experimental results show that the\nproposed sequential algorithm can achieve up to $2\\times$ to $100\\times$\nspeedup over the state-of-the-art sequential algorithms on most benchmark\ngraphs. The results also demonstrate the high scalability of the proposed\nparallel algorithm. For example, on a large real-world graph with more than 200\nmillion edges, our parallel algorithm can finish the computation within two\nminutes, while the state-of-the-art parallel algorithm cannot terminate within\n24 hours.",
          "arxiv_id": "2203.10760v3"
        }
      ],
      "11": [
        {
          "title": "Beating Bellman's Algorithm for Subset Sum",
          "year": "2024-10",
          "abstract": "Bellman's algorithm for Subset Sum is one of the earliest and simplest\nexamples of dynamic programming, dating back to 1957. For a given set of $n$\nintegers $X$ and a target $t$, it computes the set of subset sums $\\mathcal\nS(X, t)$ (i.e., the set of integers $s \\in [0\\ldots t]$ for which there is a\nsubset of $X$ summing to $s$) in time $O(|\\mathcal S(X, t)| \\cdot n)$. Since\nthen, it has been an important question whether Bellman's seminal algorithm can\nbe improved.\n  This question is addressed in many recent works. And yet, while some\nalgorithms improve upon Bellman's algorithm in specific parameter regimes, such\nas Bringmann's $\\tilde O(t + n)$-time algorithm [SODA '17] and Bringmann and\nNakos' $\\tilde O(|\\mathcal S(X, t)|^{4/3})$-time algorithm [STOC '20], none of\nthe known algorithms beats Bellman's algorithm in all regimes. In particular,\nit remained open whether Subset Sum is in time $\\tilde O(|\\mathcal S(X, t)|\n\\cdot n^{1-\\epsilon})$ (for some $\\epsilon > 0$).\n  In this work we positively resolve this question and design an algorithm that\noutperforms Bellman's algorithm in all regimes. Our algorithm runs in time\n$\\tilde O(|\\mathcal S(X, t)| \\cdot \\sqrt{n})$, thus improving the time\ncomplexity by a factor of nearly $\\sqrt n$. Our key innovation is the use of a\nresult from additive combinatorics, which has not been applied in an\nalgorithmic context before and which we believe to be of further independent\ninterest for algorithm design. To demonstrate the broader applicability of our\napproach, we extend our ideas to a variant of Subset Sum on vectors as well as\nto Unbounded Subset Sum.",
          "arxiv_id": "2410.21942v1"
        },
        {
          "title": "Improving Lagarias-Odlyzko Algorithm For Average-Case Subset Sum: Modular Arithmetic Approach",
          "year": "2024-08",
          "abstract": "Lagarias and Odlyzko (J.~ACM~1985) proposed a polynomial time algorithm for\nsolving ``\\emph{almost all}'' instances of the Subset Sum problem with $n$\nintegers of size $\\Omega(\\Gamma_{\\text{LO}})$, where\n$\\log_2(\\Gamma_{\\text{LO}}) > n^2 \\log_2(\\gamma)$ and $\\gamma$ is a parameter\nof the lattice basis reduction ($\\gamma > \\sqrt{4/3}$ for LLL). The algorithm\nof Lagarias and Odlyzko is a cornerstone result in cryptography. However, the\ntheoretical guarantee on the density of feasible instances has remained\nunimproved for almost 40 years.\n  In this paper, we propose an algorithm to solve ``almost all'' instances of\nSubset Sum with integers of size $\\Omega(\\sqrt{\\Gamma_{\\text{LO}}})$ after a\nsingle call to the lattice reduction. Additionally, our argument allows us to\nsolve the Subset Sum problem for multiple targets while the previous approach\ncould only answer one target per call to lattice basis reduction. We introduce\na modular arithmetic approach to the Subset Sum problem. The idea is to use the\nlattice reduction to solve a linear system modulo a suitably large prime. We\nshow that density guarantees can be improved, by analysing the lengths of the\nLLL reduced basis vectors, of both the primal and the dual lattices\nsimultaneously.",
          "arxiv_id": "2408.16108v1"
        },
        {
          "title": "Efficient reductions and algorithms for variants of Subset Sum",
          "year": "2021-12",
          "abstract": "Given $(a_1, \\dots, a_n, t) \\in \\mathbb{Z}_{\\geq 0}^{n + 1}$, the Subset Sum\nproblem ($\\mathsf{SSUM}$) is to decide whether there exists $S \\subseteq [n]$\nsuch that $\\sum_{i \\in S} a_i = t$. There is a close variant of the\n$\\mathsf{SSUM}$, called $\\mathsf{Subset~Product}$. Given positive integers\n$a_1, ..., a_n$ and a target integer $t$, the $\\mathsf{Subset~Product}$ problem\nasks to determine whether there exists a subset $S \\subseteq [n]$ such that\n$\\prod_{i \\in S} a_i=t$. There is a pseudopolynomial time dynamic programming\nalgorithm, due to Bellman (1957) which solves the $\\mathsf{SSUM}$ and\n$\\mathsf{Subset~Product}$ in $O(nt)$ time and $O(t)$ space.\n  In the first part, we present {\\em search} algorithms for variants of the\nSubset Sum problem. Our algorithms are parameterized by $k$, which is a given\nupper bound on the number of realisable sets (i.e.,~number of solutions,\nsumming exactly $t$). We show that $\\mathsf{SSUM}$ with a unique solution is\nalready NP-hard, under randomized reduction. This makes the regime of\nparametrized algorithms, in terms of $k$, very interesting.\n  Subsequently, we present an $\\tilde{O}(k\\cdot (n+t))$ time deterministic\nalgorithm, which finds the hamming weight of all the realisable sets for a\nsubset sum instance. We also give a poly$(knt)$-time and $O(\\log(knt))$-space\ndeterministic algorithm that finds all the realisable sets for a subset sum\ninstance.\n  In the latter part, we present a simple and elegant randomized $\\tilde{O}(n +\nt)$ time algorithm for $\\mathsf{Subset~Product}$. Moreover, we also present a\npoly$(nt)$ time and $O(\\log^2 (nt))$ space deterministic algorithm for the\nsame. We study these problems in the unbounded setting as well. Our algorithms\nuse multivariate FFT, power series and number-theoretic techniques, introduced\nby Jin and Wu (SOSA'19) and Kane (2010).",
          "arxiv_id": "2112.11020v2"
        }
      ],
      "12": [
        {
          "title": "Optimal streaming algorithm for detecting $\\ell_2$ heavy hitters in random order streams",
          "year": "2025-09",
          "abstract": "Given a stream $x_1,x_2,\\dots,x_n$ of items from a Universe $U$ of size\n$\\mathsf{poly}(n)$, and a parameter $\\epsilon>0$, an item $i\\in U$ is said to\nbe an $\\ell_2$ heavy hitter if its frequency $f_i$ in the stream is at least\n$\\sqrt{\\epsilon F_2}$, where $F_2=\\sqrt{\\sum_{i\\in U} f_i^2}$. The classical\n$\\mathsf{CountSketch}$ algorithm due to Charikar, Chen, and Farach-Colton\n[2004], was the first algorithm to detect $\\ell_2$ heavy hitters using\n$O\\left(\\frac{\\log^2 n}{\\epsilon}\\right)$ bits of space, and their algorithm is\noptimal for streams with deletions. For insertion-only streams, Braverman,\nChestnut, Ivkin, Nelson, Wang, and Woodruff [2017] gave the $\\mathsf{BPTree}$\nalgorithm which requires only $O\\left(\\frac{\\log(1/\\epsilon)}{\\epsilon}\\log n\n\\right)$ space. Note that any algorithm requires at least\n$O\\left(\\frac{1}{\\epsilon} \\log n\\right)$ space to output $O(1/\\epsilon)$ heavy\nhitters in the worst case. So for constant $\\epsilon$, the space usage of the\n$\\mathsf{BPTree}$ algorithm is optimal but their bound could be sub-optimal for\n$\\epsilon=o(1)$. In this work, we show that for random order streams, where the\nstream elements can be adversarial but their order of arrival is uniformly\nrandom, it is possible to achieve the optimal space bound of\n$O\\left(\\frac{1}{\\epsilon} \\log n\\right)$ for every $\\epsilon =\n\\Omega\\left(\\frac{1}{2^{\\sqrt{\\log n}}}\\right)$. We also show that for\npartially random order streams where only the heavy hitters are required to be\nuniformly distributed in the stream, it is possible to achieve the same space\nbound, but with an additional assumption that the algorithm is given a constant\napproximation to $F_2$ in advance.",
          "arxiv_id": "2509.07286v1"
        },
        {
          "title": "A Framework for Adversarially Robust Streaming Algorithms",
          "year": "2020-03",
          "abstract": "We investigate the adversarial robustness of streaming algorithms. In this\ncontext, an algorithm is considered robust if its performance guarantees hold\neven if the stream is chosen adaptively by an adversary that observes the\noutputs of the algorithm along the stream and can react in an online manner.\nWhile deterministic streaming algorithms are inherently robust, many central\nproblems in the streaming literature do not admit sublinear-space deterministic\nalgorithms; on the other hand, classical space-efficient randomized algorithms\nfor these problems are generally not adversarially robust. This raises the\nnatural question of whether there exist efficient adversarially robust\n(randomized) streaming algorithms for these problems.\n  In this work, we show that the answer is positive for various important\nstreaming problems in the insertion-only model, including distinct elements and\nmore generally $F_p$-estimation, $F_p$-heavy hitters, entropy estimation, and\nothers. For all of these problems, we develop adversarially robust\n$(1+\\varepsilon)$-approximation algorithms whose required space matches that of\nthe best known non-robust algorithms up to a $\\text{poly}(\\log n,\n1/\\varepsilon)$ multiplicative factor (and in some cases even up to a constant\nfactor). Towards this end, we develop several generic tools allowing one to\nefficiently transform a non-robust streaming algorithm into a robust one in\nvarious scenarios.",
          "arxiv_id": "2003.14265v3"
        },
        {
          "title": "Double-Hashing Algorithm for Frequency Estimation in Data Streams",
          "year": "2022-04",
          "abstract": "Frequency estimation of elements is an important task for summarizing data\nstreams and machine learning applications. The problem is often addressed by\nusing streaming algorithms with sublinear space data structures. These\nalgorithms allow processing of large data while using limited data storage.\nCommonly used streaming algorithms, such as count-min sketch, have many\nadvantages, but do not take into account properties of a data stream for\nperformance optimization. In the present paper we introduce a novel\ndouble-hashing algorithm that provides flexibility to optimize streaming\nalgorithms depending on the properties of a given stream. In the double-hashing\napproach, first a standard streaming algorithm is employed to obtain an\nestimate of the element frequencies. This estimate is derived using a fraction\nof the stream and allows identification of the heavy hitters. Next, it uses a\nmodified hash table where the heavy hitters are mapped into individual buckets\nand other stream elements are mapped into the remaining buckets. Finally, the\nelement frequencies are estimated based on the constructed hash table over the\nentire data stream with any streaming algorithm. We demonstrate on both\nsynthetic data and an internet query log dataset that our approach is capable\nof improving frequency estimation due to removing heavy hitters from the\nhashing process and, thus, reducing collisions in the hash table. Our approach\navoids employing additional machine learning models to identify heavy hitters\nand, thus, reduces algorithm complexity and streamlines implementation.\nMoreover, because it is not dependent on specific features of the stream\nelements for identifying heavy hitters, it is applicable to a large variety of\nstreams. In addition, we propose a procedure on how to dynamically adjust the\nproposed double-hashing algorithm when frequencies of the elements in a stream\nare changing over time.",
          "arxiv_id": "2204.00650v1"
        }
      ],
      "13": [
        {
          "title": "SARRIGUREN: a polynomial-time complete algorithm for random $k$-SAT with relatively dense clauses",
          "year": "2024-01",
          "abstract": "SARRIGUREN, a new complete algorithm for SAT based on counting clauses (which\nis valid also for Unique-SAT and #SAT) is described, analyzed and tested.\nAlthough existing complete algorithms for SAT perform slower with clauses with\nmany literals, that is an advantage for SARRIGUREN, because the more literals\nare in the clauses the bigger is the probability of overlapping among clauses,\na property that makes the clause counting process more efficient. Actually, it\nprovides a $O(m^2 \\times n/k)$ time complexity for random $k$-SAT instances of\n$n$ variables and $m$ relatively dense clauses, where that density level is\nrelative to the number of variables $n$, that is, clauses are relatively dense\nwhen $k\\geq7\\sqrt{n}$. Although theoretically there could be worst-cases with\nexponential complexity, the probability of those cases to happen in random\n$k$-SAT with relatively dense clauses is practically zero. The algorithm has\nbeen empirically tested and that polynomial time complexity maintains also for\n$k$-SAT instances with less dense clauses ($k\\geq5\\sqrt{n}$). That density\ncould, for example, be of only 0.049 working with $n=20000$ variables and\n$k=989$ literals. In addition, they are presented two more complementary\nalgorithms that provide the solutions to $k$-SAT instances and valuable\ninformation about number of solutions for each literal. Although this algorithm\ndoes not solve the NP=P problem (it is not a polynomial algorithm for 3-SAT),\nit broads the knowledge about that subject, because $k$-SAT with $k>3$ and\ndense clauses is not harder than 3-SAT. Moreover, the Python implementation of\nthe algorithms, and all the input datasets and obtained results in the\nexperiments are made available.",
          "arxiv_id": "2401.09234v2"
        },
        {
          "title": "MAJORITY-3SAT (and Related Problems) in Polynomial Time",
          "year": "2021-07",
          "abstract": "Majority-SAT is the problem of determining whether an input $n$-variable\nformula in conjunctive normal form (CNF) has at least $2^{n-1}$ satisfying\nassignments. Majority-SAT and related problems have been studied extensively in\nvarious AI communities interested in the complexity of probabilistic planning\nand inference. Although Majority-SAT has been known to be PP-complete for over\n40 years, the complexity of a natural variant has remained open:\nMajority-$k$SAT, where the input CNF formula is restricted to have clause width\nat most $k$.\n  We prove that for every $k$, Majority-$k$SAT is in P. In fact, for any\npositive integer $k$ and rational $\\rho \\in (0,1)$ with bounded denominator, we\ngive an algorithm that can determine whether a given $k$-CNF has at least $\\rho\n\\cdot 2^n$ satisfying assignments, in deterministic linear time (whereas the\nprevious best-known algorithm ran in exponential time). Our algorithms have\ninteresting positive implications for counting complexity and the complexity of\ninference, significantly reducing the known complexities of related problems\nsuch as E-MAJ-$k$SAT and MAJ-MAJ-$k$SAT. At the heart of our approach is an\nefficient method for solving threshold counting problems by extracting\nsunflowers found in the corresponding set system of a $k$-CNF.\n  We also show that the tractability of Majority-$k$SAT is somewhat fragile.\nFor the closely related GtMajority-SAT problem (where we ask whether a given\nformula has greater than $2^{n-1}$ satisfying assignments) which is known to be\nPP-complete, we show that GtMajority-$k$SAT is in P for $k\\le 3$, but becomes\nNP-complete for $k\\geq 4$. These results are counterintuitive, because the\n``natural'' classifications of these problems would have been PP-completeness,\nand because there is a stark difference in the complexity of GtMajority-$k$SAT\nand Majority-$k$SAT for all $k\\ge 4$.",
          "arxiv_id": "2107.02748v2"
        },
        {
          "title": "On the Parameterized Complexity of Diverse SAT",
          "year": "2024-12",
          "abstract": "We study the Boolean Satisfiability problem (SAT) in the framework of\ndiversity, where one asks for multiple solutions that are mutually far apart\n(i.e., sufficiently dissimilar from each other) for a suitable notion of\ndistance/dissimilarity between solutions. Interpreting assignments as bit\nvectors, we take their Hamming distance to quantify dissimilarity, and we focus\non problem of finding two solutions. Specifically, we define the problem MAX\nDIFFER SAT (resp. EXACT DIFFER SAT) as follows: Given a Boolean formula $\\phi$\non $n$ variables, decide whether $\\phi$ has two satisfying assignments that\ndiffer on at least (resp. exactly) $d$ variables. We study classical and\nparameterized (in parameters $d$ and $n-d$) complexities of MAX DIFFER SAT and\nEXACT DIFFER SAT, when restricted to some formula-classes on which SAT is known\nto be polynomial-time solvable. In particular, we consider affine formulas,\n$2$-CNF formulas and hitting formulas.\n  For affine formulas, we show the following: Both problems are polynomial-time\nsolvable when each equation has at most two variables. EXACT DIFFER SAT is\nNP-hard, even when each equation has at most three variables and each variable\nappears in at most four equations. Also, MAX DIFFER SAT is NP-hard, even when\neach equation has at most four variables. Both problems are W[1]-hard in the\nparameter $n-d$. In contrast, when parameterized by $d$, EXACT DIFFER SAT is\nW[1]-hard, but MAX DIFFER SAT admits a single-exponential FPT algorithm and a\npolynomial-kernel.\n  For 2-CNF formulas, we show the following: Both problems are polynomial-time\nsolvable when each variable appears in at most two clauses. Also, both problems\nare W[1]-hard in the parameter $d$ (and therefore, it turns out, also NP-hard),\neven on monotone inputs (i.e., formulas with no negative literals). Finally,\nfor hitting formulas, we show that both problems are polynomial-time solvable.",
          "arxiv_id": "2412.09717v1"
        }
      ],
      "14": [
        {
          "title": "Sparse Temporal Spanners with Low Stretch",
          "year": "2022-06",
          "abstract": "A temporal graph is an undirected graph $G=(V,E)$ along with a function that\nassigns a time-label to each edge in $E$. A path in $G$ with non-decreasing\ntime-labels is called temporal path and the distance from $u$ to $v$ is the\nminimum length (i.e., the number of edges) of a temporal path from $u$ to $v$.\nA temporal $\\alpha$-spanner of $G$ is a (temporal) subgraph $H$ that preserves\nthe distances between any pair of vertices in $V$, up to a multiplicative\nstretch factor of $\\alpha$. The size of $H$ is the number of its edges.\n  In this work we study the size-stretch trade-offs of temporal spanners. We\nshow that temporal cliques always admit a temporal $(2k-1)-$spanner with\n$\\tilde{O}(kn^{1+\\frac{1}{k}})$ edges, where $k>1$ is an integer parameter of\nchoice. Choosing $k=\\lfloor\\log n\\rfloor$, we obtain a temporal $O(\\log\nn)$-spanner with $\\tilde{O}(n)$ edges that has almost the same size (up to\nlogarithmic factors) as the temporal spanner in [Casteigts et al., JCSS 2021]\nwhich only preserves temporal connectivity.\n  We then consider general temporal graphs. Since $\\Omega(n^2)$ edges might be\nneeded by any connectivity-preserving temporal subgraph [Axiotis et al.,\nICALP'16], we focus on approximating distances from a single source. We show\nthat $\\tilde{O}(n/\\log(1+\\varepsilon))$ edges suffice to obtain a stretch of\n$(1+\\varepsilon)$, for any small $\\varepsilon>0$. This result is essentially\ntight since there are temporal graphs for which any temporal subgraph\npreserving exact distances from a single-source must use $\\Omega(n^2)$ edges.\nWe extend our analysis to prove an upper bound of $\\tilde{O}(n^2/\\beta)$ on the\nsize of any temporal $\\beta$-additive spanner, which is tight up to\npolylogarithmic factors.\n  Finally, we investigate how the lifetime of $G$, i.e., the number of its\ndistinct time-labels, affects the trade-off between the size and the stretch of\na temporal spanner.",
          "arxiv_id": "2206.11113v1"
        },
        {
          "title": "On Fixed-Parameter Tractability of Weighted 0-1 Timed Matching Problem on Temporal Graphs",
          "year": "2025-08",
          "abstract": "Temporal graphs are introduced to model systems where the relationships among\nthe entities of the system evolve over time. In this paper, we consider the\ntemporal graphs where the edge set changes with time and all the changes are\nknown a priori. The underlying graph of a temporal graph is a static graph\nconsisting of all the vertices and edges that exist for at least one timestep\nin the temporal graph. The concept of 0-1 timed matching in temporal graphs was\nintroduced by Mandal and Gupta [DAM2022] as an extension of the matching\nproblem in static graphs. A 0-1 timed matching of a temporal graph is a\nnon-overlapping subset of the edge set of that temporal graph. The problem of\nfinding the maximum 0-1 timed matching is proved to be NP-complete on multiple\nclasses of temporal graphs. We study the fixed-parameter tractability of the\nmaximum 0-1 timed matching problem. We prove that the problem remains to be\nNP-complete even when the underlying static graph of the temporal graph has a\nbounded treewidth. Furthermore, we establish that the problem is W[1]-hard when\nparameterized by the solution size. Finally, we present a fixed-parameter\ntractable (FPT) algorithm to address the problem when the problem is\nparameterized by the maximum vertex degree and the treewidth of the underlying\ngraph of the temporal graph.",
          "arxiv_id": "2508.10562v1"
        },
        {
          "title": "Maximum 0-1 Timed Matching on Temporal Graphs",
          "year": "2020-12",
          "abstract": "Temporal graphs are graphs where the topology and/or other properties of the\ngraph change with time. They have been used to model applications with temporal\ninformation in various domains. Problems on static graphs become more\nchallenging to solve in temporal graphs because of dynamically changing\ntopology, and many recent works have explored graph problems on temporal\ngraphs. In this paper, we define a type of matching called {\\em 0-1 timed\nmatching} for temporal graphs, and investigate the problem of finding a {\\em\nmaximum 0-1 timed matching} for different classes of temporal graphs. We first\nprove that the problem is NP-Complete for rooted temporal trees when each edge\nis associated with two or more time intervals. We then propose an $O(n \\log n)$\ntime algorithm for the problem on a rooted temporal tree with $n$ nodes when\neach edge is associated with exactly one time interval. The problem is then\nshown to be NP-Complete also for bipartite temporal graphs even when each edge\nis associated with a single time interval and degree of each node is bounded by\na constant $k \\geq 3$. We next investigate approximation algorithms for the\nproblem for temporal graphs where each edge is associated with more than one\ntime intervals. It is first shown that there is no\n$\\frac{1}{n^{1-\\epsilon}}$-factor approximation algorithm for the problem for\nany $\\epsilon > 0$ even on a rooted temporal tree with $n$ nodes unless NP =\nZPP. We then present a $\\frac{5}{2\\mathcal{N}^* + 3}$-factor approximation\nalgorithm for the problem for general temporal graphs where $\\mathcal{N^*}$ is\nthe average number of edges overlapping in time with each edge in the temporal\ngraph. The same algorithm is also a constant-factor approximation algorithm for\ndegree bounded temporal graphs.",
          "arxiv_id": "2012.08909v2"
        }
      ],
      "15": [
        {
          "title": "Agent-based Leader Election, MST, and Beyond",
          "year": "2024-03",
          "abstract": "Leader election is one of the fundamental and well-studied problems in\ndistributed computing. In this paper, we initiate the study of leader election\nusing mobile agents. Suppose $n$ agents are positioned initially arbitrarily on\nthe nodes of an arbitrary, anonymous, $n$-node, $m$-edge graph $G$. The agents\nrelocate themselves autonomously on the nodes of $G$ and elect an agent as a\nleader such that the leader agent knows it is a leader and the other agents\nknow they are not leaders. The objective is to minimize time and memory\nrequirements. Following the literature, we consider the synchronous setting in\nwhich each agent performs its operations synchronously with others and hence\nthe time complexity can be measured in rounds. The quest in this paper is to\nprovide solutions without agents knowing any graph parameter, such as $n$, a\npriori. We first establish that, without agents knowing any graph parameter a\npriori, there exists a deterministic algorithm to elect an agent as a leader in\n$O(m)$ rounds with $O(n\\log n)$ bits at each agent. Using this leader election\nresult, we develop a deterministic algorithm for agents to construct a minimum\nspanning tree of $G$ in $O(m+n\\log n)$ rounds using $O(n \\log n)$ bits memory\nat each agent, without agents knowing any graph parameter a priori. Finally,\nusing the same leader election result, we provide improved time/memory results\nfor other fundamental distributed graph problems, namely, gathering, maximal\nindependent set, and minimal dominating sets, removing the assumptions on\nagents knowing graph parameters a priori.",
          "arxiv_id": "2403.13716v2"
        },
        {
          "title": "Efficient Dispersion on an Anonymous Ring in the Presence of Weak Byzantine Robots",
          "year": "2020-04",
          "abstract": "The problem of dispersion of mobile robots on a graph asks that $n$ robots\ninitially placed arbitrarily on the nodes of an $n$-node anonymous graph,\nautonomously move to reach a final configuration where exactly each node has at\nmost one robot on it. This problem is of significant interest due to its\nrelationship to other fundamental robot coordination problems, such as\nexploration, scattering, load balancing, relocation of self-driving electric\ncars to recharge stations, etc. The robots have unique IDs, typically in the\nrange $[1,poly(n)]$ and limited memory, whereas the graph is anonymous, i.e.,\nthe nodes do not have identifiers. The objective is to simultaneously minimize\ntwo performance metrics: (i) time to achieve dispersion and (ii) memory\nrequirement at each robot. This problem has been relatively well-studied when\nrobots are non-faulty.\n  In this paper, we introduce the notion of Byzantine faults to this problem,\ni.e., we formalize the problem of dispersion in the presence of up to $f$\nByzantine robots. We then study the problem on a ring while simultaneously\noptimizing the time complexity of algorithms and the memory requirement per\nrobot. Specifically, we design deterministic algorithms that attempt to match\nthe time lower bound ($\\Omega(n)$ rounds) and memory lower bound ($\\Omega(\\log\nn)$ bits per robot).\n  Our main result is a deterministic algorithm that is both time and memory\noptimal, i.e., $O(n)$ rounds and $O(\\log n)$ bits of memory required per robot,\nsubject to certain constraints. We subsequently provide results that require\nless assumptions but are either only time or memory optimal but not both. We\nalso provide a primitive, utilized often, that takes robots initially gathered\nat a node of the ring and disperses them in a time and memory optimal manner\nwithout additional assumptions required.",
          "arxiv_id": "2004.11439v2"
        },
        {
          "title": "Distance-2-Dispersion: Dispersion with Further Constraints",
          "year": "2023-01",
          "abstract": "The aim of the dispersion problem is to place a set of $k(\\leq n)$ mobile\nrobots in the nodes of an unknown graph consisting of $n$ nodes such that in\nthe final configuration each node contains at most one robot, starting from any\narbitrary initial configuration of the robots on the graph. In this work we\npropose a variant of the dispersion problem where we start with any number of\nrobots, and put an additional constraint that no two adjacent nodes contain\nrobots in the final configuration. We name this problem as\nDistance-2-Dispersion (D-2-D). However, even if the number of robots $k$ is\nless than $n$, it may not possible for each robot to find a distinct node to\nreside, maintaining our added constraint. Specifically, if a maximal\nindependent set is already formed by the nodes which contain a robot each, then\nother robots, if any, who are searching for a node to seat, will not find one.\nHence we allow multiple robots to seat on some nodes only if there is no place\nto seat. If $k\\geq n$, it is guaranteed that the nodes with robots form a\nmaximal independent set of the underlying network.\n  The graph $G=(V, E)$ has $n$ nodes and $m$ edges, where nodes are anonymous.\nIt is a port labelled graph, i.e., each node $u$ assigns a distinct port number\nto each of its incident edges from a range $[0,\\delta-1]$ where $\\delta$ is the\ndegree of the node $u$. The robots have unique ids in the range $[1, L]$, where\n$L \\ge k$. Co-located robots can communicate among themselves. We provide an\nalgorithm that solves D-2-D starting from a rooted configuration (i.e.,\ninitially all the robots are co-located) and terminate after $2\\Delta(8m-3n+3)$\nsynchronous rounds using $O(log \\Delta)$ memory per robot without using any\nglobal knowledge of the graph parameters $m$, $n$ and $\\Delta$, the maximum\ndegree of the graph. We also provide $\\Omega(m\\Delta)$ lower bound on the\nnumber of rounds for the D-2-D problem.",
          "arxiv_id": "2301.04938v1"
        }
      ],
      "16": [
        {
          "title": "On the Line-Separable Unit-Disk Coverage and Related Problems",
          "year": "2023-09",
          "abstract": "Given a set $P$ of $n$ points and a set $S$ of $m$ disks in the plane, the\ndisk coverage problem asks for a smallest subset of disks that together cover\nall points of $P$. The problem is NP-hard. In this paper, we consider a\nline-separable unit-disk version of the problem where all disks have the same\nradius and their centers are separated from the points of $P$ by a line $\\ell$.\nWe present an $O((n+m)\\log(n+m))$ time algorithm for the problem. This improves\nthe previously best result of $O(nm+ n\\log n)$ time. Our techniques also solve\nthe line-constrained version of the problem, where centers of all disks of $S$\nare located on a line $\\ell$ while points of $P$ can be anywhere in the plane.\nOur algorithm runs in $O((n+m)\\log (m+ n)+m \\log m\\log n)$ time, which improves\nthe previously best result of $O(nm\\log(m+n))$ time. In addition, our results\nlead to an algorithm of $O(n^3\\log n)$ time for a half-plane coverage problem\n(given $n$ half-planes and $n$ points, find a smallest subset of half-planes\ncovering all points); this improves the previously best algorithm of $O(n^4\\log\nn)$ time. Further, if all half-planes are lower ones, our algorithm runs in\n$O(n\\log n)$ time while the previously best algorithm takes $O(n^2\\log n)$\ntime.",
          "arxiv_id": "2309.03162v2"
        },
        {
          "title": "On Line-Separable Weighted Unit-Disk Coverage and Related Problems",
          "year": "2024-06",
          "abstract": "Given a set $P$ of $n$ points and a set $S$ of $n$ weighted disks in the\nplane, the disk coverage problem is to compute a subset of disks of smallest\ntotal weight such that the union of the disks in the subset covers all points\nof $P$. The problem is NP-hard. In this paper, we consider a line-separable\nunit-disk version of the problem where all disks have the same radius and their\ncenters are separated from the points of $P$ by a line $\\ell$. We present an\n$O(n^{3/2}\\log^2 n)$ time algorithm for the problem. This improves the\npreviously best work of $O(n^2\\log n)$ time. Our result leads to an algorithm\nof $O(n^{{7}/{2}}\\log^2 n)$ time for the halfplane coverage problem (i.e.,\nusing $n$ weighted halfplanes to cover $n$ points), an improvement over the\nprevious $O(n^4\\log n)$ time solution. If all halfplanes are lower ones, our\nalgorithm runs in $O(n^{{3}/{2}}\\log^2 n)$ time, while the previous best\nalgorithm takes $O(n^2\\log n)$ time. Using duality, the hitting set problems\nunder the same settings can be solved with similar time complexities.",
          "arxiv_id": "2407.00329v1"
        },
        {
          "title": "Algorithms for the Line-Constrained Disk Coverage and Related Problems",
          "year": "2021-04",
          "abstract": "Given a set $P$ of $n$ points and a set $S$ of $m$ weighted disks in the\nplane, the disk coverage problem asks for a subset of disks of minimum total\nweight that cover all points of $P$. The problem is NP-hard. In this paper, we\nconsider a line-constrained version in which all disks are centered on a line\n$L$ (while points of $P$ can be anywhere in the plane). We present an\n$O((m+n)\\log(m+n)+\\kappa\\log m)$ time algorithm for the problem, where $\\kappa$\nis the number of pairs of disks that intersect. Alternatively, we can also\nsolve the problem in $O(nm\\log(m+n))$ time. For the unit-disk case where all\ndisks have the same radius, the running time can be reduced to\n$O((n+m)\\log(m+n))$. In addition, we solve in $O((m+n)\\log(m+n))$ time the\n$L_{\\infty}$ and $L_1$ cases of the problem, in which the disks are squares and\ndiamonds, respectively. As a by-product, the 1D version of the problem where\nall points of $P$ are on $L$ and the disks are line segments on $L$ is also\nsolved in $O((m+n)\\log(m+n))$ time. We also show that the problem has an\n$\\Omega((m+n)\\log (m+n))$ time lower bound even for the 1D case.\n  We further demonstrate that our techniques can also be used to solve other\ngeometric coverage problems. For example, given in the plane a set $P$ of $n$\npoints and a set $S$ of $n$ weighted half-planes, we solve in $O(n^4\\log n)$\ntime the problem of finding a subset of half-planes to cover $P$ so that their\ntotal weight is minimized. This improves the previous best algorithm of\n$O(n^5)$ time by almost a linear factor. If all half-planes are lower ones,\nthen our algorithm runs in $O(n^2\\log n)$ time, which improves the previous\nbest algorithm of $O(n^4)$ time by almost a quadratic factor.",
          "arxiv_id": "2104.14680v1"
        }
      ],
      "17": [
        {
          "title": "Optimal Multi-Pass Lower Bounds for MST in Dynamic Streams",
          "year": "2023-12",
          "abstract": "The seminal work of Ahn, Guha, and McGregor in 2012 introduced the graph\nsketching technique and used it to present the first streaming algorithms for\nvarious graph problems over dynamic streams with both insertions and deletions\nof edges. This includes algorithms for cut sparsification, spanners, matchings,\nand minimum spanning trees (MSTs). These results have since been improved or\ngeneralized in various directions, leading to a vastly rich host of efficient\nalgorithms for processing dynamic graph streams.\n  A curious omission from the list of improvements has been the MST problem.\nThe best algorithm for this problem remains the original AGM algorithm that for\nevery integer $p \\geq 1$, uses $n^{1+O(1/p)}$ space in $p$ passes on $n$-vertex\ngraphs, and thus achieves the desired semi-streaming space of $\\tilde{O}(n)$ at\na relatively high cost of $O(\\frac{\\log{n}}{\\log\\log{n}})$ passes. On the other\nhand, no lower bounds beyond a folklore one-pass lower bound is known for this\nproblem.\n  We provide a simple explanation for this lack of improvements: The AGM\nalgorithm for MSTs is optimal for the entire range of its number of passes! We\nprove that even for the simplest decision version of the problem -- deciding\nwhether the weight of MSTs is at least a given threshold or not -- any $p$-pass\ndynamic streaming algorithm requires $n^{1+\\Omega(1/p)}$ space. This implies\nthat semi-streaming algorithms do need $\\Omega(\\frac{\\log{n}}{\\log\\log{n}})$\npasses.\n  Our result relies on proving new multi-round communication complexity lower\nbounds for a variant of the universal relation problem that has been\ninstrumental in proving prior lower bounds for single-pass dynamic streaming\nalgorithms. The proof also involves proving new composition theorems in\ncommunication complexity, including majority lemmas and multi-party XOR lemmas,\nvia information complexity approaches.",
          "arxiv_id": "2312.04674v1"
        },
        {
          "title": "Polynomial Pass Semi-Streaming Lower Bounds for K-Cores and Degeneracy",
          "year": "2024-05",
          "abstract": "The following question arises naturally in the study of graph streaming\nalgorithms:\n  \"Is there any graph problem which is \"not too hard\", in that it can be solved\nefficiently with total communication (nearly) linear in the number $n$ of\nvertices, and for which, nonetheless, any streaming algorithm with\n$\\tilde{O}(n)$ space (i.e., a semi-streaming algorithm) needs a polynomial\n$n^{\\Omega(1)}$ number of passes?\"\n  Assadi, Chen, and Khanna [STOC 2019] were the first to prove that this is\nindeed the case. However, the lower bounds that they obtained are for rather\nnon-standard graph problems.\n  Our first main contribution is to present the first polynomial-pass lower\nbounds for natural \"not too hard\" graph problems studied previously in the\nstreaming model: $k$-cores and degeneracy. We devise a novel communication\nprotocol for both problems with near-linear communication, thus showing that\n$k$-cores and degeneracy are natural examples of \"not too hard\" problems.\nIndeed, previous work have developed single-pass semi-streaming algorithms for\napproximating these problems. In contrast, we prove that any semi-streaming\nalgorithm for exactly solving these problems requires (almost)\n$\\Omega(n^{1/3})$ passes.\n  Our second main contribution is improved round-communication lower bounds for\nthe underlying communication problems at the basis of these reductions:\n  * We improve the previous lower bound of Assadi, Chen, and Khanna for hidden\npointer chasing (HPC) to achieve optimal bounds.\n  * We observe that all current reductions from HPC can also work with a\ngeneralized version of this problem that we call MultiHPC, and prove an even\nstronger and optimal lower bound for this generalization.\n  These two results collectively allow us to improve the resulting pass lower\nbounds for semi-streaming algorithms by a polynomial factor, namely, from\n$n^{1/5}$ to $n^{1/3}$ passes.",
          "arxiv_id": "2405.14835v1"
        },
        {
          "title": "Settling the Pass Complexity of Approximate Matchings in Dynamic Graph Streams",
          "year": "2024-07",
          "abstract": "A semi-streaming algorithm in dynamic graph streams processes any $n$-vertex\ngraph by making one or multiple passes over a stream of insertions and\ndeletions to edges of the graph and using $O(n \\cdot \\mbox{polylog}(n))$ space.\nSemi-streaming algorithms for dynamic streams were first obtained in the\nseminal work of Ahn, Guha, and McGregor in 2012, alongside the introduction of\nthe graph sketching technique, which remains the de facto way of designing\nalgorithms in this model and a highly popular technique for designing graph\nalgorithms in general.\n  We settle the pass complexity of approximating maximum matchings in dynamic\nstreams via semi-streaming algorithms by improving the state-of-the-art in both\nupper and lower bounds.\n  We present a randomized sketching based semi-streaming algorithm for\n$O(1)$-approximation of maximum matching in dynamic streams using\n$O(\\log\\log{n})$ passes. The approximation ratio of this algorithm can be\nimproved to $(1+\\epsilon)$ for any fixed $\\epsilon > 0$ even on weighted graphs\nusing standard techniques. This exponentially improves upon several\n$O(\\log{n})$ pass algorithms developed for this problem since the introduction\nof the dynamic graph streaming model.\n  In addition, we prove that any semi-streaming algorithm (not only sketching\nbased) for $O(1)$-approximation of maximum matching in dynamic streams requires\n$\\Omega(\\log\\log{n})$ passes. This presents the first multi-pass lower bound\nfor this problem, which is already also optimal, settling a longstanding open\nquestion in this area.",
          "arxiv_id": "2407.21005v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T18:55:15Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}