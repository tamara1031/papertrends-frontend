{
  "topics": {
    "data": {
      "0": {
        "name": "0_distribution_test_distributions_tests",
        "keywords": [
          [
            "distribution",
            0.01760187575371777
          ],
          [
            "test",
            0.014972882663922816
          ],
          [
            "distributions",
            0.013464487162715484
          ],
          [
            "tests",
            0.012029373233047487
          ],
          [
            "data",
            0.011578270884571586
          ],
          [
            "multivariate",
            0.010312653897686068
          ],
          [
            "model",
            0.010116458491545377
          ],
          [
            "dependence",
            0.010022282574178651
          ],
          [
            "testing",
            0.009308025205794263
          ],
          [
            "statistics",
            0.008711569962502426
          ]
        ],
        "count": 1301
      },
      "1": {
        "name": "1_treatment_causal_effect_effects",
        "keywords": [
          [
            "treatment",
            0.038569501065539843
          ],
          [
            "causal",
            0.02403428852113061
          ],
          [
            "effect",
            0.023891565256976395
          ],
          [
            "effects",
            0.020851239890111003
          ],
          [
            "outcome",
            0.01564678537921745
          ],
          [
            "inference",
            0.01450104196245555
          ],
          [
            "treatment effect",
            0.013975294455067209
          ],
          [
            "estimators",
            0.013382715802865158
          ],
          [
            "estimator",
            0.012738615037204637
          ],
          [
            "data",
            0.01216976000538226
          ]
        ],
        "count": 636
      },
      "2": {
        "name": "2_Gaussian_kernel_regression_inverse",
        "keywords": [
          [
            "Gaussian",
            0.01711027928324697
          ],
          [
            "kernel",
            0.013499632323793372
          ],
          [
            "regression",
            0.01327992609190875
          ],
          [
            "inverse",
            0.012570419915306871
          ],
          [
            "function",
            0.01188070984561549
          ],
          [
            "data",
            0.011769215983567066
          ],
          [
            "processes",
            0.011377049579449398
          ],
          [
            "spatial",
            0.010726063970715053
          ],
          [
            "problems",
            0.010396977534815185
          ],
          [
            "process",
            0.010337980142576406
          ]
        ],
        "count": 538
      },
      "3": {
        "name": "3_graph_network_graphs_model",
        "keywords": [
          [
            "graph",
            0.028317061674522293
          ],
          [
            "network",
            0.02617141854458686
          ],
          [
            "graphs",
            0.02167611185488564
          ],
          [
            "model",
            0.018650138144391648
          ],
          [
            "networks",
            0.015976170075793154
          ],
          [
            "community",
            0.015718653650966946
          ],
          [
            "random",
            0.014940553460296905
          ],
          [
            "degree",
            0.012405444525477363
          ],
          [
            "vertices",
            0.011655718278553315
          ],
          [
            "models",
            0.01164671252839648
          ]
        ],
        "count": 511
      },
      "4": {
        "name": "4_change_time_series_time series",
        "keywords": [
          [
            "change",
            0.03430663329184555
          ],
          [
            "time",
            0.02949049314670213
          ],
          [
            "series",
            0.026375097209914912
          ],
          [
            "time series",
            0.025191441139615148
          ],
          [
            "detection",
            0.020860109491188297
          ],
          [
            "change point",
            0.01732039164808567
          ],
          [
            "point",
            0.01590491005909614
          ],
          [
            "data",
            0.013634010518643352
          ],
          [
            "stationary",
            0.012816685998803194
          ],
          [
            "test",
            0.01243447909362467
          ]
        ],
        "count": 477
      },
      "5": {
        "name": "5_matrix_rank_tensor_noise",
        "keywords": [
          [
            "matrix",
            0.023316717959476378
          ],
          [
            "rank",
            0.021298266588847307
          ],
          [
            "tensor",
            0.019741307195942553
          ],
          [
            "noise",
            0.015414305334703088
          ],
          [
            "low",
            0.014701002057617764
          ],
          [
            "signal",
            0.013581510052106848
          ],
          [
            "low rank",
            0.013151883584457224
          ],
          [
            "Gaussian",
            0.012760944225496642
          ],
          [
            "problem",
            0.011983892020151606
          ],
          [
            "algorithm",
            0.011767627732786121
          ]
        ],
        "count": 440
      },
      "6": {
        "name": "6_transport_Wasserstein_distance_metric",
        "keywords": [
          [
            "transport",
            0.019727991701156005
          ],
          [
            "Wasserstein",
            0.019640254941662928
          ],
          [
            "distance",
            0.017081946659863713
          ],
          [
            "metric",
            0.01586898900624379
          ],
          [
            "manifold",
            0.015493072599977615
          ],
          [
            "optimal transport",
            0.015446670045598668
          ],
          [
            "optimal",
            0.014738070172544575
          ],
          [
            "measures",
            0.013942063676971081
          ],
          [
            "space",
            0.013846378743781746
          ],
          [
            "data",
            0.01298865964179594
          ]
        ],
        "count": 433
      },
      "7": {
        "name": "7_regression_dimensional_high_linear",
        "keywords": [
          [
            "regression",
            0.02309500634894803
          ],
          [
            "dimensional",
            0.017958343196688514
          ],
          [
            "high",
            0.017175427984738104
          ],
          [
            "linear",
            0.014779443525262613
          ],
          [
            "model",
            0.01434416631634012
          ],
          [
            "selection",
            0.013478364204523096
          ],
          [
            "Lasso",
            0.013453287202506648
          ],
          [
            "sparse",
            0.012638310347494689
          ],
          [
            "estimator",
            0.01258871818771402
          ],
          [
            "sparsity",
            0.01143623568894426
          ]
        ],
        "count": 348
      },
      "8": {
        "name": "8_neural_networks_neural networks_deep",
        "keywords": [
          [
            "neural",
            0.03797165311863781
          ],
          [
            "networks",
            0.033208082427461795
          ],
          [
            "neural networks",
            0.029370386854526583
          ],
          [
            "deep",
            0.020808799626292392
          ],
          [
            "learning",
            0.018513178729009382
          ],
          [
            "network",
            0.01731451696778722
          ],
          [
            "training",
            0.01611006798028675
          ],
          [
            "generalization",
            0.01588167290197811
          ],
          [
            "layer",
            0.014832203750480825
          ],
          [
            "regression",
            0.01357614403961558
          ]
        ],
        "count": 310
      },
      "9": {
        "name": "9_policy_regret_learning_optimal",
        "keywords": [
          [
            "policy",
            0.03398973250868166
          ],
          [
            "regret",
            0.029355354984854785
          ],
          [
            "learning",
            0.021115515954512717
          ],
          [
            "optimal",
            0.017238645458566015
          ],
          [
            "bandit",
            0.01686721218510473
          ],
          [
            "bandits",
            0.01681971838279776
          ],
          [
            "RL",
            0.016449028501996334
          ],
          [
            "reward",
            0.016258897254985957
          ],
          [
            "algorithm",
            0.015906422798328747
          ],
          [
            "arm",
            0.014143312028696587
          ]
        ],
        "count": 285
      },
      "10": {
        "name": "10_Langevin_sampling_Carlo_Monte",
        "keywords": [
          [
            "Langevin",
            0.030913186461439812
          ],
          [
            "sampling",
            0.029870833473660333
          ],
          [
            "Carlo",
            0.021804406336815463
          ],
          [
            "Monte",
            0.021798582940515187
          ],
          [
            "Markov",
            0.02103798201798612
          ],
          [
            "convergence",
            0.020237246705916916
          ],
          [
            "Metropolis",
            0.019457595993306976
          ],
          [
            "target",
            0.017969185593652102
          ],
          [
            "algorithm",
            0.017926652191040036
          ],
          [
            "chain",
            0.017733659331749835
          ]
        ],
        "count": 231
      },
      "11": {
        "name": "11_diffusion_process_processes_drift",
        "keywords": [
          [
            "diffusion",
            0.02734877296880412
          ],
          [
            "process",
            0.02727026530881821
          ],
          [
            "processes",
            0.02661482071716155
          ],
          [
            "drift",
            0.0262707198281004
          ],
          [
            "fractional",
            0.025647693455829724
          ],
          [
            "Brownian",
            0.022069584998108054
          ],
          [
            "motion",
            0.019245433177626357
          ],
          [
            "Ornstein",
            0.018574611986966236
          ],
          [
            "Uhlenbeck",
            0.018550179822080893
          ],
          [
            "estimator",
            0.01817596832710032
          ]
        ],
        "count": 212
      },
      "12": {
        "name": "12_privacy_private_differential privacy_differential",
        "keywords": [
          [
            "privacy",
            0.07798002636369915
          ],
          [
            "private",
            0.04013732365893178
          ],
          [
            "differential privacy",
            0.03372649016164528
          ],
          [
            "differential",
            0.028205115282080993
          ],
          [
            "data",
            0.022528673135751267
          ],
          [
            "DP",
            0.02073480312482226
          ],
          [
            "mechanism",
            0.015699518278297016
          ],
          [
            "estimation",
            0.0137460816587586
          ],
          [
            "Privacy",
            0.013341530296182498
          ],
          [
            "Private",
            0.012542419325832695
          ]
        ],
        "count": 200
      },
      "13": {
        "name": "13_random_inequalities_bounds_inequality",
        "keywords": [
          [
            "random",
            0.02872320589168182
          ],
          [
            "inequalities",
            0.022416993574070867
          ],
          [
            "bounds",
            0.021856580526682927
          ],
          [
            "inequality",
            0.019820318032321215
          ],
          [
            "random variables",
            0.017033790580765817
          ],
          [
            "variables",
            0.015577035914501478
          ],
          [
            "statistics",
            0.015100729062533002
          ],
          [
            "concentration",
            0.015007561639827175
          ],
          [
            "Gaussian",
            0.01465231287810282
          ],
          [
            "results",
            0.013393736705988516
          ]
        ],
        "count": 198
      },
      "14": {
        "name": "14_matrix_matrices_eigenvalues_covariance",
        "keywords": [
          [
            "matrix",
            0.04543648903518015
          ],
          [
            "matrices",
            0.038254102140361707
          ],
          [
            "eigenvalues",
            0.034756899527382215
          ],
          [
            "covariance",
            0.03336554059544346
          ],
          [
            "sample",
            0.026682914746442753
          ],
          [
            "spectral",
            0.02371805654707624
          ],
          [
            "spiked",
            0.0215129285793114
          ],
          [
            "sample covariance",
            0.020530155324150638
          ],
          [
            "Wishart",
            0.01921392062352261
          ],
          [
            "eigenvalue",
            0.018519546486678442
          ]
        ],
        "count": 164
      },
      "15": {
        "name": "15_models_likelihood_algebraic_toric",
        "keywords": [
          [
            "models",
            0.03862567061987083
          ],
          [
            "likelihood",
            0.020846390709809846
          ],
          [
            "algebraic",
            0.020667172240188155
          ],
          [
            "toric",
            0.020644435092148562
          ],
          [
            "degree",
            0.019446388880933363
          ],
          [
            "maximum likelihood",
            0.016516456763870396
          ],
          [
            "linear",
            0.016291569122201758
          ],
          [
            "maximum",
            0.01542918422717687
          ],
          [
            "polytopes",
            0.01512543547075036
          ],
          [
            "rational",
            0.014541882293836184
          ]
        ],
        "count": 148
      },
      "16": {
        "name": "16_prediction_conformal_conformal prediction_coverage",
        "keywords": [
          [
            "prediction",
            0.048275726642690704
          ],
          [
            "conformal",
            0.04594892348656072
          ],
          [
            "conformal prediction",
            0.029121690947122532
          ],
          [
            "coverage",
            0.027267683802295772
          ],
          [
            "calibration",
            0.022561642459824966
          ],
          [
            "learning",
            0.01686385140937286
          ],
          [
            "Conformal",
            0.01660472864657955
          ],
          [
            "sets",
            0.015574818085366354
          ],
          [
            "prediction sets",
            0.014529761006827442
          ],
          [
            "data",
            0.014143018040247644
          ]
        ],
        "count": 145
      },
      "17": {
        "name": "17_causal_graph_variables_graphs",
        "keywords": [
          [
            "causal",
            0.05860302235775827
          ],
          [
            "graph",
            0.028675987280201813
          ],
          [
            "variables",
            0.027378944788357153
          ],
          [
            "graphs",
            0.02470509983634743
          ],
          [
            "models",
            0.024608155584656573
          ],
          [
            "DAG",
            0.02297325191036994
          ],
          [
            "acyclic",
            0.022469045401591135
          ],
          [
            "equivalence",
            0.02166513417482544
          ],
          [
            "latent",
            0.021528049925753405
          ],
          [
            "graphical",
            0.019098450478575595
          ]
        ],
        "count": 109
      }
    },
    "correlations": [
      [
        1.0,
        -0.6876900512905204,
        -0.7217434706610535,
        -0.6764617108852538,
        -0.7017174768799468,
        -0.705926048724389,
        -0.6987314644820375,
        -0.6994791457819914,
        -0.7399747048149661,
        -0.7266024789447729,
        -0.6928988019204472,
        -0.7157676590494593,
        -0.7034003757117651,
        -0.6762303165893324,
        -0.6819123124244938,
        -0.6437221221328358,
        -0.7370725637541234,
        -0.7160001602337511
      ],
      [
        -0.6876900512905204,
        1.0,
        -0.7207334701232596,
        -0.6883476393845919,
        -0.7131145225078055,
        -0.7254157973228643,
        -0.7195522856867118,
        -0.6885154946893015,
        -0.7255199953905682,
        -0.7028080063305533,
        -0.7181316915814691,
        -0.7315617419238847,
        -0.7141589733606759,
        -0.7147562829894901,
        -0.7025384452991348,
        -0.38279912294568524,
        -0.7300385261540187,
        -0.5045178212044918
      ],
      [
        -0.7217434706610535,
        -0.7207334701232596,
        1.0,
        -0.7284671723712033,
        -0.7324103681199201,
        -0.7354472002169142,
        -0.7228366589928655,
        -0.7221977745806345,
        -0.7372532318334403,
        -0.7450695209977465,
        -0.7270749646005752,
        -0.7152866621323603,
        -0.7373778737570802,
        -0.7251296577427804,
        -0.7181498005166975,
        -0.7020795132027209,
        -0.7374108280031051,
        -0.7425276008019828
      ],
      [
        -0.6764617108852538,
        -0.6883476393845919,
        -0.7284671723712033,
        1.0,
        -0.70723002287996,
        -0.6947138399022971,
        -0.7231833197272337,
        -0.6890941502996035,
        -0.6558759506461693,
        -0.7348963725162184,
        -0.7198441212438309,
        -0.7230511019517947,
        -0.7143872016735,
        -0.705126770793274,
        -0.7086795151136445,
        -0.6063249326122458,
        -0.7336963966160441,
        -0.5806715730754224
      ],
      [
        -0.7017174768799468,
        -0.7131145225078055,
        -0.7324103681199201,
        -0.70723002287996,
        1.0,
        -0.7355262976621729,
        -0.730672742769552,
        -0.7033759330019322,
        -0.7376209536179024,
        -0.7373311783492968,
        -0.7205929359999625,
        -0.6905210176801683,
        -0.7219690204783911,
        -0.726718313012628,
        -0.7156783994533251,
        -0.6956879738843356,
        -0.7268989665673222,
        -0.7095076928250206
      ],
      [
        -0.705926048724389,
        -0.7254157973228643,
        -0.7354472002169142,
        -0.6947138399022971,
        -0.7355262976621729,
        1.0,
        -0.7226683481907848,
        -0.6944122945948329,
        -0.7464692588021471,
        -0.7375161815246338,
        -0.7363845920602279,
        -0.7512444078836232,
        -0.7347881151957127,
        -0.7079017801292287,
        -0.484595959898925,
        -0.688436444637412,
        -0.7501116450658858,
        -0.734223290938228
      ],
      [
        -0.6987314644820375,
        -0.7195522856867118,
        -0.7228366589928655,
        -0.7231833197272337,
        -0.730672742769552,
        -0.7226683481907848,
        1.0,
        -0.7289413070389192,
        -0.7401751756681398,
        -0.7452228059342787,
        -0.7001788036404657,
        -0.7394516933817847,
        -0.738669310630439,
        -0.6920032218850386,
        -0.7308927084819272,
        -0.6935069775450668,
        -0.7549423761163256,
        -0.7467062501789272
      ],
      [
        -0.6994791457819914,
        -0.6885154946893015,
        -0.7221977745806345,
        -0.6890941502996035,
        -0.7033759330019322,
        -0.6944122945948329,
        -0.7289413070389192,
        1.0,
        -0.7183264014292345,
        -0.7342104257350146,
        -0.7103370716321278,
        -0.7412061912689552,
        -0.6934953001411264,
        -0.7214485786153646,
        -0.5777919418274916,
        -0.6482526100795225,
        -0.7095526136124393,
        -0.7062755976813433
      ],
      [
        -0.7399747048149661,
        -0.7255199953905682,
        -0.7372532318334403,
        -0.6558759506461693,
        -0.7376209536179024,
        -0.7464692588021471,
        -0.7401751756681398,
        -0.7183264014292345,
        1.0,
        -0.738395397255377,
        -0.7335535140264837,
        -0.7529064214010992,
        -0.7388439180434019,
        -0.7437523573987446,
        -0.7370113316264082,
        -0.7113316961789123,
        -0.7265201026515454,
        -0.7279274216788234
      ],
      [
        -0.7266024789447729,
        -0.7028080063305533,
        -0.7450695209977465,
        -0.7348963725162184,
        -0.7373311783492968,
        -0.7375161815246338,
        -0.7452228059342787,
        -0.7342104257350146,
        -0.738395397255377,
        1.0,
        -0.7340620653386376,
        -0.746455523767465,
        -0.7336225502274449,
        -0.7391822071473159,
        -0.7404612405305253,
        -0.7014201321678495,
        -0.7409790893185866,
        -0.7441217510137929
      ],
      [
        -0.6928988019204472,
        -0.7181316915814691,
        -0.7270749646005752,
        -0.7198441212438309,
        -0.7205929359999625,
        -0.7363845920602279,
        -0.7001788036404657,
        -0.7103370716321278,
        -0.7335535140264837,
        -0.7340620653386376,
        1.0,
        -0.7040424896866962,
        -0.7373699638320181,
        -0.7212872561971322,
        -0.7123864678778946,
        -0.6863434346888488,
        -0.7467375419500787,
        -0.7355584553701324
      ],
      [
        -0.7157676590494593,
        -0.7315617419238847,
        -0.7152866621323603,
        -0.7230511019517947,
        -0.6905210176801683,
        -0.7512444078836232,
        -0.7394516933817847,
        -0.7412061912689552,
        -0.7529064214010992,
        -0.746455523767465,
        -0.7040424896866962,
        1.0,
        -0.7391664199022383,
        -0.7346022926422117,
        -0.7340315467179311,
        -0.6889101785801932,
        -0.748457855740722,
        -0.7326297340667651
      ],
      [
        -0.7034003757117651,
        -0.7141589733606759,
        -0.7373778737570802,
        -0.7143872016735,
        -0.7219690204783911,
        -0.7347881151957127,
        -0.738669310630439,
        -0.6934953001411264,
        -0.7388439180434019,
        -0.7336225502274449,
        -0.7373699638320181,
        -0.7391664199022383,
        1.0,
        -0.7386794728425119,
        -0.7167994434353104,
        -0.6970360207845419,
        -0.728193878213775,
        -0.7283909701895497
      ],
      [
        -0.6762303165893324,
        -0.7147562829894901,
        -0.7251296577427804,
        -0.705126770793274,
        -0.726718313012628,
        -0.7079017801292287,
        -0.6920032218850386,
        -0.7214485786153646,
        -0.7437523573987446,
        -0.7391822071473159,
        -0.7212872561971322,
        -0.7346022926422117,
        -0.7386794728425119,
        1.0,
        -0.6928107784322874,
        -0.6935201955130657,
        -0.7431304587052306,
        -0.7075439820257055
      ],
      [
        -0.6819123124244938,
        -0.7025384452991348,
        -0.7181498005166975,
        -0.7086795151136445,
        -0.7156783994533251,
        -0.484595959898925,
        -0.7308927084819272,
        -0.5777919418274916,
        -0.7370113316264082,
        -0.7404612405305253,
        -0.7123864678778946,
        -0.7340315467179311,
        -0.7167994434353104,
        -0.6928107784322874,
        1.0,
        -0.6625362918216281,
        -0.7344281799937105,
        -0.712317175134701
      ],
      [
        -0.6437221221328358,
        -0.38279912294568524,
        -0.7020795132027209,
        -0.6063249326122458,
        -0.6956879738843356,
        -0.688436444637412,
        -0.6935069775450668,
        -0.6482526100795225,
        -0.7113316961789123,
        -0.7014201321678495,
        -0.6863434346888488,
        -0.6889101785801932,
        -0.6970360207845419,
        -0.6935201955130657,
        -0.6625362918216281,
        1.0,
        -0.7208343470448033,
        -0.5111306161758453
      ],
      [
        -0.7370725637541234,
        -0.7300385261540187,
        -0.7374108280031051,
        -0.7336963966160441,
        -0.7268989665673222,
        -0.7501116450658858,
        -0.7549423761163256,
        -0.7095526136124393,
        -0.7265201026515454,
        -0.7409790893185866,
        -0.7467375419500787,
        -0.748457855740722,
        -0.728193878213775,
        -0.7431304587052306,
        -0.7344281799937105,
        -0.7208343470448033,
        1.0,
        -0.7342044923755723
      ],
      [
        -0.7160001602337511,
        -0.5045178212044918,
        -0.7425276008019828,
        -0.5806715730754224,
        -0.7095076928250206,
        -0.734223290938228,
        -0.7467062501789272,
        -0.7062755976813433,
        -0.7279274216788234,
        -0.7441217510137929,
        -0.7355584553701324,
        -0.7326297340667651,
        -0.7283909701895497,
        -0.7075439820257055,
        -0.712317175134701,
        -0.5111306161758453,
        -0.7342044923755723,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        17,
        5,
        6,
        5,
        12,
        6,
        6,
        6,
        3,
        2,
        3,
        8,
        1,
        8,
        13,
        9,
        6,
        2
      ],
      "2020-02": [
        37,
        7,
        3,
        11,
        19,
        6,
        3,
        12,
        8,
        9,
        10,
        5,
        8,
        7,
        10,
        20,
        5,
        6
      ],
      "2020-03": [
        36,
        2,
        2,
        5,
        20,
        1,
        4,
        15,
        8,
        6,
        4,
        9,
        5,
        8,
        8,
        16,
        2,
        1
      ],
      "2020-04": [
        35,
        6,
        3,
        9,
        14,
        2,
        3,
        11,
        4,
        1,
        10,
        7,
        2,
        7,
        8,
        15,
        6,
        6
      ],
      "2020-05": [
        24,
        3,
        2,
        7,
        10,
        6,
        3,
        11,
        6,
        4,
        6,
        8,
        5,
        7,
        15,
        21,
        5,
        2
      ],
      "2020-06": [
        40,
        7,
        6,
        7,
        17,
        5,
        7,
        18,
        13,
        6,
        14,
        3,
        5,
        12,
        14,
        19,
        9,
        5
      ],
      "2020-07": [
        35,
        4,
        6,
        8,
        13,
        2,
        7,
        20,
        6,
        5,
        11,
        12,
        5,
        7,
        11,
        12,
        3,
        3
      ],
      "2020-08": [
        31,
        3,
        1,
        8,
        8,
        9,
        4,
        7,
        3,
        3,
        6,
        7,
        3,
        7,
        16,
        13,
        1,
        5
      ],
      "2020-09": [
        38,
        11,
        2,
        6,
        8,
        8,
        2,
        14,
        5,
        0,
        6,
        9,
        5,
        10,
        10,
        12,
        4,
        1
      ],
      "2020-10": [
        35,
        6,
        2,
        5,
        15,
        4,
        5,
        11,
        13,
        5,
        12,
        9,
        8,
        4,
        14,
        25,
        5,
        6
      ],
      "2020-11": [
        32,
        11,
        0,
        9,
        10,
        1,
        8,
        10,
        6,
        8,
        10,
        10,
        8,
        6,
        6,
        26,
        5,
        2
      ],
      "2020-12": [
        35,
        4,
        3,
        10,
        10,
        9,
        6,
        9,
        4,
        3,
        13,
        6,
        0,
        10,
        16,
        18,
        7,
        1
      ],
      "2021-01": [
        27,
        4,
        3,
        3,
        13,
        3,
        4,
        8,
        6,
        5,
        18,
        4,
        8,
        9,
        15,
        9,
        0,
        4
      ],
      "2021-02": [
        34,
        8,
        1,
        8,
        18,
        0,
        5,
        7,
        8,
        7,
        8,
        10,
        3,
        14,
        9,
        13,
        4,
        2
      ],
      "2021-03": [
        35,
        6,
        1,
        4,
        10,
        4,
        2,
        10,
        8,
        5,
        11,
        6,
        5,
        12,
        18,
        13,
        4,
        5
      ],
      "2021-04": [
        32,
        11,
        3,
        5,
        15,
        1,
        5,
        5,
        5,
        2,
        12,
        4,
        3,
        9,
        19,
        7,
        3,
        6
      ],
      "2021-05": [
        28,
        10,
        4,
        9,
        19,
        6,
        6,
        9,
        5,
        4,
        11,
        7,
        5,
        2,
        13,
        10,
        4,
        4
      ],
      "2021-06": [
        39,
        5,
        1,
        6,
        15,
        2,
        7,
        16,
        10,
        2,
        13,
        7,
        4,
        9,
        16,
        13,
        7,
        5
      ],
      "2021-07": [
        39,
        8,
        2,
        9,
        15,
        3,
        8,
        10,
        13,
        6,
        9,
        3,
        9,
        4,
        16,
        14,
        3,
        3
      ],
      "2021-08": [
        24,
        3,
        4,
        11,
        11,
        3,
        2,
        9,
        7,
        3,
        7,
        6,
        5,
        2,
        20,
        11,
        1,
        2
      ],
      "2021-09": [
        33,
        10,
        0,
        5,
        13,
        1,
        2,
        9,
        3,
        5,
        10,
        6,
        4,
        5,
        17,
        13,
        4,
        5
      ],
      "2021-10": [
        39,
        7,
        3,
        17,
        17,
        2,
        4,
        18,
        12,
        15,
        13,
        6,
        5,
        1,
        19,
        18,
        5,
        4
      ],
      "2021-11": [
        21,
        12,
        1,
        6,
        4,
        6,
        5,
        5,
        4,
        2,
        12,
        8,
        9,
        7,
        8,
        14,
        4,
        3
      ],
      "2021-12": [
        31,
        7,
        5,
        9,
        22,
        2,
        6,
        9,
        10,
        7,
        7,
        6,
        4,
        12,
        16,
        16,
        3,
        3
      ],
      "2022-01": [
        30,
        6,
        5,
        4,
        5,
        5,
        6,
        8,
        3,
        3,
        6,
        7,
        6,
        3,
        7,
        3,
        0,
        5
      ],
      "2022-02": [
        30,
        8,
        1,
        6,
        7,
        1,
        5,
        13,
        9,
        6,
        12,
        2,
        3,
        10,
        12,
        17,
        5,
        3
      ],
      "2022-03": [
        29,
        7,
        2,
        6,
        8,
        3,
        5,
        13,
        4,
        4,
        9,
        7,
        5,
        5,
        16,
        15,
        5,
        5
      ],
      "2022-04": [
        32,
        5,
        0,
        9,
        8,
        2,
        3,
        8,
        4,
        4,
        8,
        8,
        6,
        5,
        6,
        12,
        1,
        3
      ],
      "2022-05": [
        36,
        6,
        3,
        3,
        7,
        3,
        3,
        13,
        6,
        4,
        13,
        7,
        12,
        5,
        8,
        23,
        5,
        5
      ],
      "2022-06": [
        41,
        11,
        4,
        10,
        17,
        4,
        9,
        11,
        14,
        6,
        17,
        10,
        7,
        10,
        9,
        23,
        3,
        4
      ],
      "2022-07": [
        37,
        3,
        5,
        8,
        8,
        4,
        3,
        10,
        6,
        1,
        7,
        10,
        9,
        7,
        10,
        21,
        2,
        3
      ],
      "2022-08": [
        39,
        7,
        6,
        5,
        14,
        5,
        3,
        13,
        3,
        4,
        9,
        5,
        7,
        7,
        8,
        13,
        6,
        3
      ],
      "2022-09": [
        27,
        7,
        3,
        11,
        10,
        1,
        11,
        14,
        2,
        7,
        12,
        9,
        4,
        11,
        23,
        24,
        8,
        5
      ],
      "2022-10": [
        34,
        10,
        0,
        6,
        12,
        4,
        3,
        22,
        5,
        6,
        16,
        11,
        7,
        10,
        18,
        16,
        13,
        6
      ],
      "2022-11": [
        34,
        6,
        3,
        7,
        21,
        7,
        5,
        10,
        2,
        11,
        10,
        8,
        4,
        11,
        12,
        21,
        7,
        3
      ],
      "2022-12": [
        32,
        14,
        6,
        3,
        17,
        3,
        6,
        15,
        6,
        6,
        7,
        5,
        1,
        9,
        14,
        15,
        2,
        2
      ],
      "2023-01": [
        29,
        8,
        6,
        11,
        11,
        1,
        5,
        13,
        3,
        6,
        10,
        7,
        9,
        5,
        12,
        8,
        1,
        6
      ],
      "2023-02": [
        27,
        8,
        4,
        9,
        12,
        8,
        7,
        7,
        7,
        10,
        6,
        6,
        7,
        6,
        7,
        9,
        5,
        3
      ],
      "2023-03": [
        44,
        3,
        3,
        9,
        19,
        3,
        3,
        11,
        7,
        2,
        12,
        8,
        5,
        9,
        9,
        16,
        8,
        1
      ],
      "2023-04": [
        21,
        6,
        6,
        5,
        9,
        5,
        4,
        8,
        3,
        4,
        6,
        7,
        3,
        10,
        10,
        11,
        3,
        3
      ],
      "2023-05": [
        27,
        9,
        4,
        10,
        14,
        8,
        8,
        14,
        10,
        14,
        17,
        6,
        7,
        5,
        16,
        23,
        6,
        6
      ],
      "2023-06": [
        30,
        9,
        1,
        4,
        12,
        1,
        5,
        16,
        9,
        8,
        6,
        8,
        7,
        10,
        19,
        14,
        7,
        11
      ],
      "2023-07": [
        23,
        9,
        5,
        12,
        13,
        2,
        14,
        9,
        6,
        8,
        11,
        13,
        2,
        8,
        12,
        13,
        5,
        5
      ],
      "2023-08": [
        32,
        7,
        4,
        8,
        7,
        3,
        1,
        4,
        4,
        2,
        6,
        3,
        1,
        5,
        13,
        11,
        2,
        5
      ],
      "2023-09": [
        34,
        7,
        3,
        4,
        11,
        0,
        3,
        16,
        11,
        1,
        8,
        4,
        5,
        5,
        10,
        13,
        8,
        5
      ],
      "2023-10": [
        40,
        14,
        5,
        14,
        18,
        3,
        4,
        17,
        8,
        5,
        13,
        3,
        10,
        13,
        10,
        16,
        8,
        7
      ],
      "2023-11": [
        43,
        8,
        7,
        8,
        12,
        3,
        5,
        12,
        7,
        2,
        10,
        2,
        5,
        6,
        13,
        24,
        3,
        8
      ],
      "2023-12": [
        24,
        9,
        7,
        6,
        6,
        5,
        7,
        12,
        7,
        3,
        12,
        5,
        4,
        8,
        7,
        11,
        4,
        2
      ],
      "2024-01": [
        21,
        7,
        3,
        12,
        8,
        2,
        5,
        10,
        6,
        5,
        10,
        8,
        11,
        6,
        12,
        17,
        3,
        8
      ],
      "2024-02": [
        45,
        10,
        2,
        13,
        13,
        3,
        4,
        16,
        10,
        6,
        18,
        10,
        7,
        16,
        12,
        27,
        12,
        3
      ],
      "2024-03": [
        33,
        7,
        6,
        8,
        15,
        4,
        5,
        14,
        6,
        6,
        14,
        9,
        11,
        9,
        15,
        15,
        5,
        7
      ],
      "2024-04": [
        37,
        4,
        2,
        4,
        11,
        7,
        5,
        13,
        5,
        3,
        9,
        5,
        7,
        6,
        15,
        17,
        6,
        5
      ],
      "2024-05": [
        44,
        11,
        1,
        10,
        18,
        5,
        5,
        13,
        11,
        2,
        14,
        6,
        11,
        11,
        20,
        15,
        8,
        4
      ],
      "2024-06": [
        37,
        7,
        5,
        11,
        9,
        1,
        3,
        17,
        13,
        6,
        13,
        6,
        14,
        7,
        18,
        10,
        6,
        7
      ],
      "2024-07": [
        42,
        10,
        2,
        9,
        10,
        2,
        7,
        6,
        6,
        5,
        10,
        6,
        5,
        10,
        22,
        15,
        4,
        3
      ],
      "2024-08": [
        23,
        2,
        1,
        6,
        10,
        4,
        2,
        14,
        4,
        0,
        10,
        12,
        5,
        5,
        15,
        15,
        1,
        5
      ],
      "2024-09": [
        38,
        7,
        1,
        4,
        13,
        1,
        6,
        21,
        9,
        4,
        13,
        4,
        7,
        8,
        14,
        14,
        6,
        2
      ],
      "2024-10": [
        36,
        12,
        5,
        8,
        17,
        5,
        10,
        21,
        10,
        10,
        18,
        6,
        5,
        5,
        18,
        17,
        7,
        9
      ],
      "2024-11": [
        30,
        16,
        2,
        5,
        9,
        6,
        4,
        11,
        10,
        2,
        10,
        7,
        5,
        6,
        13,
        14,
        3,
        9
      ],
      "2024-12": [
        33,
        10,
        2,
        10,
        14,
        5,
        2,
        14,
        8,
        6,
        9,
        10,
        6,
        3,
        6,
        9,
        6,
        3
      ],
      "2025-01": [
        36,
        5,
        2,
        7,
        13,
        5,
        2,
        18,
        9,
        7,
        12,
        8,
        7,
        8,
        10,
        14,
        4,
        6
      ],
      "2025-02": [
        45,
        6,
        3,
        9,
        17,
        4,
        5,
        17,
        9,
        6,
        10,
        8,
        8,
        10,
        15,
        26,
        11,
        9
      ],
      "2025-03": [
        32,
        9,
        2,
        14,
        13,
        6,
        3,
        11,
        3,
        5,
        10,
        13,
        3,
        10,
        11,
        19,
        8,
        6
      ],
      "2025-04": [
        34,
        11,
        3,
        9,
        10,
        1,
        3,
        18,
        10,
        5,
        9,
        6,
        7,
        12,
        9,
        19,
        6,
        7
      ],
      "2025-05": [
        41,
        7,
        6,
        9,
        17,
        4,
        12,
        20,
        16,
        9,
        10,
        8,
        11,
        14,
        19,
        15,
        6,
        4
      ],
      "2025-06": [
        45,
        17,
        3,
        12,
        17,
        6,
        9,
        11,
        6,
        3,
        16,
        7,
        6,
        7,
        13,
        17,
        3,
        8
      ],
      "2025-07": [
        44,
        5,
        6,
        7,
        12,
        3,
        7,
        10,
        4,
        3,
        12,
        4,
        8,
        5,
        18,
        16,
        8,
        10
      ],
      "2025-08": [
        40,
        11,
        3,
        5,
        13,
        1,
        4,
        7,
        4,
        6,
        9,
        9,
        8,
        12,
        9,
        10,
        7,
        10
      ],
      "2025-09": [
        24,
        4,
        3,
        3,
        8,
        3,
        2,
        6,
        3,
        3,
        4,
        3,
        2,
        5,
        8,
        10,
        5,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Testing independence and conditional independence in high dimensions via coordinatewise Gaussianization",
          "year": "2025-04",
          "abstract": "We propose new statistical tests, in high-dimensional settings, for testing\nthe independence of two random vectors and their conditional independence given\na third random vector. The key idea is simple, i.e., we first transform each\ncomponent variable to standard normal via its marginal empirical distribution,\nand we then test for independence and conditional independence of the\ntransformed random vectors using appropriate $L_\\infty$-type test statistics.\nWhile we are testing some necessary conditions of the independence or the\nconditional independence, the new tests outperform the 13 frequently used\ntesting methods in a large scale simulation comparison. The advantage of the\nnew tests can be summarized as follows: (i) they do not require any moment\nconditions, (ii) they allow arbitrary dependence structures of the components\namong the random vectors, and (iii) they allow the dimensions of random vectors\ndiverge at the exponential rates of the sample size. The critical values of the\nproposed tests are determined by a computationally efficient multiplier\nbootstrap procedure. Theoretical analysis shows that the sizes of the proposed\ntests can be well controlled by the nominal significance level, and the\nproposed tests are also consistent under certain local alternatives. The finite\nsample performance of the new tests is illustrated via extensive simulation\nstudies and a real data application.",
          "arxiv_id": "2504.02233v1"
        },
        {
          "title": "A new goodness of fit test for normal distribution based on Stein's characterization",
          "year": "2020-01",
          "abstract": "In this paper, we develop a simple non-parametric test for testing normal\ndistribution based on the distance between empirical zero-bias transformation\nand empirical distribution. The asymptotic properties of the test statistic are\nstudied. The finite sample performance of the proposed test is evaluated\nthrough a Monte Carlo simulation study. The power of our test is compared with\nseveral other tests for normality. We illustrate the test procedure using two\nreal data sets. We also develop a jackknife empirical likelihood ratio test for\nstandard normal distribution.",
          "arxiv_id": "2001.07932v4"
        },
        {
          "title": "Family of mean-mixtures of multivariate normal distributions: properties, inference and assessment of multivariate skewness",
          "year": "2020-06",
          "abstract": "In this paper, a new mixture family of multivariate normal distributions,\nformed by mixing multivariate normal distribution and skewed distribution, is\nconstructed. Some properties of this family, such as characteristic function,\nmoment generating function, and the first four moments are derived. The\ndistributions of affine transformations and canonical forms of the model are\nalso derived. An EM type algorithm is developed for the maximum likelihood\nestimation of model parameters. We have considered in detail, some special\ncases of the family, using standard gamma and standard exponential mixture\ndistributions, denoted by MMNG and MMNE, respectively. For the proposed family\nof distributions, different multivariate measures of skewness are computed. In\norder to examine the performance of the developed estimation method, some\nsimulation studies are carried out to show that the maximum likelihood\nestimates based on the EM type algorithm do provide good performance. For\ndifferent choices of parameters of MMNE distribution, several multivariate\nmeasures of skewness are computed and compared. Because some measures of\nskewness are scalar and some are vectors, in order to evaluate them properly,\nwe have carried out a simulation study to determine the power of tests, based\non sample versions of skewness measures as test statistics to test the fit of\nthe MMNE distribution. Finally, two real data sets are used to illustrate the\nusefulness of the proposed family of distributions and the associated\ninferential method.",
          "arxiv_id": "2006.10018v2"
        }
      ],
      "1": [
        {
          "title": "Covariate-adjusted Fisher randomization tests for the average treatment effect",
          "year": "2020-10",
          "abstract": "Fisher's randomization test (FRT) delivers exact $p$-values under the strong\nnull hypothesis of no treatment effect on any units whatsoever and allows for\nflexible covariate adjustment to improve the power. Of interest is whether the\nprocedure could also be valid for testing the weak null hypothesis of zero\naverage treatment effect. Towards this end, we evaluate two general strategies\nfor FRT with covariate-adjusted test statistics: that based on the residuals\nfrom an outcome model with only the covariates, and that based on the output\nfrom an outcome model with both the treatment and the covariates. Based on\ntheory and simulation, we recommend using the ordinary least squares (OLS) fit\nof the observed outcome on the treatment, centered covariates, and their\ninteractions for covariate adjustment, and conducting FRT with the robust\n$t$-value of the treatment as the test statistic. The resulting FRT is\nfinite-sample exact for the strong null hypothesis, asymptotically valid for\nthe weak null hypothesis, and more powerful than the unadjusted analog under\nalternatives, all irrespective of whether the linear model is correctly\nspecified or not. We develop the theory for complete randomization, cluster\nrandomization, stratified randomization, and rerandomization, respectively, and\ngive a recommendation for the test procedure and test statistic under each\ndesign. We first focus on the finite-population perspective and then extend the\nresult to the super-population perspective, highlighting the difference in\nstandard errors. Motivated by the similarity in procedure, we also evaluate the\ndesign-based properties of five existing permutation tests originally for\nlinear models and show the superiority of the proposed FRT for testing the\ntreatment effects.",
          "arxiv_id": "2010.14555v3"
        },
        {
          "title": "What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness",
          "year": "2025-06",
          "abstract": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions - popularly known\nas Regression Discontinuity designs - violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and necessary for the identification of ATE. Moreover, this\ncondition also characterizes the identification of the average treatment effect\non the treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms.",
          "arxiv_id": "2506.04194v2"
        },
        {
          "title": "The CATT SATT on the MATT: semiparametric inference for sample treatment effects on the treated",
          "year": "2024-02",
          "abstract": "We study variants of the average treatment effect on the treated with\npopulation parameters replaced by their sample counterparts. For each estimand,\nwe derive the limiting distribution with respect to a semiparametric efficient\nestimator of the population effect and provide guidance on variance estimation.\nIncluded in our analysis is the well-known sample average treatment effect on\nthe treated, for which we obtain some unexpected results. Unlike the ordinary\nsample average treatment effect, we find that the asymptotic variance for the\nsample average treatment effect on the treated is point-identified and\nconsistently estimable, but it potentially exceeds that of the population\nestimand. To address this shortcoming, we propose a modification that yields a\nnew estimand, the mixed average treatment effect on the treated, which is\nalways estimated more precisely than both the population and sample effects. We\nalso introduce a second new estimand that arises from an alternative\ninterpretation of the treatment effect on the treated with which all\nindividuals are weighted by the propensity score.",
          "arxiv_id": "2402.05844v2"
        }
      ],
      "2": [
        {
          "title": "Convergence rates of non-stationary and deep Gaussian process regression",
          "year": "2023-12",
          "abstract": "The focus of this work is the convergence of non-stationary and deep Gaussian\nprocess regression. More precisely, we follow a Bayesian approach to regression\nor interpolation, where the prior placed on the unknown function $f$ is a\nnon-stationary or deep Gaussian process, and we derive convergence rates of the\nposterior mean to the true function $f$ in terms of the number of observed\ntraining points. In some cases, we also show convergence of the posterior\nvariance to zero. The only assumption imposed on the function $f$ is that it is\nan element of a certain reproducing kernel Hilbert space, which we in\nparticular cases show to be norm-equivalent to a Sobolev space. Our analysis\nincludes the case of estimated hyper-parameters in the covariance kernels\nemployed, both in an empirical Bayes' setting and the particular hierarchical\nsetting constructed through deep Gaussian processes. We consider the settings\nof noise-free or noisy observations on deterministic or random training points.\nWe establish general assumptions sufficient for the convergence of deep\nGaussian process regression, along with explicit examples demonstrating the\nfulfilment of these assumptions. Specifically, our examples require that the\nH\\\"older or Sobolev norms of the penultimate layer are bounded almost surely.",
          "arxiv_id": "2312.07320v4"
        },
        {
          "title": "Posterior Concentration for Gaussian Process Priors under Rescaled and Hierarchical Matrn and Confluent Hypergeometric Covariance Functions",
          "year": "2023-12",
          "abstract": "In nonparameteric Bayesian approaches, Gaussian stochastic processes can\nserve as priors on real-valued function spaces. Existing literature on the\nposterior convergence rates under Gaussian process priors shows that it is\npossible to achieve optimal or near-optimal posterior contraction rates if the\nsmoothness of the Gaussian process matches that of the target function. Among\nthose priors, Gaussian processes with a parametric Mat\\'ern covariance function\nis particularly notable in that its degree of smoothness can be determined by a\ndedicated smoothness parameter. \\citet{ma2022beyond} recently introduced a new\nfamily of covariance functions called the Confluent Hypergeometric (CH) class\nthat simultaneously possess two parameters: one controls the tail index of the\npolynomially decaying covariance function, and the other parameter controls the\ndegree of mean-squared smoothness analogous to the Mat\\'ern class. In this\npaper, we show that with proper choice of rescaling parameters in the Mat\\'ern\nand CH covariance functions, it is possible to obtain the minimax optimal\nposterior contraction rate for $\\eta$-regular functions for nonparametric\nregression model with fixed design. Unlike the previous results for unrescaled\ncases, the smoothness parameter of the covariance function need not equal\n$\\eta$ for achieving the optimal minimax rate, for either rescaled Mat\\'ern or\nrescaled CH covariances, illustrating a key benefit for rescaling. We also\nconsider a fully Bayesian treatment of the rescaling parameters and show the\nresulting posterior distributions still contract at the minimax-optimal rate.\nThe resultant hierarchical Bayesian procedure is fully adaptive to the unknown\ntrue smoothness.",
          "arxiv_id": "2312.07502v3"
        },
        {
          "title": "Equivalence of Convergence Rates of Posterior Distributions and Bayes Estimators for Functions and Nonparametric Functionals",
          "year": "2020-11",
          "abstract": "We study the posterior contraction rates of a Bayesian method with Gaussian\nprocess priors in nonparametric regression and its plug-in property for\ndifferential operators. For a general class of kernels, we establish\nconvergence rates of the posterior measure of the regression function and its\nderivatives, which are both minimax optimal up to a logarithmic factor for\nfunctions in certain classes. Our calculation shows that the rate-optimal\nestimation of the regression function and its derivatives share the same choice\nof hyperparameter, indicating that the Bayes procedure remarkably adapts to the\norder of derivatives and enjoys a generalized plug-in property that extends\nreal-valued functionals to function-valued functionals. This leads to a\npractically simple method for estimating the regression function and its\nderivatives, whose finite sample performance is assessed using simulations.\n  Our proof shows that, under certain conditions, to any convergence rate of\nBayes estimators there corresponds the same convergence rate of the posterior\ndistributions (i.e., posterior contraction rate), and vice versa. This\nequivalence holds for a general class of Gaussian processes and covers the\nregression function and its derivative functionals, under both the $L_2$ and\n$L_{\\infty}$ norms. In addition to connecting these two fundamental large\nsample properties in Bayesian and non-Bayesian regimes, such equivalence\nenables a new routine to establish posterior contraction rates by calculating\nconvergence rates of nonparametric point estimators.\n  At the core of our argument is an operator-theoretic framework for kernel\nridge regression and equivalent kernel techniques. We derive a range of sharp\nnon-asymptotic bounds that are pivotal in establishing convergence rates of\nnonparametric point estimators and the equivalence theory, which may be of\nindependent interest.",
          "arxiv_id": "2011.13967v1"
        }
      ],
      "3": [
        {
          "title": "Spectral Recovery of Binary Censored Block Models",
          "year": "2021-07",
          "abstract": "Community detection is the problem of identifying community structure in\ngraphs. Often the graph is modeled as a sample from the Stochastic Block Model,\nin which each vertex belongs to a community. The probability that two vertices\nare connected by an edge depends on the communities of those vertices. In this\npaper, we consider a model of {\\em censored} community detection with two\ncommunities, where most of the data is missing as the status of only a small\nfraction of the potential edges is revealed. In this model, vertices in the\nsame community are connected with probability $p$ while vertices in opposite\ncommunities are connected with probability $q$. The connectivity status of a\ngiven pair of vertices $\\{u,v\\}$ is revealed with probability $\\alpha$,\nindependently across all pairs, where $\\alpha = \\frac{t \\log(n)}{n}$. We\nestablish the information-theoretic threshold $t_c(p,q)$, such that no\nalgorithm succeeds in recovering the communities exactly when $t < t_c(p,q)$.\nWe show that when $t > t_c(p,q)$, a simple spectral algorithm based on a\nweighted, signed adjacency matrix succeeds in recovering the communities\nexactly.\n  While spectral algorithms are shown to have near-optimal performance in the\nsymmetric case, we show that they may fail in the asymmetric case where the\nconnection probabilities inside the two communities are allowed to be\ndifferent. In particular, we show the existence of a parameter regime where a\nsimple two-phase algorithm succeeds but any algorithm based on the top two\neigenvectors of the weighted, signed adjacency matrix fails.",
          "arxiv_id": "2107.06338v2"
        },
        {
          "title": "The Power of Two Matrices in Spectral Algorithms for Community Recovery",
          "year": "2022-10",
          "abstract": "Spectral algorithms are some of the main tools in optimization and inference\nproblems on graphs. Typically, the graph is encoded as a matrix and\neigenvectors and eigenvalues of the matrix are then used to solve the given\ngraph problem. Spectral algorithms have been successfully used for graph\npartitioning, hidden clique recovery and graph coloring. In this paper, we\nstudy the power of spectral algorithms using two matrices in a graph\npartitioning problem. We use two different matrices resulting from two\ndifferent encodings of the same graph and then combine the spectral information\ncoming from these two matrices.\n  We analyze a two-matrix spectral algorithm for the problem of identifying\nlatent community structure in large random graphs. In particular, we consider\nthe problem of recovering community assignments exactly in the censored\nstochastic block model, where each edge status is revealed independently with\nsome probability. We show that spectral algorithms based on two matrices are\noptimal and succeed in recovering communities up to the information theoretic\nthreshold. Further, we show that for most choices of the parameters, any\nspectral algorithm based on one matrix is suboptimal. The latter observation is\nin contrast to our prior works (2022a, 2022b) which showed that for the\nsymmetric Stochastic Block Model and the Planted Dense Subgraph problem, a\nspectral algorithm based on one matrix achieves the information theoretic\nthreshold. We additionally provide more general geometric conditions for the\n(sub)-optimality of spectral algorithms.",
          "arxiv_id": "2210.05893v3"
        },
        {
          "title": "Exact Community Recovery in Correlated Stochastic Block Models",
          "year": "2022-03",
          "abstract": "We consider the problem of learning latent community structure from multiple\ncorrelated networks. We study edge-correlated stochastic block models with two\nbalanced communities, focusing on the regime where the average degree is\nlogarithmic in the number of vertices. Our main result derives the precise\ninformation-theoretic threshold for exact community recovery using multiple\ncorrelated graphs. This threshold captures the interplay between the community\nrecovery and graph matching tasks. In particular, we uncover and characterize a\nregion of the parameter space where exact community recovery is possible using\nmultiple correlated graphs, even though (1) this is information-theoretically\nimpossible using a single graph and (2) exact graph matching is also\ninformation-theoretically impossible. In this regime, we develop a novel\nalgorithm that carefully synthesizes algorithms from the community recovery and\ngraph matching literatures.",
          "arxiv_id": "2203.15736v1"
        }
      ],
      "4": [
        {
          "title": "Optimal Change-Point Detection and Localization",
          "year": "2020-10",
          "abstract": "Given a times series ${\\bf Y}$ in $\\mathbb{R}^n$, with a piece-wise contant\nmean and independent components, the twin problems of change-point detection\nand change-point localization respectively amount to detecting the existence of\ntimes where the mean varies and estimating the positions of those\nchange-points. In this work, we tightly characterize optimal rates for both\nproblems and uncover the phase transition phenomenon from a global testing\nproblem to a local estimation problem. Introducing a suitable definition of the\nenergy of a change-point, we first establish in the single change-point setting\nthat the optimal detection threshold is $\\sqrt{2\\log\\log(n)}$. When the energy\nis just above the detection threshold, then the problem of localizing the\nchange-point becomes purely parametric: it only depends on the difference in\nmeans and not on the position of the change-point anymore. Interestingly, for\nmost change-point positions, it is possible to detect and localize them at a\nmuch smaller energy level. In the multiple change-point setting, we establish\nthe energy detection threshold and show similarly that the optimal localization\nerror of a specific change-point becomes purely parametric. Along the way,\ntight optimal rates for Hausdorff and $l_1$ estimation losses of the vector of\nall change-points positions are also established. Two procedures achieving\nthese optimal rates are introduced. The first one is a least-squares estimator\nwith a new multiscale penalty that favours well spread change-points. The\nsecond one is a two-step multiscale post-processing procedure whose\ncomputational complexity can be as low as $O(n\\log(n))$. Notably, these two\nprocedures accommodate with the presence of possibly many low-energy and\ntherefore undetectable change-points and are still able to detect and localize\nhigh-energy change-points even with the presence of those nuisance parameters.",
          "arxiv_id": "2010.11470v2"
        },
        {
          "title": "Poisson QMLE for change-point detection in general integer-valued time series models",
          "year": "2020-07",
          "abstract": "We consider together the retrospective and the sequential change-point\ndetection in a general class of integer-valued time series.\n  The conditional mean of the process depends on a parameter $\\theta^*$ which\nmay change over time. We propose procedures which are based on the Poisson\nquasi-maximum likelihood estimator of the parameter, and where the updated\nestimator is computed without the historical observations in the sequential\nframework. For both the retrospective and the sequential detection, the test\nstatistics converge to some distributions obtained from the standard Brownian\nmotion under the null hypothesis of no change and diverge to infinity under the\nalternative; that is, these procedures are consistent.\n  Some results of simulations as well as real data application are provided.",
          "arxiv_id": "2007.13858v1"
        },
        {
          "title": "Optimal multiple change-point detection for high-dimensional data",
          "year": "2020-11",
          "abstract": "This manuscript makes two contributions to the field of change-point\ndetection. In a generalchange-point setting, we provide a generic algorithm for\naggregating local homogeneity testsinto an estimator of change-points in a time\nseries. Interestingly, we establish that the errorrates of the collection of\ntests directly translate into detection properties of the\nchange-pointestimator. This generic scheme is then applied to various problems\nincluding covariance change-point detection, nonparametric change-point\ndetection and sparse multivariate mean change-point detection. For the latter,\nwe derive minimax optimal rates that are adaptive to theunknown sparsity and to\nthe distance between change-points when the noise is Gaussian. Forsub-Gaussian\nnoise, we introduce a variant that is optimal in almost all sparsity regimes.",
          "arxiv_id": "2011.07818v2"
        }
      ],
      "5": [
        {
          "title": "Low solution rank of the matrix LASSO under RIP with consequences for rank-constrained algorithms",
          "year": "2024-04",
          "abstract": "We show that solutions to the popular convex matrix LASSO problem\n(nuclear-norm--penalized linear least-squares) have low rank under similar\nassumptions as required by classical low-rank matrix sensing error bounds.\nAlthough the purpose of the nuclear norm penalty is to promote low solution\nrank, a proof has not yet (to our knowledge) been provided outside very\nspecific circumstances. Furthermore, we show that this result has significant\ntheoretical consequences for nonconvex rank-constrained optimization\napproaches. Specifically, we show that if (a) the ground truth matrix has low\nrank, (b) the (linear) measurement operator has the matrix restricted isometry\nproperty (RIP), and (c) the measurement error is small enough relative to the\nnuclear norm penalty, then the (unique) LASSO solution has rank (approximately)\nbounded by that of the ground truth. From this, we show (a) that a\nlow-rank--projected proximal gradient descent algorithm will converge linearly\nto the LASSO solution from any initialization, and (b) that the nonconvex\nlandscape of the low-rank Burer-Monteiro--factored problem formulation is\nbenign in the sense that all second-order critical points are globally optimal\nand yield the LASSO solution.",
          "arxiv_id": "2404.12828v2"
        },
        {
          "title": "A High-Dimensional Statistical Theory for Convex and Nonconvex Matrix Sensing",
          "year": "2025-06",
          "abstract": "The problem of matrix sensing, or trace regression, is a problem wherein one\nwishes to estimate a low-rank matrix from linear measurements perturbed with\nnoise. A number of existing works have studied both convex and nonconvex\napproaches to this problem, establishing minimax error rates when the number of\nmeasurements is sufficiently large relative to the rank and dimension of the\nlow-rank matrix, though a precise comparison of these procedures still remains\nunexplored. In this work we provide a high-dimensional statistical analysis for\nsymmetric low-rank matrix sensing observed under Gaussian measurements and\nnoise. Our main result describes a novel phenomenon: in this statistical model\nand in an appropriate asymptotic regime, the behavior of any local minimum of\nthe nonconvex factorized approach (with known rank) is approximately equivalent\nto that of the matrix hard-thresholding of a corresponding matrix denoising\nproblem, and the behavior of the convex nuclear-norm regularized least squares\napproach is approximately equivalent to that of matrix soft-thresholding of the\nsame matrix denoising problem. Here \"approximately equivalent\" is understood in\nthe sense of concentration of Lipchitz functions. As a consequence, the\nnonconvex procedure uniformly dominates the convex approach in mean squared\nerror. Our arguments are based on a matrix operator generalization of the\nConvex Gaussian Min-Max Theorem (CGMT) together with studying the interplay\nbetween local minima of the convex and nonconvex formulations and their\n\"debiased\" counterparts, and several of these results may be of independent\ninterest.",
          "arxiv_id": "2506.20659v1"
        },
        {
          "title": "Approximately low-rank recovery from noisy and local measurements by convex program",
          "year": "2021-10",
          "abstract": "Low-rank matrix models have been universally useful for numerous\napplications, from classical system identification to more modern matrix\ncompletion in signal processing and statistics. The nuclear norm has been\nemployed as a convex surrogate of the low-rankness since it induces a low-rank\nsolution to inverse problems. While the nuclear norm for low rankness has an\nexcellent analogy with the $\\ell_1$ norm for sparsity through the singular\nvalue decomposition, other matrix norms also induce low-rankness. Particularly\nas one interprets a matrix as a linear operator between Banach spaces, various\ntensor product norms generalize the role of the nuclear norm. We provide a\ntensor-norm-constrained estimator for the recovery of approximately low-rank\nmatrices from local measurements corrupted with noise. A tensor-norm\nregularizer is designed to adapt to the local structure. We derive statistical\nanalysis of the estimator over matrix completion and decentralized sketching by\napplying Maurey's empirical method to tensor products of Banach spaces. The\nestimator provides a near-optimal error bound in a minimax sense and admits a\npolynomial-time algorithm for these applications.",
          "arxiv_id": "2110.15205v2"
        }
      ],
      "6": [
        {
          "title": "Minimax Rates of Estimation for Optimal Transport Map between Infinite-Dimensional Spaces",
          "year": "2025-05",
          "abstract": "We investigate the estimation of an optimal transport map between probability\nmeasures on an infinite-dimensional space and reveal its minimax optimal rate.\nOptimal transport theory defines distances within a space of probability\nmeasures, utilizing an optimal transport map as its key component. Estimating\nthe optimal transport map from samples finds several applications, such as\nsimulating dynamics between probability measures and functional data analysis.\nHowever, some transport maps on infinite-dimensional spaces require\nexponential-order data for estimation, which undermines their applicability. In\nthis paper, we investigate the estimation of an optimal transport map between\ninfinite-dimensional spaces, focusing on optimal transport maps characterized\nby the notion of $\\gamma$-smoothness. Consequently, we show that the order of\nthe minimax risk is polynomial rate in the sample size even in the\ninfinite-dimensional setup. We also develop an estimator whose estimation error\nmatches the minimax optimal rate. With these results, we obtain a class of\nreasonably estimable optimal transport maps on infinite-dimensional spaces and\na method for their estimation. Our experiments validate the theory and\npractical utility of our approach with application to functional data analysis.",
          "arxiv_id": "2505.13570v2"
        },
        {
          "title": "Gromov-Wasserstein Distances: Entropic Regularization, Duality, and Sample Complexity",
          "year": "2022-12",
          "abstract": "The Gromov-Wasserstein (GW) distance, rooted in optimal transport (OT)\ntheory, quantifies dissimilarity between metric measure spaces and provides a\nframework for aligning heterogeneous datasets. While computational aspects of\nthe GW problem have been widely studied, a duality theory and fundamental\nstatistical questions concerning empirical convergence rates remained obscure.\nThis work closes these gaps for the quadratic GW distance over Euclidean spaces\nof different dimensions $d_x$ and $d_y$. We treat both the standard and the\nentropically regularized GW distance, and derive dual forms that represent them\nin terms of the well-understood OT and entropic OT (EOT) problems,\nrespectively. This enables employing proof techniques from statistical OT based\non regularity analysis of dual potentials and empirical process theory, using\nwhich we establish the first GW empirical convergence rates. The derived\ntwo-sample rates are $n^{-2/\\max\\{\\min\\{d_x,d_y\\},4\\}}$ (up to a log factor\nwhen $\\min\\{d_x,d_y\\}=4$) for standard GW and $n^{-1/2}$ for EGW, which matches\nthe corresponding rates for standard and entropic OT. The parametric rate for\nEGW is evidently optimal, while for standard GW we provide matching lower\nbounds, which establish sharpness of the derived rates. We also study stability\nof EGW in the entropic regularization parameter and prove approximation and\ncontinuity results for the cost and optimal couplings. Lastly, the duality is\nleveraged to shed new light on the open problem of the one-dimensional GW\ndistance between uniform distributions on $n$ points, illuminating why the\nidentity and anti-identity permutations may not be optimal. Our results serve\nas a first step towards a comprehensive statistical theory as well as\ncomputational advancements for GW distances, based on the discovered dual\nformulations.",
          "arxiv_id": "2212.12848v3"
        },
        {
          "title": "Plugin Estimation of Smooth Optimal Transport Maps",
          "year": "2021-07",
          "abstract": "We analyze a number of natural estimators for the optimal transport map\nbetween two distributions and show that they are minimax optimal. We adopt the\nplugin approach: our estimators are simply optimal couplings between measures\nderived from our observations, appropriately extended so that they define\nfunctions on $\\mathbb{R}^d$. When the underlying map is assumed to be\nLipschitz, we show that computing the optimal coupling between the empirical\nmeasures, and extending it using linear smoothers, already gives a minimax\noptimal estimator. When the underlying map enjoys higher regularity, we show\nthat the optimal coupling between appropriate nonparametric density estimates\nyields faster rates. Our work also provides new bounds on the risk of\ncorresponding plugin estimators for the quadratic Wasserstein distance, and we\nshow how this problem relates to that of estimating optimal transport maps\nusing stability arguments for smooth and strongly convex Brenier potentials. As\nan application of our results, we derive central limit theorems for plugin\nestimators of the squared Wasserstein distance, which are centered at their\npopulation counterpart when the underlying distributions have sufficiently\nsmooth densities. In contrast to known central limit theorems for empirical\nestimators, this result easily lends itself to statistical inference for the\nquadratic Wasserstein distance.",
          "arxiv_id": "2107.12364v3"
        }
      ],
      "7": [
        {
          "title": "On regularization methods based on Rnyi's pseudodistances for sparse high-dimensional linear regression models",
          "year": "2020-07",
          "abstract": "Several regularization methods have been considered over the last decade for\nsparse high-dimensional linear regression models, but the most common ones use\nthe least square (quadratic) or likelihood loss and hence are not robust\nagainst data contamination. Some authors have overcome the problem of\nnon-robustness by considering suitable loss function based on divergence\nmeasures (e.g., density power divergence, gamma-divergence, etc.) instead of\nthe quadratic loss. In this paper we shall consider a loss function based on\nthe R\\'enyi's pseudodistance jointly with non-concave penalties in order to\nsimultaneously perform variable selection and get robust estimators of the\nparameters in a high-dimensional linear regression model of non-polynomial\ndimensionality. The desired oracle properties of our proposed method are\nderived theoretically and its usefulness is illustustrated numerically through\nsimulations and real data examples.",
          "arxiv_id": "2007.15929v1"
        },
        {
          "title": "A generalized Bayesian approach for high-dimensional robust regression with serially correlated errors and predictors",
          "year": "2024-12",
          "abstract": "This paper introduces a loss-based generalized Bayesian methodology for\nhigh-dimensional robust regression with serially correlated errors and\npredictors. The proposed framework employs a novel scaled pseudo-Huber (SPH)\nloss function, which smooths the well-known Huber loss, effectively balancing\nquadratic ($\\ell_2$) and absolute linear ($\\ell_1$) loss behaviors. This\nflexibility enables the framework to accommodate both thin-tailed and\nheavy-tailed data efficiently. The generalized Bayesian approach constructs a\nworking likelihood based on the SPH loss, facilitating efficient and stable\nestimation while providing rigorous uncertainty quantification for all model\nparameters. Notably, this approach allows formal statistical inference without\nrequiring ad hoc tuning parameter selection while adaptively addressing a wide\nrange of tail behavior in the errors. By specifying appropriate prior\ndistributions for the regression coefficients--such as ridge priors for small\nor moderate-dimensional settings and spike-and-slab priors for high-dimensional\nsettings--the framework ensures principled inference. We establish rigorous\ntheoretical guarantees for accurate parameter estimation and correct predictor\nselection under sparsity assumptions for a wide range of data generating\nsetups. Extensive simulation studies demonstrate the superior performance of\nour approach compared to traditional Bayesian regression methods based on\n$\\ell_2$ and $\\ell_1$-loss functions. The results highlight its flexibility and\nrobustness, particularly in challenging high-dimensional settings characterized\nby data contamination.",
          "arxiv_id": "2412.05673v3"
        },
        {
          "title": "Robust adaptive variable selection in ultra-high dimensional linear regression models",
          "year": "2020-04",
          "abstract": "We consider the problem of simultaneous variable selection and estimation of\nthe corresponding regression coefficients in an ultra-high dimensional linear\nregression models, an extremely important problem in the recent era. The\nadaptive penalty functions are used in this regard to achieve the oracle\nvariable selection property along with easier computational burden. However,\nthe usual adaptive procedures (e.g., adaptive LASSO) based on the squared error\nloss function is extremely non-robust in the presence of data contamination\nwhich are quite common with large-scale data (e.g., noisy gene expression data,\nspectra and spectral data). In this paper, we present a regularization\nprocedure for the ultra-high dimensional data using a robust loss function\nbased on the popular density power divergence (DPD) measure along with the\nadaptive LASSO penalty. We theoretically study the robustness and the\nlarge-sample properties of the proposed adaptive robust estimators for a\ngeneral class of error distributions; in particular, we show that the proposed\nadaptive DPD-LASSO estimator is highly robust, satisfies the oracle variable\nselection property, and the corresponding estimators of the regression\ncoefficients are consistent and asymptotically normal under easily verifiable\nset of assumptions. Numerical illustrations are provided for the mostly used\nnormal error density. Finally, the proposal is applied to analyze an\ninteresting spectral dataset, in the field of chemometrics, regarding the\nelectron-probe X-ray microanalysis (EPXMA) of archaeological glass vessels from\nthe 16th and 17th centuries.",
          "arxiv_id": "2004.05470v3"
        }
      ],
      "8": [
        {
          "title": "Deep Nonparametric Regression on Approximate Manifolds: Non-Asymptotic Error Bounds with Polynomial Prefactors",
          "year": "2021-04",
          "abstract": "We study the properties of nonparametric least squares regression using deep\nneural networks. We derive non-asymptotic upper bounds for the prediction error\nof the empirical risk minimizer of feedforward deep neural regression. Our\nerror bounds achieve minimax optimal rate and significantly improve over the\nexisting ones in the sense that they depend polynomially on the dimension of\nthe predictor, instead of exponentially on dimension. We show that the neural\nregression estimator can circumvent the curse of dimensionality under the\nassumption that the predictor is supported on an approximate low-dimensional\nmanifold or a set with low Minkowski dimension. We also establish the optimal\nconvergence rate under the exact manifold support assumption. We investigate\nhow the prediction error of the neural regression estimator depends on the\nstructure of neural networks and propose a notion of network relative\nefficiency between two types of neural networks, which provides a quantitative\nmeasure for evaluating the relative merits of different network structures. To\nestablish these results, we derive a novel approximation error bound for the\nH\\\"older smooth functions with a positive smoothness index using ReLU activated\nneural networks, which may be of independent interest. Our results are derived\nunder weaker assumptions on the data distribution and the neural network\nstructure than those in the existing literature.",
          "arxiv_id": "2104.06708v6"
        },
        {
          "title": "Mathematical Models of Overparameterized Neural Networks",
          "year": "2020-12",
          "abstract": "Deep learning has received considerable empirical successes in recent years.\nHowever, while many ad hoc tricks have been discovered by practitioners, until\nrecently, there has been a lack of theoretical understanding for tricks\ninvented in the deep learning literature. Known by practitioners that\noverparameterized neural networks are easy to learn, in the past few years\nthere have been important theoretical developments in the analysis of\noverparameterized neural networks. In particular, it was shown that such\nsystems behave like convex systems under various restricted settings, such as\nfor two-layer NNs, and when learning is restricted locally in the so-called\nneural tangent kernel space around specialized initializations. This paper\ndiscusses some of these recent progresses leading to significant better\nunderstanding of neural networks. We will focus on the analysis of two-layer\nneural networks, and explain the key mathematical models, with their\nalgorithmic implications. We will then discuss challenges in understanding deep\nneural networks and some current research directions.",
          "arxiv_id": "2012.13982v1"
        },
        {
          "title": "Information-theoretic reduction of deep neural networks to linear models in the overparametrized proportional regime",
          "year": "2025-05",
          "abstract": "We rigorously analyse fully-trained neural networks of arbitrary depth in the\nBayesian optimal setting in the so-called proportional scaling regime where the\nnumber of training samples and width of the input and all inner layers diverge\nproportionally. We prove an information-theoretic equivalence between the\nBayesian deep neural network model trained from data generated by a teacher\nwith matching architecture, and a simpler model of optimal inference in a\ngeneralized linear model. This equivalence enables us to compute the optimal\ngeneralization error for deep neural networks in this regime. We thus prove the\n\"deep Gaussian equivalence principle\" conjectured in Cui et al. (2023)\n(arXiv:2302.00375). Our result highlights that in order to escape this\n\"trivialisation\" of deep neural networks (in the sense of reduction to a linear\nmodel) happening in the strongly overparametrized proportional regime, models\ntrained from much more data have to be considered.",
          "arxiv_id": "2505.03577v1"
        }
      ],
      "9": [
        {
          "title": "A Simple and Optimal Policy Design with Safety against Heavy-Tailed Risk for Stochastic Bandits",
          "year": "2022-06",
          "abstract": "We study the stochastic multi-armed bandit problem and design new policies\nthat enjoy both worst-case optimality for expected regret and light-tailed risk\nfor regret distribution. Specifically, our policy design (i) enjoys the\nworst-case optimality for the expected regret at order $O(\\sqrt{KT\\ln T})$ and\n(ii) has the worst-case tail probability of incurring a regret larger than any\n$x>0$ being upper bounded by $\\exp(-\\Omega(x/\\sqrt{KT}))$, a rate that we prove\nto be best achievable with respect to $T$ for all worst-case optimal policies.\nOur proposed policy achieves a delicate balance between doing more exploration\nat the beginning of the time horizon and doing more exploitation when\napproaching the end, compared to standard confidence-bound-based policies. We\nalso enhance the policy design to accommodate the \"any-time\" setting where $T$\nis unknown a priori, and prove equivalently desired policy performances as\ncompared to the \"fixed-time\" setting with known $T$. Numerical experiments are\nconducted to illustrate the theoretical findings. We find that from a\nmanagerial perspective, our new policy design yields better tail distributions\nand is preferable than celebrated policies especially when (i) there is a risk\nof under-estimating the volatility profile, or (ii) there is a challenge of\ntuning policy hyper-parameters. We conclude by extending our proposed policy\ndesign to the stochastic linear bandit setting that leads to both worst-case\noptimality in terms of expected regret and light-tailed risk on the regret\ndistribution.",
          "arxiv_id": "2206.02969v6"
        },
        {
          "title": "High-probability sample complexities for policy evaluation with linear function approximation",
          "year": "2023-05",
          "abstract": "This paper is concerned with the problem of policy evaluation with linear\nfunction approximation in discounted infinite horizon Markov decision\nprocesses. We investigate the sample complexities required to guarantee a\npredefined estimation error of the best linear coefficients for two widely-used\npolicy evaluation algorithms: the temporal difference (TD) learning algorithm\nand the two-timescale linear TD with gradient correction (TDC) algorithm. In\nboth the on-policy setting, where observations are generated from the target\npolicy, and the off-policy setting, where samples are drawn from a behavior\npolicy potentially different from the target policy, we establish the first\nsample complexity bound with high-probability convergence guarantee that\nattains the optimal dependence on the tolerance level. We also exhihit an\nexplicit dependence on problem-related quantities, and show in the on-policy\nsetting that our upper bound matches the minimax lower bound on crucial problem\nparameters, including the choice of the feature maps and the problem dimension.",
          "arxiv_id": "2305.19001v2"
        },
        {
          "title": "Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective",
          "year": "2020-10",
          "abstract": "In the classical multi-armed bandit problem, instance-dependent algorithms\nattain improved performance on \"easy\" problems with a gap between the best and\nsecond-best arm. Are similar guarantees possible for contextual bandits? While\npositive results are known for certain special cases, there is no general\ntheory characterizing when and how instance-dependent regret bounds for\ncontextual bandits can be achieved for rich, general classes of policies. We\nintroduce a family of complexity measures that are both sufficient and\nnecessary to obtain instance-dependent regret bounds. We then introduce new\noracle-efficient algorithms which adapt to the gap whenever possible, while\nalso attaining the minimax rate in the worst case. Finally, we provide\nstructural results that tie together a number of complexity measures previously\nproposed throughout contextual bandits, reinforcement learning, and active\nlearning and elucidate their role in determining the optimal instance-dependent\nregret. In a large-scale empirical evaluation, we find that our approach often\ngives superior results for challenging exploration problems.\n  Turning our focus to reinforcement learning with function approximation, we\ndevelop new oracle-efficient algorithms for reinforcement learning with rich\nobservations that obtain optimal gap-dependent sample complexity.",
          "arxiv_id": "2010.03104v1"
        }
      ],
      "10": [
        {
          "title": "Upper and lower bounds on the subgeometric convergence of adaptive Markov chain Monte Carlo",
          "year": "2024-11",
          "abstract": "We investigate lower bounds on the subgeometric convergence of adaptive\nMarkov chain Monte Carlo under any adaptation strategy. In particular, we prove\ngeneral lower bounds in total variation and on the weak convergence rate under\ngeneral adaptation plans. If the adaptation diminishes sufficiently fast, we\nalso develop comparable convergence rate upper bounds that are capable of\napproximately matching the convergence rate in the subgeometric lower bound.\nThese results provide insight into the optimal design of adaptation strategies\nand also limitations on the convergence behavior of adaptive Markov chain Monte\nCarlo. Applications to an adaptive unadjusted Langevin algorithm as well as\nadaptive Metropolis-Hastings with independent proposals and random-walk\nproposals are explored.",
          "arxiv_id": "2411.17084v2"
        },
        {
          "title": "Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling",
          "year": "2024-07",
          "abstract": "We consider the outstanding problem of sampling from an unnormalized density\nthat may be non-log-concave and multimodal. To enhance the performance of\nsimple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type\nhave been widely used. However, quantitative theoretical guarantees of these\ntechniques are under-explored. This study takes a first step toward providing a\nnon-asymptotic analysis of annealed MCMC. Specifically, we establish, for the\nfirst time, an oracle complexity of $\\widetilde{O}\\left(\\frac{d\\beta^2{\\cal\nA}^2}{\\varepsilon^6}\\right)$ for the simple annealed Langevin Monte Carlo\nalgorithm to achieve $\\varepsilon^2$ accuracy in Kullback-Leibler divergence to\nthe target distribution $\\pi\\propto{\\rm e}^{-V}$ on $\\mathbb{R}^d$ with\n$\\beta$-smooth potential $V$. Here, ${\\cal A}$ represents the action of a curve\nof probability measures interpolating the target distribution $\\pi$ and a\nreadily sampleable distribution.",
          "arxiv_id": "2407.16936v2"
        },
        {
          "title": "Wasserstein Control of Mirror Langevin Monte Carlo",
          "year": "2020-02",
          "abstract": "Discretized Langevin diffusions are efficient Monte Carlo methods for\nsampling from high dimensional target densities that are log-Lipschitz-smooth\nand (strongly) log-concave. In particular, the Euclidean Langevin Monte Carlo\nsampling algorithm has received much attention lately, leading to a detailed\nunderstanding of its non-asymptotic convergence properties and of the role that\nsmoothness and log-concavity play in the convergence rate. Distributions that\ndo not possess these regularity properties can be addressed by considering a\nRiemannian Langevin diffusion with a metric capturing the local geometry of the\nlog-density. However, the Monte Carlo algorithms derived from discretizations\nof such Riemannian Langevin diffusions are notoriously difficult to analyze. In\nthis paper, we consider Langevin diffusions on a Hessian-type manifold and\nstudy a discretization that is closely related to the mirror-descent scheme. We\nestablish for the first time a non-asymptotic upper-bound on the sampling error\nof the resulting Hessian Riemannian Langevin Monte Carlo algorithm. This bound\nis measured according to a Wasserstein distance induced by a Riemannian metric\nground cost capturing the Hessian structure and closely related to a\nself-concordance-like condition. The upper-bound implies, for instance, that\nthe iterates contract toward a Wasserstein ball around the target density whose\nradius is made explicit. Our theory recovers existing Euclidean results and can\ncope with a wide variety of Hessian metrics related to highly non-flat\ngeometries.",
          "arxiv_id": "2002.04363v1"
        }
      ],
      "11": [
        {
          "title": "Increasing Domain Infill Asymptotics for Stochastic Differential Equations Driven by Fractional Brownian Motion",
          "year": "2020-05",
          "abstract": "Although statistical inference in stochastic differential equations (SDEs)\ndriven by Wiener process has received significant attention in the literature,\ninference in those driven by fractional Brownian motion seem to have seen much\nless development in comparison, despite their importance in modeling long range\ndependence. In this article, we consider both classical and Bayesian inference\nin such fractional Brownian motion based SDEs. In particular, we consider\nasymptotic inference for two parameters in this regard; a multiplicative\nparameter associated with the drift function, and the so-called \"Hurst\nparameter\" of the fractional Brownian motion, when the time domain tends to\ninfinity. For unknown Hurst parameter, the likelihood does not lend itself\namenable to the popular Girsanov form, rendering usual asymptotic development\ndifficult. As such, we develop increasing domain infill asymptotic theory, by\ndiscretizing the SDE. In this setup, we establish consistency and asymptotic\nnormality of the maximum likelihood estimators, as well as consistency and\nasymptotic normality of the Bayesian posterior distributions. However,\nclassical or Bayesian asymptotic normality with respect to the Hurst parameter\ncould not be established. We supplement our theoretical investigations with\nsimulation studies in a non-asymptotic setup, prescribing suitable\nmethodologies for classical and Bayesian analyses of SDEs driven by fractional\nBrownian motion. Applications to a real, close price data, along with\ncomparison with standard SDE driven by Wiener process, is also considered. As\nexpected, it turned out that our Bayesian fractional SDE triumphed over the\nother model and methods, in both simulated and real data applications.",
          "arxiv_id": "2005.09577v4"
        },
        {
          "title": "Parameter estimation of stochastic differential equation driven by small fractional noise",
          "year": "2022-01",
          "abstract": "We study the problem of parametric estimation for continuously observed\nstochastic processes driven by additive small fractional Brownian motion with\nHurst index 0<H<1/2 and 1/2<H<1. Under some assumptions on the drift\ncoefficient, we obtain the asymptotic normality and moment convergence of\nmaximum likelihood estimator of the drift parameter .",
          "arxiv_id": "2201.00372v1"
        },
        {
          "title": "The maximum likelihood type estimator of SDEs with fractional Brownian motion under small noise asymptotics in the rough case",
          "year": "2024-06",
          "abstract": "We study the problem of parametric estimation for continuously observed\nstochastic differential equation driven by fractional Brownian motion. Under\nsome assumptions on drift and diffusion coefficients, we construct maximum\nlikelihood estimator and establish its the asymptotic normality and moment\nconvergence of the drift parameter when a small dispersion coefficient\nvanishes.",
          "arxiv_id": "2406.07804v3"
        }
      ],
      "12": [
        {
          "title": "Optimal estimation in private distributed functional data analysis",
          "year": "2024-12",
          "abstract": "We systematically investigate the preservation of differential privacy in\nfunctional data analysis, beginning with functional mean estimation and\nextending to varying coefficient model estimation. Our work introduces a\ndistributed learning framework involving multiple servers, each responsible for\ncollecting several sparsely observed functions. This hierarchical setup\nintroduces a mixed notion of privacy. Within each function, user-level\ndifferential privacy is applied to $m$ discrete observations. At the server\nlevel, central differential privacy is deployed to account for the centralised\nnature of data collection. Across servers, only private information is\nexchanged, adhering to federated differential privacy constraints. To address\nthis complex hierarchy, we employ minimax theory to reveal several fundamental\nphenomena: from sparse to dense functional data analysis, from user-level to\ncentral and federated differential privacy costs, and the intricate interplay\nbetween different regimes of functional data analysis and privacy preservation.\n  To the best of our knowledge, this is the first study to rigorously examine\nfunctional data estimation under multiple privacy constraints. Our theoretical\nfindings are complemented by efficient private algorithms and extensive\nnumerical evidence, providing a comprehensive exploration of this challenging\nproblem.",
          "arxiv_id": "2412.06582v1"
        },
        {
          "title": "About the Cost of Central Privacy in Density Estimation",
          "year": "2023-06",
          "abstract": "We study non-parametric density estimation for densities in Lipschitz and\nSobolev spaces, and under central privacy. In particular, we investigate\nregimes where the privacy budget is not supposed to be constant. We consider\nthe classical definition of central differential privacy, but also the more\nrecent notion of central concentrated differential privacy. We recover the\nresult of Barber and Duchi (2014) stating that histogram estimators are optimal\nagainst Lipschitz distributions for the L2 risk, and under regular differential\nprivacy, and we extend it to other norms and notions of privacy. Then, we\ninvestigate higher degrees of smoothness, drawing two conclusions: First, and\ncontrary to what happens with constant privacy budget (Wasserman and Zhou,\n2010), there are regimes where imposing privacy degrades the regular minimax\nrisk of estimation on Sobolev densities. Second, so-called projection\nestimators are near-optimal against the same classes of densities in this new\nsetup with pure differential privacy, but contrary to the constant privacy\nbudget case, it comes at the cost of relaxation. With zero concentrated\ndifferential privacy, there is no need for relaxation, and we prove that the\nestimation is optimal.",
          "arxiv_id": "2306.14535v4"
        },
        {
          "title": "Private sampling: a noiseless approach for generating differentially private synthetic data",
          "year": "2021-09",
          "abstract": "In a world where artificial intelligence and data science become omnipresent,\ndata sharing is increasingly locking horns with data-privacy concerns.\nDifferential privacy has emerged as a rigorous framework for protecting\nindividual privacy in a statistical database, while releasing useful\nstatistical information about the database. The standard way to implement\ndifferential privacy is to inject a sufficient amount of noise into the data.\nHowever, in addition to other limitations of differential privacy, this process\nof adding noise will affect data accuracy and utility. Another approach to\nenable privacy in data sharing is based on the concept of synthetic data. The\ngoal of synthetic data is to create an as-realistic-as-possible dataset, one\nthat not only maintains the nuances of the original data, but does so without\nrisk of exposing sensitive information. The combination of differential privacy\nwith synthetic data has been suggested as a best-of-both-worlds solutions. In\nthis work, we propose the first noisefree method to construct differentially\nprivate synthetic data; we do this through a mechanism called \"private\nsampling\". Using the Boolean cube as benchmark data model, we derive explicit\nbounds on accuracy and privacy of the constructed synthetic data. The key\nmathematical tools are hypercontractivity, duality, and empirical processes. A\ncore ingredient of our private sampling mechanism is a rigorous \"marginal\ncorrection\" method, which has the remarkable property that importance\nreweighting can be utilized to exactly match the marginals of the sample to the\nmarginals of the population.",
          "arxiv_id": "2109.14839v1"
        }
      ],
      "13": [
        {
          "title": "On the links between Stein transforms and concentration inequalities for dependent random variables",
          "year": "2022-11",
          "abstract": "In this paper, we explore some links between transforms derived by Stein's\nmethod and concentration inequalities. In particular, we show that the\nstochastic domination of the zero bias transform of a random variable is\nequivalent to sub-Gaussian concentration. For this purpose a new stochastic\norder is considered. In a second time, we study the case of functions of\nslightly dependent light-tailed random variables. We are able to recover the\nfamous McDiarmid type of concentration inequality for functions with the\nbounded difference property. Additionally, we obtain new concentration bounds\nwhen we authorize a light dependence between the random variables. Finally, we\ngive a analogous result for another type of Stein's transform, the so-called\nsize bias transform.",
          "arxiv_id": "2211.13211v1"
        },
        {
          "title": "Sharp Concentration Results for Heavy-Tailed Distributions",
          "year": "2020-03",
          "abstract": "We obtain concentration and large deviation for the sums of independent and\nidentically distributed random variables with heavy-tailed distributions. Our\nconcentration results are concerned with random variables whose distributions\nsatisfy $\\mathbb{P}(X>t) \\leq {\\rm e}^{- I(t)}$, where $I: \\mathbb{R}\n\\rightarrow \\mathbb{R}$ is an increasing function and $I(t)/t \\rightarrow\n\\alpha \\in [0, \\infty)$ as $t \\rightarrow \\infty$. Our main theorem can not\nonly recover some of the existing results, such as the concentration of the sum\nof subWeibull random variables, but it can also produce new results for the sum\nof random variables with heavier tails. We show that the concentration\ninequalities we obtain are sharp enough to offer large deviation results for\nthe sums of independent random variables as well. Our analyses which are based\non standard truncation arguments simplify, unify and generalize the existing\nresults on the concentration and large deviation of heavy-tailed random\nvariables.",
          "arxiv_id": "2003.13819v3"
        },
        {
          "title": "Tight Concentration Inequality for Sub-Weibull Random Variables with Generalized Bernstien Orlicz norm",
          "year": "2023-02",
          "abstract": "Recent development in high-dimensional statistical inference has necessitated\nconcentration inequalities for a broader range of random variables. We focus on\nsub-Weibull random variables, which extend sub-Gaussian or sub-exponential\nrandom variables to allow heavy-tailed distributions. This paper presents\nconcentration inequalities for independent sub-Weibull random variables with\nfinite Generalized Bernstein-Orlicz norms, providing generalized Bernstein's\ninequalities and Rosenthal-type moment bounds. The tightness of the proposed\nbounds is shown through lower bounds of the concentration inequalities obtained\nvia the Paley-Zygmund inequality. The results are applied to a graphical model\ninference problem, improving previous sample complexity bounds.",
          "arxiv_id": "2302.03850v2"
        }
      ],
      "14": [
        {
          "title": "Large sample correlation matrices: a comparison theorem and its applications",
          "year": "2022-01",
          "abstract": "In this paper, we show that the diagonal of a high-dimensional sample\ncovariance matrix stemming from $n$ independent observations of a\n$p$-dimensional time series with finite fourth moments can be approximated in\nspectral norm by the diagonal of the population covariance matrix. We assume\nthat $n,p\\to \\infty$ with $p/n$ tending to a constant which might be positive\nor zero. As applications, we provide an approximation of the sample correlation\nmatrix ${\\mathbf R}$ and derive a variety of results for its eigenvalues. We\nidentify the limiting spectral distribution of ${\\mathbf R}$ and construct an\nestimator for the population correlation matrix and its eigenvalues. Finally,\nthe almost sure limits of the extreme eigenvalues of ${\\mathbf R}$ in a\ngeneralized spiked correlation model are analyzed.",
          "arxiv_id": "2201.00916v1"
        },
        {
          "title": "A CLT for the LSS of large dimensional sample covariance matrices with diverging spikes",
          "year": "2022-12",
          "abstract": "In this paper, we establish the central limit theorem (CLT) for linear\nspectral statistics (LSSs) of a large-dimensional sample covariance matrix when\nthe population covariance matrices are involved with diverging spikes. This\nconstitutes a nontrivial extension of the Bai-Silverstein theorem (BST) (Ann\nProbab 32(1):553--605, 2004), a theorem that has strongly influenced the\ndevelopment of high-dimensional statistics, especially in the applications of\nrandom matrix theory to statistics. Recently, there has been a growing\nrealization that the assumption of uniform boundedness of the population\ncovariance matrices in the BST is not satisfied in some fields, such as\neconomics, where the variances of principal components may diverge as the\ndimension tends to infinity. Therefore, in this paper, we aim to eliminate this\nobstacle to applications of the BST. Our new CLT accommodates spiked\neigenvalues, which may either be bounded or tend to infinity. A distinguishing\nfeature of our result is that the variance in the new CLT is related to both\nspiked eigenvalues and bulk eigenvalues, with dominance being determined by the\ndivergence rate of the largest spiked eigenvalues. The new CLT for LSS is then\napplied to test the hypothesis that the population covariance matrix is the\nidentity matrix or a generalized spiked model. The asymptotic distributions of\nthe corrected likelihood ratio test statistic and the corrected Nagao's trace\ntest statistic are derived under the alternative hypothesis. Moreover, we\npresent power comparisons between these two LSSs and Roy's largest root test.\nIn particular, we demonstrate that except for the case in which the number of\nspikes is equal to one, the LSSs could exhibit higher asymptotic power than\nRoy's largest root test.",
          "arxiv_id": "2212.05896v4"
        },
        {
          "title": "A CLT for the LSS of large dimensional sample covariance matrices with unbounded dispersions",
          "year": "2022-05",
          "abstract": "In this paper, we establish the central limit theorem (CLT) for linear\nspectral statistics (LSS) of large-dimensional sample covariance matrix when\nthe population covariance matrices are not uniformly bounded, which is a\nnontrivial extension of the Bai-Silverstein theorem (BST) (2004). The latter\nhas strongly stimulated the development of high-dimensional statistics,\nespecially the application of random matrix theory to statistics. However, the\nassumption of uniform boundedness of the population covariance matrices is\nfound strongly limited to the applications of BST. The aim of this paper is to\nremove the blockages to the applications of BST. The new CLT, allows the spiked\neigenvalues to exist and tend to infinity. It is interesting to note that the\nroles of either spiked eigenvalues or the bulk eigenvalues or both of the two\nare dominating in the CLT.\n  Moreover, the results are checked by simulation studies with various\npopulation settings. The CLT for LSS is then applied for testing the hypothesis\nthat a covariance matrix $ \\bSi $ is equal to an identity matrix. For this, the\nasymptotic distributions for the corrected likelihood ratio test (LRT) and\nNagao's trace test (NT) under alternative are derived, and we also propose the\nasymptotic power of LRT and NT under certain alternatives.",
          "arxiv_id": "2205.07280v1"
        }
      ],
      "15": [
        {
          "title": "On the maximum likelihood degree for Gaussian graphical models",
          "year": "2024-10",
          "abstract": "In this paper we revisit the likelihood geometry of Gaussian graphical\nmodels. We give a detailed proof that the ML-degree behaves monotonically on\ninduced subgraphs. Furthermore, we complete a missing argument that the\nML-degree of the $n$-th cycle is larger than one for any $n\\geq 4$, therefore\ncompleting the characterization that the only Gaussian graphical models with\nrational maximum likelihood estimator are the ones corresponding to chordal\n(decomposable) graphs. Finally, we prove that the formula for the ML-degree of\na cycle conjectured by Drton, Sturmfels and Sullivant provides a correct lower\nbound.",
          "arxiv_id": "2410.07007v1"
        },
        {
          "title": "Families of polytopes with rational linear precision in higher dimensions",
          "year": "2021-09",
          "abstract": "In this article we introduce a new family of lattice polytopes with rational\nlinear precision. For this purpose, we define a new class of discrete\nstatistical models that we call multinomial staged tree models. We prove that\nthese models have rational maximum likelihood estimators (MLE) and give a\ncriterion for these models to be log-linear. Our main result is then obtained\nby applying Garcia-Puente and Sottile's theorem that establishes a\ncorrespondence between polytopes with rational linear precision and log-linear\nmodels with rational MLE. Throughout this article we also study the interplay\nbetween the primitive collections of the normal fan of a polytope with rational\nlinear precision and the shape of the Horn matrix of its corresponding\nstatistical model. Finally, we investigate lattice polytopes arising from toric\nmultinomial staged tree models, in terms of the combinatorics of their tree\nrepresentations.",
          "arxiv_id": "2109.08151v2"
        },
        {
          "title": "Homaloidal Polynomials and Gaussian Models of Maximum Likelihood Degree One",
          "year": "2024-02",
          "abstract": "We study the Gaussian statistical models whose log-likelihood function has a\nunique complex critical point, i.e., has maximum likelihood degree one. We\nexploit the connection developed by Am\\'endola et. al. between the models\nhaving maximum likelihood degree one and homaloidal polynomials. We study the\nspanning tree generating function of a graph and show this polynomial is\nhomaloidal when the graph is chordal. When the graph is a cycle on $n$\nvertices, $n \\geq 4$, we prove the polynomial is not homaloidal, and show that\nthe maximum likelihood degree of the resulting model is the $n$th Eulerian\nnumber. These results support our conjecture that the spanning tree generating\nfunction is a homaloidal polynomial if and only if the graph is chordal. We\nalso provide an algebraic formulation for the defining equations of these\nmodels. Using existing results, we provide a computational study on\nconstructing new families of homaloidal polynomials. In the end, we analyze the\nsymmetric determinantal representation of such polynomials and provide an upper\nbound on the size of the matrices involved.",
          "arxiv_id": "2402.06090v3"
        }
      ],
      "16": [
        {
          "title": "Multi-Scale Conformal Prediction: A Theoretical Framework with Coverage Guarantees",
          "year": "2025-02",
          "abstract": "We propose a multi-scale extension of conformal prediction, an approach that\nconstructs prediction sets with finite-sample coverage guarantees under minimal\nstatistical assumptions. Classic conformal prediction relies on a single notion\nof conformity, overlooking the multi-level structures that arise in\napplications such as image analysis, hierarchical data exploration, and\nmulti-resolution time series modeling. In contrast, the proposed framework\ndefines a distinct conformity function at each relevant scale or resolution,\nproducing multiple conformal predictors whose prediction sets are then\nintersected to form the final multi-scale output. We establish theoretical\nresults confirming that the multi-scale prediction set retains the marginal\ncoverage guarantees of the original conformal framework and can, in fact, yield\nsmaller or more precise sets in practice. By distributing the total miscoverage\nprobability across scales in proportion to their informative power, the method\nfurther refines the set sizes. We also show that dependence between scales can\nlead to conservative coverage, ensuring that the actual coverage exceeds the\nnominal level. Numerical experiments in a synthetic classification setting\ndemonstrate that multi-scale conformal prediction achieves or surpasses the\nnominal coverage level while generating smaller prediction sets compared to\nsingle-scale conformal methods.",
          "arxiv_id": "2502.05565v1"
        },
        {
          "title": "Localized Conformal Prediction: A Generalized Inference Framework for Conformal Prediction",
          "year": "2021-06",
          "abstract": "We propose a new inference framework called localized conformal prediction.\nIt generalizes the framework of conformal prediction by offering a\nsingle-test-sample adaptive construction that emphasizes a local region around\nthis test sample, and can be combined with different conformal score\nconstructions. The proposed framework enjoys an assumption-free finite sample\nmarginal coverage guarantee, and it also offers additional local coverage\nguarantees under suitable assumptions. We demonstrate how to change from\nconformal prediction to localized conformal prediction using several conformal\nscores, and we illustrate a potential gain via numerical examples.",
          "arxiv_id": "2106.08460v2"
        },
        {
          "title": "Conditional validity and a fast approximation formula of full conformal prediction sets",
          "year": "2025-08",
          "abstract": "Prediction sets based on full conformal prediction have seen an increasing\ninterest in statistical learning due to their universal marginal coverage\nguarantees. However, practitioners have refrained from using it in applications\nfor two reasons: Firstly, it comes at very high computational costs, exceeding\neven that of cross-validation. Secondly, an applicant is typically not\ninterested in a marginal coverage guarantee which averages over all possible\n(but not available) training data sets, but rather in a guarantee conditional\non the specific training data. This work tackles these problems by, firstly,\nshowing that full conformal prediction sets are conditionally conservative\ngiven the training data if the conformity score is stochastically bounded and\nsatisfies a stability condition. Secondly, we propose an approximation for the\nfull conformal prediction set that has asymptotically the same training\nconditional coverage as full conformal prediction under the stability\nassumption derived before, and can be computed more easily. Furthermore, we\nshow that under the stability assumption, $n$-fold cross-conformal prediction\nalso has the same asymptotic training conditional coverage guarantees as full\nconformal prediction. If the conformity score is defined as the out-of-sample\nprediction error, our approximation of the full conformal set coincides with\nthe symmetrized Jackknife. We conclude that for this conformity score, if based\non a stable prediction algorithm, full-conformal, $n$-fold cross-conformal, the\nJackknife+, our approximation formula, and hence also the Jackknife, all yield\nthe same asymptotic training conditional coverage guarantees.",
          "arxiv_id": "2508.05272v1"
        }
      ],
      "17": [
        {
          "title": "Confidence in Causal Discovery with Linear Causal Models",
          "year": "2021-06",
          "abstract": "Structural causal models postulate noisy functional relations among a set of\ninteracting variables. The causal structure underlying each such model is\nnaturally represented by a directed graph whose edges indicate for each\nvariable which other variables it causally depends upon. Under a number of\ndifferent model assumptions, it has been shown that this causal graph and, thus\nalso, causal effects are identifiable from mere observational data. For these\nmodels, practical algorithms have been devised to learn the graph. Moreover,\nwhen the graph is known, standard techniques may be used to give estimates and\nconfidence intervals for causal effects. We argue, however, that a two-step\nmethod that first learns a graph and then treats the graph as known yields\nconfidence intervals that are overly optimistic and can drastically fail to\naccount for the uncertain causal structure. To address this issue we lay out a\nframework based on test inversion that allows us to give confidence regions for\ntotal causal effects that capture both sources of uncertainty: causal structure\nand numerical size of nonzero effects. Our ideas are developed in the context\nof bivariate linear causal models with homoscedastic errors, but as we\nexemplify they are generalizable to larger systems as well as other settings\nsuch as, in particular, linear non-Gaussian models.",
          "arxiv_id": "2106.05694v1"
        },
        {
          "title": "Partial Homoscedasticity in Causal Discovery with Linear Models",
          "year": "2023-08",
          "abstract": "Recursive linear structural equation models and the associated directed\nacyclic graphs (DAGs) play an important role in causal discovery. The classic\nidentifiability result for this class of models states that when only\nobservational data is available, each DAG can be identified only up to a Markov\nequivalence class. In contrast, recent work has shown that the DAG can be\nuniquely identified if the errors in the model are homoscedastic, i.e., all\nhave the same variance. This equal variance assumption yields methods that, if\nappropriate, are highly scalable and also sheds light on fundamental\ninformation-theoretic limits and optimality in causal discovery. In this paper,\nwe fill the gap that exists between the two previously considered cases, which\nassume the error variances to be either arbitrary or all equal. Specifically,\nwe formulate a framework of partial homoscedasticity, in which the variables\nare partitioned into blocks and each block shares the same error variance. For\nany such groupwise equal variances assumption, we characterize when two DAGs\ngive rise to identical Gaussian linear structural equation models. Furthermore,\nwe show how the resulting distributional equivalence classes may be represented\nusing a completed partially directed acyclic graph (CPDAG), and we give an\nalgorithm to efficiently construct this CPDAG. In a simulation study, we\ndemonstrate that greedy search provides an effective way to learn the CPDAG and\nexploit partial knowledge about homoscedasticity of errors in structural\nequation models.",
          "arxiv_id": "2308.08959v1"
        },
        {
          "title": "Confidence in Causal Inference under Structure Uncertainty in Linear Causal Models with Equal Variances",
          "year": "2023-09",
          "abstract": "Inferring the effect of interventions within complex systems is a fundamental\nproblem of statistics. A widely studied approach employs structural causal\nmodels that postulate noisy functional relations among a set of interacting\nvariables. The underlying causal structure is then naturally represented by a\ndirected graph whose edges indicate direct causal dependencies. In a recent\nline of work, additional assumptions on the causal models have been shown to\nrender this causal graph identifiable from observational data alone. One\nexample is the assumption of linear causal relations with equal error variances\nthat we will take up in this work. When the graph structure is known, classical\nmethods may be used for calculating estimates and confidence intervals for\ncausal effects. However, in many applications, expert knowledge that provides\nan a priori valid causal structure is not available. Lacking alternatives, a\ncommonly used two-step approach first learns a graph and then treats the graph\nas known in inference. This, however, yields confidence intervals that are\noverly optimistic and fail to account for the data-driven model choice. We\nargue that to draw reliable conclusions, it is necessary to incorporate the\nremaining uncertainty about the underlying causal structure in confidence\nstatements about causal effects. To address this issue, we present a framework\nbased on test inversion that allows us to give confidence regions for total\ncausal effects that capture both sources of uncertainty: causal structure and\nnumerical size of nonzero effects.",
          "arxiv_id": "2309.04298v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T20:09:15Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}