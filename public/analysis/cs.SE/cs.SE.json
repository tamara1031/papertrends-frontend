{
  "topics": {
    "data": {
      "0": {
        "name": "0_code_LLMs_models_tasks",
        "keywords": [
          [
            "code",
            0.026112029315627394
          ],
          [
            "LLMs",
            0.018101209080969106
          ],
          [
            "models",
            0.013248991593216713
          ],
          [
            "tasks",
            0.012399877979388022
          ],
          [
            "generation",
            0.011977897468757503
          ],
          [
            "language",
            0.011181677422876759
          ],
          [
            "LLM",
            0.010977331214520169
          ],
          [
            "Code",
            0.010608471096069435
          ],
          [
            "code generation",
            0.009831864481477872
          ],
          [
            "programming",
            0.00879238697209395
          ]
        ],
        "count": 2575
      },
      "1": {
        "name": "1_systems_safety_IoT_Systems",
        "keywords": [
          [
            "systems",
            0.02080374805063655
          ],
          [
            "safety",
            0.018996360113269058
          ],
          [
            "IoT",
            0.01168986175514111
          ],
          [
            "Systems",
            0.010730275791115084
          ],
          [
            "testing",
            0.010603277113619346
          ],
          [
            "autonomous",
            0.010482052045219546
          ],
          [
            "scenarios",
            0.009982616789486538
          ],
          [
            "approach",
            0.009615197999256945
          ],
          [
            "paper",
            0.009604138805959919
          ],
          [
            "driving",
            0.009464091370499403
          ]
        ],
        "count": 1205
      },
      "2": {
        "name": "2_software_development_research_Software",
        "keywords": [
          [
            "software",
            0.027262309638869168
          ],
          [
            "development",
            0.016104216656878462
          ],
          [
            "research",
            0.016041188625284374
          ],
          [
            "Software",
            0.015085314545247626
          ],
          [
            "engineering",
            0.01374413848666192
          ],
          [
            "study",
            0.0136586129542492
          ],
          [
            "teams",
            0.013538603835451801
          ],
          [
            "software engineering",
            0.01319766227894607
          ],
          [
            "agile",
            0.012220444294332108
          ],
          [
            "software development",
            0.011758733433227865
          ]
        ],
        "count": 779
      },
      "3": {
        "name": "3_apps_app_Android_privacy",
        "keywords": [
          [
            "apps",
            0.030321571017525875
          ],
          [
            "app",
            0.02780533870214166
          ],
          [
            "Android",
            0.02423912181774085
          ],
          [
            "privacy",
            0.01861252327326721
          ],
          [
            "mobile",
            0.0165589247594822
          ],
          [
            "user",
            0.014801453059508385
          ],
          [
            "game",
            0.012153386978920747
          ],
          [
            "data",
            0.010888960033161825
          ],
          [
            "users",
            0.010720186519849295
          ],
          [
            "developers",
            0.0100909793957397
          ]
        ],
        "count": 743
      },
      "4": {
        "name": "4_test_tests_testing_test cases",
        "keywords": [
          [
            "test",
            0.049052205823120344
          ],
          [
            "tests",
            0.021023248734910925
          ],
          [
            "testing",
            0.020759853665747476
          ],
          [
            "test cases",
            0.013904570697776316
          ],
          [
            "cases",
            0.013778963539331794
          ],
          [
            "Test",
            0.013184737695604446
          ],
          [
            "coverage",
            0.012822564927678498
          ],
          [
            "generation",
            0.011346368691452413
          ],
          [
            "unit",
            0.01031791199636173
          ],
          [
            "approach",
            0.00956828960704144
          ]
        ],
        "count": 713
      },
      "5": {
        "name": "5_Rust_programs_program_verification",
        "keywords": [
          [
            "Rust",
            0.02287798116151447
          ],
          [
            "programs",
            0.018384178664641763
          ],
          [
            "program",
            0.018294179530402688
          ],
          [
            "verification",
            0.016862554855429968
          ],
          [
            "memory",
            0.014012697855249735
          ],
          [
            "proof",
            0.012673454560146565
          ],
          [
            "language",
            0.012480928524224203
          ],
          [
            "code",
            0.010994053313596862
          ],
          [
            "analysis",
            0.010207395793758
          ],
          [
            "formal",
            0.009787102838869034
          ]
        ],
        "count": 461
      },
      "6": {
        "name": "6_requirements_Requirements_RE_software",
        "keywords": [
          [
            "requirements",
            0.05517560354707932
          ],
          [
            "Requirements",
            0.023837469652974383
          ],
          [
            "RE",
            0.018534659320177258
          ],
          [
            "software",
            0.01416312580136144
          ],
          [
            "engineering",
            0.012489743259716957
          ],
          [
            "requirements engineering",
            0.010964654051380985
          ],
          [
            "development",
            0.009718431423464192
          ],
          [
            "requirement",
            0.009689516736356844
          ],
          [
            "quality",
            0.00955916117389932
          ],
          [
            "user",
            0.009111086005255878
          ]
        ],
        "count": 395
      },
      "7": {
        "name": "7_security_software_dependencies_vulnerabilities",
        "keywords": [
          [
            "security",
            0.044596019474868645
          ],
          [
            "software",
            0.02138110180755675
          ],
          [
            "dependencies",
            0.017084666455889155
          ],
          [
            "vulnerabilities",
            0.016543100838122913
          ],
          [
            "packages",
            0.01599576120628935
          ],
          [
            "dependency",
            0.013701668982327669
          ],
          [
            "package",
            0.012921422867119975
          ],
          [
            "Security",
            0.012440239315202188
          ],
          [
            "developers",
            0.011073636147836866
          ],
          [
            "supply",
            0.010710749935154135
          ]
        ],
        "count": 389
      },
      "8": {
        "name": "8_smart_contracts_blockchain_smart contracts",
        "keywords": [
          [
            "smart",
            0.05410972121168034
          ],
          [
            "contracts",
            0.052702320184492106
          ],
          [
            "blockchain",
            0.04350483891438077
          ],
          [
            "smart contracts",
            0.03978555783255978
          ],
          [
            "contract",
            0.03493090371794917
          ],
          [
            "smart contract",
            0.024137858295557556
          ],
          [
            "Ethereum",
            0.021350195302391226
          ],
          [
            "Smart",
            0.021172532546940005
          ],
          [
            "Blockchain",
            0.01742469203836695
          ],
          [
            "vulnerabilities",
            0.015588902379189517
          ]
        ],
        "count": 371
      },
      "9": {
        "name": "9_microservices_cloud_microservice_applications",
        "keywords": [
          [
            "microservices",
            0.03509536895297276
          ],
          [
            "cloud",
            0.028437295800048137
          ],
          [
            "microservice",
            0.025510732057634575
          ],
          [
            "applications",
            0.01644519774459038
          ],
          [
            "serverless",
            0.014508550628479321
          ],
          [
            "application",
            0.014106328012437296
          ],
          [
            "Microservices",
            0.013668422766434091
          ],
          [
            "service",
            0.01316893765315334
          ],
          [
            "architecture",
            0.012102530786806467
          ],
          [
            "systems",
            0.011682162357259183
          ]
        ],
        "count": 339
      },
      "10": {
        "name": "10_vulnerability_vulnerabilities_vulnerability detection_detection",
        "keywords": [
          [
            "vulnerability",
            0.04514470685179436
          ],
          [
            "vulnerabilities",
            0.024678589947069396
          ],
          [
            "vulnerability detection",
            0.02099503515799008
          ],
          [
            "detection",
            0.02053994645371718
          ],
          [
            "Vulnerability",
            0.017862208350676246
          ],
          [
            "code",
            0.01587663098435486
          ],
          [
            "security",
            0.015418901312857475
          ],
          [
            "vulnerable",
            0.012672264577996323
          ],
          [
            "learning",
            0.011632998734350343
          ],
          [
            "models",
            0.010994989610814227
          ]
        ],
        "count": 335
      },
      "11": {
        "name": "11_ML_Machine_machine learning_learning",
        "keywords": [
          [
            "ML",
            0.07449001930853243
          ],
          [
            "Machine",
            0.022991485350766117
          ],
          [
            "machine learning",
            0.022250183810018233
          ],
          [
            "learning",
            0.021965369595185667
          ],
          [
            "machine",
            0.021568450302874156
          ],
          [
            "Learning",
            0.018423511838619896
          ],
          [
            "data",
            0.018099903869665174
          ],
          [
            "systems",
            0.01686075387442939
          ],
          [
            "MLOps",
            0.016438319071609347
          ],
          [
            "model",
            0.014462721018070562
          ]
        ],
        "count": 295
      },
      "12": {
        "name": "12_DNN_DL_Deep_DNNs",
        "keywords": [
          [
            "DNN",
            0.0346714268173825
          ],
          [
            "DL",
            0.024272455678617046
          ],
          [
            "Deep",
            0.021591349126866728
          ],
          [
            "DNNs",
            0.019435640283508714
          ],
          [
            "test",
            0.018224156003099838
          ],
          [
            "model",
            0.016726230511181106
          ],
          [
            "testing",
            0.014905215464481548
          ],
          [
            "Neural",
            0.014789959963368631
          ],
          [
            "neural",
            0.014619588468944242
          ],
          [
            "inputs",
            0.014137472971118056
          ]
        ],
        "count": 253
      },
      "13": {
        "name": "13_bug_defect_prediction_reports",
        "keywords": [
          [
            "bug",
            0.034369737827349374
          ],
          [
            "defect",
            0.027756993508151927
          ],
          [
            "prediction",
            0.02636882591391418
          ],
          [
            "reports",
            0.01992290761841924
          ],
          [
            "defect prediction",
            0.019443265259420684
          ],
          [
            "issue",
            0.01626782106206544
          ],
          [
            "bug reports",
            0.015181000309534504
          ],
          [
            "software",
            0.014861865036619366
          ],
          [
            "models",
            0.013005969256307908
          ],
          [
            "projects",
            0.011901395650927432
          ]
        ],
        "count": 245
      },
      "14": {
        "name": "14_log_logs_anomaly_parsing",
        "keywords": [
          [
            "log",
            0.06991358431675175
          ],
          [
            "logs",
            0.027921280441842024
          ],
          [
            "anomaly",
            0.02362994872755447
          ],
          [
            "parsing",
            0.019020180819951032
          ],
          [
            "Log",
            0.018580380358498395
          ],
          [
            "data",
            0.016605047921496634
          ],
          [
            "detection",
            0.016561029140819576
          ],
          [
            "log parsing",
            0.012259402331691423
          ],
          [
            "systems",
            0.01112489686707399
          ],
          [
            "anomalies",
            0.010902698237681818
          ]
        ],
        "count": 234
      },
      "15": {
        "name": "15_refactoring_code_developers_refactorings",
        "keywords": [
          [
            "refactoring",
            0.0475990297925992
          ],
          [
            "code",
            0.028568035619079044
          ],
          [
            "developers",
            0.01823085338638079
          ],
          [
            "refactorings",
            0.017880459094576834
          ],
          [
            "review",
            0.01663882095331273
          ],
          [
            "software",
            0.016045603924154904
          ],
          [
            "SATD",
            0.015019092516591446
          ],
          [
            "debt",
            0.013818352809435497
          ],
          [
            "Refactoring",
            0.013639627640437382
          ],
          [
            "quality",
            0.012325229834621965
          ]
        ],
        "count": 225
      },
      "16": {
        "name": "16_quantum_Quantum_quantum software_computing",
        "keywords": [
          [
            "quantum",
            0.1393039323233231
          ],
          [
            "Quantum",
            0.05614252251960186
          ],
          [
            "quantum software",
            0.03533650579169919
          ],
          [
            "computing",
            0.028740954840834713
          ],
          [
            "classical",
            0.027879165406594
          ],
          [
            "software",
            0.0258347486050244
          ],
          [
            "quantum computing",
            0.023416541667335505
          ],
          [
            "programs",
            0.01789077173248893
          ],
          [
            "computers",
            0.015496297897309323
          ],
          [
            "circuit",
            0.015371240381360155
          ]
        ],
        "count": 221
      },
      "17": {
        "name": "17_scientific_software_data_research",
        "keywords": [
          [
            "scientific",
            0.027358941830640103
          ],
          [
            "software",
            0.01926139883272446
          ],
          [
            "data",
            0.01684849057840917
          ],
          [
            "research",
            0.015953313071460765
          ],
          [
            "HPC",
            0.01537905622971054
          ],
          [
            "workflows",
            0.015323149450334955
          ],
          [
            "reproducibility",
            0.014748043954419946
          ],
          [
            "notebooks",
            0.014319429954464703
          ],
          [
            "Jupyter",
            0.012785766848152502
          ],
          [
            "computational",
            0.012312472007344106
          ]
        ],
        "count": 217
      },
      "18": {
        "name": "18_process_business_modeling_Process",
        "keywords": [
          [
            "process",
            0.05088969364708593
          ],
          [
            "business",
            0.02762179749956969
          ],
          [
            "modeling",
            0.02723161539875496
          ],
          [
            "Process",
            0.021611231388350874
          ],
          [
            "TM",
            0.02136580831894808
          ],
          [
            "processes",
            0.02102401702339238
          ],
          [
            "model",
            0.0202861595925476
          ],
          [
            "event",
            0.018557642458877692
          ],
          [
            "models",
            0.014565669837630659
          ],
          [
            "business process",
            0.014321743745945512
          ]
        ],
        "count": 206
      },
      "19": {
        "name": "19_OSS_projects_source_open",
        "keywords": [
          [
            "OSS",
            0.048934374990298304
          ],
          [
            "projects",
            0.03330195174402746
          ],
          [
            "source",
            0.025675087999305967
          ],
          [
            "open",
            0.025466489461488297
          ],
          [
            "open source",
            0.024985346028827623
          ],
          [
            "Open",
            0.021410838475734488
          ],
          [
            "software",
            0.019681021015494737
          ],
          [
            "GitHub",
            0.018773161729443617
          ],
          [
            "project",
            0.01791386976246215
          ],
          [
            "Source",
            0.01749217805143197
          ]
        ],
        "count": 199
      },
      "20": {
        "name": "20_code_models_security_LLMs",
        "keywords": [
          [
            "code",
            0.035383945677860115
          ],
          [
            "models",
            0.02312297826026167
          ],
          [
            "security",
            0.020262653760141174
          ],
          [
            "LLMs",
            0.017971554934982457
          ],
          [
            "adversarial",
            0.016995288574964174
          ],
          [
            "Code",
            0.0154510690358764
          ],
          [
            "generation",
            0.014127536612948093
          ],
          [
            "code generation",
            0.01370899926701213
          ],
          [
            "attacks",
            0.012435978051028954
          ],
          [
            "generated",
            0.012011094727497963
          ]
        ],
        "count": 173
      },
      "21": {
        "name": "21_fuzzing_Fuzzing_fuzzers_coverage",
        "keywords": [
          [
            "fuzzing",
            0.05726114185692315
          ],
          [
            "Fuzzing",
            0.02785638583331969
          ],
          [
            "fuzzers",
            0.027200531904491156
          ],
          [
            "coverage",
            0.021159546009541737
          ],
          [
            "bugs",
            0.0210751314220266
          ],
          [
            "fuzzer",
            0.020434791945180646
          ],
          [
            "fuzz",
            0.0160958469782078
          ],
          [
            "testing",
            0.015735080399415225
          ],
          [
            "AFL",
            0.015210075315994083
          ],
          [
            "inputs",
            0.014386153393049566
          ]
        ],
        "count": 164
      },
      "22": {
        "name": "22_AI_systems_ethical_development",
        "keywords": [
          [
            "AI",
            0.0892374043273809
          ],
          [
            "systems",
            0.026572119426835916
          ],
          [
            "ethical",
            0.01650988599188527
          ],
          [
            "development",
            0.015754376686030674
          ],
          [
            "ethics",
            0.014071168457942346
          ],
          [
            "software",
            0.0133253849576948
          ],
          [
            "Artificial",
            0.012167599122319855
          ],
          [
            "intelligence",
            0.011568801154578711
          ],
          [
            "challenges",
            0.011086808408182073
          ],
          [
            "engineering",
            0.010870585122979712
          ]
        ],
        "count": 159
      },
      "23": {
        "name": "23_comments_review_code_comment",
        "keywords": [
          [
            "comments",
            0.042654552613536234
          ],
          [
            "review",
            0.03804305696212045
          ],
          [
            "code",
            0.03683819085606641
          ],
          [
            "comment",
            0.03060847173433705
          ],
          [
            "code review",
            0.02593685287634439
          ],
          [
            "Code",
            0.0190412344423989
          ],
          [
            "quality",
            0.013010291232946811
          ],
          [
            "Review",
            0.012359692698215194
          ],
          [
            "reviews",
            0.012166269705038013
          ],
          [
            "reviewers",
            0.011690339059527256
          ]
        ],
        "count": 156
      }
    },
    "correlations": [
      [
        1.0,
        -0.7302082714676481,
        -0.6661738593258151,
        -0.7393537500588594,
        -0.6348505493668437,
        -0.7129524047839347,
        -0.697073716587956,
        -0.6829200512195901,
        -0.7445364801364535,
        -0.739015662552764,
        -0.6896441444940911,
        -0.7149297505863804,
        -0.7166858813883462,
        -0.7051828509698823,
        -0.7403355742677284,
        -0.48144071369926367,
        -0.7610283786647565,
        -0.7156191300869496,
        -0.6838591401132718,
        -0.6675483339160431,
        0.6663716256933871,
        -0.7256879357473508,
        -0.6426784789558846,
        -0.4655269911010258
      ],
      [
        -0.7302082714676481,
        1.0,
        -0.6972784680983282,
        -0.7559284607821082,
        -0.6995034209894913,
        -0.7383649616714211,
        -0.6905487593066516,
        -0.6992549066904585,
        -0.7451055143158992,
        -0.7304034499891658,
        -0.7515902493395933,
        -0.7228203227424324,
        -0.7070417447487448,
        -0.7448655113559548,
        -0.7471138204053882,
        -0.73924066268467,
        -0.7598098544438732,
        -0.712120391502857,
        -0.7109615114411217,
        -0.7273845983368803,
        -0.7269480070992753,
        -0.7411635858382706,
        -0.5576899957046819,
        -0.7418253482742456
      ],
      [
        -0.6661738593258151,
        -0.6972784680983282,
        1.0,
        -0.7425505555750143,
        -0.6959128127640524,
        -0.7385722288415113,
        -0.670253169739061,
        -0.45246519351910996,
        -0.7483749832952568,
        -0.7312480297817152,
        -0.7383291096603781,
        -0.703356025049312,
        -0.7292297573799637,
        -0.7264893222620126,
        -0.754122242717306,
        -0.676373893436405,
        -0.7439463053562583,
        -0.5280656568133324,
        -0.6545949366409101,
        -0.6683958279337117,
        -0.6621069985863348,
        -0.747755986245525,
        -0.6769926896929306,
        -0.6794967824065737
      ],
      [
        -0.7393537500588594,
        -0.7559284607821082,
        -0.7425505555750143,
        1.0,
        -0.7342839104478571,
        -0.7640579565120165,
        -0.7292722356057266,
        -0.7190260711508671,
        -0.7559952803350951,
        -0.7438302820952647,
        -0.7472239675110585,
        -0.7461870132568205,
        -0.7464031853470512,
        -0.7321422848212769,
        -0.7576753043446641,
        -0.7367067455361537,
        -0.7640647201880431,
        -0.7482383589796635,
        -0.7481338639179023,
        -0.7437800166552231,
        -0.7376997399428247,
        -0.7520553188851377,
        -0.750913790353722,
        -0.7373765087021962
      ],
      [
        -0.6348505493668437,
        -0.6995034209894913,
        -0.6959128127640524,
        -0.7342839104478571,
        1.0,
        -0.7046953771425075,
        -0.7115946761745201,
        -0.7167537426684576,
        -0.7527463360635269,
        -0.7442473275044316,
        -0.7435851864585665,
        -0.7216444991466087,
        -0.6220327294142163,
        -0.7110386700021647,
        -0.7522790576263396,
        -0.7041458365061355,
        -0.7559046044037784,
        -0.7180793160029298,
        -0.7147569520997511,
        -0.7004466990553952,
        -0.6377723972247338,
        -0.669275096731832,
        -0.7228818062273111,
        -0.7158451885283512
      ],
      [
        -0.7129524047839347,
        -0.7383649616714211,
        -0.7385722288415113,
        -0.7640579565120165,
        -0.7046953771425075,
        1.0,
        -0.7440828491123462,
        -0.7355724993453228,
        -0.7517277002976139,
        -0.7510432876970562,
        -0.7398527591068733,
        -0.7521791251151364,
        -0.7396550730559814,
        -0.7416619588560699,
        -0.7620330813885801,
        -0.7304869025802836,
        -0.7566932443399197,
        -0.7344444534290471,
        -0.7295190005416543,
        -0.7318748457100803,
        -0.7093459140990989,
        -0.712382760523828,
        -0.7511789649799306,
        -0.7425366721614851
      ],
      [
        -0.697073716587956,
        -0.6905487593066516,
        -0.670253169739061,
        -0.7292722356057266,
        -0.7115946761745201,
        -0.7440828491123462,
        1.0,
        -0.7013216687984376,
        -0.7484591557845761,
        -0.7290034212581761,
        -0.7547098211989849,
        -0.7233128621651937,
        -0.7382613438500284,
        -0.752985925762649,
        -0.7607222948484509,
        -0.7369695491308752,
        -0.7598056256319643,
        -0.7183655569605591,
        -0.6803641100755168,
        -0.730096122921764,
        -0.6986117862383692,
        -0.7578874949999805,
        -0.7005257907485742,
        -0.732705775528059
      ],
      [
        -0.6829200512195901,
        -0.6992549066904585,
        -0.45246519351910996,
        -0.7190260711508671,
        -0.7167537426684576,
        -0.7355724993453228,
        -0.7013216687984376,
        1.0,
        -0.7196030235911444,
        -0.7180608198176186,
        -0.5408657452652265,
        -0.7224132718127638,
        -0.7367804190092878,
        -0.7353562568271703,
        -0.7478227448591463,
        -0.6899512492420462,
        -0.7575477709508025,
        -0.5726430066461683,
        -0.6967617787413529,
        -0.652457567942587,
        -0.6377164428475164,
        -0.7209875675171977,
        -0.707710058389053,
        -0.6998577235871517
      ],
      [
        -0.7445364801364535,
        -0.7451055143158992,
        -0.7483749832952568,
        -0.7559952803350951,
        -0.7527463360635269,
        -0.7517277002976139,
        -0.7484591557845761,
        -0.7196030235911444,
        1.0,
        -0.7504409304259312,
        -0.7030335494690892,
        -0.7570532906199104,
        -0.7607294137015936,
        -0.761394294002351,
        -0.759403338214026,
        -0.7506011121310252,
        -0.7632629733113413,
        -0.7529130949462697,
        -0.7380693881282083,
        -0.7509713539238336,
        -0.7425200549508726,
        -0.7416154552796753,
        -0.7510380701509493,
        -0.7501508627930589
      ],
      [
        -0.739015662552764,
        -0.7304034499891658,
        -0.7312480297817152,
        -0.7438302820952647,
        -0.7442473275044316,
        -0.7510432876970562,
        -0.7290034212581761,
        -0.7180608198176186,
        -0.7504409304259312,
        1.0,
        -0.7555590094902809,
        -0.7389700714992554,
        -0.7520261195066567,
        -0.7603403930645054,
        -0.7330081098111181,
        -0.733836660733702,
        -0.7545287759147618,
        -0.7247051681633456,
        -0.7279489698857984,
        -0.7286436903342183,
        -0.7383070879609444,
        -0.7509761373582671,
        -0.7355351052860764,
        -0.7440161883047649
      ],
      [
        -0.6896441444940911,
        -0.7515902493395933,
        -0.7383291096603781,
        -0.7472239675110585,
        -0.7435851864585665,
        -0.7398527591068733,
        -0.7547098211989849,
        -0.5408657452652265,
        -0.7030335494690892,
        -0.7555590094902809,
        1.0,
        -0.7354775742865183,
        -0.7195031216611477,
        -0.739139083110322,
        -0.7599874536350011,
        -0.7200189234227432,
        -0.7644155955538593,
        -0.7454930732324987,
        -0.7441051846064005,
        -0.7080961875085106,
        -0.6763279329817931,
        -0.713733554133478,
        -0.741588329080463,
        -0.7205132981096167
      ],
      [
        -0.7149297505863804,
        -0.7228203227424324,
        -0.703356025049312,
        -0.7461870132568205,
        -0.7216444991466087,
        -0.7521791251151364,
        -0.7233128621651937,
        -0.7224132718127638,
        -0.7570532906199104,
        -0.7389700714992554,
        -0.7354775742865183,
        1.0,
        -0.7135586013764987,
        -0.7106486550489051,
        -0.7445672214213161,
        -0.7238501962844696,
        -0.7494226853113615,
        -0.711368745141282,
        -0.7187111343323984,
        -0.7103272242773636,
        -0.7073908829117252,
        -0.7498309854487893,
        -0.6798982079444706,
        -0.7120510317720221
      ],
      [
        -0.7166858813883462,
        -0.7070417447487448,
        -0.7292297573799637,
        -0.7464031853470512,
        -0.6220327294142163,
        -0.7396550730559814,
        -0.7382613438500284,
        -0.7367804190092878,
        -0.7607294137015936,
        -0.7520261195066567,
        -0.7195031216611477,
        -0.7135586013764987,
        1.0,
        -0.7104093060295309,
        -0.7477269605257914,
        -0.7202795879988783,
        -0.760403713015237,
        -0.7306536867988066,
        -0.7273166240274023,
        -0.7369887026668297,
        -0.7000842737195125,
        -0.7168985122981153,
        -0.7342882878207969,
        -0.7209938649515666
      ],
      [
        -0.7051828509698823,
        -0.7448655113559548,
        -0.7264893222620126,
        -0.7321422848212769,
        -0.7110386700021647,
        -0.7416619588560699,
        -0.752985925762649,
        -0.7353562568271703,
        -0.761394294002351,
        -0.7603403930645054,
        -0.739139083110322,
        -0.7106486550489051,
        -0.7104093060295309,
        1.0,
        -0.7537914614630328,
        -0.7066201125491351,
        -0.757929890245214,
        -0.7301522369384118,
        -0.724442477443295,
        -0.6853186452165414,
        -0.7026122394522822,
        -0.6885586586152415,
        -0.7472256138264516,
        -0.7160897720212853
      ],
      [
        -0.7403355742677284,
        -0.7471138204053882,
        -0.754122242717306,
        -0.7576753043446641,
        -0.7522790576263396,
        -0.7620330813885801,
        -0.7607222948484509,
        -0.7478227448591463,
        -0.759403338214026,
        -0.7330081098111181,
        -0.7599874536350011,
        -0.7445672214213161,
        -0.7477269605257914,
        -0.7537914614630328,
        1.0,
        -0.751981027016535,
        -0.7645938261183569,
        -0.7486718514814221,
        -0.717706856688838,
        -0.7427995217022003,
        -0.7400431300848682,
        -0.7624053293457846,
        -0.7551825749674382,
        -0.7541759044707248
      ],
      [
        -0.48144071369926367,
        -0.73924066268467,
        -0.676373893436405,
        -0.7367067455361537,
        -0.7041458365061355,
        -0.7304869025802836,
        -0.7369695491308752,
        -0.6899512492420462,
        -0.7506011121310252,
        -0.733836660733702,
        -0.7200189234227432,
        -0.7238501962844696,
        -0.7202795879988783,
        -0.7066201125491351,
        -0.751981027016535,
        1.0,
        -0.75999975006916,
        -0.7190206702719988,
        -0.7092819340134175,
        -0.6483680998334949,
        -0.4600323011071741,
        -0.7364846555055511,
        -0.7285184556466666,
        -0.31757415828709307
      ],
      [
        -0.7610283786647565,
        -0.7598098544438732,
        -0.7439463053562583,
        -0.7640647201880431,
        -0.7559046044037784,
        -0.7566932443399197,
        -0.7598056256319643,
        -0.7575477709508025,
        -0.7632629733113413,
        -0.7545287759147618,
        -0.7644155955538593,
        -0.7494226853113615,
        -0.760403713015237,
        -0.757929890245214,
        -0.7645938261183569,
        -0.75999975006916,
        1.0,
        -0.7531070859526308,
        -0.7611646367247142,
        -0.7605336506489151,
        -0.7607693422455577,
        -0.7608607583364244,
        -0.7540514880968616,
        -0.7633862272691867
      ],
      [
        -0.7156191300869496,
        -0.712120391502857,
        -0.5280656568133324,
        -0.7482383589796635,
        -0.7180793160029298,
        -0.7344444534290471,
        -0.7183655569605591,
        -0.5726430066461683,
        -0.7529130949462697,
        -0.7247051681633456,
        -0.7454930732324987,
        -0.711368745141282,
        -0.7306536867988066,
        -0.7301522369384118,
        -0.7486718514814221,
        -0.7190206702719988,
        -0.7531070859526308,
        1.0,
        -0.6944131210455936,
        -0.6959305643479952,
        -0.7134874971426732,
        -0.7489767227699886,
        -0.7294472184738843,
        -0.7195222893126005
      ],
      [
        -0.6838591401132718,
        -0.7109615114411217,
        -0.6545949366409101,
        -0.7481338639179023,
        -0.7147569520997511,
        -0.7295190005416543,
        -0.6803641100755168,
        -0.6967617787413529,
        -0.7380693881282083,
        -0.7279489698857984,
        -0.7441051846064005,
        -0.7187111343323984,
        -0.7273166240274023,
        -0.724442477443295,
        -0.717706856688838,
        -0.7092819340134175,
        -0.7611646367247142,
        -0.6944131210455936,
        1.0,
        -0.7121197197432185,
        -0.6796381312110666,
        -0.752259645677533,
        -0.715957124034889,
        -0.7144719741604135
      ],
      [
        -0.6675483339160431,
        -0.7273845983368803,
        -0.6683958279337117,
        -0.7437800166552231,
        -0.7004466990553952,
        -0.7318748457100803,
        -0.730096122921764,
        -0.652457567942587,
        -0.7509713539238336,
        -0.7286436903342183,
        -0.7080961875085106,
        -0.7103272242773636,
        -0.7369887026668297,
        -0.6853186452165414,
        -0.7427995217022003,
        -0.6483680998334949,
        -0.7605336506489151,
        -0.6959305643479952,
        -0.7121197197432185,
        1.0,
        -0.6626050703701818,
        -0.7281383263446713,
        -0.7175255563985949,
        -0.6610864761601002
      ],
      [
        0.6663716256933871,
        -0.7269480070992753,
        -0.6621069985863348,
        -0.7376997399428247,
        -0.6377723972247338,
        -0.7093459140990989,
        -0.6986117862383692,
        -0.6377164428475164,
        -0.7425200549508726,
        -0.7383070879609444,
        -0.6763279329817931,
        -0.7073908829117252,
        -0.7000842737195125,
        -0.7026122394522822,
        -0.7400431300848682,
        -0.4600323011071741,
        -0.7607693422455577,
        -0.7134874971426732,
        -0.6796381312110666,
        -0.6626050703701818,
        1.0,
        -0.7267378484563314,
        -0.6418817294493019,
        -0.4438904346308661
      ],
      [
        -0.7256879357473508,
        -0.7411635858382706,
        -0.747755986245525,
        -0.7520553188851377,
        -0.669275096731832,
        -0.712382760523828,
        -0.7578874949999805,
        -0.7209875675171977,
        -0.7416154552796753,
        -0.7509761373582671,
        -0.713733554133478,
        -0.7498309854487893,
        -0.7168985122981153,
        -0.6885586586152415,
        -0.7624053293457846,
        -0.7364846555055511,
        -0.7608607583364244,
        -0.7489767227699886,
        -0.752259645677533,
        -0.7281383263446713,
        -0.7267378484563314,
        1.0,
        -0.7550991790298845,
        -0.7450317765413961
      ],
      [
        -0.6426784789558846,
        -0.5576899957046819,
        -0.6769926896929306,
        -0.750913790353722,
        -0.7228818062273111,
        -0.7511789649799306,
        -0.7005257907485742,
        -0.707710058389053,
        -0.7510380701509493,
        -0.7355351052860764,
        -0.741588329080463,
        -0.6798982079444706,
        -0.7342882878207969,
        -0.7472256138264516,
        -0.7551825749674382,
        -0.7285184556466666,
        -0.7540514880968616,
        -0.7294472184738843,
        -0.715957124034889,
        -0.7175255563985949,
        -0.6418817294493019,
        -0.7550991790298845,
        1.0,
        -0.717270786463396
      ],
      [
        -0.4655269911010258,
        -0.7418253482742456,
        -0.6794967824065737,
        -0.7373765087021962,
        -0.7158451885283512,
        -0.7425366721614851,
        -0.732705775528059,
        -0.6998577235871517,
        -0.7501508627930589,
        -0.7440161883047649,
        -0.7205132981096167,
        -0.7120510317720221,
        -0.7209938649515666,
        -0.7160897720212853,
        -0.7541759044707248,
        -0.31757415828709307,
        -0.7633862272691867,
        -0.7195222893126005,
        -0.7144719741604135,
        -0.6610864761601002,
        -0.4438904346308661,
        -0.7450317765413961,
        -0.717270786463396,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        15,
        4,
        6,
        4,
        9,
        3,
        7,
        4,
        5,
        3,
        0,
        6,
        2,
        1,
        3,
        3,
        1,
        1,
        4,
        4,
        1,
        2,
        3,
        2
      ],
      "2020-02": [
        18,
        2,
        10,
        5,
        12,
        1,
        6,
        2,
        6,
        3,
        0,
        6,
        4,
        1,
        0,
        5,
        0,
        1,
        1,
        7,
        4,
        1,
        0,
        0
      ],
      "2020-03": [
        17,
        5,
        10,
        4,
        5,
        1,
        8,
        4,
        4,
        2,
        0,
        4,
        1,
        5,
        3,
        5,
        0,
        5,
        4,
        10,
        4,
        0,
        5,
        3
      ],
      "2020-04": [
        23,
        5,
        10,
        2,
        12,
        5,
        8,
        2,
        8,
        9,
        2,
        5,
        3,
        3,
        4,
        6,
        0,
        4,
        10,
        2,
        5,
        4,
        6,
        3
      ],
      "2020-05": [
        22,
        8,
        8,
        7,
        13,
        2,
        11,
        5,
        8,
        2,
        1,
        9,
        5,
        5,
        0,
        11,
        1,
        5,
        16,
        14,
        4,
        3,
        1,
        3
      ],
      "2020-06": [
        12,
        3,
        10,
        4,
        9,
        3,
        11,
        7,
        1,
        3,
        4,
        7,
        3,
        1,
        2,
        3,
        1,
        2,
        7,
        10,
        3,
        1,
        2,
        4
      ],
      "2020-07": [
        22,
        4,
        12,
        6,
        10,
        8,
        12,
        1,
        9,
        6,
        1,
        5,
        1,
        4,
        1,
        6,
        1,
        2,
        11,
        10,
        3,
        2,
        4,
        1
      ],
      "2020-08": [
        13,
        8,
        10,
        13,
        7,
        3,
        5,
        2,
        9,
        4,
        4,
        10,
        5,
        4,
        3,
        4,
        0,
        2,
        13,
        13,
        5,
        1,
        3,
        1
      ],
      "2020-09": [
        12,
        7,
        8,
        2,
        9,
        0,
        10,
        5,
        6,
        5,
        2,
        9,
        4,
        2,
        4,
        4,
        0,
        7,
        9,
        13,
        7,
        1,
        4,
        7
      ],
      "2020-10": [
        13,
        5,
        13,
        4,
        4,
        1,
        5,
        1,
        2,
        3,
        2,
        8,
        6,
        5,
        1,
        4,
        0,
        7,
        7,
        16,
        2,
        2,
        3,
        7
      ],
      "2020-11": [
        11,
        1,
        14,
        5,
        9,
        4,
        5,
        0,
        2,
        1,
        1,
        9,
        5,
        6,
        1,
        3,
        0,
        1,
        11,
        8,
        3,
        1,
        3,
        4
      ],
      "2020-12": [
        27,
        4,
        11,
        7,
        14,
        2,
        6,
        7,
        4,
        3,
        1,
        9,
        2,
        7,
        1,
        3,
        0,
        3,
        21,
        8,
        3,
        3,
        3,
        6
      ],
      "2021-01": [
        21,
        6,
        19,
        5,
        7,
        1,
        11,
        2,
        5,
        3,
        1,
        10,
        5,
        5,
        4,
        7,
        0,
        2,
        14,
        7,
        6,
        2,
        4,
        6
      ],
      "2021-02": [
        21,
        4,
        27,
        11,
        15,
        1,
        16,
        3,
        5,
        4,
        4,
        13,
        7,
        6,
        5,
        10,
        0,
        3,
        9,
        19,
        16,
        3,
        2,
        10
      ],
      "2021-03": [
        43,
        8,
        35,
        16,
        26,
        1,
        25,
        7,
        10,
        6,
        4,
        27,
        9,
        11,
        9,
        20,
        6,
        7,
        19,
        38,
        16,
        5,
        10,
        13
      ],
      "2021-04": [
        17,
        3,
        24,
        7,
        23,
        1,
        10,
        5,
        2,
        6,
        6,
        8,
        1,
        10,
        4,
        9,
        2,
        3,
        12,
        16,
        4,
        1,
        5,
        3
      ],
      "2021-05": [
        16,
        3,
        16,
        1,
        16,
        4,
        15,
        6,
        4,
        5,
        0,
        11,
        4,
        8,
        1,
        4,
        2,
        2,
        15,
        14,
        0,
        0,
        2,
        1
      ],
      "2021-06": [
        16,
        11,
        9,
        2,
        8,
        5,
        11,
        5,
        6,
        6,
        1,
        7,
        4,
        5,
        3,
        8,
        3,
        1,
        6,
        11,
        8,
        1,
        6,
        2
      ],
      "2021-07": [
        20,
        5,
        11,
        4,
        18,
        6,
        11,
        4,
        3,
        3,
        3,
        13,
        8,
        4,
        2,
        14,
        1,
        1,
        7,
        9,
        6,
        1,
        3,
        8
      ],
      "2021-08": [
        34,
        5,
        9,
        5,
        22,
        8,
        14,
        2,
        4,
        7,
        3,
        13,
        1,
        4,
        2,
        15,
        2,
        2,
        9,
        13,
        8,
        1,
        8,
        15
      ],
      "2021-09": [
        17,
        5,
        12,
        3,
        13,
        5,
        10,
        1,
        1,
        6,
        2,
        9,
        4,
        7,
        3,
        4,
        0,
        5,
        11,
        10,
        8,
        6,
        3,
        8
      ],
      "2021-10": [
        19,
        6,
        19,
        2,
        14,
        1,
        8,
        2,
        7,
        3,
        2,
        12,
        0,
        8,
        3,
        9,
        1,
        3,
        7,
        12,
        5,
        2,
        2,
        7
      ],
      "2021-11": [
        18,
        4,
        6,
        7,
        8,
        4,
        6,
        4,
        4,
        1,
        0,
        7,
        2,
        3,
        1,
        7,
        1,
        1,
        2,
        3,
        4,
        3,
        7,
        1
      ],
      "2021-12": [
        19,
        0,
        12,
        3,
        10,
        1,
        10,
        10,
        5,
        5,
        4,
        8,
        14,
        5,
        8,
        6,
        3,
        5,
        6,
        8,
        3,
        6,
        3,
        2
      ],
      "2022-01": [
        14,
        6,
        14,
        5,
        6,
        4,
        9,
        6,
        4,
        3,
        3,
        13,
        6,
        0,
        4,
        4,
        2,
        4,
        14,
        9,
        11,
        8,
        7,
        9
      ],
      "2022-02": [
        10,
        1,
        12,
        2,
        13,
        2,
        8,
        5,
        2,
        4,
        3,
        9,
        5,
        9,
        7,
        6,
        2,
        1,
        9,
        13,
        8,
        4,
        7,
        14
      ],
      "2022-03": [
        17,
        6,
        13,
        7,
        12,
        1,
        11,
        1,
        6,
        6,
        6,
        15,
        4,
        7,
        2,
        9,
        4,
        4,
        8,
        22,
        10,
        5,
        7,
        9
      ],
      "2022-04": [
        27,
        6,
        24,
        8,
        8,
        2,
        8,
        4,
        5,
        6,
        2,
        13,
        10,
        7,
        7,
        4,
        4,
        4,
        12,
        20,
        7,
        5,
        3,
        10
      ],
      "2022-05": [
        26,
        3,
        9,
        5,
        14,
        0,
        14,
        5,
        4,
        5,
        1,
        6,
        7,
        3,
        2,
        7,
        3,
        2,
        10,
        17,
        9,
        2,
        7,
        8
      ],
      "2022-06": [
        20,
        1,
        12,
        4,
        20,
        2,
        15,
        3,
        5,
        4,
        2,
        10,
        5,
        4,
        2,
        6,
        2,
        2,
        7,
        13,
        8,
        3,
        4,
        9
      ],
      "2022-07": [
        16,
        0,
        7,
        1,
        15,
        3,
        7,
        11,
        2,
        11,
        2,
        10,
        8,
        7,
        2,
        5,
        0,
        0,
        5,
        23,
        7,
        2,
        3,
        4
      ],
      "2022-08": [
        15,
        4,
        7,
        6,
        20,
        10,
        8,
        12,
        4,
        4,
        2,
        16,
        8,
        3,
        5,
        8,
        2,
        4,
        8,
        18,
        18,
        1,
        3,
        8
      ],
      "2022-09": [
        19,
        7,
        11,
        9,
        12,
        4,
        14,
        7,
        3,
        1,
        5,
        17,
        1,
        11,
        3,
        3,
        3,
        3,
        13,
        8,
        8,
        6,
        5,
        5
      ],
      "2022-10": [
        21,
        4,
        7,
        9,
        9,
        2,
        7,
        6,
        3,
        3,
        3,
        9,
        5,
        3,
        1,
        2,
        4,
        0,
        9,
        10,
        8,
        2,
        5,
        4
      ],
      "2022-11": [
        17,
        2,
        10,
        5,
        13,
        1,
        13,
        4,
        6,
        3,
        2,
        12,
        6,
        7,
        0,
        2,
        1,
        7,
        5,
        3,
        9,
        3,
        4,
        2
      ],
      "2022-12": [
        18,
        3,
        13,
        5,
        17,
        2,
        0,
        4,
        5,
        5,
        4,
        4,
        5,
        9,
        1,
        5,
        2,
        2,
        5,
        9,
        9,
        5,
        9,
        3
      ],
      "2023-01": [
        20,
        4,
        13,
        8,
        12,
        1,
        14,
        4,
        3,
        4,
        5,
        15,
        7,
        6,
        2,
        7,
        0,
        3,
        8,
        7,
        5,
        1,
        8,
        3
      ],
      "2023-02": [
        19,
        4,
        24,
        5,
        18,
        1,
        9,
        6,
        1,
        7,
        4,
        12,
        6,
        5,
        3,
        3,
        5,
        3,
        3,
        10,
        12,
        2,
        11,
        4
      ],
      "2023-03": [
        30,
        4,
        18,
        11,
        11,
        7,
        8,
        7,
        10,
        8,
        7,
        13,
        9,
        5,
        2,
        8,
        4,
        2,
        10,
        14,
        19,
        2,
        16,
        8
      ],
      "2023-04": [
        26,
        6,
        17,
        5,
        15,
        1,
        14,
        4,
        9,
        4,
        7,
        14,
        9,
        3,
        2,
        5,
        2,
        2,
        6,
        17,
        8,
        3,
        7,
        4
      ],
      "2023-05": [
        44,
        7,
        15,
        6,
        26,
        1,
        10,
        7,
        9,
        10,
        7,
        18,
        9,
        6,
        3,
        12,
        1,
        5,
        9,
        17,
        15,
        6,
        25,
        9
      ],
      "2023-06": [
        27,
        9,
        11,
        3,
        11,
        1,
        8,
        6,
        6,
        9,
        8,
        13,
        7,
        4,
        4,
        7,
        6,
        8,
        4,
        16,
        12,
        6,
        14,
        9
      ],
      "2023-07": [
        38,
        6,
        14,
        9,
        22,
        5,
        18,
        4,
        4,
        2,
        3,
        22,
        12,
        3,
        3,
        5,
        8,
        3,
        12,
        14,
        10,
        5,
        17,
        11
      ],
      "2023-08": [
        61,
        6,
        12,
        10,
        22,
        5,
        16,
        8,
        9,
        9,
        6,
        17,
        9,
        12,
        11,
        10,
        6,
        6,
        16,
        16,
        14,
        4,
        7,
        10
      ],
      "2023-09": [
        33,
        1,
        9,
        4,
        15,
        4,
        12,
        9,
        5,
        12,
        3,
        18,
        5,
        7,
        8,
        5,
        4,
        2,
        11,
        21,
        16,
        4,
        14,
        6
      ],
      "2023-10": [
        48,
        4,
        11,
        8,
        26,
        5,
        10,
        9,
        7,
        11,
        9,
        15,
        6,
        7,
        5,
        5,
        3,
        3,
        4,
        16,
        14,
        3,
        14,
        10
      ],
      "2023-11": [
        42,
        6,
        13,
        3,
        10,
        2,
        14,
        4,
        12,
        4,
        2,
        11,
        3,
        3,
        4,
        5,
        4,
        6,
        10,
        18,
        12,
        2,
        18,
        6
      ],
      "2023-12": [
        49,
        4,
        17,
        3,
        19,
        5,
        6,
        8,
        4,
        3,
        6,
        13,
        5,
        4,
        4,
        2,
        2,
        3,
        7,
        13,
        14,
        4,
        13,
        5
      ],
      "2024-01": [
        77,
        6,
        24,
        6,
        27,
        2,
        13,
        6,
        6,
        4,
        17,
        9,
        4,
        7,
        7,
        9,
        6,
        3,
        11,
        26,
        18,
        7,
        18,
        10
      ],
      "2024-02": [
        58,
        3,
        26,
        5,
        24,
        3,
        12,
        12,
        6,
        5,
        6,
        15,
        3,
        8,
        3,
        8,
        3,
        3,
        11,
        17,
        23,
        6,
        29,
        14
      ],
      "2024-03": [
        74,
        4,
        10,
        2,
        21,
        5,
        15,
        6,
        13,
        6,
        9,
        14,
        6,
        7,
        4,
        9,
        3,
        3,
        9,
        14,
        18,
        3,
        19,
        4
      ],
      "2024-04": [
        70,
        9,
        13,
        9,
        23,
        5,
        18,
        9,
        9,
        8,
        10,
        18,
        6,
        4,
        2,
        4,
        9,
        3,
        3,
        16,
        25,
        5,
        28,
        4
      ],
      "2024-05": [
        54,
        8,
        17,
        4,
        21,
        5,
        20,
        7,
        4,
        5,
        9,
        12,
        2,
        7,
        4,
        7,
        11,
        5,
        11,
        18,
        20,
        3,
        24,
        8
      ],
      "2024-06": [
        61,
        4,
        20,
        5,
        29,
        5,
        13,
        11,
        10,
        7,
        12,
        16,
        3,
        2,
        6,
        8,
        3,
        1,
        11,
        12,
        27,
        3,
        27,
        15
      ],
      "2024-07": [
        43,
        6,
        26,
        10,
        24,
        4,
        15,
        8,
        14,
        11,
        9,
        20,
        7,
        6,
        3,
        5,
        2,
        6,
        10,
        23,
        29,
        1,
        17,
        7
      ],
      "2024-08": [
        85,
        5,
        13,
        11,
        28,
        7,
        18,
        6,
        7,
        6,
        13,
        14,
        5,
        0,
        7,
        7,
        6,
        2,
        13,
        17,
        23,
        2,
        33,
        10
      ],
      "2024-09": [
        61,
        7,
        13,
        16,
        16,
        4,
        12,
        9,
        10,
        2,
        8,
        8,
        7,
        5,
        7,
        8,
        7,
        1,
        12,
        23,
        15,
        7,
        27,
        8
      ],
      "2024-10": [
        78,
        6,
        12,
        5,
        31,
        4,
        12,
        6,
        7,
        8,
        4,
        14,
        4,
        6,
        7,
        7,
        8,
        1,
        13,
        17,
        26,
        4,
        29,
        5
      ],
      "2024-11": [
        57,
        3,
        15,
        8,
        19,
        5,
        13,
        9,
        8,
        4,
        12,
        7,
        4,
        3,
        5,
        6,
        9,
        4,
        7,
        15,
        25,
        6,
        24,
        9
      ],
      "2024-12": [
        67,
        4,
        18,
        3,
        31,
        5,
        14,
        10,
        9,
        7,
        11,
        15,
        12,
        7,
        8,
        7,
        3,
        3,
        9,
        16,
        19,
        4,
        26,
        5
      ],
      "2025-01": [
        71,
        6,
        20,
        5,
        39,
        2,
        13,
        9,
        15,
        8,
        12,
        20,
        8,
        2,
        7,
        7,
        5,
        5,
        11,
        19,
        19,
        8,
        34,
        6
      ],
      "2025-02": [
        97,
        4,
        20,
        10,
        33,
        1,
        17,
        13,
        7,
        10,
        7,
        16,
        5,
        12,
        4,
        9,
        7,
        6,
        4,
        26,
        25,
        4,
        30,
        8
      ],
      "2025-03": [
        105,
        1,
        27,
        5,
        36,
        11,
        23,
        19,
        6,
        8,
        16,
        13,
        9,
        10,
        4,
        4,
        6,
        8,
        18,
        22,
        34,
        4,
        25,
        11
      ],
      "2025-04": [
        112,
        1,
        28,
        11,
        35,
        7,
        14,
        12,
        19,
        9,
        6,
        18,
        2,
        22,
        5,
        12,
        7,
        4,
        13,
        28,
        21,
        5,
        33,
        11
      ],
      "2025-05": [
        94,
        12,
        19,
        6,
        26,
        10,
        17,
        11,
        12,
        7,
        15,
        11,
        2,
        7,
        3,
        5,
        5,
        5,
        12,
        33,
        29,
        9,
        45,
        13
      ],
      "2025-06": [
        115,
        7,
        18,
        5,
        31,
        8,
        20,
        6,
        16,
        9,
        16,
        16,
        1,
        5,
        6,
        5,
        12,
        6,
        12,
        20,
        27,
        12,
        42,
        7
      ],
      "2025-07": [
        114,
        9,
        16,
        7,
        33,
        1,
        30,
        12,
        5,
        4,
        16,
        14,
        8,
        4,
        6,
        11,
        11,
        5,
        16,
        30,
        27,
        9,
        51,
        15
      ],
      "2025-08": [
        103,
        5,
        10,
        12,
        22,
        5,
        24,
        6,
        10,
        10,
        8,
        17,
        5,
        6,
        6,
        9,
        7,
        2,
        9,
        20,
        18,
        3,
        42,
        13
      ],
      "2025-09": [
        44,
        2,
        10,
        3,
        13,
        1,
        8,
        4,
        7,
        2,
        12,
        4,
        3,
        1,
        3,
        4,
        3,
        3,
        7,
        16,
        15,
        5,
        21,
        6
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding",
          "year": "2025-05",
          "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.",
          "arxiv_id": "2505.07768v1"
        },
        {
          "title": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
          "year": "2024-07",
          "abstract": "Code generation aims to automatically generate code snippets that meet given\nnatural language requirements and plays an important role in software\ndevelopment. Although Code LLMs have shown excellent performance in this\ndomain, their long generation time poses a signification limitation in practice\nuse. In this paper, we first conduct an in-depth preliminary study with\ndifferent Code LLMs on code generation tasks and identify a significant\nefficiency issue, i.e., continual generation of excess tokens. It harms the\ndeveloper productivity and leads to huge computational wastes. To address it,\nwe introduce CodeFast, an inference acceleration approach for Code LLMs on code\ngeneration. The key idea of CodeFast is to terminate the inference process in\ntime when unnecessary excess tokens are detected. First, we propose an\nautomatic data construction framework to obtain training data. Then, we train a\nunified lightweight model GenGuard applicable to multiple programming languages\nto predict whether to terminate inference at the current step. Finally, we\nenhance Code LLM with GenGuard to accelerate its inference in code generation\ntasks. We conduct extensive experiments with CodeFast on five representative\nCode LLMs across four widely used code generation datasets. Experimental\nresults show that (1) CodeFast can significantly improve the inference speed of\nvarious Code LLMs in code generation, ranging form 34% to 452%, without\ncompromising the quality of generated code. (2) CodeFast is stable across\ndifferent parameter settings and can generalize to untrained datasets. Our code\nand data are available at https://github.com/DeepSoftwareAnalytics/CodeFast",
          "arxiv_id": "2407.20042v1"
        },
        {
          "title": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
          "year": "2024-05",
          "abstract": "Large Language Models (LLMs) trained on code are revolutionizing the software\ndevelopment process. Increasingly, code LLMs are being integrated into software\ndevelopment environments to improve the productivity of human programmers, and\nLLM-based agents are beginning to show promise for handling complex tasks\nautonomously. Realizing the full potential of code LLMs requires a wide range\nof capabilities, including code generation, fixing bugs, explaining and\ndocumenting code, maintaining repositories, and more. In this work, we\nintroduce the Granite series of decoder-only code models for code generative\ntasks, trained with code written in 116 programming languages. The Granite Code\nmodels family consists of models ranging in size from 3 to 34 billion\nparameters, suitable for applications ranging from complex application\nmodernization tasks to on-device memory-constrained use cases. Evaluation on a\ncomprehensive set of tasks demonstrates that Granite Code models consistently\nreaches state-of-the-art performance among available open-source code LLMs. The\nGranite Code model family was optimized for enterprise software development\nworkflows and performs well across a range of coding tasks (e.g. code\ngeneration, fixing and explanation), making it a versatile all around code\nmodel. We release all our Granite Code models under an Apache 2.0 license for\nboth research and commercial use.",
          "arxiv_id": "2405.04324v1"
        }
      ],
      "1": [
        {
          "title": "A Roadmap for Simulation-Based Testing of Autonomous Cyber-Physical Systems: Challenges and Future Direction",
          "year": "2024-05",
          "abstract": "As the era of autonomous cyber-physical systems (ACPSs), such as unmanned\naerial vehicles and self-driving cars, unfolds, the demand for robust testing\nmethodologies is key to realizing the adoption of such systems in real-world\nscenarios. However, traditional software testing paradigms face unprecedented\nchallenges in ensuring the safety and reliability of these systems. In\nresponse, this paper pioneers a strategic roadmap for simulation-based testing\nof ACPSs, specifically focusing on autonomous systems. Our paper discusses the\nrelevant challenges and obstacles of ACPSs, focusing on test automation and\nquality assurance, hence advocating for tailored solutions to address the\nunique demands of autonomous systems. While providing concrete definitions of\ntest cases within simulation environments, we also accentuate the need to\ncreate new benchmark assets and the development of automated tools tailored\nexplicitly for autonomous systems in the software engineering community. This\npaper not only highlights the relevant, pressing issues the software\nengineering community should focus on (in terms of practices, expected\nautomation, and paradigms), but it also outlines ways to tackle them. By\noutlining the various domains and challenges of simulation-based\ntesting/development for ACPSs, we provide directions for future research\nefforts.",
          "arxiv_id": "2405.01064v1"
        },
        {
          "title": "LMM-enhanced Safety-Critical Scenario Generation for Autonomous Driving System Testing From Non-Accident Traffic Videos",
          "year": "2024-06",
          "abstract": "Safety testing serves as the fundamental pillar for the development of\nautonomous driving systems (ADSs). To ensure the safety of ADSs, it is\nparamount to generate a diverse range of safety-critical test scenarios. While\nexisting ADS practitioners primarily focus on reproducing real-world traffic\naccidents in simulation environments to create test scenarios, it's essential\nto highlight that many of these accidents do not directly result in safety\nviolations for ADSs due to the differences between human driving and autonomous\ndriving. More importantly, we observe that some accident-free real-world\nscenarios can not only lead to misbehaviors in ADSs but also be leveraged for\nthe generation of ADS violations during simulation testing. Therefore, it is of\nsignificant importance to discover safety violations of ADSs from routine\ntraffic scenarios (i.e., non-crash scenarios).\n  We introduce LEADE, a novel methodology to achieve the above goal. It\nautomatically generates abstract and concrete scenarios from real-traffic\nvideos. Then it optimizes these scenarios to search for safety violations of\nthe ADS in semantically consistent scenarios where human-driving worked safely.\nSpecifically, LEADE enhances the ability of Large Multimodal Models (LMMs) to\naccurately construct abstract scenarios from traffic videos and generate\nconcrete scenarios by multi-modal few-shot Chain of Thought (CoT). Based on\nthem, LEADE assesses and increases the behavior differences between the ego\nvehicle and human-driving in semantic equivalent scenarios (here equivalent\nsemantics means that each participant in test scenarios has the same behaviors\nas those observed in the original real traffic scenarios). We implement and\nevaluate LEADE on the industrial-grade Level-4 ADS, Apollo.",
          "arxiv_id": "2406.10857v2"
        },
        {
          "title": "Generating Critical Scenarios for Testing Automated Driving Systems",
          "year": "2024-12",
          "abstract": "Autonomous vehicles (AVs) have demonstrated significant potential in\nrevolutionizing transportation, yet ensuring their safety and reliability\nremains a critical challenge, especially when exposed to dynamic and\nunpredictable environments. Real-world testing of an Autonomous Driving System\n(ADS) is both expensive and risky, making simulation-based testing a preferred\napproach. In this paper, we propose AVASTRA, a Reinforcement Learning\n(RL)-based approach to generate realistic critical scenarios for testing ADSs\nin simulation environments. To capture the complexity of driving scenarios,\nAVASTRA comprehensively represents the environment by both the internal states\nof an ADS under-test (e.g., the status of the ADS's core components, speed, or\nacceleration) and the external states of the surrounding factors in the\nsimulation environment (e.g., weather, traffic flow, or road condition).\nAVASTRA trains the RL agent to effectively configure the simulation environment\nthat places the AV in dangerous situations and potentially leads it to\ncollisions. We introduce a diverse set of actions that allows the RL agent to\nsystematically configure both environmental conditions and traffic\nparticipants. Additionally, based on established safety requirements, we\nenforce heuristic constraints to ensure the realism and relevance of the\ngenerated test scenarios. AVASTRA is evaluated on two popular simulation maps\nwith four different road configurations. Our results show AVASTRA's ability to\noutperform the state-of-the-art approach by generating 30% to 115% more\ncollision scenarios. Compared to the baseline based on Random Search, AVASTRA\nachieves up to 275% better performance. These results highlight the\neffectiveness of AVASTRA in enhancing the safety testing of AVs through\nrealistic comprehensive critical scenario generation.",
          "arxiv_id": "2412.02574v1"
        }
      ],
      "2": [
        {
          "title": "A Grounded Theory of Coordination in Remote-First and Hybrid Software Teams",
          "year": "2022-02",
          "abstract": "While the long-term effects of the COVID-19 pandemic on software\nprofessionals and organizations are difficult to predict, it seems likely that\nworking from home, remote-first teams, distributed teams, and hybrid\n(part-remote/part-office) teams will be more common. It is therefore important\nto investigate the challenges that software teams and organizations face with\nnew remote and hybrid work. Consequently, this paper reports a year-long,\nparticipant-observation, constructivist grounded theory study investigating the\nimpact of working from home on software development. This study resulted in a\ntheory of software team coordination. Briefly, shifting from in-office to\nat-home work fundamentally altered coordination within software teams. While\ngroup cohesion and more effective communication appear protective, coordination\nis undermined by distrust, parenting and communication bricolage. Poor\ncoordination leads to numerous problems including misunderstandings, help\nrequests, lower job satisfaction among team members, and more ill-defined\ntasks. These problems, in turn, reduce overall project success and prompt\nprofessionals to alter their software development processes (in this case, from\nScrum to Kanban). Our findings suggest that software organizations with many\nremote employees can improve performance by encouraging greater engagement\nwithin teams and supporting employees with family and childcare\nresponsibilities.",
          "arxiv_id": "2202.10445v2"
        },
        {
          "title": "Software development in startup companies: A systematic mapping study",
          "year": "2023-07",
          "abstract": "Context: Software startups are newly created companies with no operating\nhistory and fast in producing cutting-edge technologies. These companies\ndevelop software under highly uncertain conditions, tackling fast-growing\nmarkets under severe lack of resources. Therefore, software startups present an\nunique combination of characteristics which pose several challenges to software\ndevelopment activities. Objective: This study aims to structure and analyze the\nliterature on software development in startup companies, determining thereby\nthe potential for technology transfer and identifying software development work\npractices reported by practitioners and researchers. Method: We conducted a\nsystematic mapping study, developing a classification schema, ranking the\nselected primary studies according their rigor and relevance, and analyzing\nreported software development work practices in startups. Results: A total of\n43 primary studies were identified and mapped, synthesizing the available\nevidence on software development in startups. Only 16 studies are entirely\ndedicated to software development in startups, of which 10 result in a weak\ncontribution (advice and implications (6); lesson learned (3); tool (1)).\nNineteen studies focus on managerial and organizational factors. Moreover, only\n9 studies exhibit high scientific rigor and relevance. From the reviewed\nprimary studies, 213 software engineering work practices were extracted,\ncategorized and analyzed. Conclusion: This mapping study provides the first\nsystematic exploration of the state-of-art on software startup research. The\nexisting body of knowledge is limited to a few high quality studies.\nFurthermore, the results indicate that software engineering work practices are\nchosen opportunistically, adapted and configured to provide value under the\nconstrains imposed by the startup context.",
          "arxiv_id": "2307.13104v1"
        },
        {
          "title": "Work-from-home and its implication for project management, resilience and innovation -- a global survey on software companies",
          "year": "2022-02",
          "abstract": "[Context] The COVID-19 pandemic has had a disruptive impact on how people\nwork and collaborate across all global economic sectors, including the software\nbusiness. While remote working is not new for software engineers, forced\nWork-from-home situations to come with both constraints, limitations, and\nopportunities for individuals, software teams and software companies. As the\n\"new normal\" for working might be based on the current state of Work From Home\n(WFH), it is useful to understand what has happened and learn from that.\n[Objective] The goal of this study is to gain insights on how their WFH\nenvironment impacts software projects and software companies. We are also\ninterested in understanding if the impact differs between software startups and\nestablished companies. [Method] We conducted a global-scale, cross-sectional\nsurvey during spring and summer 2021. Our results are based on quantitative and\nqualitative analysis of 297 valid responses. [Results] We observed a mixed\nperception of the impact of WFH on software project management, resilience, and\ninnovation. Certain patterns on WFH, control and coordination mechanisms and\ncollaborative tools are observed globally. We find that team, agility and\nleadership are the three most important factors for achieving resilience during\nthe pandemic. Although startups do not perceive the impact of WFH differently,\nthere is a difference between engineers who work in a small team context and\nthose who work in a large team context. [Conclusion] The result suggests a\ncontingency approach in studying and improving WFH practices and environment in\nthe future software industry.",
          "arxiv_id": "2202.04950v1"
        }
      ],
      "3": [
        {
          "title": "Visualizing Privacy-Relevant Data Flows in Android Applications",
          "year": "2025-03",
          "abstract": "Android applications collecting data from users must protect it according to\nthe current legal frameworks. Such data protection has become even more\nimportant since in 2018 the European Union rolled out the General Data\nProtection Regulation (GDPR). Since app developers are not legal experts, they\nfind it difficult to integrate privacy-aware practices into source code\ndevelopment. Despite these legal obligations, developers have limited tool\nsupport to reason about data protection throughout their app development\nprocess.\n  This paper explores the use of static program slicing and software\nvisualization to analyze privacy-relevant data flows in Android apps. We\nintroduce SliceViz, a web tool that analyzes an Android app by slicing all\nprivacy-relevant data sources detected in the source code on the back-end. It\nthen helps developers by visualizing these privacy-relevant program slices.\n  We conducted a user study with 12 participants demonstrating that SliceViz\neffectively aids developers in identifying privacy-relevant properties in\nAndroid apps.\n  Our findings indicate that program slicing can be employed to identify and\nreason about privacy-relevant data flows in Android applications. With further\nusability improvements, developers can be better equipped to handle\nprivacy-sensitive information.",
          "arxiv_id": "2503.16640v1"
        },
        {
          "title": "Mining user reviews of COVID contact-tracing apps: An exploratory analysis of nine European apps",
          "year": "2020-12",
          "abstract": "Context: More than 50 countries have developed COVID contact-tracing apps to\nlimit the spread of coronavirus. However, many experts and scientists cast\ndoubt on the effectiveness of those apps. For each app, a large number of\nreviews have been entered by end-users in app stores. Objective: Our goal is to\ngain insights into the user reviews of those apps, and to find out the main\nproblems that users have reported. Our focus is to assess the \"software in\nsociety\" aspects of the apps, based on user reviews. Method: We selected nine\nEuropean national apps for our analysis and used a commercial app-review\nanalytics tool to extract and mine the user reviews. For all the apps combined,\nour dataset includes 39,425 user reviews. Results: Results show that users are\ngenerally dissatisfied with the nine apps under study, except the Scottish\n(\"Protect Scotland\") app. Some of the major issues that users have complained\nabout are high battery drainage and doubts on whether apps are really working.\nConclusion: Our results show that more work is needed by the stakeholders\nbehind the apps (e.g., app developers, decision-makers, public health experts)\nto improve the public adoption, software quality and public perception of these\napps.",
          "arxiv_id": "2012.13589v1"
        },
        {
          "title": "SeMA: Extending and Analyzing Storyboards to Develop Secure Android Apps",
          "year": "2020-01",
          "abstract": "Mobile apps provide various critical services, such as banking,\ncommunication, and healthcare. To this end, they have access to our personal\ninformation and have the ability to perform actions on our behalf. Hence,\nsecuring mobile apps is crucial to ensuring the privacy and safety of its\nusers.\n  Recent research efforts have focused on developing solutions to secure mobile\necosystems (i.e., app platforms, apps, and app stores), specifically in the\ncontext of detecting vulnerabilities in Android apps. Despite this attention,\nknown vulnerabilities are often found in mobile apps, which can be exploited by\nmalicious apps to harm the user. Further, fixing vulnerabilities after\ndeveloping an app has downsides in terms of time, resources, user\ninconvenience, and information loss.\n  In an attempt to address this concern, we have developed SeMA, a mobile app\ndevelopment methodology that builds on existing mobile app design artifacts\nsuch as storyboards. With SeMA, security is a first-class citizen in an app's\ndesign -- app designers and developers can collaborate to specify and reason\nabout the security properties of an app at an abstract level without being\ndistracted by implementation level details. Our realization of SeMA using\nAndroid Studio tooling demonstrates the methodology is complementary to\nexisting design and development practices. An evaluation of the effectiveness\nof SeMA shows the methodology can detect and help prevent 49 vulnerabilities\nknown to occur in Android apps. Further, a usability study of the methodology\ninvolving ten real-world developers shows the methodology is likely to reduce\nthe development time and help developers uncover and prevent known\nvulnerabilities while designing apps.",
          "arxiv_id": "2001.10052v4"
        }
      ],
      "4": [
        {
          "title": "TestART: Improving LLM-based Unit Testing via Co-evolution of Automated Generation and Repair Iteration",
          "year": "2024-08",
          "abstract": "Unit testing is crucial for detecting bugs in individual program units but\nconsumes time and effort. Recently, large language models (LLMs) have\ndemonstrated remarkable capabilities in generating unit test cases. However,\nseveral problems limit their ability to generate high-quality unit test cases:\n(1) compilation and runtime errors caused by the hallucination of LLMs; (2)\nlack of testing and coverage feedback information restricting the increase of\ncode coverage;(3) the repetitive suppression problem causing invalid LLM-based\nrepair and generation attempts. To address these limitations, we propose\nTestART, a novel unit test generation method. TestART improves LLM-based unit\ntesting via co-evolution of automated generation and repair iteration,\nrepresenting a significant advancement in automated unit test generation.\nTestART leverages the template-based repair strategy to effectively fix bugs in\nLLM-generated test cases for the first time. Meanwhile, TestART extracts\ncoverage information from successful test cases and uses it as coverage-guided\ntesting feedback. It also incorporates positive prompt injection to prevent\nrepetition suppression, thereby enhancing the sufficiency of the final test\ncase. This synergy between generation and repair elevates the correctness and\nsufficiency of the produced test cases significantly beyond previous methods.\nIn comparative experiments, TestART demonstrates an 18% improvement in pass\nrate and a 20% enhancement in coverage across three types of datasets compared\nto baseline models. Additionally, it achieves better coverage rates than\nEvoSuite with only half the number of test cases. These results demonstrate\nTestART's superior ability to produce high-quality unit test cases by\nharnessing the power of LLMs while overcoming their inherent flaws.",
          "arxiv_id": "2408.03095v6"
        },
        {
          "title": "Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models",
          "year": "2023-10",
          "abstract": "Software testing is a core discipline in software engineering where a large\narray of research results has been produced, notably in the area of automatic\ntest generation. Because existing approaches produce test cases that either can\nbe qualified as simple (e.g. unit tests) or that require precise\nspecifications, most testing procedures still rely on test cases written by\nhumans to form test suites. Such test suites, however, are incomplete: they\nonly cover parts of the project or they are produced after the bug is fixed.\nYet, several research challenges, such as automatic program repair, and\npractitioner processes, build on the assumption that available test suites are\nsufficient. There is thus a need to break existing barriers in automatic test\ncase generation. While prior work largely focused on random unit testing\ninputs, we propose to consider generating test cases that realistically\nrepresent complex user execution scenarios, which reveal buggy behaviour. Such\nscenarios are informally described in bug reports, which should therefore be\nconsidered as natural inputs for specifying bug-triggering test cases. In this\nwork, we investigate the feasibility of performing this generation by\nleveraging large language models (LLMs) and using bug reports as inputs. Our\nexperiments include the use of ChatGPT, as an online service, as well as\nCodeGPT, a code-related pre-trained LLM that was fine-tuned for our task.\nOverall, we experimentally show that bug reports associated to up to 50% of\nDefects4J bugs can prompt ChatGPT to generate an executable test case. We show\nthat even new bug reports can indeed be used as input for generating executable\ntest cases. Finally, we report experimental results which confirm that\nLLM-generated test cases are immediately useful in software engineering tasks\nsuch as fault localization as well as patch validation in automated program\nrepair.",
          "arxiv_id": "2310.06320v1"
        },
        {
          "title": "Unit Test Case Generation with Transformers and Focal Context",
          "year": "2020-09",
          "abstract": "Automated unit test case generation tools facilitate test-driven development\nand support developers by suggesting tests intended to identify flaws in their\ncode. Existing approaches are usually guided by the test coverage criteria,\ngenerating synthetic test cases that are often difficult for developers to read\nor understand. In this paper we propose AthenaTest, an approach that aims to\ngenerate unit test cases by learning from real-world focal methods and\ndeveloper-written testcases. We formulate unit test case generation as a\nsequence-to-sequence learning task, adopting a two-step training procedure\nconsisting of denoising pretraining on a large unsupervised Java corpus, and\nsupervised finetuning for a downstream translation task of generating unit\ntests. We investigate the impact of natural language and source code\npretraining, as well as the focal context information surrounding the focal\nmethod. Both techniques provide improvements in terms of validation loss, with\npretraining yielding 25% relative improvement and focal context providing\nadditional 11.1% improvement. We also introduce Methods2Test, the largest\npublicly available supervised parallel corpus of unit test case methods and\ncorresponding focal methods in Java, which comprises 780K test cases mined from\n91K open-source repositories from GitHub. We evaluate AthenaTest on five\ndefects4j projects, generating 25K passing test cases covering 43.7% of the\nfocal methods with only 30 attempts. We execute the test cases, collect test\ncoverage information, and compare them with test cases generated by EvoSuite\nand GPT-3, finding that our approach outperforms GPT-3 and has comparable\ncoverage w.r.t. EvoSuite. Finally, we survey professional developers on their\npreference in terms of readability, understandability, and testing\neffectiveness of the generated tests, showing overwhelmingly preference towards\nAthenaTest.",
          "arxiv_id": "2009.05617v2"
        }
      ],
      "5": [
        {
          "title": "RustMap: Towards Project-Scale C-to-Rust Migration via Program Analysis and LLM",
          "year": "2025-03",
          "abstract": "Migrating existing C programs into Rust is increasingly desired, as Rust\noffers superior memory safety while maintaining C's high performance. However,\nvastly different features between C and Rust--e.g., distinct definitions and\nusages of pointers and references--pose significant challenges beyond mere\nsyntactic translation. Existing automated translation tools, such as C2Rust,\nmay rely too much on syntactic, template-based translation and generate unsafe\nRust code that is hard for human developers to read, maintain, or even compile.\nMore semantic-aware translation that produces safer, idiomatic, and runnable\nRust code is much needed. This paper introduces a novel dependency-guided and\nlarge language model (LLM)-based C-to-Rust translation approach, RustMap, based\non three key ideas: (1) Utilize LLM capabilities to produce idiomatic Rust code\nfrom given small pieces of C code, (2) Mitigate LLM limitations in handling\nlarge codebases by breaking project-scale C programs into smaller units for\ntranslation according to their usage dependencies and composing them into a\nrunnable Rust program, and (3) Enhance the correctness of the translated Rust\nprogram by using test cases to check input/output equivalence, isolate faulty\ncode when execution states deviate, and iteratively refine the translation\nusing feedback from compilation and test errors. We empirically evaluate\nRustMap on 126 real-world programs, including 125 from Rosetta Code and a 7000+\nline bzip2 implementation using GPT-4o as the LLM. RustMap shows promising\nresults, guiding GPT-4o to produce idiomatic, readable, and functional Rust\ncode with significantly less unsafe code than other tools, and revealing\nnon-trivial translation patterns reusable for future research.",
          "arxiv_id": "2503.17741v1"
        },
        {
          "title": "Is Rust Used Safely by Software Developers?",
          "year": "2020-07",
          "abstract": "Rust, an emerging programming language with explosive growth, provides a\nrobust type system that enables programmers to write memory-safe and data-race\nfree code. To allow access to a machine's hardware and to support low-level\nperformance optimizations, a second language, Unsafe Rust, is embedded in Rust.\nIt contains support for operations that are difficult to statically check, such\nas C-style pointers for access to arbitrary memory locations and mutable global\nvariables. When a program uses these features, the compiler is unable to\nstatically guarantee the safety properties Rust promotes. In this work, we\nperform a large-scale empirical study to explore how software developers are\nusing Unsafe Rust in real-world Rust libraries and applications. Our results\nindicate that software engineers use the keyword unsafe in less than 30% of\nRust libraries, but more than half cannot be entirely statically checked by the\nRust compiler because of Unsafe Rust hidden somewhere in a library's call\nchain. We conclude that although the use of the keyword unsafe is limited, the\npropagation of unsafeness offers a challenge to the claim of Rust as a\nmemory-safe language. Furthermore, we recommend changes to the Rust compiler\nand to the central Rust repository's interface to help Rust software developers\nbe aware of when their Rust code is unsafe.",
          "arxiv_id": "2007.00752v1"
        },
        {
          "title": "VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners",
          "year": "2024-04",
          "abstract": "Rust is a programming language that combines memory safety and low-level\ncontrol, providing C-like performance while guaranteeing the absence of\nundefined behaviors by default. Rust's growing popularity has prompted research\non safe and correct transpiling of existing code-bases to Rust. Existing work\nfalls into two categories: rule-based and large language model (LLM)-based.\nWhile rule-based approaches can theoretically produce correct transpilations\nthat maintain input-output equivalence to the original, they often yield\nunreadable Rust code that uses unsafe subsets of the Rust language. On the\nother hand, while LLM-based approaches typically produce more readable,\nmaintainable, and safe code, they do not provide any guarantees about\ncorrectness. In this work, we present VERT, a tool that can produce readable\nRust transpilations with formal guarantees of correctness. VERT's only\nrequirement is that there is Web Assembly compiler for the source language,\nwhich is true for most major languages. VERT first uses the Web Assembly\ncompiler to obtain an oracle Rust program. In parallel, VERT uses an LLM to\ngenerate a readable candidate Rust program. This candidate is verified against\nthe oracle, and if verification fails, we regenerate a new candidate\ntranspilation until verification succeeds. We evaluate VERT by transpiling a\nsuite of 1,394 programs taken from competitive programming style benchmarks.\nCombining Anthropic's Claude-2 and VERT increases Rust transpilations passing\nproperty-based testing from 31% to 54% and bounded model-checking from 1% to\n42% compared to using Claude alone. In addition, we evaluate VERT's ability to\ngenerate non-trivial safe Rust on programs taken from real-world C projects\nthat make significant use of pointers. Our results provide insights into the\nlimitations of LLMs to write safe Rust.",
          "arxiv_id": "2404.18852v2"
        }
      ],
      "6": [
        {
          "title": "A multi-case study of agile requirements engineering and the use of test cases as requirements",
          "year": "2023-08",
          "abstract": "Context: It is an enigma that agile projects can succeed 'without\nrequirements' when weak requirements engineering is a known cause for project\nfailures. While agile development projects often manage well without extensive\nrequirements test cases are commonly viewed as requirements and detailed\nrequirements are documented as test cases. Objective: We have investigated this\nagile practice of using test cases as requirements to understand how test cases\ncan support the main requirements activities, and how this practice varies.\nMethod: We performed an iterative case study at three companies and collected\ndata through 14 interviews and two focus groups. Results: The use of test cases\nas requirements poses both benefits and challenges when eliciting, validating,\nverifying, and managing requirements, and when used as a documented agreement.\nWe have identified five variants of the test-cases-as-requirements practice,\nnamely de facto, behaviour-driven, story-test driven, stand-alone strict and\nstand-alone manual for which the application of the practice varies concerning\nthe time frame of requirements documentation, the requirements format, the\nextent to which the test cases are a machine executable specification and the\nuse of tools which provide specific support for the practice of using test\ncases as requirements. Conclusions: The findings provide empirical insight into\nhow agile development projects manage and communicate requirements. The\nidentified variants of the practice of using test cases as requirements can be\nused to perform in-depth investigations into agile requirements engineering.\nPractitioners can use the provided recommendations as a guide in designing and\nimproving their agile requirements practices based on project characteristics\nsuch as number of stakeholders and rate of change.",
          "arxiv_id": "2308.11747v1"
        },
        {
          "title": "UOOR: Seamless and Traceable Requirements",
          "year": "2025-02",
          "abstract": "In industrial practice, requirements are an indispensable element of any\nserious software project. In the academic study of software engineering,\nrequirements are one of the heavily researched subjects. And yet requirements\nengineering, as practiced in industry, makes shockingly sparse use of the\nconcepts propounded in the requirements literature. The present paper starts\nfrom an assumption about the causes for this situation and proposes a remedy to\nredress it. The posited explanation is that change is the major factor\naffecting the practical application of even the best-intentioned requirements\ntechniques. No sooner has the ink dried on the specifications than the system\nenvironment and stakeholders' views of the system begin to evolve.\n  The proposed solution is a requirements engineering method, called UOOR,\nwhich unifies many known requirements concepts and a few new ones in a\nframework entirely devised to accommodate and support seamless change\nthroughout the project lifecycle.\n  The method encompasses the commonly used requirements techniques, namely,\nscenarios, and integrates them into the seamless software development process.\nThe work presented here introduces the notion of seamless requirements\ntraceability, which relies on the propagation of traceability links, themselves\nbased on formal properties of relations between project artifacts. As a proof\nof concept, the paper presents a traceability tool to be integrated into a\ngeneral-purpose IDE that provides the ability to link requirements to other\nsoftware project artifacts, display notifications of changes in requirements,\nand trace those changes to the related project elements.\n  The UOOR approach is not just a theoretical proposal but has been designed\nfor practical use and has been applied to a significant real-world case study:\nRoborace, a competition of autonomous racing cars.",
          "arxiv_id": "2502.18617v2"
        },
        {
          "title": "A Framework for Aspectual Requirements Validation: An Experimental Study",
          "year": "2021-10",
          "abstract": "Requirements engineering is a discipline of software engineering that is\nconcerned with the identification and handling of user and system requirements.\nAspect-Oriented Requirements Engineering (AORE) extends the existing\nrequirements engineering approaches to cope with the issue of tangling and\nscattering resulted from crosscutting concerns. Crosscutting concerns are\nconsidered as potential aspects and can lead to the phenomena tyranny of the\ndominant decomposition. Requirements-level aspects are responsible for\nproducing scattered and tangled descriptions of requirements in the\nrequirements document. Validation of requirements artefacts is an essential\ntask in software development. This task ensures that requirements are correct\nand valid in terms of completeness and consistency, hence, reducing the\ndevelopment cost, maintenance and establish an approximately correct estimate\nof effort and completion time of the project. In this paper, we present a\nvalidation framework to validate the aspectual requirements and the\ncrosscutting relationship of concerns that are resulted from the requirements\nengineering phase. The proposed framework comprises a high-level and low-level\nvalidation to implement on software requirements specification (SRS). The\nhigh-level validation validates the concerns with stakeholders, whereas the\nlow-level validation validates the aspectual requirement by requirements\nengineers and analysts using a checklist. The approach has been evaluated using\nan experimental study on two AORE approaches. The approaches are\nviewpoint-based called AORE with ArCaDe and lexical analysis based on Theme/Doc\napproach. The results obtained from the study demonstrate that the proposed\nframework is an effective validation model for AORE artefacts.",
          "arxiv_id": "2110.03952v1"
        }
      ],
      "7": [
        {
          "title": "Implementation of Security Features in Software Development Phases",
          "year": "2020-12",
          "abstract": "Security holds an important role in a software. Most people are not aware of\nthe significance of security in software system and tend to assume that they\nwill be fine without security in their software systems. However, the lack of\nsecurity features causes to expose all the vulnerabilities possible to the\npublic. This provides opportunities for the attackers to perform dangerous\nactivities to the vulnerable insecure systems. This is the reason why many\norganizations are reported for being victims of system security attacks. In\norder to achieve the security requirement, developers must take time to study\nso that they truly understand the consequences and importance of security.\nHence, this paper is written to discuss how secure software development can be\nperformed. To reach the goal of this paper, relevant researches have been\nreviewed. Multiple case study papers have been studied to find out the answers\nto how the vulnerabilities are identified, how to eliminate them, when to\nimplement security features, why do we implement them. Finally, the paper is\nconcluded with final remarks on implementation of security features during\nsoftware development process. It is expected that this paper will be a\ncontribution towards the aforementioned software security domain which is often\nignored during practical application.",
          "arxiv_id": "2012.13108v1"
        },
        {
          "title": "Dependency Practices for Vulnerability Mitigation",
          "year": "2023-10",
          "abstract": "Relying on dependency packages accelerates software development, but it also\nincreases the exposure to security vulnerabilities that may be present in\ndependencies. While developers have full control over which dependency packages\n(and which version) they use, they have no control over the dependencies of\ntheir dependencies. Such transitive dependencies, which often amount to a\ngreater number than direct dependencies, can become infected with\nvulnerabilities and put software projects at risk. To mitigate this risk,\nPractitioners need to select dependencies that respond quickly to\nvulnerabilities to prevent the propagation of vulnerable code to their project.\nTo identify such dependencies, we analyze more than 450 vulnerabilities in the\nnpm ecosystem to understand why dependent packages remain vulnerable. We\nidentify over 200,000 npm packages that are infected through their dependencies\nand use 9 features to build a prediction model that identifies packages that\nquickly adopt the vulnerability fix and prevent further propagation of\nvulnerabilities. We also study the relationship between these features and the\nresponse speed of vulnerable packages. We complement our work with a\npractitioner survey to understand the applicability of our findings. Developers\ncan incorporate our findings into their dependency management practices to\nmitigate the impact of vulnerabilities from their dependency supply chain.",
          "arxiv_id": "2310.07847v1"
        },
        {
          "title": "Do Software Security Practices Yield Fewer Vulnerabilities?",
          "year": "2022-10",
          "abstract": "Due to the ever-increasing security breaches, practitioners are motivated to\nproduce more secure software. In the United States, the White House Office\nreleased a memorandum on Executive Order (EO) 14028 that mandates organizations\nprovide self-attestation of the use of secure software development practices.\nThe OpenSSF Scorecard project allows practitioners to measure the use of\nsoftware security practices automatically. However, little research has been\ndone to determine whether the use of security practices improves package\nsecurity, particularly which security practices have the biggest impact on\nsecurity outcomes. The goal of this study is to assist practitioners and\nresearchers making informed decisions on which security practices to adopt\nthrough the development of models between software security practice scores and\nsecurity vulnerability counts.\n  To that end, we developed five supervised machine learning models for npm and\nPyPI packages using the OpenSSF Scorecared security practices scores and\naggregate security scores as predictors and the number of externally-reported\nvulnerabilities as a target variable. Our models found four security practices\n(Maintained, Code Review, Branch Protection, and Security Policy) were the most\nimportant practices influencing vulnerability count. However, we had low R^2\n(ranging from 9% to 12%) when we tested the models to predict vulnerability\ncounts. Additionally, we observed that the number of reported vulnerabilities\nincreased rather than reduced as the aggregate security score of the packages\nincreased. Both findings indicate that additional factors may influence the\npackage vulnerability count. We suggest that vulnerability count and security\nscore data be refined such that these measures may be used to provide\nactionable guidance on security practices.",
          "arxiv_id": "2210.14884v2"
        }
      ],
      "8": [
        {
          "title": "DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing EVM Bytecode",
          "year": "2020-09",
          "abstract": "Smart contracts are Turing-complete programs running on the blockchain. They\nare immutable and cannot be modified, even when bugs are detected. Therefore,\nensuring smart contracts are bug-free and well-designed before deploying them\nto the blockchain is extremely important. A contract defect is an error, flaw\nor fault in a smart contract that causes it to produce an incorrect or\nunexpected result, or to behave in unintended ways. Detecting and removing\ncontract defects can avoid potential bugs and make programs more robust. Our\nprevious work defined 20 contract defects for smart contracts and divided them\ninto five impact levels. According to our classification, contract defects with\nseriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract\nbeing controlled by attackers. In this paper, we propose DefectChecker, a\nsymbolic execution-based approach and tool to detect eight contract defects\nthat can cause unwanted behaviors of smart contracts on the Ethereum blockchain\nplatform. DefectChecker can detect contract defects from smart contracts\nbytecode. We compare DefectChecker with key previous works, including Oyente,\nMythril and Securify by using an open-source dataset. Our experimental results\nshow that DefectChecker performs much better than these tools in terms of both\nspeed and accuracy. We also applied DefectChecker to 165,621 distinct smart\ncontracts on the Ethereum platform. We found that 25,815 of these smart\ncontracts contain at least one of the contract defects that belongs to impact\nlevel 1-3, including some real-world attacks.",
          "arxiv_id": "2009.02663v2"
        },
        {
          "title": "EOSFuzzer: Fuzzing EOSIO Smart Contracts for Vulnerability Detection",
          "year": "2020-07",
          "abstract": "EOSIO is one typical public blockchain platform. It is scalable in terms of\ntransaction speeds and has a growing ecosystem supporting smart contracts and\ndecentralized applications. However, the vulnerabilities within the EOSIO smart\ncontracts have led to serious attacks, which caused serious financial loss to\nits end users. In this work, we systematically analyzed three typical EOSIO\nsmart contract vulnerabilities and their related attacks. Then we presented\nEOSFuzzer, a general black-box fuzzing framework to detect vulnerabilities\nwithin EOSIO smart contracts. In particular, EOSFuzzer proposed effective\nattacking scenarios and test oracles for EOSIO smart contract fuzzing. Our\nfuzzing experiment on 3963 EOSIO smart contracts shows that EOSFuzzer is both\neffective and efficient to detect EOSIO smart contract vulnerabilities with\nhigh accuracy.",
          "arxiv_id": "2007.14903v3"
        },
        {
          "title": "Smart Contracts for SMEs and Large Companies",
          "year": "2025-05",
          "abstract": "Research on blockchains addresses multiple issues, with one being writing\nsmart contracts. In our previous research we described methodology and a tool\nto generate, in automated fashion, smart contracts from BPMN models. The\ngenerated smart contracts provide support for multi-step transactions that\nfacilitate repair/upgrade of smart contracts. In this paper we show how the\napproach is used to support collaborations via smart contracts for companies\nranging from SMEs with little IT capabilities to companies with IT using\nblockchain smart contracts. Furthermore, we also show how the approach is used\nfor certain applications to generate smart contracts by a BPMN modeler who does\nnot need any knowledge of blockchain technology or smart contract development -\nthus we are hoping to facilitate democratization of smart contracts and\nblockchain technology.",
          "arxiv_id": "2505.22619v1"
        }
      ],
      "9": [
        {
          "title": "Systematic Mapping of Monolithic Applications to Microservices Architecture",
          "year": "2023-09",
          "abstract": "The aim of this paper to provide the solution microservices architecture as a\npopular alternative to monolithic architecture. It discusses the advantages of\nmicroservices and the challenges that organizations face when transitioning\nfrom a monolithic system. It presents a case study of a financial application\nand proposed techniques for identifying microservices on monolithic systems\nusing domain-driven development concepts. In recent years, microservices\narchitecture has emerged as a new architectural style in the software\ndevelopment industry. As legacy monolithic software becomes too large to\nmanage, many large corporations are considering converting their traditional\nmonolithic systems into small-scale, self-contained microservices. However,\nmigrating from monolithic to microservices architecture is a difficult and\nchallenging task. It presents a comparison of the two architectural styles and\ndiscusses the difficulties that led companies to switch to microservices. The\nstudy's findings suggest that the proposed technique can improve work\nperformance and establish clear models, but it may not be useful for systems\nwith lower levels of complexity. This research paper has practical implications\nfor software architects and developers who are considering migrating from\nmonolithic to microservices architecture.",
          "arxiv_id": "2309.03796v1"
        },
        {
          "title": "MONO2REST: Identifying and Exposing Microservices: a Reusable RESTification Approach",
          "year": "2025-03",
          "abstract": "The microservices architectural style has become the de facto standard for\nlarge-scale cloud applications, offering numerous benefits in scalability,\nmaintainability, and deployment flexibility. Many organizations are pursuing\nthe migration of legacy monolithic systems to a microservices architecture.\nHowever, this process is challenging, risky, time-intensive, and\nprone-to-failure while several organizations lack necessary financial\nresources, time, or expertise to set up this migration process. So, rather than\ntrying to migrate a legacy system where migration is risky or not feasible, we\nsuggest exposing it as a microservice application without without having to\nmigrate it. In this paper, we present a reusable, automated, two-phase approach\nthat combines evolutionary algorithms with machine learning techniques. In the\nfirst phase, we identify microservices at the method level using a\nmulti-objective genetic algorithm that considers both structural and semantic\ndependencies between methods. In the second phase, we generate REST APIs for\neach identified microservice using a classification algorithm to assign HTTP\nmethods and endpoints. We evaluated our approach with a case study on the\nSpring PetClinic application, which has both monolithic and microservices\nimplementations that serve as ground truth for comparison. Results demonstrate\nthat our approach successfully aligns identified microservices with those in\nthe reference microservices implementation, highlighting its effectiveness in\nservice identification and API generation.",
          "arxiv_id": "2503.21522v1"
        },
        {
          "title": "Proposing a Dynamic Executive Microservices Architecture Model for AI Systems",
          "year": "2023-08",
          "abstract": "Microservices architecture is one of the new architectural styles that has\nimproved in recent years. It has become a popular architectural style among\nsystem architects and developers. This popularity increased with the advent of\nnew technologies and technological advancements in cloud computing. These\nadvancements caused the emergence of new design and development challenges for\nservice-based software systems. The increasing use of microservices\narchitecture in large organizations and teams has increased the need to find\nappropriate solutions for architecture challenges. Orchestration of the\ncomponents in the microservices architecture is one of the main challenges in\ndistributed systems and affects the software quality in factors such as\nefficiency, compatibility, stability, and reusability. In such systems,\nsoftware architecture consists of fine-grained components. Due to the\nincreasing number of microservices in a large-scale system, proper management\nand communication orchestration of microservice components can become a point\nof failure. In this article, the challenges of Microservices architecture have\nbeen identified. To resolve the component orchestration challenges, an\nappropriate model to maintain and improve quality is proposed. The presented\nmodel, as a pattern, can be used at the both design and development level of\nthe system. The Dynamicity of software at runtime is the main achievement of\nthis pattern. In this model, microservice components orchestration tasks are\nperformed by using a BPMN-based workflow engine as the orchestrator component.\nThe orchestrator design gives the ability to create, track and modify new\ncomposite microservices without the need to change platform infrastructure.",
          "arxiv_id": "2308.05833v1"
        }
      ],
      "10": [
        {
          "title": "A Survey on Automated Software Vulnerability Detection Using Machine Learning and Deep Learning",
          "year": "2023-06",
          "abstract": "Software vulnerability detection is critical in software security because it\nidentifies potential bugs in software systems, enabling immediate remediation\nand mitigation measures to be implemented before they may be exploited.\nAutomatic vulnerability identification is important because it can evaluate\nlarge codebases more efficiently than manual code auditing. Many Machine\nLearning (ML) and Deep Learning (DL) based models for detecting vulnerabilities\nin source code have been presented in recent years. However, a survey that\nsummarises, classifies, and analyses the application of ML/DL models for\nvulnerability detection is missing. It may be difficult to discover gaps in\nexisting research and potential for future improvement without a comprehensive\nsurvey. This could result in essential areas of research being overlooked or\nunder-represented, leading to a skewed understanding of the state of the art in\nvulnerability detection. This work address that gap by presenting a systematic\nsurvey to characterize various features of ML/DL-based source code level\nsoftware vulnerability detection approaches via five primary research questions\n(RQs). Specifically, our RQ1 examines the trend of publications that leverage\nML/DL for vulnerability detection, including the evolution of research and the\ndistribution of publication venues. RQ2 describes vulnerability datasets used\nby existing ML/DL-based models, including their sources, types, and\nrepresentations, as well as analyses of the embedding techniques used by these\napproaches. RQ3 explores the model architectures and design assumptions of\nML/DL-based vulnerability detection approaches. RQ4 summarises the type and\nfrequency of vulnerabilities that are covered by existing studies. Lastly, RQ5\npresents a list of current challenges to be researched and an outline of a\npotential research roadmap that highlights crucial opportunities for future\nwork.",
          "arxiv_id": "2306.11673v1"
        },
        {
          "title": "DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection",
          "year": "2023-04",
          "abstract": "We propose and release a new vulnerable source code dataset. We curate the\ndataset by crawling security issue websites, extracting vulnerability-fixing\ncommits and source codes from the corresponding projects. Our new dataset\ncontains 18,945 vulnerable functions spanning 150 CWEs and 330,492\nnon-vulnerable functions extracted from 7,514 commits. Our dataset covers 295\nmore projects than all previous datasets combined.\n  Combining our new dataset with previous datasets, we present an analysis of\nthe challenges and promising research directions of using deep learning for\ndetecting software vulnerabilities. We study 11 model architectures belonging\nto 4 families. Our results show that deep learning is still not ready for\nvulnerability detection, due to high false positive rate, low F1 score, and\ndifficulty of detecting hard CWEs. In particular, we demonstrate an important\ngeneralization challenge for the deployment of deep learning-based models. We\nshow that increasing the volume of training data may not further improve the\nperformance of deep learning models for vulnerability detection, but might be\nuseful to improve the generalization ability to unseen projects.\n  We also identify hopeful future research directions. We demonstrate that\nlarge language models (LLMs) are a promising research direction for ML-based\nvulnerability detection, outperforming Graph Neural Networks (GNNs) with\ncode-structure features in our experiments. Moreover, developing source code\nspecific pre-training objectives is a promising research direction to improve\nthe vulnerability detection performance.",
          "arxiv_id": "2304.00409v2"
        },
        {
          "title": "Security Vulnerability Detection with Multitask Self-Instructed Fine-Tuning of Large Language Models",
          "year": "2024-06",
          "abstract": "Software security vulnerabilities allow attackers to perform malicious\nactivities to disrupt software operations. Recent Transformer-based language\nmodels have significantly advanced vulnerability detection, surpassing the\ncapabilities of static analysis based deep learning models. However, language\nmodels trained solely on code tokens do not capture either the explanation of\nvulnerability type or the data flow structure information of code, both of\nwhich are crucial for vulnerability detection. We propose a novel technique\nthat integrates a multitask sequence-to-sequence LLM with pro-gram control flow\ngraphs encoded as a graph neural network to achieve sequence-to-classification\nvulnerability detection. We introduce MSIVD, multitask self-instructed\nfine-tuning for vulnerability detection, inspired by chain-of-thought prompting\nand LLM self-instruction. Our experiments demonstrate that MSIVD achieves\nsuperior performance, outperforming the highest LLM-based vulnerability\ndetector baseline (LineVul), with a F1 score of 0.92 on the BigVul dataset, and\n0.48 on the PreciseBugs dataset. By training LLMs and GNNs simultaneously using\na combination of code and explanatory metrics of a vulnerable program, MSIVD\nrepresents a promising direction for advancing LLM-based vulnerability\ndetection that generalizes to unseen data. Based on our findings, we further\ndiscuss the necessity for new labelled security vulnerability datasets, as\nrecent LLMs have seen or memorized prior datasets' held-out evaluation data.",
          "arxiv_id": "2406.05892v1"
        }
      ],
      "11": [
        {
          "title": "ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study",
          "year": "2024-01",
          "abstract": "Machine learning (ML), especially with the emergence of large language models\n(LLMs), has significantly transformed various industries. However, the\ntransition from ML model prototyping to production use within software systems\npresents several challenges. These challenges primarily revolve around ensuring\nsafety, security, and transparency, subsequently influencing the overall\nrobustness and trustworthiness of ML models. In this paper, we introduce\nML-On-Rails, a protocol designed to safeguard ML models, establish a\nwell-defined endpoint interface for different ML tasks, and clear communication\nbetween ML providers and ML consumers (software engineers). ML-On-Rails\nenhances the robustness of ML models via incorporating detection capabilities\nto identify unique challenges specific to production ML. We evaluated the\nML-On-Rails protocol through a real-world case study of the MoveReminder\napplication. Through this evaluation, we emphasize the importance of\nsafeguarding ML models in production.",
          "arxiv_id": "2401.06513v1"
        },
        {
          "title": "Bug Characterization in Machine Learning-based Systems",
          "year": "2023-07",
          "abstract": "Rapid growth of applying Machine Learning (ML) in different domains,\nespecially in safety-critical areas, increases the need for reliable ML\ncomponents, i.e., a software component operating based on ML. Understanding the\nbugs characteristics and maintenance challenges in ML-based systems can help\ndevelopers of these systems to identify where to focus maintenance and testing\nefforts, by giving insights into the most error-prone components, most common\nbugs, etc. In this paper, we investigate the characteristics of bugs in\nML-based software systems and the difference between ML and non-ML bugs from\nthe maintenance viewpoint. We extracted 447,948 GitHub repositories that used\none of the three most popular ML frameworks, i.e., TensorFlow, Keras, and\nPyTorch. After multiple filtering steps, we select the top 300 repositories\nwith the highest number of closed issues. We manually investigate the extracted\nrepositories to exclude non-ML-based systems. Our investigation involved a\nmanual inspection of 386 sampled reported issues in the identified ML-based\nsystems to indicate whether they affect ML components or not. Our analysis\nshows that nearly half of the real issues reported in ML-based systems are ML\nbugs, indicating that ML components are more error-prone than non-ML\ncomponents. Next, we thoroughly examined 109 identified ML bugs to identify\ntheir root causes, symptoms, and calculate their required fixing time. The\nresults also revealed that ML bugs have significantly different characteristics\ncompared to non-ML bugs, in terms of the complexity of bug-fixing (number of\ncommits, changed files, and changed lines of code). Based on our results,\nfixing ML bugs are more costly and ML components are more error-prone, compared\nto non-ML bugs and non-ML components respectively. Hence, paying a significant\nattention to the reliability of the ML components is crucial in ML-based\nsystems.",
          "arxiv_id": "2307.14512v1"
        },
        {
          "title": "A Large-Scale Study of Model Integration in ML-Enabled Software Systems",
          "year": "2024-08",
          "abstract": "The rise of machine learning (ML) and its integration into software systems\nhas drastically changed development practices. While software engineering\ntraditionally focused on manually created code artifacts with dedicated\nprocesses and architectures, ML-enabled systems require additional data-science\nmethods and tools to create ML artifacts -- especially ML models and training\ndata. However, integrating models into systems, and managing the many different\nartifacts involved, is far from trivial. ML-enabled systems can easily have\nmultiple ML models that interact with each other and with traditional code in\nintricate ways. Unfortunately, while challenges and practices of building\nML-enabled systems have been studied, little is known about the characteristics\nof real-world ML-enabled systems beyond isolated examples. Improving\nengineering processes and architectures for ML-enabled systems requires\nimproving the empirical understanding of these systems. We present a\nlarge-scale study of 2,928 open-source ML-enabled software systems. We\nclassified and analyzed them to determine system characteristics, model and\ncode reuse practices, and architectural aspects of integrating ML models. Our\nfindings show that these systems still mainly consist of traditional source\ncode, and that ML model reuse through code duplication or pre-trained models is\ncommon. We also identified different ML integration patterns and related\nimplementation practices. We hope that our results help improve practices for\nintegrating ML models, bringing data science and software engineering closer\ntogether.",
          "arxiv_id": "2408.06226v2"
        }
      ],
      "12": [
        {
          "title": "Supporting DNN Safety Analysis and Retraining through Heatmap-based Unsupervised Learning",
          "year": "2020-02",
          "abstract": "Deep neural networks (DNNs) are increasingly important in safety-critical\nsystems, for example in their perception layer to analyze images.\nUnfortunately, there is a lack of methods to ensure the functional safety of\nDNN-based components. We observe three major challenges with existing practices\nregarding DNNs in safety-critical systems: (1) scenarios that are\nunderrepresented in the test set may lead to serious safety violation risks,\nbut may, however, remain unnoticed; (2) characterizing such high-risk scenarios\nis critical for safety analysis; (3) retraining DNNs to address these risks is\npoorly supported when causes of violations are difficult to determine. To\naddress these problems in the context of DNNs analyzing images, we propose\nHUDD, an approach that automatically supports the identification of root causes\nfor DNN errors. HUDD identifies root causes by applying a clustering algorithm\nto heatmaps capturing the relevance of every DNN neuron on the DNN outcome.\nAlso, HUDD retrains DNNs with images that are automatically selected based on\ntheir relatedness to the identified image clusters. We evaluated HUDD with DNNs\nfrom the automotive domain. HUDD was able to identify all the distinct root\ncauses of DNN errors, thus supporting safety analysis. Also, our retraining\napproach has shown to be more effective at improving DNN accuracy than existing\napproaches.",
          "arxiv_id": "2002.00863v4"
        },
        {
          "title": "Black-Box Testing of Deep Neural Networks Through Test Case Diversity",
          "year": "2021-12",
          "abstract": "Deep Neural Networks (DNNs) have been extensively used in many areas\nincluding image processing, medical diagnostics, and autonomous driving.\nHowever, DNNs can exhibit erroneous behaviours that may lead to critical\nerrors, especially when used in safety-critical systems. Inspired by testing\ntechniques for traditional software systems, researchers have proposed neuron\ncoverage criteria, as an analogy to source code coverage, to guide the testing\nof DNN models. Despite very active research on DNN coverage, several recent\nstudies have questioned the usefulness of such criteria in guiding DNN testing.\nFurther, from a practical standpoint, these criteria are white-box as they\nrequire access to the internals or training data of DNN models, which is in\nmany contexts not feasible or convenient. In this paper, we investigate\nblack-box input diversity metrics as an alternative to white-box coverage\ncriteria. To this end, we first select and adapt three diversity metrics and\nstudy, in a controlled manner, their capacity to measure actual diversity in\ninput sets. We then analyse their statistical association with fault detection\nusing four datasets and five DNN models. We further compare diversity with\nstate-of-the-art white-box coverage criteria. Our experiments show that relying\non the diversity of image features embedded in test input sets is a more\nreliable indicator than coverage criteria to effectively guide the testing of\nDNNs. Indeed, we found that one of our selected black-box diversity metrics far\noutperforms existing coverage criteria in terms of fault-revealing capability\nand computational time. Results also confirm the suspicions that\nstate-of-the-art coverage metrics are not adequate to guide the construction of\ntest input sets to detect as many faults as possible with natural inputs.",
          "arxiv_id": "2112.12591v5"
        },
        {
          "title": "DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks",
          "year": "2023-03",
          "abstract": "Deep neural networks (DNNs) are widely used in various application domains\nsuch as image processing, speech recognition, and natural language processing.\nHowever, testing DNN models may be challenging due to the complexity and size\nof their input domain. Particularly, testing DNN models often requires\ngenerating or exploring large unlabeled datasets. In practice, DNN test\noracles, which identify the correct outputs for inputs, often require expensive\nmanual effort to label test data, possibly involving multiple experts to ensure\nlabeling correctness. In this paper, we propose DeepGD, a black-box\nmulti-objective test selection approach for DNN models. It reduces the cost of\nlabeling by prioritizing the selection of test inputs with high fault revealing\npower from large unlabeled datasets. DeepGD not only selects test inputs with\nhigh uncertainty scores to trigger as many mispredicted inputs as possible but\nalso maximizes the probability of revealing distinct faults in the DNN model by\nselecting diverse mispredicted inputs. The experimental results conducted on\nfour widely used datasets and five DNN models show that in terms of\nfault-revealing ability: (1) White-box, coverage-based approaches fare poorly,\n(2) DeepGD outperforms existing black-box test selection approaches in terms of\nfault detection, and (3) DeepGD also leads to better guidance for DNN model\nretraining when using selected inputs to augment the training set.",
          "arxiv_id": "2303.04878v5"
        }
      ],
      "13": [
        {
          "title": "Feature Importance in the Context of Traditional and Just-In-Time Software Defect Prediction Models",
          "year": "2024-11",
          "abstract": "Software defect prediction models can assist software testing initiatives by\nprioritizing testing error-prone modules. In recent years, in addition to the\ntraditional defect prediction model approach of predicting defects from class,\nmodules, etc., Just-In-Time defect prediction research, which focuses on the\nchange history of software products is getting prominent. For building these\ndefect prediction models, it is important to understand which features are\nprimary contributors to these classifiers. This study considered developing\ndefect prediction models incorporating the traditional and the Just-In-Time\napproaches from the publicly available dataset of the Apache Camel project. A\nmulti-layer deep learning algorithm was applied to these datasets in comparison\nwith machine learning algorithms. The deep learning algorithm achieved\naccuracies of 80% and 86%, with the area under receiving operator curve (AUC)\nscores of 66% and 78% for traditional and Just-In-Time defect prediction,\nrespectively. Finally, the feature importance of these models was identified\nusing a model-specific integrated gradient method and a model-agnostic Shapley\nAdditive Explanation (SHAP) technique.",
          "arxiv_id": "2411.05230v1"
        },
        {
          "title": "The Impact of Dormant Defects on Defect Prediction: a Study of 19 Apache Projects",
          "year": "2021-05",
          "abstract": "Defect prediction models can be beneficial to prioritize testing, analysis,\nor code review activities, and has been the subject of a substantial effort in\nacademia, and some applications in industrial contexts. A necessary\nprecondition when creating a defect prediction model is the availability of\ndefect data from the history of projects. If this data is noisy, the resulting\ndefect prediction model could result to be unreliable. One of the causes of\nnoise for defect datasets is the presence of \"dormant defects\", i.e., of\ndefects discovered several releases after their introduction. This can cause a\nclass to be labeled as defect-free while it is not, and is, therefore\n\"snoring\". In this paper, we investigate the impact of snoring on classifiers'\naccuracy and the effectiveness of a possible countermeasure, i.e., dropping too\nrecent data from a training set. We analyze the accuracy of 15 machine learning\ndefect prediction classifiers, on data from more than 4,000 defects and 600\nreleases of 19 open source projects from the Apache ecosystem. Our results show\nthat on average across projects: (i) the presence of dormant defects decreases\nthe recall of defect prediction classifiers, and (ii) removing from the\ntraining set the classes that in the last release are labeled as not defective\nsignificantly improves the accuracy of the classifiers. In summary, this paper\nprovides insights on how to create defects datasets by mitigating the negative\neffect of dormant defects on defect prediction.",
          "arxiv_id": "2105.12372v1"
        },
        {
          "title": "Toward a consistent performance evaluation for defect prediction models",
          "year": "2023-02",
          "abstract": "In defect prediction community, many defect prediction models have been\nproposed and indeed more new models are continuously being developed. However,\nthere is no consensus on how to evaluate the performance of a newly proposed\nmodel. In this paper, we aim to propose MATTER, a fraMework towArd a consisTenT\npErformance compaRison, which makes model performance directly comparable\nacross different studies. We take three actions to build a consistent\nevaluation framework for defect prediction models. First, we propose a simple\nand easy-to-use unsupervised baseline model ONE (glObal baseliNe modEl) to\nprovide \"a single point of comparison\". Second, we propose using the\nSQA-effort-aligned threshold setting to make a fair comparison. Third, we\nsuggest reporting the evaluation results in a unified way and provide a set of\ncore performance indicators for this purpose, thus enabling an across-study\ncomparison to attain real progress. The experimental results show that MATTER\ncan serve as an effective framework to support a consistent performance\nevaluation for defect prediction models and hence can help determine whether a\nnewly proposed defect prediction model is practically useful for practitioners\nand inform the real progress in the road of defect prediction. Furthermore,\nwhen applying MATTER to evaluate the representative defect prediction models\nproposed in recent years, we find that most of them (if not all) are not\nsuperior to the simple baseline model ONE in terms of the SQA-effort awareness\nprediction performance. This reveals that the real progress in defect\nprediction has been overestimated. We hence recommend that, in future studies,\nwhen any new defect prediction model is proposed, MATTER should be used to\nevaluate its actual usefulness (on the same benchmark test data sets) to\nadvance scientific progress in defect prediction.",
          "arxiv_id": "2302.00394v2"
        }
      ],
      "14": [
        {
          "title": "Log-based Anomaly Detection Without Log Parsing",
          "year": "2021-08",
          "abstract": "Software systems often record important runtime information in system logs\nfor troubleshooting purposes. There have been many studies that use log data to\nconstruct machine learning models for detecting system anomalies. Through our\nempirical study, we find that existing log-based anomaly detection approaches\nare significantly affected by log parsing errors that are introduced by 1) OOV\n(out-of-vocabulary) words, and 2) semantic misunderstandings. The log parsing\nerrors could cause the loss of important information for anomaly detection. To\naddress the limitations of existing methods, we propose NeuralLog, a novel\nlog-based anomaly detection approach that does not require log parsing.\nNeuralLog extracts the semantic meaning of raw log messages and represents them\nas semantic vectors. These representation vectors are then used to detect\nanomalies through a Transformer-based classification model, which can capture\nthe contextual information from log sequences. Our experimental results show\nthat the proposed approach can effectively understand the semantic meaning of\nlog messages and achieve accurate anomaly detection results. Overall, NeuralLog\nachieves F1-scores greater than 0.95 on four public datasets, outperforming the\nexisting approaches.",
          "arxiv_id": "2108.01955v3"
        },
        {
          "title": "Impact of Log Parsing on Deep Learning-Based Anomaly Detection",
          "year": "2023-05",
          "abstract": "Software systems log massive amounts of data, recording important runtime\ninformation. Such logs are used, for example, for log-based anomaly detection,\nwhich aims to automatically detect abnormal behaviors of the system under\nanalysis by processing the information recorded in its logs. Many log-based\nanomaly detection techniques based on deep learning models include a\npre-processing step called log parsing. However, understanding the impact of\nlog parsing on the accuracy of anomaly detection techniques has received\nsurprisingly little attention so far. Investigating what are the key properties\nlog parsing techniques should ideally have to help anomaly detection is\ntherefore warranted.\n  In this paper, we report on a comprehensive empirical study on the impact of\nlog parsing on anomaly detection accuracy, using 13 log parsing techniques,\nseven anomaly detection techniques (five based on deep learning and two based\non traditional machine learning) on three publicly available log datasets. Our\nempirical results show that, despite what is widely assumed, there is no strong\ncorrelation between log parsing accuracy and anomaly detection accuracy,\nregardless of the metric used for measuring log parsing accuracy. Moreover, we\nexperimentally confirm existing theoretical results showing that it is a\nproperty that we refer to as distinguishability in log parsing results as\nopposed to their accuracy that plays an essential role in achieving accurate\nanomaly detection.",
          "arxiv_id": "2305.15897v4"
        },
        {
          "title": "UniParser: A Unified Log Parser for Heterogeneous Log Data",
          "year": "2022-02",
          "abstract": "Logs provide first-hand information for engineers to diagnose failures in\nlarge-scale online service systems. Log parsing, which transforms\nsemi-structured raw log messages into structured data, is a prerequisite of\nautomated log analysis such as log-based anomaly detection and diagnosis.\nAlmost all existing log parsers follow the general idea of extracting the\ncommon part as templates and the dynamic part as parameters. However, these log\nparsing methods, often neglect the semantic meaning of log messages.\nFurthermore, high diversity among various log sources also poses an obstacle in\nthe generalization of log parsing across different systems. In this paper, we\npropose UniParser to capture the common logging behaviours from heterogeneous\nlog data. UniParser utilizes a Token Encoder module and a Context Encoder\nmodule to learn the patterns from the log token and its neighbouring context. A\nContext Similarity module is specially designed to model the commonalities of\nlearned patterns. We have performed extensive experiments on 16 public log\ndatasets and our results show that UniParser outperperforms state-of-the-art\nlog parsers by a large margin.",
          "arxiv_id": "2202.06569v1"
        }
      ],
      "15": [
        {
          "title": "An Empirical Study on Refactoring-Inducing Pull Requests",
          "year": "2021-08",
          "abstract": "Background: Pull-based development has shaped the practice of Modern Code\nReview (MCR), in which reviewers can contribute code improvements, such as\nrefactorings, through comments and commits in Pull Requests (PRs). Past MCR\nstudies uniformly treat all PRs, regardless of whether they induce refactoring\nor not. We define a PR as refactoring-inducing, when refactoring edits are\nperformed after the initial commit(s), as either a result of discussion among\nreviewers or spontaneous actions carried out by the PR developer. Aims: This\nmixed study (quantitative and qualitative) explores code reviewing-related\naspects intending to characterize refactoring-inducing PRs. Method: We\nhypothesize that refactoring-inducing PRs have distinct characteristics than\nnon-refactoring-inducing ones and thus deserve special attention and treatment\nfrom researchers, practitioners, and tool builders. To investigate our\nhypothesis, we mined a sample of 1,845 Apache's merged PRs from GitHub, mined\nrefactoring edits in these PRs, and ran a comparative study between\nrefactoring-inducing and non-refactoring-inducing PRs. We also manually\nexamined 2,096 review comments and 1,891 detected refactorings from 228\nrefactoring-inducing PRs. Results: We found 30.2% of refactoring-inducing PRs\nin our sample and that they significantly differ from non-refactoring-inducing\nones in terms of number of commits, code churn, number of file changes, number\nof review comments, length of discussion, and time to merge. However, we found\nno statistical evidence that the number of reviewers is related to\nrefactoring-inducement. Our qualitative analysis revealed that at least one\nrefactoring edit was induced by review in 133 (58.3%) of the\nrefactoring-inducing PRs examined. Conclusions: Our findings suggest directions\nfor researchers, practitioners, and tool builders to improve practices around\npull-based code review.",
          "arxiv_id": "2108.10994v1"
        },
        {
          "title": "On the Documentation of Refactoring Types",
          "year": "2021-12",
          "abstract": "Commit messages are the atomic level of software documentation. They provide\na natural language description of the code change and its purpose. Messages are\ncritical for software maintenance and program comprehension. Unlike documenting\nfeature updates and bug fixes, little is known about how developers document\ntheir refactoring activities. Developers can perform multiple refactoring\noperations, including moving methods, extracting classes, for various reasons.\nYet, there is no systematic study that analyzes the extent to which the\ndocumentation of refactoring accurately describes the refactoring operations\nperformed at the source code level. Therefore, this paper challenges the\nability of refactoring documentation to adequately predict the refactoring\ntypes, performed at the commit level. Our analysis relies on the text mining of\ncommit messages to extract the corresponding features that better represent\neach class. The extraction of text patterns, specific to each refactoring\nallows the design of a model that verifies the consistency of these patterns\nwith their corresponding refactoring. Such verification process can be achieved\nvia automatically predicting the method-level type of refactoring being\napplied, namely Extract Method, Inline Method, Move Method, Pull-up Method,\nPush-down Method, and Rename Method. We compared various classifiers, and a\nbaseline keyword-based approach, in terms of their prediction performance,\nusing a dataset of 5,004 commits. Our main findings show that the complexity of\nrefactoring type prediction varies from one type to another. Rename method and\nExtract method were found to be the best documented refactoring activities,\nwhile Pull-up Method and Push-down Method were the hardest to be identified via\ntextual descriptions. Such findings bring the attention of developers to the\nnecessity of paying more attention to the documentation of these types.",
          "arxiv_id": "2112.01581v1"
        },
        {
          "title": "State of Refactoring Adoption: Better Understanding Developer Perception of Refactoring",
          "year": "2023-06",
          "abstract": "We aim to explore how developers document their refactoring activities during\nthe software life cycle. We call such activity Self-Affirmed Refactoring (SAR),\nwhich indicates developers' documentation of their refactoring activities. SAR\nis crucial in understanding various aspects of refactoring, including the\nmotivation, procedure, and consequences of the performed code change. After\nthat, we propose an approach to identify whether a commit describes\ndeveloper-related refactoring events to classify them according to the\nrefactoring common quality improvement categories. To complement this goal, we\naim to reveal insights into how reviewers decide to accept or reject a\nsubmitted refactoring request and what makes such a review challenging.Our SAR\ntaxonomy and model can work with refactoring detectors to report any early\ninconsistency between refactoring types and their documentation. They can serve\nas a solid background for various empirical investigations. Our survey with\ncode reviewers has revealed several difficulties related to understanding the\nrefactoring intent and implications on the functional and non-functional\naspects of the software. In light of our findings from the industrial case\nstudy, we recommended a procedure to properly document refactoring activities,\nas part of our survey feedback.",
          "arxiv_id": "2306.06019v1"
        }
      ],
      "16": [
        {
          "title": "A Reference Architecture for Quantum Computing as a Service",
          "year": "2023-06",
          "abstract": "Quantum computers (QCs) aim to disrupt the status-quo of computing --\nreplacing traditional systems and platforms that are driven by digital circuits\nand modular software -- with hardware and software that operates on the\nprinciple of quantum mechanics. QCs that rely on quantum mechanics can exploit\nquantum circuits (i.e., quantum bits for manipulating quantum gates) to achieve\n\"quantum computational supremacy\" over traditional, i.e., digital computing\nsystems. Currently, the issues that impede mass-scale adoption of quantum\nsystems are rooted in the fact that building, maintaining, and/or programming\nQCs is a complex and radically distinct engineering paradigm when compared to\nchallenges of classical computing and software engineering. Quantum service\norientation is seen as a solution that synergises the research on service\ncomputing and quantum software engineering (QSE) to allow developers and users\nto build and utilise quantum software services based on pay-per-shot utility\ncomputing model. The pay-per-shot model represents a single execution of\ninstruction on quantum processing unit and it allows vendors (e.g., Amazon\nBraket) to offer their QC platforms, simulators, software services etc. to\nenterprises and individuals who do not need to own or maintain quantum systems.\nThis research contributes by 1) developing a reference architecture for\nenabling quantum computing as a service, 2) implementing microservices with the\nquantum-classic split pattern as an architectural use-case, and 3) evaluating\nthe reference architecture based on feedback by 22 practitioners. In the QSE\ncontext, the research focuses on unifying architectural methods and\nservice-orientation patterns to promote reuse knowledge and best practices to\ntackle emerging and futuristic challenges of architecting and implementing\nQuantum Computing as a Service (QCaaS).",
          "arxiv_id": "2306.04578v1"
        },
        {
          "title": "Quantum Software Engineering: A New Genre of Computing",
          "year": "2022-11",
          "abstract": "Quantum computing (QC) is no longer only a scientific interest but is rapidly\nbecoming an industrially available technology that can potentially tackle the\nlimitations of classical computing. Over the last few years, major technology\ngiants have invested in developing hardware and programming frameworks to\ndevelop quantum-specific applications. QC hardware technologies are gaining\nmomentum, however, operationalizing the QC technologies trigger the need for\nsoftware-intensive methodologies, techniques, processes, tools, roles, and\nresponsibilities for developing industrial-centric quantum software\napplications. This paper presents the vision of the quantum software\nengineering (QSE) life cycle consisting of quantum requirements engineering,\nquantum software design, quantum software implementation, quantum software\ntesting, and quantum software maintenance. This paper particularly calls for\njoint contributions of software engineering research and industrial community\nto present real-world solutions to support the entire quantum software\ndevelopment activities. The proposed vision facilitates the researchers and\npractitioners to propose new processes, reference architectures, novel tools,\nand practices to leverage quantum computers and develop emerging and next\ngenerations of quantum software.",
          "arxiv_id": "2211.13990v1"
        },
        {
          "title": "Bug Characteristics in Quantum Software Ecosystem",
          "year": "2022-04",
          "abstract": "With the advance in quantum computing in recent years, quantum software\nbecomes vital for exploring the full potential of quantum computing systems.\nQuantum programming is different from classical programming, for example, the\nstate of a quantum program is probabilistic in nature, and a quantum computer\nis error-prone due to the instability of quantum mechanisms. Therefore, the\ncharacteristics of bugs in quantum software projects may be very different from\nthat of classical software projects. This work aims to understand the\ncharacteristics of bugs in quantum software projects, in order to provide\ninsights to help devise effective testing and debugging mechanisms. To achieve\nthis goal, we conduct an empirical study on the bug reports of 125 quantum\nsoftware projects. We observe that quantum software projects are more buggy\nthan classical software projects and that quantum project bugs are more costly\nto fix than classical project bugs. We also identify the types of the bugs and\nthe quantum programming components where they occurred. Our study shows that\nthe bugs are spread across different components, but quantum-specific bugs\nparticularly appear in the compiler, gate operation, and state preparation\ncomponents. The three most occurring types of bugs are Program anomaly bugs,\nConfiguration bugs, and Data type and structure bugs. Our study highlights some\nparticularly challenging areas in quantum software development, such as the\nlack of scientific quantum computation libraries that implement comprehensive\nmathematical functions for quantum computing. Quantum developers also seek\nspecialized data manipulation libraries for quantum software engineering like\nNumpy for quantum computing. Our findings also provide insights for future work\nto advance the quantum program development, testing, and debugging of quantum\nsoftware, such as providing tooling support for debugging low-level circuits.",
          "arxiv_id": "2204.11965v1"
        }
      ],
      "17": [
        {
          "title": "A Research Software Engineering Workflow for Computational Science and Engineering",
          "year": "2022-08",
          "abstract": "University research groups in Computational Science and Engineering (CSE)\ngenerally lack dedicated funding and personnel for Research Software\nEngineering (RSE), which, combined with the pressure to maximize the number of\nscientific publications, shifts the focus away from sustainable research\nsoftware development and reproducible results. The neglect of RSE in CSE at\nUniversity research groups negatively impacts the scientific output: research\ndata - including research software - related to a CSE publication cannot be\nfound, reproduced, or re-used, different ideas are not combined easily into new\nideas, and published methods must very often be re-implemented to be\ninvestigated further. This slows down CSE research significantly, resulting in\nconsiderable losses in time and, consequentially, public funding.\n  We propose a RSE workflow for Computational Science and Engineering (CSE)\nthat addresses these challenges, that improves the quality of research output\nin CSE. Our workflow applies established software engineering practices adapted\nfor CSE: software testing, result visualization, and periodical cross-linking\nof software with reports/publications and data, timed by milestones in the\nscientific publication process. The workflow introduces minimal work overhead,\ncrucial for university research groups, and delivers modular and tested\nsoftware linked to publications whose results can easily be reproduced. We\ndefine research software quality from a perspective of a pragmatic researcher:\nthe ability to quickly find the publication, data, and software related to a\npublished research idea, quickly reproduce results, understand or re-use a CSE\nmethod, and finally extend the method with new research ideas.",
          "arxiv_id": "2208.07460v1"
        },
        {
          "title": "Jup2Kub: algorithms and a system to translate a Jupyter Notebook pipeline to a fault tolerant distributed Kubernetes deployment",
          "year": "2023-11",
          "abstract": "Scientific workflows facilitate computational, data manipulation, and\nsometimes visualization steps for scientific data analysis. They are vital for\nreproducing and validating experiments, usually involving computational steps\nin scientific simulations and data analysis. These workflows are often\ndeveloped by domain scientists using Jupyter notebooks, which are convenient\nyet face limitations: they struggle to scale with larger data sets, lack\nfailure tolerance, and depend heavily on the stability of underlying tools and\npackages. To address these issues, Jup2Kup has been developed. This software\nsystem translates workflows from Jupyter notebooks into a distributed,\nhigh-performance Kubernetes environment, enhancing fault tolerance. It also\nmanages software dependencies to maintain operational stability amidst changes\nin tools and packages.",
          "arxiv_id": "2311.12308v1"
        },
        {
          "title": "A pragmatic workflow for research software engineering in computational science",
          "year": "2023-10",
          "abstract": "University research groups in Computational Science and Engineering (CSE)\ngenerally lack dedicated funding and personnel for Research Software\nEngineering (RSE), which, combined with the pressure to maximize the number of\nscientific publications, shifts the focus away from sustainable research\nsoftware development and reproducible results. The neglect of RSE in CSE at\nUniversity research groups negatively impacts the scientific output: research\ndata - including research software - related to a CSE publication cannot be\nfound, reproduced, or re-used, different ideas are not combined easily into new\nideas, and published methods must very often be re-implemented to be\ninvestigated further. This slows down CSE research significantly, resulting in\nconsiderable losses in time and, consequentially, public funding.\n  We propose a RSE workflow for Computational Science and Engineering (CSE)\nthat addresses these challenges, that improves the quality of research output\nin CSE. Our workflow applies established software engineering practices adapted\nfor CSE: software testing, result visualization, and periodical cross-linking\nof software with reports/publications and data, timed by milestones in the\nscientific publication process. The workflow introduces minimal work overhead,\ncrucial for university research groups, and delivers modular and tested\nsoftware linked to publications whose results can easily be reproduced. We\ndefine research software quality from a perspective of a pragmatic researcher:\nthe ability to quickly find the publication, data, and software related to a\npublished research idea, quickly reproduce results, understand or re-use a CSE\nmethod, and finally extend the method with new research ideas.",
          "arxiv_id": "2310.00960v1"
        }
      ],
      "18": [
        {
          "title": "Conceptual Model with Built-in Process Mining",
          "year": "2021-03",
          "abstract": "Process mining involves discovering, monitoring, and improving real processes\nby extracting knowledge from event logs in information systems. Process mining\nhas become an important topic in recent years, as evidenced by a growing number\nof case studies and commercial tools. Current studies in this area assume that\nevent records are created separately from a conceptual model (CM). Techniques\nare then used to discover missing processes and conformance with the CM, as\nwell as for checks and enhancements. By contrast, in this paper we focus on\nmodeling events as part of a tight multilevel CM that includes a static\ndescription, dynamics, events-log scheme, and monitoring and control system. If\nthere is an out-of-model event log, it is treated as a requirement needed to\nbuild or enrich the CM. The motivation for such a unified system is our thesis\nthat process mining is an essential component of a CM with built-in mining\ncapabilities to perform self-process mining and attain completeness.\nAccordingly, our proposed conceptual model facilitates collecting data\ngenerated about itself. The resultant framework emphasizes an integrated\nrepresentation of systems to include process-mining functionalities. Case\nstudies that start with event logs are recast to evolve around a model-first\napproach that is not limited to the initial event log. The result presents a\nframework that achieves the aims of process mining in a more comprehensive way",
          "arxiv_id": "2103.16956v1"
        },
        {
          "title": "Automated Discovery of Process Models with True Concurrency and Inclusive Choices",
          "year": "2021-05",
          "abstract": "Enterprise information systems allow companies to maintain detailed records\nof their business process executions. These records can be extracted in the\nform of event logs, which capture the execution of activities across multiple\ninstances of a business process. Event logs may be used to analyze business\nprocesses at a fine level of detail using process mining techniques. Among\nother things, process mining techniques allow us to discover a process model\nfrom an event log -- an operation known as automated process discovery. Despite\na rich body of research in the field, existing automated process discovery\ntechniques do not fully capture the concurrency inherent in a business process.\nSpecifically, the bulk of these techniques treat two activities A and B as\nconcurrent if sometimes A completes before B and other times B completes before\nA. Typically though, activities in a business process are executed in a true\nconcurrency setting, meaning that two or more activity executions overlap\ntemporally. This paper addresses this gap by presenting a refined version of an\nautomated process discovery technique, namely Split Miner, that discovers true\nconcurrency relations from event logs containing start and end timestamps for\neach activity. The proposed technique is also able to differentiate between\nexclusive and inclusive choices. We evaluate the proposed technique relative to\nexisting baselines using 11 real-life logs drawn from different industries.",
          "arxiv_id": "2105.06016v1"
        },
        {
          "title": "BPCE: A Prototype for Co-Evolution between Business Process Variants through Configurable Process Model",
          "year": "2023-03",
          "abstract": "With the continuous development of business process management technology,\nthe increasing business process models are usually owned by large enterprises.\nIn large enterprises, different stakeholders may modify the same business\nprocess model. In order to better manage the changeability of processes, they\nadopt configurable business process models to manage process variants. However,\nthe process variants will vary with the change in enterprise business demands.\nTherefore, it is necessary to explore the co-evolution of the process variants\nso as to effectively manage the business process family. To this end, a novel\nframework for co-evolution between business process variants through a\nconfigurable process model is proposed in this work. First, the mapping\nrelationship between process variants and configurable models is standardized\nin this study. A series of change operations and change propagation operations\nbetween process variants and configurable models are further defined for\nachieving propagation. Then, an overall algorithm is proposed for achieving\nco-evolution of process variants. Next, a prototype is developed for managing\nchange synchronization between process variants and configurable process\nmodels. Finally, the effectiveness and efficiency of our proposed process\nchange propagation method are verified based on experiments on two business\nprocess datasets. The experimental results show that our approach implements\nthe co-evolution of process variants with high accuracy and efficiency.",
          "arxiv_id": "2303.17388v1"
        }
      ],
      "19": [
        {
          "title": "Open Source Software Lifecycle Classification: Developing Wrangling Techniques for Complex Sociotechnical Systems",
          "year": "2025-04",
          "abstract": "Open source software is a rapidly evolving center for distributed work, and\nunderstanding the characteristics of this work across its different contexts is\nvital for informing policy, economics, and the design of enabling software. The\nsteep increase in open source projects and corporate participation have\ntransformed a peripheral, cottage industry component of the global technology\necosystem into a large, infinitely complex \"technology parts supplier\" wired\ninto every corner of contemporary life. The lack of theory and tools for\nbreaking this complexity down into identifiable project types or strategies for\nunderstanding them more systematically is incommensurate with current industry,\nsociety, and developer needs. This paper reviews previous attempts to classify\nopen source software and other organizational ecosystems, using open source\nscientific software ecosystems in contrast with those found in corporatized\nopen source software. It then examines the divergent and sometimes conflicting\npurposes that may exist for classifying open source projects and how these\ncompeting interests impede our progress in developing a comprehensive\nunderstanding of how open source software projects and companies operate.\nFinally, we will present an empirical, mixed-methods study demonstrating how to\nclassify open-source projects by their lifecycle position. This is the first\nstep forward, advancing our scientific and practical knowledge of open source\nsoftware through the lens of dynamic and evolving open source genres. It\nconcludes with examples and a proposed path forward.",
          "arxiv_id": "2504.16670v1"
        },
        {
          "title": "A Benchmarking Proposal for DevOps Practices on Open Source Software Projects",
          "year": "2023-04",
          "abstract": "The popularity of open-source software (OSS) projects has grown significantly\nover the last few years with more organizations relying on them. As these\nprojects become larger, the need for higher quality also increases. DevOps\npractices have been shown to improve quality and performance. The DORA\nbenchmarking reports provide useful information to compare DevOps practices\nperformance between organizations, but they focus on continuous deployment and\ndelivery to production, while OSS projects focus on the continuous release of\ncode and its impact on third parties. The DORA reports mention the increasing\npresence of OSS projects as they are widely used in the industry, but they have\nnever been used to measure OSS projects performance levels. This study reveals\nthat the DORA benchmark cannot be applied to OSS projects and proposes\nbenchmarking metrics for OSS projects, being the first one that adapts the DORA\nmetrics and applies them in OSS projects. The metrics proposed in this study\nfor benchmarking OSS projects include Release Frequency and Lead Time For\nReleased Changes to measure throughput, and Time To Repair Code and Bug Issues\nRate to assess stability. In contrast to the DORA reports, where data is\ncollected through manual surveys, in our proposal, data is collected\nautomatically by a tool we developed that retrieves information from public\nGitHub repositories. This reduces the risk of survey-based data collection. Our\nstudy also shows the benchmark feasibility by applying it to four popular OSS\nprojects: Angular, Kubernetes, Tensorflow, and VS Code. In addition, we\nproposed challenges that address the topics and future works to expand the\nknowledge and findings of this study. Overall, the findings of the study can\nhelp to improve future research on OSS projects and provide a better\nunderstanding and challenges of the role of DevOps practices in OSS projects.",
          "arxiv_id": "2304.14790v1"
        },
        {
          "title": "Towards a Critical Open-Source Software Database",
          "year": "2023-05",
          "abstract": "Open-source software (OSS) plays a vital role in the modern software\necosystem. However, the maintenance and sustainability of OSS projects can be\nchallenging. In this paper, we present the CrOSSD project, which aims to build\na database of OSS projects and measure their current project \"health\" status.\nIn the project, we will use both quantitative and qualitative metrics to\nevaluate the health of OSS projects. The quantitative metrics will be gathered\nthrough automated crawling of meta information such as the number of\ncontributors, commits and lines of code. Qualitative metrics will be gathered\nfor selected \"critical\" projects through manual analysis and automated tools,\nincluding aspects such as sustainability, funding, community engagement and\nadherence to security policies. The results of the analysis will be presented\non a user-friendly web platform, which will allow users to view the health of\nindividual OSS projects as well as the overall health of the OSS ecosystem.\nWith this approach, the CrOSSD project provides a comprehensive and up-to-date\nview of the health of OSS projects, making it easier for developers,\nmaintainers and other stakeholders to understand the health of OSS projects and\nmake informed decisions about their use and maintenance.",
          "arxiv_id": "2305.01311v1"
        }
      ],
      "20": [
        {
          "title": "Is Your AI-Generated Code Really Safe? Evaluating Large Language Models on Secure Code Generation with CodeSecEval",
          "year": "2024-07",
          "abstract": "Large language models (LLMs) have brought significant advancements to code\ngeneration and code repair, benefiting both novice and experienced developers.\nHowever, their training using unsanitized data from open-source repositories,\nlike GitHub, raises the risk of inadvertently propagating security\nvulnerabilities. Despite numerous studies investigating the safety of code\nLLMs, there remains a gap in comprehensively addressing their security\nfeatures. In this work, we aim to present a comprehensive study aimed at\nprecisely evaluating and enhancing the security aspects of code LLMs. To\nsupport our research, we introduce CodeSecEval, a meticulously curated dataset\ndesigned to address 44 critical vulnerability types with 180 distinct samples.\nCodeSecEval serves as the foundation for the automatic evaluation of code\nmodels in two crucial tasks: code generation and code repair, with a strong\nemphasis on security. Our experimental results reveal that current models\nfrequently overlook security issues during both code generation and repair\nprocesses, resulting in the creation of vulnerable code. In response, we\npropose different strategies that leverage vulnerability-aware information and\ninsecure code explanations to mitigate these security vulnerabilities.\nFurthermore, our findings highlight that certain vulnerability types\nparticularly challenge model performance, influencing their effectiveness in\nreal-world applications. Based on these findings, we believe our study will\nhave a positive impact on the software engineering community, inspiring the\ndevelopment of improved methods for training and utilizing LLMs, thereby\nleading to safer and more trustworthy model deployment.",
          "arxiv_id": "2407.02395v2"
        },
        {
          "title": "CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models",
          "year": "2023-02",
          "abstract": "Large language models (LLMs) for automatic code generation have achieved\nbreakthroughs in several programming tasks. Their advances in competition-level\nprogramming problems have made them an essential pillar of AI-assisted pair\nprogramming, and tools such as GitHub Copilot have emerged as part of the daily\nprogramming workflow used by millions of developers. The training data for\nthese models is usually collected from the Internet (e.g., from open-source\nrepositories) and is likely to contain faults and security vulnerabilities.\nThis unsanitized training data can cause the language models to learn these\nvulnerabilities and propagate them during the code generation procedure. While\nthese models have been extensively assessed for their ability to produce\nfunctionally correct programs, there remains a lack of comprehensive\ninvestigations and benchmarks addressing the security aspects of these models.\n  In this work, we propose a method to systematically study the security issues\nof code language models to assess their susceptibility to generating vulnerable\ncode. To this end, we introduce the first approach to automatically find\ngenerated code that contains vulnerabilities in black-box code generation\nmodels. To achieve this, we present an approach to approximate inversion of the\nblack-box code generation models based on few-shot prompting. We evaluate the\neffectiveness of our approach by examining code language models in generating\nhigh-risk security weaknesses. Furthermore, we establish a collection of\ndiverse non-secure prompts for various vulnerability scenarios using our\nmethod. This dataset forms a benchmark for evaluating and comparing the\nsecurity weaknesses in code language models.",
          "arxiv_id": "2302.04012v2"
        },
        {
          "title": "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)",
          "year": "2024-09",
          "abstract": "The capability of generating high-quality source code using large language\nmodels (LLMs) reduces software development time and costs. However, they often\nintroduce security vulnerabilities due to training on insecure open-source\ndata. This highlights the need for ensuring secure and functional code\ngeneration. This paper introduces PromSec, an algorithm for prom optimization\nfor secure and functioning code generation using LLMs. In PromSec, we combine\n1) code vulnerability clearing using a generative adversarial graph neural\nnetwork, dubbed as gGAN, to fix and reduce security vulnerabilities in\ngenerated codes and 2) code generation using an LLM into an interactive loop,\nsuch that the outcome of the gGAN drives the LLM with enhanced prompts to\ngenerate secure codes while preserving their functionality. Introducing a new\ncontrastive learning approach in gGAN, we formulate code-clearing and\ngeneration as a dual-objective optimization problem, enabling PromSec to\nnotably reduce the number of LLM inferences. PromSec offers a cost-effective\nand practical solution for generating secure, functional code. Extensive\nexperiments conducted on Python and Java code datasets confirm that PromSec\neffectively enhances code security while upholding its intended functionality.\nOur experiments show that while a state-of-the-art approach fails to address\nall code vulnerabilities, PromSec effectively resolves them. Moreover, PromSec\nachieves more than an order-of-magnitude reduction in operation time, number of\nLLM queries, and security analysis costs. Furthermore, prompts optimized with\nPromSec for a certain LLM are transferable to other LLMs across programming\nlanguages and generalizable to unseen vulnerabilities in training. This study\nis a step in enhancing the trustworthiness of LLMs for secure and functional\ncode generation, supporting their integration into real-world software\ndevelopment.",
          "arxiv_id": "2409.12699v1"
        }
      ],
      "21": [
        {
          "title": "Large Language Model assisted Hybrid Fuzzing",
          "year": "2024-12",
          "abstract": "Greybox fuzzing is one of the most popular methods for detecting software\nvulnerabilities, which conducts a biased random search within the program input\nspace. To enhance its effectiveness in achieving deep coverage of program\nbehaviors, greybox fuzzing is often combined with concolic execution, which\nperforms a path-sensitive search over the domain of program inputs. In hybrid\nfuzzing, conventional greybox fuzzing is followed by concolic execution in an\niterative loop, where reachability roadblocks encountered by greybox fuzzing\nare tackled by concolic execution. However, such hybrid fuzzing still suffers\nfrom difficulties conventionally faced by symbolic execution, such as the need\nfor environment modeling and system call support. In this work, we show how to\nachieve the effect of concolic execution without having to compute and solve\nsymbolic path constraints. When coverage-based greybox fuzzing reaches a\nroadblock in terms of reaching certain branches, we conduct a slicing on the\nexecution trace and suggest modifications of the input to reach the relevant\nbranches. A Large Language Model (LLM) is used as a solver to generate the\nmodified input for reaching the desired branches. Compared with both the\nvanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based\nhybrid fuzzer HyLLfuzz (pronounced \"hill fuzz\") demonstrates superior coverage.\nFurthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is\n4-19 times faster than the concolic execution running in existing hybrid\nfuzzing tools. This experience shows that LLMs can be effectively inserted into\nthe iterative loop of hybrid fuzzers, to efficiently expose more program\nbehaviors.",
          "arxiv_id": "2412.15931v1"
        },
        {
          "title": "Better Pay Attention Whilst Fuzzing",
          "year": "2021-12",
          "abstract": "Fuzzing is one of the prevailing methods for vulnerability detection.\nHowever, even state-of-the-art fuzzing methods become ineffective after some\nperiod of time, i.e., the coverage hardly improves as existing methods are\nineffective to focus the attention of fuzzing on covering the hard-to-trigger\nprogram paths. In other words, they cannot generate inputs that can break the\nbottleneck due to the fundamental difficulty in capturing the complex relations\nbetween the test inputs and program coverage. In particular, existing fuzzers\nsuffer from the following main limitations: 1) lacking an overall analysis of\nthe program to identify the most \"rewarding\" seeds, and 2) lacking an effective\nmutation strategy which could continuously select and mutates the more relevant\n\"bytes\" of the seeds.\n  In this work, we propose an approach called ATTuzz to address these two\nissues systematically. First, we propose a lightweight dynamic analysis\ntechnique which estimates the \"reward\" of covering each basic block and selects\nthe most rewarding seeds accordingly. Second, we mutate the selected seeds\naccording to a neural network model which predicts whether a certain\n\"rewarding\" block will be covered given certain mutation on certain bytes of a\nseed. The model is a deep learning model equipped with attention mechanism\nwhich is learned and updated periodically whilst fuzzing. Our evaluation shows\nthat ATTuzz significantly outperforms 5 state-of-the-art grey-box fuzzers on 13\npopular real-world programs at achieving higher edge coverage and finding new\nbugs. In particular, ATTuzz achieved 2X edge coverage and 4X bugs detected than\nAFL over 24-hour runs. Moreover, ATTuzz persistently improves the edge coverage\nin the long run, i.e., achieving 50% more coverage than AFL in 5 days.",
          "arxiv_id": "2112.07143v1"
        },
        {
          "title": "DARWIN: Survival of the Fittest Fuzzing Mutators",
          "year": "2022-10",
          "abstract": "Fuzzing is an automated software testing technique broadly adopted by the\nindustry. A popular variant is mutation-based fuzzing, which discovers a large\nnumber of bugs in practice. While the research community has studied\nmutation-based fuzzing for years now, the algorithms' interactions within the\nfuzzer are highly complex and can, together with the randomness in every\ninstance of a fuzzer, lead to unpredictable effects. Most efforts to improve\nthis fragile interaction focused on optimizing seed scheduling. However,\nreal-world results like Google's FuzzBench highlight that these approaches do\nnot consistently show improvements in practice. Another approach to improve the\nfuzzing process algorithmically is optimizing mutation scheduling.\nUnfortunately, existing mutation scheduling approaches also failed to convince\nbecause of missing real-world improvements or too many user-controlled\nparameters whose configuration requires expert knowledge about the target\nprogram. This leaves the challenging problem of cleverly processing test cases\nand achieving a measurable improvement unsolved.\n  We present DARWIN, a novel mutation scheduler and the first to show fuzzing\nimprovements in a realistic scenario without the need to introduce additional\nuser-configurable parameters, opening this approach to the broad fuzzing\ncommunity. DARWIN uses an Evolution Strategy to systematically optimize and\nadapt the probability distribution of the mutation operators during fuzzing. We\nimplemented a prototype based on the popular general-purpose fuzzer AFL. DARWIN\nsignificantly outperforms the state-of-the-art mutation scheduler and the AFL\nbaseline in our own coverage experiment, in FuzzBench, and by finding 15 out of\n21 bugs the fastest in the MAGMA benchmark. Finally, DARWIN found 20 unique\nbugs (including one novel bug), 66% more than AFL, in widely-used real-world\napplications.",
          "arxiv_id": "2210.11783v1"
        }
      ],
      "22": [
        {
          "title": "Responsible Design Patterns for Machine Learning Pipelines",
          "year": "2023-05",
          "abstract": "Integrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
          "arxiv_id": "2306.01788v3"
        },
        {
          "title": "POLARIS: A framework to guide the development of Trustworthy AI systems",
          "year": "2024-02",
          "abstract": "In the ever-expanding landscape of Artificial Intelligence (AI), where\ninnovation thrives and new products and services are continuously being\ndelivered, ensuring that AI systems are designed and developed responsibly\nthroughout their entire lifecycle is crucial. To this end, several AI ethics\nprinciples and guidelines have been issued to which AI systems should conform.\nNevertheless, relying solely on high-level AI ethics principles is far from\nsufficient to ensure the responsible engineering of AI systems. In this field,\nAI professionals often navigate by sight. Indeed, while recommendations\npromoting Trustworthy AI (TAI) exist, these are often high-level statements\nthat are difficult to translate into concrete implementation strategies. There\nis a significant gap between high-level AI ethics principles and low-level\nconcrete practices for AI professionals. To address this challenge, our work\npresents an experience report where we develop a novel holistic framework for\nTrustworthy AI - designed to bridge the gap between theory and practice - and\nreport insights from its application in an industrial case study. The framework\nis built on the result of a systematic review of the state of the practice, a\nsurvey, and think-aloud interviews with 34 AI practitioners. The framework,\nunlike most of those already in the literature, is designed to provide\nactionable guidelines and tools to support different types of stakeholders\nthroughout the entire Software Development Life Cycle (SDLC). Our goal is to\nempower AI professionals to confidently navigate the ethical dimensions of TAI\nthrough practical insights, ensuring that the vast potential of AI is exploited\nresponsibly for the benefit of society as a whole.",
          "arxiv_id": "2402.05340v1"
        },
        {
          "title": "Ethics in the Age of AI: An Analysis of AI Practitioners' Awareness and Challenges",
          "year": "2023-07",
          "abstract": "Ethics in AI has become a debated topic of public and expert discourse in\nrecent years. But what do people who build AI - AI practitioners - have to say\nabout their understanding of AI ethics and the challenges associated with\nincorporating it in the AI-based systems they develop? Understanding AI\npractitioners' views on AI ethics is important as they are the ones closest to\nthe AI systems and can bring about changes and improvements. We conducted a\nsurvey aimed at understanding AI practitioners' awareness of AI ethics and\ntheir challenges in incorporating ethics. Based on 100 AI practitioners'\nresponses, our findings indicate that majority of AI practitioners had a\nreasonable familiarity with the concept of AI ethics, primarily due to\nworkplace rules and policies. Privacy protection and security was the ethical\nprinciple that majority of them were aware of. Formal education/training was\nconsidered somewhat helpful in preparing practitioners to incorporate AI\nethics. The challenges that AI practitioners faced in the development of\nethical AI-based systems included (i) general challenges, (ii)\ntechnology-related challenges and (iii) human-related challenges. We also\nidentified areas needing further investigation and provided recommendations to\nassist AI practitioners and companies in incorporating ethics into AI\ndevelopment.",
          "arxiv_id": "2307.10057v1"
        }
      ],
      "23": [
        {
          "title": "Comments on Comments: Where Code Review and Documentation Meet",
          "year": "2022-03",
          "abstract": "A central function of code review is to increase understanding; helping\nreviewers understand a code change aids in knowledge transfer and finding bugs.\nComments in code largely serve a similar purpose, helping future readers\nunderstand the program. It is thus natural to study what happens when these two\nforms of understanding collide. We ask: what documentation-related comments do\nreviewers make and how do they affect understanding of the contribution? We\nanalyze ca.700K review comments on 2,000 (Java and Python) GitHub projects, and\npropose several filters to identify which comments are likely to be either in\nresponse to a change in documentation and/or call for such a change. We\nidentify 65K such cases. We next develop a taxonomy of the reviewer intents\nbehind such \"comments on comments\". We find that achieving a shared\nunderstanding of the code is key: reviewer comments most often focused on\nclarification, followed by pointing out issues to fix, such as typos and\noutdated comments. Curiously, clarifying comments were frequently suggested\n(often verbatim) by the reviewer, indicating a desire to persist their\nunderstanding acquired during code review. We conclude with a discussion of\nimplications of our comments-on-comments dataset for research on improving code\nreview, including the potential benefits for automating code review.",
          "arxiv_id": "2204.00107v1"
        },
        {
          "title": "Exploring the Advances in Identifying Useful Code Review Comments",
          "year": "2023-07",
          "abstract": "Effective peer code review in collaborative software development necessitates\nuseful reviewer comments and supportive automated tools. Code review comments\nare a central component of the Modern Code Review process in the industry and\nopen-source development. Therefore, it is important to ensure these comments\nserve their purposes. This paper reflects the evolution of research on the\nusefulness of code review comments. It examines papers that define the\nusefulness of code review comments, mine and annotate datasets, study\ndevelopers' perceptions, analyze factors from different aspects, and use\nmachine learning classifiers to automatically predict the usefulness of code\nreview comments. Finally, it discusses the open problems and challenges in\nrecognizing useful code review comments for future research.",
          "arxiv_id": "2307.00692v2"
        },
        {
          "title": "Too Noisy To Learn: Enhancing Data Quality for Code Review Comment Generation",
          "year": "2025-02",
          "abstract": "Code review is an important practice in software development, yet it is\ntime-consuming and requires substantial effort. While open-source datasets have\nbeen used to train neural models for automating code review tasks, including\nreview comment generation, these datasets contain a significant amount of noisy\ncomments (e.g., vague or non-actionable feedback) that persist despite cleaning\nmethods using heuristics and machine learning approaches. Such remaining noise\nmay lead models to generate low-quality review comments, yet removing them\nrequires a complex semantic understanding of both code changes and natural\nlanguage comments. In this paper, we investigate the impact of such noise on\nreview comment generation and propose a novel approach using large language\nmodels (LLMs) to further clean these datasets. Based on an empirical study on a\nlarge-scale code review dataset, our LLM-based approach achieves 66-85%\nprecision in detecting valid comments. Using the predicted valid comments to\nfine-tune the state-of-the-art code review models (cleaned models) can generate\nreview comments that are 13.0% - 12.4% more similar to valid human-written\ncomments than the original models. We also find that the cleaned models can\ngenerate more informative and relevant comments than the original models. Our\nfindings underscore the critical impact of dataset quality on the performance\nof review comment generation. We advocate for further research into cleaning\ntraining data to enhance the practical utility and quality of automated code\nreview.",
          "arxiv_id": "2502.02757v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-25T19:43:54Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}